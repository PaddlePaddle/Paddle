// This file is generated by "paddle/fluid/pir/dialect/op_generator/op_gen.py"
#include "/home/aistudio/fix_op/Paddle/paddle/fluid/pir/dialect/operator/ir/pd_op.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_type.h"
#include "paddle/fluid/pir/dialect/operator/ir/op_attribute.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_tensor.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_selected_rows.h"
#include "paddle/fluid/pir/dialect/operator/ir/ir_meta_tensor.h"
#include "paddle/pir/core/builtin_attribute.h"
#include "paddle/pir/core/builtin_type.h"
#include "paddle/pir/core/builtin_op.h"
#include "paddle/pir/core/ir_context.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/api/lib/utils/allocator.h"
#include "paddle/fluid/primitive/rule/vjp/vjp.h"
#include "paddle/pir/core/op_base.h"

namespace paddle {
namespace dialect {

const char *Adadelta_Op::attributes_name[3] = { "rho", "epsilon", "multi_precision" };

OpInfoTuple Adadelta_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("avg_squared_grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("avg_squared_update", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("rho", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("inf_norm_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AdadeltaInferMeta", {"param", "grad", "avg_squared_grad", "avg_squared_update", "learning_rate", "master_param", "rho", "epsilon", "multi_precision"}, "adadelta", {"param", "grad", "avg_squared_grad", "avg_squared_update", "learning_rate", "master_param", "rho", "epsilon", "multi_precision"}, {"param"}, {}, {{"param_out", "param"},{"moment_out", "avg_squared_grad"},{"inf_norm_out", "avg_squared_update"},{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "adadelta_");
}

void Adadelta_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value avg_squared_grad_, pir::Value avg_squared_update_, pir::Value learning_rate_, pir::Value master_param_, float rho, float epsilon, bool multi_precision) {
  VLOG(4) << "Start build Adadelta_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, avg_squared_grad_, avg_squared_update_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_rho = pir::FloatAttribute::get(pir::IrContext::Instance(), rho);
  argument.AddAttribute("rho", attr_rho);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType avg_squared_grad = avg_squared_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)avg_squared_grad;
  paddle::dialect::DenseTensorType avg_squared_update = avg_squared_update_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)avg_squared_update;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_avg_squared_grad";
  paddle::dialect::IrTensor ir_tensor_avg_squared_grad(paddle::dialect::TransToPhiDataType(avg_squared_grad.dtype()),
                                                      avg_squared_grad.dims(),
                                                      avg_squared_grad.data_layout(),
                                                      avg_squared_grad.lod(),
                                                      avg_squared_grad.offset());
  VLOG(4) << "Builder construction  meta_avg_squared_grad";
  paddle::dialect::IrMetaTensor meta_avg_squared_grad(&ir_tensor_avg_squared_grad);

  VLOG(4) << "Builder construction  dense_avg_squared_update";
  paddle::dialect::IrTensor ir_tensor_avg_squared_update(paddle::dialect::TransToPhiDataType(avg_squared_update.dtype()),
                                                      avg_squared_update.dims(),
                                                      avg_squared_update.data_layout(),
                                                      avg_squared_update.lod(),
                                                      avg_squared_update.offset());
  VLOG(4) << "Builder construction  meta_avg_squared_update";
  paddle::dialect::IrMetaTensor meta_avg_squared_update(&ir_tensor_avg_squared_update);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_inf_norm_out;
  paddle::dialect::IrMetaTensor meta_inf_norm_out(&dense_inf_norm_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdadeltaInferMeta(meta_param, meta_grad, meta_avg_squared_grad, meta_avg_squared_update, meta_learning_rate, meta_master_param, rho, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_inf_norm_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type inf_norm_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_inf_norm_out.dtype()), dense_inf_norm_out.dims(), dense_inf_norm_out.layout(), dense_inf_norm_out.lod(), dense_inf_norm_out.offset());
  argument_outputs.push_back(inf_norm_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adadelta_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value avg_squared_grad_, pir::Value avg_squared_update_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Adadelta_Op";


  IR_ENFORCE(
      attributes.find("rho") != attributes.end(),
          "'rho' Attribute is expected for Adadelta_Op. ");
  float rho = attributes.at("rho").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for Adadelta_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for Adadelta_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, avg_squared_grad_, avg_squared_update_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_rho = pir::FloatAttribute::get(pir::IrContext::Instance(), rho);
  argument.AddAttribute("rho", attr_rho);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType avg_squared_grad = avg_squared_grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)avg_squared_grad;
  paddle::dialect::DenseTensorType avg_squared_update = avg_squared_update_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)avg_squared_update;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_avg_squared_grad";
  paddle::dialect::IrTensor ir_tensor_avg_squared_grad(paddle::dialect::TransToPhiDataType(avg_squared_grad.dtype()),
                                                      avg_squared_grad.dims(),
                                                      avg_squared_grad.data_layout(),
                                                      avg_squared_grad.lod(),
                                                      avg_squared_grad.offset());
  VLOG(4) << "Builder construction  meta_avg_squared_grad";
  paddle::dialect::IrMetaTensor meta_avg_squared_grad(&ir_tensor_avg_squared_grad);

  VLOG(4) << "Builder construction  dense_avg_squared_update";
  paddle::dialect::IrTensor ir_tensor_avg_squared_update(paddle::dialect::TransToPhiDataType(avg_squared_update.dtype()),
                                                      avg_squared_update.dims(),
                                                      avg_squared_update.data_layout(),
                                                      avg_squared_update.lod(),
                                                      avg_squared_update.offset());
  VLOG(4) << "Builder construction  meta_avg_squared_update";
  paddle::dialect::IrMetaTensor meta_avg_squared_update(&ir_tensor_avg_squared_update);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  paddle::dialect::IrMetaTensor meta_master_param;
  paddle::dialect::IrTensor ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    paddle::dialect::DenseTensorType master_param = master_param_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_master_param";
    ir_tensor_master_param = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param.dtype()),
                                                        master_param.dims(),
                                                        master_param.data_layout(),
                                                        master_param.lod(),
                                                        master_param.offset());
    VLOG(4) << "Builder construction  meta_master_param";
    meta_master_param = paddle::dialect::IrMetaTensor(&ir_tensor_master_param);
  }

  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);
  paddle::dialect::IrTensor dense_inf_norm_out;
  paddle::dialect::IrMetaTensor meta_inf_norm_out(&dense_inf_norm_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::AdadeltaInferMeta(meta_param, meta_grad, meta_avg_squared_grad, meta_avg_squared_update, meta_learning_rate, meta_master_param, rho, epsilon, multi_precision, &meta_param_out, &meta_moment_out, &meta_inf_norm_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);

  pir::Type inf_norm_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_inf_norm_out.dtype()), dense_inf_norm_out.dims(), dense_inf_norm_out.layout(), dense_inf_norm_out.lod(), dense_inf_norm_out.offset());
  argument_outputs.push_back(inf_norm_out_dense_tensor_type);

  if (master_param_.impl() != nullptr) {
    pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
    argument_outputs.push_back(master_param_out_dense_tensor_type);
  } else {
    pir::Type master_param_out_type;
    argument_outputs.push_back(master_param_out_type);
  }

  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Adadelta_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Adadelta_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("rho")>0,
                 "rho does not exist.");
  IR_ENFORCE(attributes.at("rho").isa<pir::FloatAttribute>(),
                 "Type of attribute: rho is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: Adadelta_Op.";
}

void Adadelta_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AdadeltaInferMeta);
  fn(infer_meta);
}

phi::DataType Adadelta_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Adadelta_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AddOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "add", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add");
}

void AddOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build AddOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AddOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AddOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AddOp.";
}

void AddOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType AddOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AddOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

bool AddOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: AddOp";
  return AddOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple Add_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "add", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "add");
}

void Add_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Add_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Add_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Add_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Add_Op.";
}

void Add_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType Add_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Add_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

bool Add_Op::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: Add_Op";
  return Add_OpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *AllOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple AllOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceInferMeta", {"x", "axis", "keepdim"}, "all", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "all");
}

void AllOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build AllOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AllOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AllOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AllOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for AllOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AllOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AllOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AllOp.";
}

void AllOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceInferMeta);
  fn(infer_meta);
}

phi::DataType AllOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AllOp";
  


  return expected_kernel_dtype;
}

const char *AmaxOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple AmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceInferMeta", {"x", "axis", "keepdim"}, "amax", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "amax");
}

void AmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build AmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AmaxOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AmaxOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for AmaxOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AmaxOp.";
}

void AmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceInferMeta);
  fn(infer_meta);
}

phi::DataType AmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AmaxOp";
  


  return expected_kernel_dtype;
}

const char *AminOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple AminOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceInferMeta", {"x", "axis", "keepdim"}, "amin", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "amin");
}

void AminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build AminOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AminOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AminOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AminOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for AminOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AminOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AminOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AminOp.";
}

void AminOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceInferMeta);
  fn(infer_meta);
}

phi::DataType AminOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AminOp";
  


  return expected_kernel_dtype;
}

const char *AnyOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple AnyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceInferMeta", {"x", "axis", "keepdim"}, "any", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "any");
}

void AnyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build AnyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AnyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AnyOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for AnyOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for AnyOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceInferMeta(meta_x, axis, keepdim, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AnyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AnyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AnyOp.";
}

void AnyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceInferMeta);
  fn(infer_meta);
}

phi::DataType AnyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AnyOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple AssignOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "assign", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign");
}

void AssignOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build AssignOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AssignOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AssignOp.";
}

void AssignOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AssignOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Assign_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "assign", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign");
}

void Assign_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build Assign_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Assign_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Assign_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Assign_Op.";
}

void Assign_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Assign_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Assign_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple AssignOut_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("output", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "assign", {"x"}, {}, {}, {{"out", "output"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign_out_");
}

void AssignOut_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value output_) {
  VLOG(4) << "Start build AssignOut_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, output_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignOut_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AssignOut_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AssignOut_Op.";
}

void AssignOut_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType AssignOut_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignOut_Op";
  


  return expected_kernel_dtype;
}

const char *AssignValueOp::attributes_name[4] = { "shape", "dtype", "values", "place" };

OpInfoTuple AssignValueOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("values", "pir::ArrayAttribute<paddle::dialect::ScalarAttribute>", "std::vector<Scalar>"), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AssignValueInferMeta", {"shape", "dtype"}, "assign_value", {"shape", "dtype", "values"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign_value");
}

void AssignValueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int>& shape, phi::DataType dtype, std::vector<phi::Scalar> values, const Place& place) {
  VLOG(4) << "Start build AssignValueOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AssignValueInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignValueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AssignValueOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for AssignValueOp. ");
  std::vector<int> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for AssignValueOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("values") != attributes.end(),
          "'values' Attribute is expected for AssignValueOp. ");
  std::vector<phi::Scalar> values;
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    values.push_back(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<paddle::dialect::ScalarAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for AssignValueOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AssignValueInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignValueOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AssignValueOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("values")>0,
                 "values does not exist.");
  IR_ENFORCE(attributes.at("values").isa<pir::ArrayAttribute>(),
                 "Type of attribute: values is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).isa<paddle::dialect::ScalarAttribute>(),
                   "Type of attribute: values is not right.");
  }
  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AssignValueOp.";
}

void AssignValueOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AssignValueInferMeta);
  fn(infer_meta);
}

phi::DataType AssignValueOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignValueOp";
  


  return expected_kernel_dtype;
}

const char *AssignValue_Op::attributes_name[4] = { "shape", "dtype", "values", "place" };

OpInfoTuple AssignValue_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("output", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("values", "pir::ArrayAttribute<paddle::dialect::ScalarAttribute>", "std::vector<Scalar>"), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AssignValueInferMeta", {"shape", "dtype"}, "assign_value", {"shape", "dtype", "values"}, {"dtype"}, {"place", "output"}, {{"out", "output"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "assign_value_");
}

void AssignValue_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value output_, const std::vector<int>& shape, phi::DataType dtype, std::vector<phi::Scalar> values, const Place& place) {
  VLOG(4) << "Start build AssignValue_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {output_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AssignValueInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignValue_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value output_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build AssignValue_Op";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for AssignValue_Op. ");
  std::vector<int> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for AssignValue_Op. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("values") != attributes.end(),
          "'values' Attribute is expected for AssignValue_Op. ");
  std::vector<phi::Scalar> values;
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    values.push_back(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<paddle::dialect::ScalarAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for AssignValue_Op. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {output_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AssignValueInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void AssignValue_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: AssignValue_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("values")>0,
                 "values does not exist.");
  IR_ENFORCE(attributes.at("values").isa<pir::ArrayAttribute>(),
                 "Type of attribute: values is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).isa<paddle::dialect::ScalarAttribute>(),
                   "Type of attribute: values is not right.");
  }
  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: AssignValue_Op.";
}

void AssignValue_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AssignValueInferMeta);
  fn(infer_meta);
}

phi::DataType AssignValue_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: AssignValue_Op";
  


  return expected_kernel_dtype;
}

const char *BatchNormOp::attributes_name[6] = { "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics" };

OpInfoTuple BatchNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BatchNormInferMeta", {"x", "mean", "variance", "scale", "bias", "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics"}, "batch_norm", {"x", "mean", "variance", "scale", "bias", "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "batch_norm");
}

void BatchNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, bool is_test, float momentum, float epsilon, const std::string& data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build BatchNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::BatchNormInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BatchNormOp";


  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for BatchNormOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for BatchNormOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for BatchNormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BatchNormOp. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for BatchNormOp. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for BatchNormOp. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::BatchNormInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BatchNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("use_global_stats")>0,
                 "use_global_stats does not exist.");
  IR_ENFORCE(attributes.at("use_global_stats").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_stats is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("trainable_statistics")>0,
                 "trainable_statistics does not exist.");
  IR_ENFORCE(attributes.at("trainable_statistics").isa<pir::BoolAttribute>(),
                 "Type of attribute: trainable_statistics is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: BatchNormOp.";
}

void BatchNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BatchNormInferMeta);
  fn(infer_meta);
}

phi::DataType BatchNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BatchNormOp";
  


  return expected_kernel_dtype;
}

const char *BatchNorm_Op::attributes_name[6] = { "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics" };

OpInfoTuple BatchNorm_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BatchNormInferMeta", {"x", "mean", "variance", "scale", "bias", "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics"}, "batch_norm", {"x", "mean", "variance", "scale", "bias", "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics"}, {"x"}, {}, {}, {{"mean_out", "mean"},{"variance_out", "variance"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "batch_norm");
}

void BatchNorm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, bool is_test, float momentum, float epsilon, const std::string& data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build BatchNorm_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::BatchNormInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNorm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build BatchNorm_Op";


  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for BatchNorm_Op. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for BatchNorm_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for BatchNorm_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for BatchNorm_Op. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for BatchNorm_Op. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for BatchNorm_Op. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  paddle::dialect::IrMetaTensor meta_scale;
  paddle::dialect::IrTensor ir_tensor_scale;
  if (scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_scale";
    ir_tensor_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                        scale.dims(),
                                                        scale.data_layout(),
                                                        scale.lod(),
                                                        scale.offset());
    VLOG(4) << "Builder construction  meta_scale";
    meta_scale = paddle::dialect::IrMetaTensor(&ir_tensor_scale);
  }


  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::BatchNormInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void BatchNorm_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: BatchNorm_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("use_global_stats")>0,
                 "use_global_stats does not exist.");
  IR_ENFORCE(attributes.at("use_global_stats").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_stats is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("trainable_statistics")>0,
                 "trainable_statistics does not exist.");
  IR_ENFORCE(attributes.at("trainable_statistics").isa<pir::BoolAttribute>(),
                 "Type of attribute: trainable_statistics is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: BatchNorm_Op.";
}

void BatchNorm_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BatchNormInferMeta);
  fn(infer_meta);
}

phi::DataType BatchNorm_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: BatchNorm_Op";
  


  return expected_kernel_dtype;
}

const char *CAllgatherOp::attributes_name[3] = { "ring_id", "nranks", "use_calc_stream" };

OpInfoTuple CAllgatherOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AllGatherInferMeta", {"x", "nranks"}, "c_allgather", {"x", "ring_id", "nranks", "use_calc_stream"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_allgather");
}

void CAllgatherOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int nranks, bool use_calc_stream) {
  VLOG(4) << "Start build CAllgatherOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllGatherInferMeta(meta_x, nranks, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllgatherOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CAllgatherOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CAllgatherOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for CAllgatherOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CAllgatherOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllGatherInferMeta(meta_x, nranks, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllgatherOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CAllgatherOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nranks")>0,
                 "nranks does not exist.");
  IR_ENFORCE(attributes.at("nranks").isa<pir::Int32Attribute>(),
                 "Type of attribute: nranks is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CAllgatherOp.";
}

void CAllgatherOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AllGatherInferMeta);
  fn(infer_meta);
}

phi::DataType CAllgatherOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CAllgatherOp";
  


  return expected_kernel_dtype;
}

const char *CAllreduceMaxOp::attributes_name[3] = { "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CAllreduceMaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AllReduceInferMeta", {"x"}, "c_allreduce_max", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_allreduce_max");
}

void CAllreduceMaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CAllreduceMaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceMaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CAllreduceMaxOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CAllreduceMaxOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CAllreduceMaxOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CAllreduceMaxOp. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceMaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CAllreduceMaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CAllreduceMaxOp.";
}

void CAllreduceMaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AllReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CAllreduceMaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CAllreduceMaxOp";
  


  return expected_kernel_dtype;
}

const char *CAllreduceMax_Op::attributes_name[3] = { "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CAllreduceMax_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AllReduceInferMeta", {"x"}, "c_allreduce_max", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_allreduce_max");
}

void CAllreduceMax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CAllreduceMax_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceMax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CAllreduceMax_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CAllreduceMax_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CAllreduceMax_Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CAllreduceMax_Op. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceMax_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CAllreduceMax_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CAllreduceMax_Op.";
}

void CAllreduceMax_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AllReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CAllreduceMax_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CAllreduceMax_Op";
  


  return expected_kernel_dtype;
}

const char *CAllreduceSumOp::attributes_name[3] = { "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CAllreduceSumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AllReduceInferMeta", {"x"}, "c_allreduce_sum", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_allreduce_sum");
}

void CAllreduceSumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CAllreduceSumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceSumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CAllreduceSumOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CAllreduceSumOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CAllreduceSumOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CAllreduceSumOp. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceSumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CAllreduceSumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CAllreduceSumOp.";
}

void CAllreduceSumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AllReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CAllreduceSumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CAllreduceSumOp";
  


  return expected_kernel_dtype;
}

const char *CAllreduceSum_Op::attributes_name[3] = { "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CAllreduceSum_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("AllReduceInferMeta", {"x"}, "c_allreduce_sum", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_allreduce_sum");
}

void CAllreduceSum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CAllreduceSum_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceSum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CAllreduceSum_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CAllreduceSum_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CAllreduceSum_Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CAllreduceSum_Op. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::AllReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CAllreduceSum_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CAllreduceSum_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CAllreduceSum_Op.";
}

void CAllreduceSum_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::AllReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CAllreduceSum_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CAllreduceSum_Op";
  


  return expected_kernel_dtype;
}

const char *CBroadcastOp::attributes_name[3] = { "ring_id", "root", "use_calc_stream" };

OpInfoTuple CBroadcastOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("root", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "c_broadcast", {"x", "ring_id", "root", "use_calc_stream"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_broadcast");
}

void CBroadcastOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int root, bool use_calc_stream) {
  VLOG(4) << "Start build CBroadcastOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root = pir::Int32Attribute::get(pir::IrContext::Instance(), root);
  argument.AddAttribute("root", attr_root);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CBroadcastOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CBroadcastOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CBroadcastOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("root") != attributes.end(),
          "'root' Attribute is expected for CBroadcastOp. ");
  int root = attributes.at("root").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CBroadcastOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root = pir::Int32Attribute::get(pir::IrContext::Instance(), root);
  argument.AddAttribute("root", attr_root);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CBroadcastOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CBroadcastOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("root")>0,
                 "root does not exist.");
  IR_ENFORCE(attributes.at("root").isa<pir::Int32Attribute>(),
                 "Type of attribute: root is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CBroadcastOp.";
}

void CBroadcastOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CBroadcastOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CBroadcastOp";
  


  return expected_kernel_dtype;
}

const char *CBroadcast_Op::attributes_name[3] = { "ring_id", "root", "use_calc_stream" };

OpInfoTuple CBroadcast_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("root", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "c_broadcast", {"x", "ring_id", "root", "use_calc_stream"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_broadcast");
}

void CBroadcast_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int root, bool use_calc_stream) {
  VLOG(4) << "Start build CBroadcast_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root = pir::Int32Attribute::get(pir::IrContext::Instance(), root);
  argument.AddAttribute("root", attr_root);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CBroadcast_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CBroadcast_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CBroadcast_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("root") != attributes.end(),
          "'root' Attribute is expected for CBroadcast_Op. ");
  int root = attributes.at("root").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CBroadcast_Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root = pir::Int32Attribute::get(pir::IrContext::Instance(), root);
  argument.AddAttribute("root", attr_root);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CBroadcast_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CBroadcast_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("root")>0,
                 "root does not exist.");
  IR_ENFORCE(attributes.at("root").isa<pir::Int32Attribute>(),
                 "Type of attribute: root is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CBroadcast_Op.";
}

void CBroadcast_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CBroadcast_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CBroadcast_Op";
  


  return expected_kernel_dtype;
}

const char *CConcatOp::attributes_name[5] = { "rank", "nranks", "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CConcatOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CConcatInferMeta", {"x", "nranks"}, "c_concat", {"x", "rank", "nranks", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_concat");
}

void CConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int rank, int nranks, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CConcatOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CConcatInferMeta(meta_x, nranks, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CConcatOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CConcatOp";


  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for CConcatOp. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for CConcatOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CConcatOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CConcatOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CConcatOp. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CConcatInferMeta(meta_x, nranks, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CConcatOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CConcatOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("rank")>0,
                 "rank does not exist.");
  IR_ENFORCE(attributes.at("rank").isa<pir::Int32Attribute>(),
                 "Type of attribute: rank is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nranks")>0,
                 "nranks does not exist.");
  IR_ENFORCE(attributes.at("nranks").isa<pir::Int32Attribute>(),
                 "Type of attribute: nranks is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CConcatOp.";
}

void CConcatOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CConcatInferMeta);
  fn(infer_meta);
}

phi::DataType CConcatOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CConcatOp";
  


  return expected_kernel_dtype;
}

const char *CEmbeddingOp::attributes_name[2] = { "start_index", "vocab_size" };

OpInfoTuple CEmbeddingOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("start_index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("vocab_size", "pir::Int64Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CEmbeddingInferMeta", {"weight", "x", "start_index"}, "c_embedding", {"weight", "x", "start_index", "vocab_size"}, {"weight"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_embedding");
}

void CEmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value x_, int64_t start_index, int64_t vocab_size) {
  VLOG(4) << "Start build CEmbeddingOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_index = pir::Int64Attribute::get(pir::IrContext::Instance(), start_index);
  argument.AddAttribute("start_index", attr_start_index);
  pir::Attribute attr_vocab_size = pir::Int64Attribute::get(pir::IrContext::Instance(), vocab_size);
  argument.AddAttribute("vocab_size", attr_vocab_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CEmbeddingInferMeta(meta_weight, meta_x, start_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CEmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value weight_, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CEmbeddingOp";


  IR_ENFORCE(
      attributes.find("start_index") != attributes.end(),
          "'start_index' Attribute is expected for CEmbeddingOp. ");
  int64_t start_index = attributes.at("start_index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("vocab_size") != attributes.end(),
          "'vocab_size' Attribute is expected for CEmbeddingOp. ");
  int64_t vocab_size = attributes.at("vocab_size").dyn_cast<pir::Int64Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {weight_, x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_start_index = pir::Int64Attribute::get(pir::IrContext::Instance(), start_index);
  argument.AddAttribute("start_index", attr_start_index);
  pir::Attribute attr_vocab_size = pir::Int64Attribute::get(pir::IrContext::Instance(), vocab_size);
  argument.AddAttribute("vocab_size", attr_vocab_size);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CEmbeddingInferMeta(meta_weight, meta_x, start_index, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CEmbeddingOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CEmbeddingOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("start_index")>0,
                 "start_index does not exist.");
  IR_ENFORCE(attributes.at("start_index").isa<pir::Int64Attribute>(),
                 "Type of attribute: start_index is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("vocab_size")>0,
                 "vocab_size does not exist.");
  IR_ENFORCE(attributes.at("vocab_size").isa<pir::Int64Attribute>(),
                 "Type of attribute: vocab_size is not pir::Int64Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CEmbeddingOp.";
}

void CEmbeddingOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CEmbeddingInferMeta);
  fn(infer_meta);
}

phi::DataType CEmbeddingOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CEmbeddingOp";
  


  return expected_kernel_dtype;
}

const char *CIdentityOp::attributes_name[3] = { "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CIdentityOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CIdentityInferMeta", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, "c_identity", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_identity");
}

void CIdentityOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CIdentityOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CIdentityInferMeta(meta_x, ring_id, use_calc_stream, use_model_parallel, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CIdentityOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CIdentityOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CIdentityOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CIdentityOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CIdentityOp. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CIdentityInferMeta(meta_x, ring_id, use_calc_stream, use_model_parallel, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CIdentityOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CIdentityOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CIdentityOp.";
}

void CIdentityOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CIdentityInferMeta);
  fn(infer_meta);
}

phi::DataType CIdentityOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CIdentityOp";
  


  return expected_kernel_dtype;
}

const char *CIdentity_Op::attributes_name[3] = { "ring_id", "use_calc_stream", "use_model_parallel" };

OpInfoTuple CIdentity_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_model_parallel", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CIdentityInferMeta", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, "c_identity", {"x", "ring_id", "use_calc_stream", "use_model_parallel"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_identity");
}

void CIdentity_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, bool use_calc_stream, bool use_model_parallel) {
  VLOG(4) << "Start build CIdentity_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CIdentityInferMeta(meta_x, ring_id, use_calc_stream, use_model_parallel, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CIdentity_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CIdentity_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CIdentity_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CIdentity_Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_model_parallel") != attributes.end(),
          "'use_model_parallel' Attribute is expected for CIdentity_Op. ");
  bool use_model_parallel = attributes.at("use_model_parallel").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_use_model_parallel = pir::BoolAttribute::get(pir::IrContext::Instance(), use_model_parallel);
  argument.AddAttribute("use_model_parallel", attr_use_model_parallel);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CIdentityInferMeta(meta_x, ring_id, use_calc_stream, use_model_parallel, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CIdentity_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CIdentity_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_model_parallel")>0,
                 "use_model_parallel does not exist.");
  IR_ENFORCE(attributes.at("use_model_parallel").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_model_parallel is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CIdentity_Op.";
}

void CIdentity_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CIdentityInferMeta);
  fn(infer_meta);
}

phi::DataType CIdentity_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CIdentity_Op";
  


  return expected_kernel_dtype;
}

const char *CReduceMinOp::attributes_name[3] = { "ring_id", "root_id", "use_calc_stream" };

OpInfoTuple CReduceMinOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("root_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DistReduceInferMeta", {"x"}, "c_reduce_min", {"x", "ring_id", "root_id", "use_calc_stream"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_reduce_min");
}

void CReduceMinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int root_id, bool use_calc_stream) {
  VLOG(4) << "Start build CReduceMinOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceMinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CReduceMinOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CReduceMinOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("root_id") != attributes.end(),
          "'root_id' Attribute is expected for CReduceMinOp. ");
  int root_id = attributes.at("root_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CReduceMinOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceMinOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CReduceMinOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("root_id")>0,
                 "root_id does not exist.");
  IR_ENFORCE(attributes.at("root_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: root_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CReduceMinOp.";
}

void CReduceMinOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DistReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CReduceMinOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CReduceMinOp";
  


  return expected_kernel_dtype;
}

const char *CReduceMin_Op::attributes_name[3] = { "ring_id", "root_id", "use_calc_stream" };

OpInfoTuple CReduceMin_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("root_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DistReduceInferMeta", {"x"}, "c_reduce_min", {"x", "ring_id", "root_id", "use_calc_stream"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_reduce_min");
}

void CReduceMin_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int root_id, bool use_calc_stream) {
  VLOG(4) << "Start build CReduceMin_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceMin_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CReduceMin_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CReduceMin_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("root_id") != attributes.end(),
          "'root_id' Attribute is expected for CReduceMin_Op. ");
  int root_id = attributes.at("root_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CReduceMin_Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceMin_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CReduceMin_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("root_id")>0,
                 "root_id does not exist.");
  IR_ENFORCE(attributes.at("root_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: root_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CReduceMin_Op.";
}

void CReduceMin_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DistReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CReduceMin_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CReduceMin_Op";
  


  return expected_kernel_dtype;
}

const char *CReduceSumOp::attributes_name[3] = { "ring_id", "root_id", "use_calc_stream" };

OpInfoTuple CReduceSumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("root_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DistReduceInferMeta", {"x"}, "c_reduce_sum", {"x", "ring_id", "root_id", "use_calc_stream"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_reduce_sum");
}

void CReduceSumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int root_id, bool use_calc_stream) {
  VLOG(4) << "Start build CReduceSumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceSumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CReduceSumOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CReduceSumOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("root_id") != attributes.end(),
          "'root_id' Attribute is expected for CReduceSumOp. ");
  int root_id = attributes.at("root_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CReduceSumOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceSumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CReduceSumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("root_id")>0,
                 "root_id does not exist.");
  IR_ENFORCE(attributes.at("root_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: root_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CReduceSumOp.";
}

void CReduceSumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DistReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CReduceSumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CReduceSumOp";
  


  return expected_kernel_dtype;
}

const char *CReduceSum_Op::attributes_name[3] = { "ring_id", "root_id", "use_calc_stream" };

OpInfoTuple CReduceSum_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("root_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DistReduceInferMeta", {"x"}, "c_reduce_sum", {"x", "ring_id", "root_id", "use_calc_stream"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_reduce_sum");
}

void CReduceSum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int root_id, bool use_calc_stream) {
  VLOG(4) << "Start build CReduceSum_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceSum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CReduceSum_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CReduceSum_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("root_id") != attributes.end(),
          "'root_id' Attribute is expected for CReduceSum_Op. ");
  int root_id = attributes.at("root_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CReduceSum_Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_root_id = pir::Int32Attribute::get(pir::IrContext::Instance(), root_id);
  argument.AddAttribute("root_id", attr_root_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DistReduceInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReduceSum_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CReduceSum_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("root_id")>0,
                 "root_id does not exist.");
  IR_ENFORCE(attributes.at("root_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: root_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CReduceSum_Op.";
}

void CReduceSum_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DistReduceInferMeta);
  fn(infer_meta);
}

phi::DataType CReduceSum_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CReduceSum_Op";
  


  return expected_kernel_dtype;
}

const char *CReducescatterOp::attributes_name[3] = { "ring_id", "nranks", "use_calc_stream" };

OpInfoTuple CReducescatterOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceScatterInferMeta", {"x", "nranks"}, "reduce_scatter", {"x", "nranks"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_reducescatter");
}

void CReducescatterOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int nranks, bool use_calc_stream) {
  VLOG(4) << "Start build CReducescatterOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceScatterInferMeta(meta_x, nranks, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReducescatterOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CReducescatterOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CReducescatterOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for CReducescatterOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for CReducescatterOp. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceScatterInferMeta(meta_x, nranks, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CReducescatterOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CReducescatterOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nranks")>0,
                 "nranks does not exist.");
  IR_ENFORCE(attributes.at("nranks").isa<pir::Int32Attribute>(),
                 "Type of attribute: nranks is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CReducescatterOp.";
}

void CReducescatterOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceScatterInferMeta);
  fn(infer_meta);
}

phi::DataType CReducescatterOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CReducescatterOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CSyncCalcStreamOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "c_sync_calc_stream", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_sync_calc_stream");
}

void CSyncCalcStreamOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build CSyncCalcStreamOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSyncCalcStreamOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CSyncCalcStreamOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CSyncCalcStreamOp.";
}

void CSyncCalcStreamOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CSyncCalcStreamOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CSyncCalcStreamOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple CSyncCalcStream_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "c_sync_calc_stream", {"x"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_sync_calc_stream");
}

void CSyncCalcStream_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build CSyncCalcStream_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSyncCalcStream_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CSyncCalcStream_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CSyncCalcStream_Op.";
}

void CSyncCalcStream_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CSyncCalcStream_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CSyncCalcStream_Op";
  


  return expected_kernel_dtype;
}

const char *CSyncCommStreamOp::attributes_name[1] = { "ring_id" };

OpInfoTuple CSyncCommStreamOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "c_sync_comm_stream", {"x", "ring_id"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_sync_comm_stream");
}

void CSyncCommStreamOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id) {
  VLOG(4) << "Start build CSyncCommStreamOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSyncCommStreamOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CSyncCommStreamOp";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CSyncCommStreamOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSyncCommStreamOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CSyncCommStreamOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CSyncCommStreamOp.";
}

void CSyncCommStreamOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CSyncCommStreamOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CSyncCommStreamOp";
  


  return expected_kernel_dtype;
}

const char *CSyncCommStream_Op::attributes_name[1] = { "ring_id" };

OpInfoTuple CSyncCommStream_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "c_sync_comm_stream", {"x", "ring_id"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_sync_comm_stream");
}

void CSyncCommStream_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id) {
  VLOG(4) << "Start build CSyncCommStream_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSyncCommStream_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CSyncCommStream_Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CSyncCommStream_Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSyncCommStream_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CSyncCommStream_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CSyncCommStream_Op.";
}

void CSyncCommStream_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType CSyncCommStream_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CSyncCommStream_Op";
  


  return expected_kernel_dtype;
}

const char *CastOp::attributes_name[1] = { "dtype" };

OpInfoTuple CastOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CastInferMeta", {"x", "dtype"}, "cast", {"x", "dtype"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cast");
}

void CastOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType dtype) {
  VLOG(4) << "Start build CastOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CastInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CastOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CastOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for CastOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CastInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CastOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CastOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CastOp.";
}

void CastOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CastInferMeta);
  fn(infer_meta);
}

phi::DataType CastOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CastOp";
  


  return expected_kernel_dtype;
}

bool CastOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: CastOp";
  return CastOpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *Cast_Op::attributes_name[1] = { "dtype" };

OpInfoTuple Cast_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CastInferMeta", {"x", "dtype"}, "cast", {"x", "dtype"}, {"x"}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "cast");
}

void Cast_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType dtype) {
  VLOG(4) << "Start build Cast_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CastInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cast_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Cast_Op";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for Cast_Op. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CastInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Cast_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Cast_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Cast_Op.";
}

void Cast_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CastInferMeta);
  fn(infer_meta);
}

phi::DataType Cast_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Cast_Op";
  


  return expected_kernel_dtype;
}

bool Cast_Op::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: Cast_Op";
  return Cast_OpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *ChannelShuffleOp::attributes_name[2] = { "groups", "data_format" };

OpInfoTuple ChannelShuffleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ChannelShuffleInferMeta", {"x", "groups", "data_format"}, "channel_shuffle", {"x", "groups", "data_format"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "channel_shuffle");
}

void ChannelShuffleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int groups, const std::string& data_format) {
  VLOG(4) << "Start build ChannelShuffleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ChannelShuffleInferMeta(meta_x, groups, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ChannelShuffleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ChannelShuffleOp";


  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for ChannelShuffleOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for ChannelShuffleOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ChannelShuffleInferMeta(meta_x, groups, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ChannelShuffleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ChannelShuffleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ChannelShuffleOp.";
}

void ChannelShuffleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ChannelShuffleInferMeta);
  fn(infer_meta);
}

phi::DataType ChannelShuffleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ChannelShuffleOp";
  


  return expected_kernel_dtype;
}

const char *Conv2dTransposeOp::attributes_name[7] = { "strides", "paddings", "output_padding", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple Conv2dTransposeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dTransposeInferMeta", {"x", "filter", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "conv2d_transpose", {"x", "filter", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "conv2d_transpose");
}

void Conv2dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int64_t>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dTransposeOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv2dTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Conv2dTransposeOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Conv2dTransposeOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Conv2dTransposeOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for Conv2dTransposeOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for Conv2dTransposeOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Conv2dTransposeOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for Conv2dTransposeOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for Conv2dTransposeOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Conv2dTransposeOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv2dTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value output_size_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build Conv2dTransposeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv2dTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Conv2dTransposeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Conv2dTransposeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("output_padding")>0,
                 "output_padding does not exist.");
  IR_ENFORCE(attributes.at("output_padding").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_padding is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_padding is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Conv2dTransposeOp.";
}

void Conv2dTransposeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dTransposeInferMeta);
  fn(infer_meta);
}

phi::DataType Conv2dTransposeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Conv2dTransposeOp";
  


  return expected_kernel_dtype;
}

const char *CopyToOp::attributes_name[2] = { "place", "blocking" };

OpInfoTuple CopyToOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", ""), paddle::dialect::OpAttributeInfo("blocking", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "copy_to");
}

void CopyToOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CopyToOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  IR_ENFORCE(attributes.count("blocking")>0,
                 "blocking does not exist.");
  IR_ENFORCE(attributes.at("blocking").isa<pir::BoolAttribute>(),
                 "Type of attribute: blocking is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: CopyToOp.";
}

phi::DataType CopyToOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CopyToOp";
  


  return expected_kernel_dtype;
}

const char *DecayedAdagradOp::attributes_name[2] = { "decay", "epsilon" };

OpInfoTuple DecayedAdagradOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moment", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("decay", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("moment_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DecayedAdagradInferMeta", {"param", "grad", "moment", "learning_rate", "decay", "epsilon"}, "decayed_adagrad", {"param", "grad", "moment", "learning_rate", "decay", "epsilon"}, {"param"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "decayed_adagrad");
}

void DecayedAdagradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, float decay, float epsilon) {
  VLOG(4) << "Start build DecayedAdagradOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, moment_, learning_rate_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), decay);
  argument.AddAttribute("decay", attr_decay);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);

  phi::DecayedAdagradInferMeta(meta_param, meta_grad, meta_moment, meta_learning_rate, decay, epsilon, &meta_param_out, &meta_moment_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DecayedAdagradOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value moment_, pir::Value learning_rate_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DecayedAdagradOp";


  IR_ENFORCE(
      attributes.find("decay") != attributes.end(),
          "'decay' Attribute is expected for DecayedAdagradOp. ");
  float decay = attributes.at("decay").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for DecayedAdagradOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, moment_, learning_rate_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), decay);
  argument.AddAttribute("decay", attr_decay);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType moment = moment_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)moment;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_moment";
  paddle::dialect::IrTensor ir_tensor_moment(paddle::dialect::TransToPhiDataType(moment.dtype()),
                                                      moment.dims(),
                                                      moment.data_layout(),
                                                      moment.lod(),
                                                      moment.offset());
  VLOG(4) << "Builder construction  meta_moment";
  paddle::dialect::IrMetaTensor meta_moment(&ir_tensor_moment);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_moment_out;
  paddle::dialect::IrMetaTensor meta_moment_out(&dense_moment_out);

  phi::DecayedAdagradInferMeta(meta_param, meta_grad, meta_moment, meta_learning_rate, decay, epsilon, &meta_param_out, &meta_moment_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type moment_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_moment_out.dtype()), dense_moment_out.dims(), dense_moment_out.layout(), dense_moment_out.lod(), dense_moment_out.offset());
  argument_outputs.push_back(moment_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DecayedAdagradOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DecayedAdagradOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("decay")>0,
                 "decay does not exist.");
  IR_ENFORCE(attributes.at("decay").isa<pir::FloatAttribute>(),
                 "Type of attribute: decay is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: DecayedAdagradOp.";
}

void DecayedAdagradOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DecayedAdagradInferMeta);
  fn(infer_meta);
}

phi::DataType DecayedAdagradOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DecayedAdagradOp";
  


  return expected_kernel_dtype;
}

const char *DecodeJpegOp::attributes_name[2] = { "mode", "place" };

OpInfoTuple DecodeJpegOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DecodeJpegInferMeta", {"x", "mode"}, "decode_jpeg", {"x", "mode"}, {}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "decode_jpeg");
}

void DecodeJpegOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& mode, const Place& place) {
  VLOG(4) << "Start build DecodeJpegOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DecodeJpegInferMeta(meta_x, mode, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DecodeJpegOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DecodeJpegOp";


  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for DecodeJpegOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for DecodeJpegOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DecodeJpegInferMeta(meta_x, mode, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DecodeJpegOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DecodeJpegOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DecodeJpegOp.";
}

void DecodeJpegOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DecodeJpegInferMeta);
  fn(infer_meta);
}

phi::DataType DecodeJpegOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DecodeJpegOp";
  


  return expected_kernel_dtype;
}

const char *DeformableConvOp::attributes_name[6] = { "strides", "paddings", "dilations", "deformable_groups", "groups", "im2col_step" };

OpInfoTuple DeformableConvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("offset", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mask", "paddle::dialect::DenseTensorType", true, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("deformable_groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("im2col_step", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DeformableConvInferMeta", {"x", "offset", "filter", "mask", "strides", "paddings", "dilations", "deformable_groups", "groups", "im2col_step"}, "deformable_conv", {"x", "offset", "filter", "mask", "strides", "paddings", "dilations", "deformable_groups", "groups", "im2col_step"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "deformable_conv");
}

void DeformableConvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value offset_, pir::Value filter_, pir::Value mask_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations, int deformable_groups, int groups, int im2col_step) {
  VLOG(4) << "Start build DeformableConvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, offset_, filter_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_deformable_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), deformable_groups);
  argument.AddAttribute("deformable_groups", attr_deformable_groups);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_im2col_step = pir::Int32Attribute::get(pir::IrContext::Instance(), im2col_step);
  argument.AddAttribute("im2col_step", attr_im2col_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType offset = offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)offset;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_offset";
  paddle::dialect::IrTensor ir_tensor_offset(paddle::dialect::TransToPhiDataType(offset.dtype()),
                                                      offset.dims(),
                                                      offset.data_layout(),
                                                      offset.lod(),
                                                      offset.offset());
  VLOG(4) << "Builder construction  meta_offset";
  paddle::dialect::IrMetaTensor meta_offset(&ir_tensor_offset);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DeformableConvInferMeta(meta_x, meta_offset, meta_filter, meta_mask, strides, paddings, dilations, deformable_groups, groups, im2col_step, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DeformableConvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value offset_, pir::Value filter_, pir::Value mask_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DeformableConvOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DeformableConvOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DeformableConvOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DeformableConvOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("deformable_groups") != attributes.end(),
          "'deformable_groups' Attribute is expected for DeformableConvOp. ");
  int deformable_groups = attributes.at("deformable_groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DeformableConvOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("im2col_step") != attributes.end(),
          "'im2col_step' Attribute is expected for DeformableConvOp. ");
  int im2col_step = attributes.at("im2col_step").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, offset_, filter_, mask_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_deformable_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), deformable_groups);
  argument.AddAttribute("deformable_groups", attr_deformable_groups);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  pir::Attribute attr_im2col_step = pir::Int32Attribute::get(pir::IrContext::Instance(), im2col_step);
  argument.AddAttribute("im2col_step", attr_im2col_step);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType offset = offset_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)offset;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_offset";
  paddle::dialect::IrTensor ir_tensor_offset(paddle::dialect::TransToPhiDataType(offset.dtype()),
                                                      offset.dims(),
                                                      offset.data_layout(),
                                                      offset.lod(),
                                                      offset.offset());
  VLOG(4) << "Builder construction  meta_offset";
  paddle::dialect::IrMetaTensor meta_offset(&ir_tensor_offset);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);

  paddle::dialect::IrMetaTensor meta_mask;
  paddle::dialect::IrTensor ir_tensor_mask;
  if (mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType mask = mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_mask";
    ir_tensor_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(mask.dtype()),
                                                        mask.dims(),
                                                        mask.data_layout(),
                                                        mask.lod(),
                                                        mask.offset());
    VLOG(4) << "Builder construction  meta_mask";
    meta_mask = paddle::dialect::IrMetaTensor(&ir_tensor_mask);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::DeformableConvInferMeta(meta_x, meta_offset, meta_filter, meta_mask, strides, paddings, dilations, deformable_groups, groups, im2col_step, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DeformableConvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DeformableConvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("deformable_groups")>0,
                 "deformable_groups does not exist.");
  IR_ENFORCE(attributes.at("deformable_groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: deformable_groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("im2col_step")>0,
                 "im2col_step does not exist.");
  IR_ENFORCE(attributes.at("im2col_step").isa<pir::Int32Attribute>(),
                 "Type of attribute: im2col_step is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DeformableConvOp.";
}

void DeformableConvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DeformableConvInferMeta);
  fn(infer_meta);
}

phi::DataType DeformableConvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DeformableConvOp";
  


  return expected_kernel_dtype;
}

const char *DepthwiseConv2dTransposeOp::attributes_name[7] = { "strides", "paddings", "output_padding", "padding_algorithm", "groups", "dilations", "data_format" };

OpInfoTuple DepthwiseConv2dTransposeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("output_padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("groups", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dilations", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Conv2dTransposeInferMeta", {"x", "filter", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, "depthwise_conv2d_transpose", {"x", "filter", "strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "depthwise_conv2d_transpose");
}

void DepthwiseConv2dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int64_t>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dTransposeOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv2dTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DepthwiseConv2dTransposeOp";


  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_padding") != attributes.end(),
          "'output_padding' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::vector<int> output_padding;
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    output_padding.push_back(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("groups") != attributes.end(),
          "'groups' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  int groups = attributes.at("groups").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dilations") != attributes.end(),
          "'dilations' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::vector<int> dilations;
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    dilations.push_back(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for DepthwiseConv2dTransposeOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv2dTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dTransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_, pir::Value output_size_, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format) {
  VLOG(4) << "Start build DepthwiseConv2dTransposeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  std::vector<pir::Attribute> vec_output_padding;
  for (size_t i = 0; i < static_cast<size_t>(output_padding.size()); i++) {
      pir::Attribute attr_output_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), output_padding[i]);

    vec_output_padding.push_back(attr_output_padding);
  }
  pir::Attribute attr_output_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_output_padding);
  argument.AddAttribute("output_padding", attr_output_padding);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);
  pir::Attribute attr_groups = pir::Int32Attribute::get(pir::IrContext::Instance(), groups);
  argument.AddAttribute("groups", attr_groups);
  std::vector<pir::Attribute> vec_dilations;
  for (size_t i = 0; i < static_cast<size_t>(dilations.size()); i++) {
      pir::Attribute attr_dilations = pir::Int32Attribute::get(pir::IrContext::Instance(), dilations[i]);

    vec_dilations.push_back(attr_dilations);
  }
  pir::Attribute attr_dilations = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_dilations);
  argument.AddAttribute("dilations", attr_dilations);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Conv2dTransposeInferMeta(meta_x, meta_filter, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DepthwiseConv2dTransposeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DepthwiseConv2dTransposeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("output_padding")>0,
                 "output_padding does not exist.");
  IR_ENFORCE(attributes.at("output_padding").isa<pir::ArrayAttribute>(),
                 "Type of attribute: output_padding is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("output_padding").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: output_padding is not right.");
  }
  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("groups")>0,
                 "groups does not exist.");
  IR_ENFORCE(attributes.at("groups").isa<pir::Int32Attribute>(),
                 "Type of attribute: groups is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dilations")>0,
                 "dilations does not exist.");
  IR_ENFORCE(attributes.at("dilations").isa<pir::ArrayAttribute>(),
                 "Type of attribute: dilations is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("dilations").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: dilations is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DepthwiseConv2dTransposeOp.";
}

void DepthwiseConv2dTransposeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Conv2dTransposeInferMeta);
  fn(infer_meta);
}

phi::DataType DepthwiseConv2dTransposeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DepthwiseConv2dTransposeOp";
  


  return expected_kernel_dtype;
}

const char *DisableCheckModelNanInfOp::attributes_name[1] = { "flag" };

OpInfoTuple DisableCheckModelNanInfOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("flag", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "check_model_nan_inf", {"x", "flag"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "disable_check_model_nan_inf");
}

void DisableCheckModelNanInfOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int flag) {
  VLOG(4) << "Start build DisableCheckModelNanInfOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flag = pir::Int32Attribute::get(pir::IrContext::Instance(), flag);
  argument.AddAttribute("flag", attr_flag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DisableCheckModelNanInfOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DisableCheckModelNanInfOp";


  IR_ENFORCE(
      attributes.find("flag") != attributes.end(),
          "'flag' Attribute is expected for DisableCheckModelNanInfOp. ");
  int flag = attributes.at("flag").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flag = pir::Int32Attribute::get(pir::IrContext::Instance(), flag);
  argument.AddAttribute("flag", attr_flag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DisableCheckModelNanInfOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DisableCheckModelNanInfOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("flag")>0,
                 "flag does not exist.");
  IR_ENFORCE(attributes.at("flag").isa<pir::Int32Attribute>(),
                 "Type of attribute: flag is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DisableCheckModelNanInfOp.";
}

void DisableCheckModelNanInfOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType DisableCheckModelNanInfOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DisableCheckModelNanInfOp";
  


  return expected_kernel_dtype;
}

const char *DistributeFpnProposalsOp::attributes_name[5] = { "min_level", "max_level", "refer_level", "refer_scale", "pixel_offset" };

OpInfoTuple DistributeFpnProposalsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("fpn_rois", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("rois_num", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("min_level", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("max_level", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("refer_level", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("refer_scale", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("pixel_offset", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("multi_fpn_rois", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("multi_level_rois_num", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false), paddle::dialect::OpOutputInfo("restore_index", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DistributeFpnProposalsInferMeta", {"fpn_rois", "rois_num", "min_level", "max_level", "refer_level", "refer_scale", "pixel_offset"}, "distribute_fpn_proposals", {"fpn_rois", "rois_num", "min_level", "max_level", "refer_level", "refer_scale", "pixel_offset"}, {"fpn_rois"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "distribute_fpn_proposals");
}

void DistributeFpnProposalsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value fpn_rois_, pir::Value rois_num_, int min_level, int max_level, int refer_level, int refer_scale, bool pixel_offset) {
  VLOG(4) << "Start build DistributeFpnProposalsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {fpn_rois_, rois_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min_level = pir::Int32Attribute::get(pir::IrContext::Instance(), min_level);
  argument.AddAttribute("min_level", attr_min_level);
  pir::Attribute attr_max_level = pir::Int32Attribute::get(pir::IrContext::Instance(), max_level);
  argument.AddAttribute("max_level", attr_max_level);
  pir::Attribute attr_refer_level = pir::Int32Attribute::get(pir::IrContext::Instance(), refer_level);
  argument.AddAttribute("refer_level", attr_refer_level);
  pir::Attribute attr_refer_scale = pir::Int32Attribute::get(pir::IrContext::Instance(), refer_scale);
  argument.AddAttribute("refer_scale", attr_refer_scale);
  pir::Attribute attr_pixel_offset = pir::BoolAttribute::get(pir::IrContext::Instance(), pixel_offset);
  argument.AddAttribute("pixel_offset", attr_pixel_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType fpn_rois = fpn_rois_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fpn_rois;

  VLOG(4) << "Builder construction  dense_fpn_rois";
  paddle::dialect::IrTensor ir_tensor_fpn_rois(paddle::dialect::TransToPhiDataType(fpn_rois.dtype()),
                                                      fpn_rois.dims(),
                                                      fpn_rois.data_layout(),
                                                      fpn_rois.lod(),
                                                      fpn_rois.offset());
  VLOG(4) << "Builder construction  meta_fpn_rois";
  paddle::dialect::IrMetaTensor meta_fpn_rois(&ir_tensor_fpn_rois);

  paddle::dialect::IrMetaTensor meta_rois_num;
  paddle::dialect::IrTensor ir_tensor_rois_num;
  if (rois_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rois_num = rois_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rois_num";
    ir_tensor_rois_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rois_num.dtype()),
                                                        rois_num.dims(),
                                                        rois_num.data_layout(),
                                                        rois_num.lod(),
                                                        rois_num.offset());
    VLOG(4) << "Builder construction  meta_rois_num";
    meta_rois_num = paddle::dialect::IrMetaTensor(&ir_tensor_rois_num);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_multi_fpn_rois((max_level - min_level + 1), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_multi_fpn_rois;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    vec_meta_multi_fpn_rois.push_back(paddle::dialect::IrMetaTensor(&vec_dense_multi_fpn_rois[i]));
  }
  std::vector<phi::MetaTensor*> meta_multi_fpn_rois;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_multi_fpn_rois.size()); i++) {
    meta_multi_fpn_rois.push_back(&vec_meta_multi_fpn_rois[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_multi_level_rois_num((max_level - min_level + 1), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_multi_level_rois_num;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    vec_meta_multi_level_rois_num.push_back(paddle::dialect::IrMetaTensor(&vec_dense_multi_level_rois_num[i]));
  }
  std::vector<phi::MetaTensor*> meta_multi_level_rois_num;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_multi_level_rois_num.size()); i++) {
    meta_multi_level_rois_num.push_back(&vec_meta_multi_level_rois_num[i]);
  }
  paddle::dialect::IrTensor dense_restore_index;
  paddle::dialect::IrMetaTensor meta_restore_index(&dense_restore_index);

  phi::DistributeFpnProposalsInferMeta(meta_fpn_rois, meta_rois_num, min_level, max_level, refer_level, refer_scale, pixel_offset, meta_multi_fpn_rois, meta_multi_level_rois_num, &meta_restore_index);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> multi_fpn_rois_types;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    multi_fpn_rois_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_multi_fpn_rois[i].dtype()), vec_dense_multi_fpn_rois[i].dims(), vec_dense_multi_fpn_rois[i].layout(), vec_dense_multi_fpn_rois[i].lod(), vec_dense_multi_fpn_rois[i].offset()));
  }
  pir::Type multi_fpn_rois_vector_type = pir::VectorType::get(pir::IrContext::Instance(), multi_fpn_rois_types);
  argument_outputs.push_back(multi_fpn_rois_vector_type);

  std::vector<pir::Type> multi_level_rois_num_types;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    multi_level_rois_num_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_multi_level_rois_num[i].dtype()), vec_dense_multi_level_rois_num[i].dims(), vec_dense_multi_level_rois_num[i].layout(), vec_dense_multi_level_rois_num[i].lod(), vec_dense_multi_level_rois_num[i].offset()));
  }
  pir::Type multi_level_rois_num_vector_type = pir::VectorType::get(pir::IrContext::Instance(), multi_level_rois_num_types);
  argument_outputs.push_back(multi_level_rois_num_vector_type);

  pir::Type restore_index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_restore_index.dtype()), dense_restore_index.dims(), dense_restore_index.layout(), dense_restore_index.lod(), dense_restore_index.offset());
  argument_outputs.push_back(restore_index_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DistributeFpnProposalsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value fpn_rois_, pir::Value rois_num_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DistributeFpnProposalsOp";


  IR_ENFORCE(
      attributes.find("min_level") != attributes.end(),
          "'min_level' Attribute is expected for DistributeFpnProposalsOp. ");
  int min_level = attributes.at("min_level").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("max_level") != attributes.end(),
          "'max_level' Attribute is expected for DistributeFpnProposalsOp. ");
  int max_level = attributes.at("max_level").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("refer_level") != attributes.end(),
          "'refer_level' Attribute is expected for DistributeFpnProposalsOp. ");
  int refer_level = attributes.at("refer_level").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("refer_scale") != attributes.end(),
          "'refer_scale' Attribute is expected for DistributeFpnProposalsOp. ");
  int refer_scale = attributes.at("refer_scale").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("pixel_offset") != attributes.end(),
          "'pixel_offset' Attribute is expected for DistributeFpnProposalsOp. ");
  bool pixel_offset = attributes.at("pixel_offset").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {fpn_rois_, rois_num_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_min_level = pir::Int32Attribute::get(pir::IrContext::Instance(), min_level);
  argument.AddAttribute("min_level", attr_min_level);
  pir::Attribute attr_max_level = pir::Int32Attribute::get(pir::IrContext::Instance(), max_level);
  argument.AddAttribute("max_level", attr_max_level);
  pir::Attribute attr_refer_level = pir::Int32Attribute::get(pir::IrContext::Instance(), refer_level);
  argument.AddAttribute("refer_level", attr_refer_level);
  pir::Attribute attr_refer_scale = pir::Int32Attribute::get(pir::IrContext::Instance(), refer_scale);
  argument.AddAttribute("refer_scale", attr_refer_scale);
  pir::Attribute attr_pixel_offset = pir::BoolAttribute::get(pir::IrContext::Instance(), pixel_offset);
  argument.AddAttribute("pixel_offset", attr_pixel_offset);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType fpn_rois = fpn_rois_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)fpn_rois;

  VLOG(4) << "Builder construction  dense_fpn_rois";
  paddle::dialect::IrTensor ir_tensor_fpn_rois(paddle::dialect::TransToPhiDataType(fpn_rois.dtype()),
                                                      fpn_rois.dims(),
                                                      fpn_rois.data_layout(),
                                                      fpn_rois.lod(),
                                                      fpn_rois.offset());
  VLOG(4) << "Builder construction  meta_fpn_rois";
  paddle::dialect::IrMetaTensor meta_fpn_rois(&ir_tensor_fpn_rois);

  paddle::dialect::IrMetaTensor meta_rois_num;
  paddle::dialect::IrTensor ir_tensor_rois_num;
  if (rois_num_.impl() != nullptr) {
    paddle::dialect::DenseTensorType rois_num = rois_num_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_rois_num";
    ir_tensor_rois_num = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(rois_num.dtype()),
                                                        rois_num.dims(),
                                                        rois_num.data_layout(),
                                                        rois_num.lod(),
                                                        rois_num.offset());
    VLOG(4) << "Builder construction  meta_rois_num";
    meta_rois_num = paddle::dialect::IrMetaTensor(&ir_tensor_rois_num);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_multi_fpn_rois((max_level - min_level + 1), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_multi_fpn_rois;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    vec_meta_multi_fpn_rois.push_back(paddle::dialect::IrMetaTensor(&vec_dense_multi_fpn_rois[i]));
  }
  std::vector<phi::MetaTensor*> meta_multi_fpn_rois;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_multi_fpn_rois.size()); i++) {
    meta_multi_fpn_rois.push_back(&vec_meta_multi_fpn_rois[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_multi_level_rois_num((max_level - min_level + 1), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_multi_level_rois_num;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    vec_meta_multi_level_rois_num.push_back(paddle::dialect::IrMetaTensor(&vec_dense_multi_level_rois_num[i]));
  }
  std::vector<phi::MetaTensor*> meta_multi_level_rois_num;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_multi_level_rois_num.size()); i++) {
    meta_multi_level_rois_num.push_back(&vec_meta_multi_level_rois_num[i]);
  }
  paddle::dialect::IrTensor dense_restore_index;
  paddle::dialect::IrMetaTensor meta_restore_index(&dense_restore_index);

  phi::DistributeFpnProposalsInferMeta(meta_fpn_rois, meta_rois_num, min_level, max_level, refer_level, refer_scale, pixel_offset, meta_multi_fpn_rois, meta_multi_level_rois_num, &meta_restore_index);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> multi_fpn_rois_types;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    multi_fpn_rois_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_multi_fpn_rois[i].dtype()), vec_dense_multi_fpn_rois[i].dims(), vec_dense_multi_fpn_rois[i].layout(), vec_dense_multi_fpn_rois[i].lod(), vec_dense_multi_fpn_rois[i].offset()));
  }
  pir::Type multi_fpn_rois_vector_type = pir::VectorType::get(pir::IrContext::Instance(), multi_fpn_rois_types);
  argument_outputs.push_back(multi_fpn_rois_vector_type);

  std::vector<pir::Type> multi_level_rois_num_types;
  for (size_t i=0; i < static_cast<size_t>(max_level - min_level + 1); i++) {
    multi_level_rois_num_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_multi_level_rois_num[i].dtype()), vec_dense_multi_level_rois_num[i].dims(), vec_dense_multi_level_rois_num[i].layout(), vec_dense_multi_level_rois_num[i].lod(), vec_dense_multi_level_rois_num[i].offset()));
  }
  pir::Type multi_level_rois_num_vector_type = pir::VectorType::get(pir::IrContext::Instance(), multi_level_rois_num_types);
  argument_outputs.push_back(multi_level_rois_num_vector_type);

  pir::Type restore_index_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_restore_index.dtype()), dense_restore_index.dims(), dense_restore_index.layout(), dense_restore_index.lod(), dense_restore_index.offset());
  argument_outputs.push_back(restore_index_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DistributeFpnProposalsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DistributeFpnProposalsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("min_level")>0,
                 "min_level does not exist.");
  IR_ENFORCE(attributes.at("min_level").isa<pir::Int32Attribute>(),
                 "Type of attribute: min_level is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("max_level")>0,
                 "max_level does not exist.");
  IR_ENFORCE(attributes.at("max_level").isa<pir::Int32Attribute>(),
                 "Type of attribute: max_level is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("refer_level")>0,
                 "refer_level does not exist.");
  IR_ENFORCE(attributes.at("refer_level").isa<pir::Int32Attribute>(),
                 "Type of attribute: refer_level is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("refer_scale")>0,
                 "refer_scale does not exist.");
  IR_ENFORCE(attributes.at("refer_scale").isa<pir::Int32Attribute>(),
                 "Type of attribute: refer_scale is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("pixel_offset")>0,
                 "pixel_offset does not exist.");
  IR_ENFORCE(attributes.at("pixel_offset").isa<pir::BoolAttribute>(),
                 "Type of attribute: pixel_offset is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  if (auto output_1_type = (*this)->result(1).type()) {
    if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th output.");
      }
    }
    else {
      IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: DistributeFpnProposalsOp.";
}

void DistributeFpnProposalsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DistributeFpnProposalsInferMeta);
  fn(infer_meta);
}

phi::DataType DistributeFpnProposalsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DistributeFpnProposalsOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple DivideOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "divide", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "divide");
}

void DivideOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build DivideOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DivideOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DivideOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DivideOp.";
}

void DivideOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType DivideOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DivideOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Divide_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "divide", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "divide");
}

void Divide_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Divide_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Divide_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Divide_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Divide_Op.";
}

void Divide_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType Divide_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Divide_Op";
  


  return expected_kernel_dtype;
}

const char *DropoutOp::attributes_name[5] = { "p", "is_test", "mode", "seed", "fix_seed" };

OpInfoTuple DropoutOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("seed_tensor", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("p", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("fix_seed", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mask", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DropoutInferMeta", {"x", "seed_tensor", "p", "is_test", "mode", "seed", "fix_seed"}, "dropout", {"x", "seed_tensor", "p", "is_test", "mode", "seed", "fix_seed"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dropout");
}

void DropoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value seed_tensor_, float p, bool is_test, const std::string& mode, int seed, bool fix_seed) {
  VLOG(4) << "Start build DropoutOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, seed_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_seed_tensor;
  paddle::dialect::IrTensor ir_tensor_seed_tensor;
  if (seed_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seed_tensor = seed_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seed_tensor";
    ir_tensor_seed_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seed_tensor.dtype()),
                                                        seed_tensor.dims(),
                                                        seed_tensor.data_layout(),
                                                        seed_tensor.lod(),
                                                        seed_tensor.offset());
    VLOG(4) << "Builder construction  meta_seed_tensor";
    meta_seed_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_seed_tensor);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mask;
  paddle::dialect::IrMetaTensor meta_mask(&dense_mask);

  phi::DropoutInferMeta(meta_x, meta_seed_tensor, p, is_test, mode, seed, fix_seed, &meta_out, &meta_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask.dtype()), dense_mask.dims(), dense_mask.layout(), dense_mask.lod(), dense_mask.offset());
  argument_outputs.push_back(mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DropoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value seed_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DropoutOp";


  IR_ENFORCE(
      attributes.find("p") != attributes.end(),
          "'p' Attribute is expected for DropoutOp. ");
  float p = attributes.at("p").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for DropoutOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for DropoutOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for DropoutOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("fix_seed") != attributes.end(),
          "'fix_seed' Attribute is expected for DropoutOp. ");
  bool fix_seed = attributes.at("fix_seed").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, seed_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_p = paddle::dialect::TransToIrAttribute(p, pir::IrContext::Instance());
  argument.AddAttribute("p", attr_p);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), fix_seed);
  argument.AddAttribute("fix_seed", attr_fix_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_seed_tensor;
  paddle::dialect::IrTensor ir_tensor_seed_tensor;
  if (seed_tensor_.impl() != nullptr) {
    paddle::dialect::DenseTensorType seed_tensor = seed_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_seed_tensor";
    ir_tensor_seed_tensor = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(seed_tensor.dtype()),
                                                        seed_tensor.dims(),
                                                        seed_tensor.data_layout(),
                                                        seed_tensor.lod(),
                                                        seed_tensor.offset());
    VLOG(4) << "Builder construction  meta_seed_tensor";
    meta_seed_tensor = paddle::dialect::IrMetaTensor(&ir_tensor_seed_tensor);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mask;
  paddle::dialect::IrMetaTensor meta_mask(&dense_mask);

  phi::DropoutInferMeta(meta_x, meta_seed_tensor, p, is_test, mode, seed, fix_seed, &meta_out, &meta_mask);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mask.dtype()), dense_mask.dims(), dense_mask.layout(), dense_mask.lod(), dense_mask.offset());
  argument_outputs.push_back(mask_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DropoutOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DropoutOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("p")>0,
                 "p does not exist.");
  IR_ENFORCE(attributes.at("p").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: p is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("fix_seed")>0,
                 "fix_seed does not exist.");
  IR_ENFORCE(attributes.at("fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: fix_seed is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: DropoutOp.";
}

void DropoutOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DropoutInferMeta);
  fn(infer_meta);
}

phi::DataType DropoutOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DropoutOp";
  


  return expected_kernel_dtype;
}

const char *EinsumOp::attributes_name[1] = { "equation" };

OpInfoTuple EinsumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("equation", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("inner_cache", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("xshape", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EinsumRawInferMeta", {"x", "equation"}, "einsum", {"x", "equation"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "einsum");
}

void EinsumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& equation) {
  VLOG(4) << "Start build EinsumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equation = pir::StrAttribute::get(pir::IrContext::Instance(), equation);
  argument.AddAttribute("equation", attr_equation);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_inner_cache((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inner_cache;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_inner_cache.push_back(paddle::dialect::IrMetaTensor(&vec_dense_inner_cache[i]));
  }
  std::vector<phi::MetaTensor*> meta_inner_cache;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inner_cache.size()); i++) {
    meta_inner_cache.push_back(&vec_meta_inner_cache[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_xshape((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_xshape;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_xshape.push_back(paddle::dialect::IrMetaTensor(&vec_dense_xshape[i]));
  }
  std::vector<phi::MetaTensor*> meta_xshape;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_xshape.size()); i++) {
    meta_xshape.push_back(&vec_meta_xshape[i]);
  }

  phi::EinsumRawInferMeta(meta_x, equation, &meta_out, meta_inner_cache, meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  std::vector<pir::Type> inner_cache_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    inner_cache_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_inner_cache[i].dtype()), vec_dense_inner_cache[i].dims(), vec_dense_inner_cache[i].layout(), vec_dense_inner_cache[i].lod(), vec_dense_inner_cache[i].offset()));
  }
  pir::Type inner_cache_vector_type = pir::VectorType::get(pir::IrContext::Instance(), inner_cache_types);
  argument_outputs.push_back(inner_cache_vector_type);

  std::vector<pir::Type> xshape_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    xshape_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_xshape[i].dtype()), vec_dense_xshape[i].dims(), vec_dense_xshape[i].layout(), vec_dense_xshape[i].lod(), vec_dense_xshape[i].offset()));
  }
  pir::Type xshape_vector_type = pir::VectorType::get(pir::IrContext::Instance(), xshape_types);
  argument_outputs.push_back(xshape_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EinsumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EinsumOp";


  IR_ENFORCE(
      attributes.find("equation") != attributes.end(),
          "'equation' Attribute is expected for EinsumOp. ");
  std::string equation = attributes.at("equation").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_equation = pir::StrAttribute::get(pir::IrContext::Instance(), equation);
  argument.AddAttribute("equation", attr_equation);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType x = x_.type().dyn_cast<pir::VectorType>(); (void)x;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_x;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_ir_tensor_x.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(x[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     x[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_x;
  for (size_t i=0; i < vec_ir_tensor_x.size(); i++) {
    vec_meta_x.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_x[i]));
  }

  std::vector<const phi::MetaTensor*> meta_x;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_x.size()); i++) {
    meta_x.push_back(&vec_meta_x[i]);
  }
   paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_inner_cache((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_inner_cache;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_inner_cache.push_back(paddle::dialect::IrMetaTensor(&vec_dense_inner_cache[i]));
  }
  std::vector<phi::MetaTensor*> meta_inner_cache;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_inner_cache.size()); i++) {
    meta_inner_cache.push_back(&vec_meta_inner_cache[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_xshape((x.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_xshape;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    vec_meta_xshape.push_back(paddle::dialect::IrMetaTensor(&vec_dense_xshape[i]));
  }
  std::vector<phi::MetaTensor*> meta_xshape;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_xshape.size()); i++) {
    meta_xshape.push_back(&vec_meta_xshape[i]);
  }

  phi::EinsumRawInferMeta(meta_x, equation, &meta_out, meta_inner_cache, meta_xshape);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  std::vector<pir::Type> inner_cache_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    inner_cache_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_inner_cache[i].dtype()), vec_dense_inner_cache[i].dims(), vec_dense_inner_cache[i].layout(), vec_dense_inner_cache[i].lod(), vec_dense_inner_cache[i].offset()));
  }
  pir::Type inner_cache_vector_type = pir::VectorType::get(pir::IrContext::Instance(), inner_cache_types);
  argument_outputs.push_back(inner_cache_vector_type);

  std::vector<pir::Type> xshape_types;
  for (size_t i=0; i < static_cast<size_t>(x.size()); i++) {
    xshape_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_xshape[i].dtype()), vec_dense_xshape[i].dims(), vec_dense_xshape[i].layout(), vec_dense_xshape[i].lod(), vec_dense_xshape[i].offset()));
  }
  pir::Type xshape_vector_type = pir::VectorType::get(pir::IrContext::Instance(), xshape_types);
  argument_outputs.push_back(xshape_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EinsumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EinsumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("equation")>0,
                 "equation does not exist.");
  IR_ENFORCE(attributes.at("equation").isa<pir::StrAttribute>(),
                 "Type of attribute: equation is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  auto output_2_type = (*this)->result(2).type();
  if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  else {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: EinsumOp.";
}

void EinsumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EinsumRawInferMeta);
  fn(infer_meta);
}

phi::DataType EinsumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EinsumOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ElementwisePowOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "elementwise_pow", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "elementwise_pow");
}

void ElementwisePowOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build ElementwisePowOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ElementwisePowOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ElementwisePowOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ElementwisePowOp.";
}

void ElementwisePowOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType ElementwisePowOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ElementwisePowOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *EmbeddingOp::attributes_name[2] = { "padding_idx", "sparse" };

OpInfoTuple EmbeddingOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("sparse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingInferMeta", {"x", "weight", "padding_idx"}, "embedding", {"x", "weight", "padding_idx"}, {"weight"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding");
}

void EmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, int64_t padding_idx, bool sparse) {
  VLOG(4) << "Start build EmbeddingOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);
  pir::Attribute attr_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), sparse);
  argument.AddAttribute("sparse", attr_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EmbeddingInferMeta(meta_x, meta_weight, padding_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EmbeddingOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for EmbeddingOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("sparse") != attributes.end(),
          "'sparse' Attribute is expected for EmbeddingOp. ");
  bool sparse = attributes.at("sparse").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);
  pir::Attribute attr_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), sparse);
  argument.AddAttribute("sparse", attr_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EmbeddingInferMeta(meta_x, meta_weight, padding_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmbeddingOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EmbeddingOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("padding_idx")>0,
                 "padding_idx does not exist.");
  IR_ENFORCE(attributes.at("padding_idx").isa<pir::Int64Attribute>(),
                 "Type of attribute: padding_idx is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("sparse")>0,
                 "sparse does not exist.");
  IR_ENFORCE(attributes.at("sparse").isa<pir::BoolAttribute>(),
                 "Type of attribute: sparse is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EmbeddingOp.";
}

void EmbeddingOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingInferMeta);
  fn(infer_meta);
}

phi::DataType EmbeddingOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EmbeddingOp";
  


  return expected_kernel_dtype;
}

const char *SparseWeightEmbeddingOp::attributes_name[2] = { "padding_idx", "sparse" };

OpInfoTuple SparseWeightEmbeddingOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::SelectedRowsType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("padding_idx", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("sparse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EmbeddingInferMeta", {"x", "weight", "padding_idx"}, "sparse_weight_embedding", {"x", "weight", "padding_idx"}, {"weight"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "embedding");
}

void SparseWeightEmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, int64_t padding_idx, bool sparse) {
  VLOG(4) << "Start build SparseWeightEmbeddingOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);
  pir::Attribute attr_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), sparse);
  argument.AddAttribute("sparse", attr_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::SelectedRowsType weight = weight_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EmbeddingInferMeta(meta_x, meta_weight, padding_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseWeightEmbeddingOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value weight_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SparseWeightEmbeddingOp";


  IR_ENFORCE(
      attributes.find("padding_idx") != attributes.end(),
          "'padding_idx' Attribute is expected for SparseWeightEmbeddingOp. ");
  int64_t padding_idx = attributes.at("padding_idx").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("sparse") != attributes.end(),
          "'sparse' Attribute is expected for SparseWeightEmbeddingOp. ");
  bool sparse = attributes.at("sparse").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, weight_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_padding_idx = pir::Int64Attribute::get(pir::IrContext::Instance(), padding_idx);
  argument.AddAttribute("padding_idx", attr_padding_idx);
  pir::Attribute attr_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), sparse);
  argument.AddAttribute("sparse", attr_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::SelectedRowsType weight = weight_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EmbeddingInferMeta(meta_x, meta_weight, padding_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseWeightEmbeddingOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SparseWeightEmbeddingOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("padding_idx")>0,
                 "padding_idx does not exist.");
  IR_ENFORCE(attributes.at("padding_idx").isa<pir::Int64Attribute>(),
                 "Type of attribute: padding_idx is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("sparse")>0,
                 "sparse does not exist.");
  IR_ENFORCE(attributes.at("sparse").isa<pir::BoolAttribute>(),
                 "Type of attribute: sparse is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SparseWeightEmbeddingOp.";
}

void SparseWeightEmbeddingOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EmbeddingInferMeta);
  fn(infer_meta);
}

phi::DataType SparseWeightEmbeddingOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SparseWeightEmbeddingOp";
  


  return expected_kernel_dtype;
}

const char *EmptyOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple EmptyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CreateInferMeta", {"shape", "dtype"}, "empty", {"shape", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "empty");
}

void EmptyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& shape, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build EmptyOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmptyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EmptyOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for EmptyOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for EmptyOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for EmptyOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmptyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shape_, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build EmptyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmptyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EmptyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EmptyOp.";
}

void EmptyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateInferMeta);
  fn(infer_meta);
}

phi::DataType EmptyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EmptyOp";
  


  return expected_kernel_dtype;
}

const char *EmptyLikeOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple EmptyLikeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CreateLikeInferMeta", {"x", "dtype"}, "empty_like", {"x", "dtype"}, {"dtype", "x"}, {"place", "x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "empty_like");
}

void EmptyLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build EmptyLikeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateLikeInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmptyLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EmptyLikeOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for EmptyLikeOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for EmptyLikeOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateLikeInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EmptyLikeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EmptyLikeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EmptyLikeOp.";
}

void EmptyLikeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateLikeInferMeta);
  fn(infer_meta);
}

phi::DataType EmptyLikeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EmptyLikeOp";
  


  return expected_kernel_dtype;
}

const char *EnableCheckModelNanInfOp::attributes_name[1] = { "flag" };

OpInfoTuple EnableCheckModelNanInfOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("flag", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "check_model_nan_inf", {"x", "flag"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "enable_check_model_nan_inf");
}

void EnableCheckModelNanInfOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int flag) {
  VLOG(4) << "Start build EnableCheckModelNanInfOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flag = pir::Int32Attribute::get(pir::IrContext::Instance(), flag);
  argument.AddAttribute("flag", attr_flag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EnableCheckModelNanInfOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EnableCheckModelNanInfOp";


  IR_ENFORCE(
      attributes.find("flag") != attributes.end(),
          "'flag' Attribute is expected for EnableCheckModelNanInfOp. ");
  int flag = attributes.at("flag").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_flag = pir::Int32Attribute::get(pir::IrContext::Instance(), flag);
  argument.AddAttribute("flag", attr_flag);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EnableCheckModelNanInfOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EnableCheckModelNanInfOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("flag")>0,
                 "flag does not exist.");
  IR_ENFORCE(attributes.at("flag").isa<pir::Int32Attribute>(),
                 "Type of attribute: flag is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EnableCheckModelNanInfOp.";
}

void EnableCheckModelNanInfOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType EnableCheckModelNanInfOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EnableCheckModelNanInfOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple EqualOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "equal", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "equal");
}

void EqualOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build EqualOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EqualOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EqualOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EqualOp.";
}

void EqualOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType EqualOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EqualOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Equal_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "equal", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "equal");
}

void Equal_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Equal_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Equal_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Equal_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Equal_Op.";
}

void Equal_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType Equal_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Equal_Op";
  


  return expected_kernel_dtype;
}

const char *Exponential_Op::attributes_name[1] = { "lam" };

OpInfoTuple Exponential_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lam", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "exponential", {"x", "lam"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "exponential_");
}

void Exponential_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float lam) {
  VLOG(4) << "Start build Exponential_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lam = pir::FloatAttribute::get(pir::IrContext::Instance(), lam);
  argument.AddAttribute("lam", attr_lam);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Exponential_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Exponential_Op";


  IR_ENFORCE(
      attributes.find("lam") != attributes.end(),
          "'lam' Attribute is expected for Exponential_Op. ");
  float lam = attributes.at("lam").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lam = pir::FloatAttribute::get(pir::IrContext::Instance(), lam);
  argument.AddAttribute("lam", attr_lam);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Exponential_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Exponential_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("lam")>0,
                 "lam does not exist.");
  IR_ENFORCE(attributes.at("lam").isa<pir::FloatAttribute>(),
                 "Type of attribute: lam is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Exponential_Op.";
}

void Exponential_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType Exponential_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Exponential_Op";
  


  return expected_kernel_dtype;
}

const char *EyeOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple EyeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("num_rows", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("num_columns", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("EyeInferMeta", {"num_rows", "num_columns", "dtype"}, "eye", {"num_rows", "num_columns", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "eye");
}

void EyeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, float num_rows, float num_columns, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build EyeOp";


  // Generate scalar mutable attribute: num_rows
  paddle::dialect::FullOp full_num_rows_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_rows, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult num_rows_ = full_num_rows_op->result(0);
      // Generate scalar mutable attribute: num_columns
  paddle::dialect::FullOp full_num_columns_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_columns, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult num_columns_ = full_num_columns_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {num_rows_, num_columns_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EyeInferMeta(num_rows, num_columns, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EyeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build EyeOp";


  IR_ENFORCE(
      attributes.find("num_rows") != attributes.end(),
          "'num_rows' Attribute is expected for EyeOp. ");
  float num_rows = attributes.at("num_rows").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("num_columns") != attributes.end(),
          "'num_columns' Attribute is expected for EyeOp. ");
  float num_columns = attributes.at("num_columns").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for EyeOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for EyeOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  // Generate scalar mutable attribute: num_rows
  paddle::dialect::FullOp full_num_rows_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_rows, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult num_rows_ = full_num_rows_op->result(0);
      // Generate scalar mutable attribute: num_columns
  paddle::dialect::FullOp full_num_columns_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_columns, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult num_columns_ = full_num_columns_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {num_rows_, num_columns_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EyeInferMeta(num_rows, num_columns, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EyeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value num_rows_, pir::Value num_columns_, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build EyeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {num_rows_, num_columns_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  phi::Scalar num_rows;
  if (num_rows_.dyn_cast<pir::OpResult>() && num_rows_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    num_rows = std::move(phi::Scalar(num_rows_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    num_rows = std::move(phi::Scalar(-1));
    num_rows.SetFromTensor(true);
  }
  phi::Scalar num_columns;
  if (num_columns_.dyn_cast<pir::OpResult>() && num_columns_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    num_columns = std::move(phi::Scalar(num_columns_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    num_columns = std::move(phi::Scalar(-1));
    num_columns.SetFromTensor(true);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::EyeInferMeta(num_rows, num_columns, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void EyeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: EyeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: EyeOp.";
}

void EyeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::EyeInferMeta);
  fn(infer_meta);
}

phi::DataType EyeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: EyeOp";
  


  return expected_kernel_dtype;
}

const char *FeedOp::attributes_name[2] = { "name", "col" };

OpInfoTuple FeedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("name", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("col", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "feed");
}

void FeedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FeedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("name")>0,
                 "name does not exist.");
  IR_ENFORCE(attributes.at("name").isa<pir::StrAttribute>(),
                 "Type of attribute: name is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("col")>0,
                 "col does not exist.");
  IR_ENFORCE(attributes.at("col").isa<pir::Int32Attribute>(),
                 "Type of attribute: col is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FeedOp.";
}

phi::DataType FeedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FeedOp";
  


  return expected_kernel_dtype;
}

const char *FetchOp::attributes_name[2] = { "name", "col" };

OpInfoTuple FetchOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("name", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("col", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "fetch", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fetch");
}

void FetchOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::string& name, int col) {
  VLOG(4) << "Start build FetchOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_name = pir::StrAttribute::get(pir::IrContext::Instance(), name);
  argument.AddAttribute("name", attr_name);
  pir::Attribute attr_col = pir::Int32Attribute::get(pir::IrContext::Instance(), col);
  argument.AddAttribute("col", attr_col);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FetchOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FetchOp";


  IR_ENFORCE(
      attributes.find("name") != attributes.end(),
          "'name' Attribute is expected for FetchOp. ");
  std::string name = attributes.at("name").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("col") != attributes.end(),
          "'col' Attribute is expected for FetchOp. ");
  int col = attributes.at("col").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_name = pir::StrAttribute::get(pir::IrContext::Instance(), name);
  argument.AddAttribute("name", attr_name);
  pir::Attribute attr_col = pir::Int32Attribute::get(pir::IrContext::Instance(), col);
  argument.AddAttribute("col", attr_col);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FetchOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FetchOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("name")>0,
                 "name does not exist.");
  IR_ENFORCE(attributes.at("name").isa<pir::StrAttribute>(),
                 "Type of attribute: name is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("col")>0,
                 "col does not exist.");
  IR_ENFORCE(attributes.at("col").isa<pir::Int32Attribute>(),
                 "Type of attribute: col is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FetchOp.";
}

void FetchOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FetchOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FetchOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple FloorDivideOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "floor_divide", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "floor_divide");
}

void FloorDivideOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build FloorDivideOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FloorDivideOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FloorDivideOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FloorDivideOp.";
}

void FloorDivideOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType FloorDivideOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FloorDivideOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

OpInfoTuple FloorDivide_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "floor_divide", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "floor_divide");
}

void FloorDivide_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build FloorDivide_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FloorDivide_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FloorDivide_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FloorDivide_Op.";
}

void FloorDivide_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType FloorDivide_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FloorDivide_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *FrobeniusNormOp::attributes_name[2] = { "keep_dim", "reduce_all" };

OpInfoTuple FrobeniusNormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keep_dim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceIntArrayAxisInferMetaBase", {"x", "axis", "keep_dim", "reduce_all"}, "frobenius_norm", {"x", "axis", "keep_dim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "frobenius_norm");
}

void FrobeniusNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build FrobeniusNormOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMetaBase(meta_x, axis, keep_dim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrobeniusNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FrobeniusNormOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FrobeniusNormOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keep_dim") != attributes.end(),
          "'keep_dim' Attribute is expected for FrobeniusNormOp. ");
  bool keep_dim = attributes.at("keep_dim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for FrobeniusNormOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMetaBase(meta_x, axis, keep_dim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrobeniusNormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build FrobeniusNormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  std::vector<int64_t> axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = paddle::dialect::GetInt64Vector(
                    axis_.dyn_cast<pir::OpResult>().owner()
                    ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                    .attribute("value"));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::vector<int64_t>(axis_size, -1);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::vector<int64_t>(axis_size, -1);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMetaBase(meta_x, axis, keep_dim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FrobeniusNormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FrobeniusNormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("keep_dim")>0,
                 "keep_dim does not exist.");
  IR_ENFORCE(attributes.at("keep_dim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keep_dim is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("reduce_all")>0,
                 "reduce_all does not exist.");
  IR_ENFORCE(attributes.at("reduce_all").isa<pir::BoolAttribute>(),
                 "Type of attribute: reduce_all is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FrobeniusNormOp.";
}

void FrobeniusNormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceIntArrayAxisInferMetaBase);
  fn(infer_meta);
}

phi::DataType FrobeniusNormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FrobeniusNormOp";
  


  return expected_kernel_dtype;
}

const char *FullOp::attributes_name[4] = { "shape", "value", "dtype", "place" };

OpInfoTuple FullOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("value", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CreateInferMeta", {"shape", "dtype"}, "full", {"shape", "value", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "full");
}

void FullOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& shape, float value, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build FullOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_shape = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(shape));
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_value = paddle::dialect::TransToIrAttribute(value, pir::IrContext::Instance());
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FullOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for FullOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FullOp. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for FullOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for FullOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_shape = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(shape));
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_value = paddle::dialect::TransToIrAttribute(value, pir::IrContext::Instance());
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FullOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: shape is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("value")>0,
                 "value does not exist.");
  IR_ENFORCE(attributes.at("value").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: value is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FullOp.";
}

void FullOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateInferMeta);
  fn(infer_meta);
}

phi::DataType FullOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FullOp";
  


  return expected_kernel_dtype;
}

const char *Full_Op::attributes_name[4] = { "shape", "value", "dtype", "place" };

OpInfoTuple Full_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("output", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("value", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CreateInferMeta", {"shape", "dtype"}, "full", {"shape", "value", "dtype"}, {"dtype"}, {"place"}, {{"out", "output"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "full_");
}

void Full_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value output_, const std::vector<int64_t>& shape, float value, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build Full_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {output_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_shape = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(shape));
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_value = paddle::dialect::TransToIrAttribute(value, pir::IrContext::Instance());
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Full_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value output_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Full_Op";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for Full_Op. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for Full_Op. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for Full_Op. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for Full_Op. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {output_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_shape = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(shape));
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_value = paddle::dialect::TransToIrAttribute(value, pir::IrContext::Instance());
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType output = output_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)output;
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Full_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Full_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: shape is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("value")>0,
                 "value does not exist.");
  IR_ENFORCE(attributes.at("value").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: value is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Full_Op.";
}

void Full_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateInferMeta);
  fn(infer_meta);
}

phi::DataType Full_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Full_Op";
  


  return expected_kernel_dtype;
}

const char *FullBatchSizeLikeOp::attributes_name[6] = { "shape", "dtype", "value", "input_dim_idx", "output_dim_idx", "place" };

OpInfoTuple FullBatchSizeLikeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("value", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("input_dim_idx", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("output_dim_idx", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FullBatchSizeLikeInferMeta", {"input", "shape", "value", "dtype", "input_dim_idx", "output_dim_idx"}, "full_batch_size_like", {"input", "shape", "value", "dtype", "input_dim_idx", "output_dim_idx"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "full_batch_size_like");
}

void FullBatchSizeLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, const std::vector<int>& shape, phi::DataType dtype, float value, int input_dim_idx, int output_dim_idx, const Place& place) {
  VLOG(4) << "Start build FullBatchSizeLikeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_value = paddle::dialect::TransToIrAttribute(value, pir::IrContext::Instance());
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_input_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), input_dim_idx);
  argument.AddAttribute("input_dim_idx", attr_input_dim_idx);
  pir::Attribute attr_output_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), output_dim_idx);
  argument.AddAttribute("output_dim_idx", attr_output_dim_idx);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FullBatchSizeLikeInferMeta(meta_input, shape, value, dtype, input_dim_idx, output_dim_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullBatchSizeLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FullBatchSizeLikeOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for FullBatchSizeLikeOp. ");
  std::vector<int> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for FullBatchSizeLikeOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FullBatchSizeLikeOp. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("input_dim_idx") != attributes.end(),
          "'input_dim_idx' Attribute is expected for FullBatchSizeLikeOp. ");
  int input_dim_idx = attributes.at("input_dim_idx").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("output_dim_idx") != attributes.end(),
          "'output_dim_idx' Attribute is expected for FullBatchSizeLikeOp. ");
  int output_dim_idx = attributes.at("output_dim_idx").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for FullBatchSizeLikeOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_value = paddle::dialect::TransToIrAttribute(value, pir::IrContext::Instance());
  argument.AddAttribute("value", attr_value);
  pir::Attribute attr_input_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), input_dim_idx);
  argument.AddAttribute("input_dim_idx", attr_input_dim_idx);
  pir::Attribute attr_output_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), output_dim_idx);
  argument.AddAttribute("output_dim_idx", attr_output_dim_idx);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FullBatchSizeLikeInferMeta(meta_input, shape, value, dtype, input_dim_idx, output_dim_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullBatchSizeLikeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FullBatchSizeLikeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("value")>0,
                 "value does not exist.");
  IR_ENFORCE(attributes.at("value").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: value is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("input_dim_idx")>0,
                 "input_dim_idx does not exist.");
  IR_ENFORCE(attributes.at("input_dim_idx").isa<pir::Int32Attribute>(),
                 "Type of attribute: input_dim_idx is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("output_dim_idx")>0,
                 "output_dim_idx does not exist.");
  IR_ENFORCE(attributes.at("output_dim_idx").isa<pir::Int32Attribute>(),
                 "Type of attribute: output_dim_idx is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FullBatchSizeLikeOp.";
}

void FullBatchSizeLikeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FullBatchSizeLikeInferMeta);
  fn(infer_meta);
}

phi::DataType FullBatchSizeLikeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FullBatchSizeLikeOp";
  


  return expected_kernel_dtype;
}

const char *FullLikeOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple FullLikeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CreateLikeInferMeta", {"x", "dtype"}, "full_like", {"x", "value", "dtype"}, {"dtype", "x"}, {"place", "x"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "full_like");
}

void FullLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float value, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build FullLikeOp";


  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateLikeInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FullLikeOp";


  IR_ENFORCE(
      attributes.find("value") != attributes.end(),
          "'value' Attribute is expected for FullLikeOp. ");
  float value = attributes.at("value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for FullLikeOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for FullLikeOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  // Generate scalar mutable attribute: value
  paddle::dialect::FullOp full_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult value_ = full_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateLikeInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value value_, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build FullLikeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar value;
  if (value_.dyn_cast<pir::OpResult>() && value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    value = std::move(phi::Scalar(value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    value = std::move(phi::Scalar(-1));
    value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CreateLikeInferMeta(meta_x, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullLikeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FullLikeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FullLikeOp.";
}

void FullLikeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateLikeInferMeta);
  fn(infer_meta);
}

phi::DataType FullLikeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FullLikeOp";
  

  // deal skip data transform
  if (var_name == "x"){
    return expected_kernel_dtype;
  }


  return expected_kernel_dtype;
}

const char *FullWithTensorOp::attributes_name[1] = { "dtype" };

OpInfoTuple FullWithTensorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("shape", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("value", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FullWithTensorInferMeta", {"shape", "dtype"}, "full_with_tensor", {"shape", "value", "dtype"}, {"dtype"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "full_with_tensor");
}

void FullWithTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shape_, pir::Value value_, phi::DataType dtype) {
  VLOG(4) << "Start build FullWithTensorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType shape = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)shape;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_shape";
  paddle::dialect::IrTensor ir_tensor_shape(paddle::dialect::TransToPhiDataType(shape.dtype()),
                                                      shape.dims(),
                                                      shape.data_layout(),
                                                      shape.lod(),
                                                      shape.offset());
  VLOG(4) << "Builder construction  meta_shape";
  paddle::dialect::IrMetaTensor meta_shape(&ir_tensor_shape);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FullWithTensorInferMeta(meta_shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullWithTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shape_, pir::Value value_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FullWithTensorOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for FullWithTensorOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_, value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType shape = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)shape;
  paddle::dialect::DenseTensorType value = value_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)value;

  VLOG(4) << "Builder construction  dense_shape";
  paddle::dialect::IrTensor ir_tensor_shape(paddle::dialect::TransToPhiDataType(shape.dtype()),
                                                      shape.dims(),
                                                      shape.data_layout(),
                                                      shape.lod(),
                                                      shape.offset());
  VLOG(4) << "Builder construction  meta_shape";
  paddle::dialect::IrMetaTensor meta_shape(&ir_tensor_shape);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FullWithTensorInferMeta(meta_shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FullWithTensorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FullWithTensorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FullWithTensorOp.";
}

void FullWithTensorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FullWithTensorInferMeta);
  fn(infer_meta);
}

phi::DataType FullWithTensorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FullWithTensorOp";
  


  return expected_kernel_dtype;
}

const char *FusedAdam_Op::attributes_name[8] = { "beta1", "beta2", "epsilon", "chunk_size", "weight_decay", "use_adamw", "multi_precision", "use_global_beta_pow" };

OpInfoTuple FusedAdam_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("params", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("grads", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("moments1", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("moments2", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("beta1_pows", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("beta2_pows", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("master_params", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false), paddle::dialect::OpInputInfo("skip_update", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("beta1", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("beta2", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("epsilon", "paddle::dialect::ScalarAttribute", "float"), paddle::dialect::OpAttributeInfo("chunk_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("weight_decay", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_adamw", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_beta_pow", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("params_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("moments1_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("moments2_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("beta1_pows_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("beta2_pows_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("master_params_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedAdamInferMeta", {"params", "grads", "learning_rate", "moments1", "moments2", "beta1_pows", "beta2_pows", "master_params", "skip_update", "beta1", "beta2", "epsilon", "chunk_size", "weight_decay", "use_adamw", "multi_precision", "use_global_beta_pow"}, "fused_adam", {"params", "grads", "learning_rate", "moments1", "moments2", "beta1_pows", "beta2_pows", "master_params", "skip_update", "beta1", "beta2", "epsilon", "chunk_size", "weight_decay", "use_adamw", "multi_precision", "use_global_beta_pow"}, {"params"}, {}, {{"params_out", "params"},{"moments1_out", "moments1"},{"moments2_out", "moments2"},{"beta1_pows_out", "beta1_pows"},{"beta2_pows_out", "beta2_pows"},{"master_params_out", "master_params"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_adam_");
}

void FusedAdam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value params_, pir::Value grads_, pir::Value learning_rate_, pir::Value moments1_, pir::Value moments2_, pir::Value beta1_pows_, pir::Value beta2_pows_, pir::Value master_params_, pir::Value skip_update_, float beta1, float beta2, float epsilon, int chunk_size, float weight_decay, bool use_adamw, bool multi_precision, bool use_global_beta_pow) {
  VLOG(4) << "Start build FusedAdam_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {params_, grads_, learning_rate_, moments1_, moments2_, beta1_pows_, beta2_pows_, master_params_, skip_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta1 = paddle::dialect::TransToIrAttribute(beta1, pir::IrContext::Instance());
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = paddle::dialect::TransToIrAttribute(beta2, pir::IrContext::Instance());
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = paddle::dialect::TransToIrAttribute(epsilon, pir::IrContext::Instance());
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_chunk_size = pir::Int32Attribute::get(pir::IrContext::Instance(), chunk_size);
  argument.AddAttribute("chunk_size", attr_chunk_size);
  pir::Attribute attr_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), weight_decay);
  argument.AddAttribute("weight_decay", attr_weight_decay);
  pir::Attribute attr_use_adamw = pir::BoolAttribute::get(pir::IrContext::Instance(), use_adamw);
  argument.AddAttribute("use_adamw", attr_use_adamw);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType params = params_.type().dyn_cast<pir::VectorType>(); (void)params;
  pir::VectorType grads = grads_.type().dyn_cast<pir::VectorType>(); (void)grads;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  pir::VectorType moments1 = moments1_.type().dyn_cast<pir::VectorType>(); (void)moments1;
  pir::VectorType moments2 = moments2_.type().dyn_cast<pir::VectorType>(); (void)moments2;
  pir::VectorType beta1_pows = beta1_pows_.type().dyn_cast<pir::VectorType>(); (void)beta1_pows;
  pir::VectorType beta2_pows = beta2_pows_.type().dyn_cast<pir::VectorType>(); (void)beta2_pows;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_params;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_ir_tensor_params.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(params[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_params;
  for (size_t i=0; i < vec_ir_tensor_params.size(); i++) {
    vec_meta_params.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_params[i]));
  }

  std::vector<const phi::MetaTensor*> meta_params;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_params.size()); i++) {
    meta_params.push_back(&vec_meta_params[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grads;
  for (size_t i=0; i < static_cast<size_t>(grads.size()); i++) {
    vec_ir_tensor_grads.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grads[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grads;
  for (size_t i=0; i < vec_ir_tensor_grads.size(); i++) {
    vec_meta_grads.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grads[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grads;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grads.size()); i++) {
    meta_grads.push_back(&vec_meta_grads[i]);
  }
 
  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moments1;
  for (size_t i=0; i < static_cast<size_t>(moments1.size()); i++) {
    vec_ir_tensor_moments1.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments1;
  for (size_t i=0; i < vec_ir_tensor_moments1.size(); i++) {
    vec_meta_moments1.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moments1[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moments1;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments1.size()); i++) {
    meta_moments1.push_back(&vec_meta_moments1[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moments2;
  for (size_t i=0; i < static_cast<size_t>(moments2.size()); i++) {
    vec_ir_tensor_moments2.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments2;
  for (size_t i=0; i < vec_ir_tensor_moments2.size(); i++) {
    vec_meta_moments2.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moments2[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moments2;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments2.size()); i++) {
    meta_moments2.push_back(&vec_meta_moments2[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta1_pows;
  for (size_t i=0; i < static_cast<size_t>(beta1_pows.size()); i++) {
    vec_ir_tensor_beta1_pows.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pows;
  for (size_t i=0; i < vec_ir_tensor_beta1_pows.size(); i++) {
    vec_meta_beta1_pows.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta1_pows[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta1_pows;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pows.size()); i++) {
    meta_beta1_pows.push_back(&vec_meta_beta1_pows[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta2_pows;
  for (size_t i=0; i < static_cast<size_t>(beta2_pows.size()); i++) {
    vec_ir_tensor_beta2_pows.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pows;
  for (size_t i=0; i < vec_ir_tensor_beta2_pows.size(); i++) {
    vec_meta_beta2_pows.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta2_pows[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta2_pows;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pows.size()); i++) {
    meta_beta2_pows.push_back(&vec_meta_beta2_pows[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_params;
  if (master_params_.impl() != nullptr) {
    pir::VectorType master_params = master_params_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_params.size()); i++) {
        vec_ir_tensor_master_params.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_params;
  for (size_t i=0; i < vec_ir_tensor_master_params.size(); i++) {
    vec_meta_master_params.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_params[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_params;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_params.size()); i++) {
    meta_master_params.push_back(&vec_meta_master_params[i]);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_params_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_params_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_params_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_params_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_params_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_params_out.size()); i++) {
    meta_params_out.push_back(&vec_meta_params_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moments1_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments1_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_moments1_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moments1_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moments1_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments1_out.size()); i++) {
    meta_moments1_out.push_back(&vec_meta_moments1_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moments2_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments2_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_moments2_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moments2_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moments2_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments2_out.size()); i++) {
    meta_moments2_out.push_back(&vec_meta_moments2_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta1_pows_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pows_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_beta1_pows_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta1_pows_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta1_pows_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pows_out.size()); i++) {
    meta_beta1_pows_out.push_back(&vec_meta_beta1_pows_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta2_pows_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pows_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_beta2_pows_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta2_pows_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta2_pows_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pows_out.size()); i++) {
    meta_beta2_pows_out.push_back(&vec_meta_beta2_pows_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_params_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_params_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_master_params_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_params_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_params_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_params_out.size()); i++) {
    meta_master_params_out.push_back(&vec_meta_master_params_out[i]);
  }

  phi::FusedAdamInferMeta(meta_params, meta_grads, meta_learning_rate, meta_moments1, meta_moments2, meta_beta1_pows, meta_beta2_pows, meta_master_params, meta_skip_update, beta1, beta2, epsilon, chunk_size, weight_decay, use_adamw, multi_precision, use_global_beta_pow, meta_params_out, meta_moments1_out, meta_moments2_out, meta_beta1_pows_out, meta_beta2_pows_out, meta_master_params_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> params_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    params_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_params_out[i].dtype()), vec_dense_params_out[i].dims(), vec_dense_params_out[i].layout(), vec_dense_params_out[i].lod(), vec_dense_params_out[i].offset()));
  }
  pir::Type params_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), params_out_types);
  argument_outputs.push_back(params_out_vector_type);

  std::vector<pir::Type> moments1_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    moments1_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moments1_out[i].dtype()), vec_dense_moments1_out[i].dims(), vec_dense_moments1_out[i].layout(), vec_dense_moments1_out[i].lod(), vec_dense_moments1_out[i].offset()));
  }
  pir::Type moments1_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moments1_out_types);
  argument_outputs.push_back(moments1_out_vector_type);

  std::vector<pir::Type> moments2_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    moments2_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moments2_out[i].dtype()), vec_dense_moments2_out[i].dims(), vec_dense_moments2_out[i].layout(), vec_dense_moments2_out[i].lod(), vec_dense_moments2_out[i].offset()));
  }
  pir::Type moments2_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moments2_out_types);
  argument_outputs.push_back(moments2_out_vector_type);

  std::vector<pir::Type> beta1_pows_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    beta1_pows_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta1_pows_out[i].dtype()), vec_dense_beta1_pows_out[i].dims(), vec_dense_beta1_pows_out[i].layout(), vec_dense_beta1_pows_out[i].lod(), vec_dense_beta1_pows_out[i].offset()));
  }
  pir::Type beta1_pows_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta1_pows_out_types);
  argument_outputs.push_back(beta1_pows_out_vector_type);

  std::vector<pir::Type> beta2_pows_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    beta2_pows_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta2_pows_out[i].dtype()), vec_dense_beta2_pows_out[i].dims(), vec_dense_beta2_pows_out[i].layout(), vec_dense_beta2_pows_out[i].lod(), vec_dense_beta2_pows_out[i].offset()));
  }
  pir::Type beta2_pows_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta2_pows_out_types);
  argument_outputs.push_back(beta2_pows_out_vector_type);

  std::vector<pir::Type> master_params_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    master_params_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_params_out[i].dtype()), vec_dense_master_params_out[i].dims(), vec_dense_master_params_out[i].layout(), vec_dense_master_params_out[i].lod(), vec_dense_master_params_out[i].offset()));
  }
  pir::Type master_params_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_params_out_types);
  argument_outputs.push_back(master_params_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedAdam_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value params_, pir::Value grads_, pir::Value learning_rate_, pir::Value moments1_, pir::Value moments2_, pir::Value beta1_pows_, pir::Value beta2_pows_, pir::Value master_params_, pir::Value skip_update_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedAdam_Op";


  IR_ENFORCE(
      attributes.find("beta1") != attributes.end(),
          "'beta1' Attribute is expected for FusedAdam_Op. ");
  float beta1 = attributes.at("beta1").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("beta2") != attributes.end(),
          "'beta2' Attribute is expected for FusedAdam_Op. ");
  float beta2 = attributes.at("beta2").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedAdam_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("chunk_size") != attributes.end(),
          "'chunk_size' Attribute is expected for FusedAdam_Op. ");
  int chunk_size = attributes.at("chunk_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("weight_decay") != attributes.end(),
          "'weight_decay' Attribute is expected for FusedAdam_Op. ");
  float weight_decay = attributes.at("weight_decay").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_adamw") != attributes.end(),
          "'use_adamw' Attribute is expected for FusedAdam_Op. ");
  bool use_adamw = attributes.at("use_adamw").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for FusedAdam_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_global_beta_pow") != attributes.end(),
          "'use_global_beta_pow' Attribute is expected for FusedAdam_Op. ");
  bool use_global_beta_pow = attributes.at("use_global_beta_pow").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {params_, grads_, learning_rate_, moments1_, moments2_, beta1_pows_, beta2_pows_, master_params_, skip_update_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_beta1 = paddle::dialect::TransToIrAttribute(beta1, pir::IrContext::Instance());
  argument.AddAttribute("beta1", attr_beta1);
  pir::Attribute attr_beta2 = paddle::dialect::TransToIrAttribute(beta2, pir::IrContext::Instance());
  argument.AddAttribute("beta2", attr_beta2);
  pir::Attribute attr_epsilon = paddle::dialect::TransToIrAttribute(epsilon, pir::IrContext::Instance());
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_chunk_size = pir::Int32Attribute::get(pir::IrContext::Instance(), chunk_size);
  argument.AddAttribute("chunk_size", attr_chunk_size);
  pir::Attribute attr_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), weight_decay);
  argument.AddAttribute("weight_decay", attr_weight_decay);
  pir::Attribute attr_use_adamw = pir::BoolAttribute::get(pir::IrContext::Instance(), use_adamw);
  argument.AddAttribute("use_adamw", attr_use_adamw);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_use_global_beta_pow = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_beta_pow);
  argument.AddAttribute("use_global_beta_pow", attr_use_global_beta_pow);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType params = params_.type().dyn_cast<pir::VectorType>(); (void)params;
  pir::VectorType grads = grads_.type().dyn_cast<pir::VectorType>(); (void)grads;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  pir::VectorType moments1 = moments1_.type().dyn_cast<pir::VectorType>(); (void)moments1;
  pir::VectorType moments2 = moments2_.type().dyn_cast<pir::VectorType>(); (void)moments2;
  pir::VectorType beta1_pows = beta1_pows_.type().dyn_cast<pir::VectorType>(); (void)beta1_pows;
  pir::VectorType beta2_pows = beta2_pows_.type().dyn_cast<pir::VectorType>(); (void)beta2_pows;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_params;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_ir_tensor_params.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(params[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     params[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_params;
  for (size_t i=0; i < vec_ir_tensor_params.size(); i++) {
    vec_meta_params.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_params[i]));
  }

  std::vector<const phi::MetaTensor*> meta_params;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_params.size()); i++) {
    meta_params.push_back(&vec_meta_params[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grads;
  for (size_t i=0; i < static_cast<size_t>(grads.size()); i++) {
    vec_ir_tensor_grads.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grads[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grads[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grads;
  for (size_t i=0; i < vec_ir_tensor_grads.size(); i++) {
    vec_meta_grads.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grads[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grads;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grads.size()); i++) {
    meta_grads.push_back(&vec_meta_grads[i]);
  }
 
  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moments1;
  for (size_t i=0; i < static_cast<size_t>(moments1.size()); i++) {
    vec_ir_tensor_moments1.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moments1[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments1;
  for (size_t i=0; i < vec_ir_tensor_moments1.size(); i++) {
    vec_meta_moments1.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moments1[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moments1;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments1.size()); i++) {
    meta_moments1.push_back(&vec_meta_moments1[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_moments2;
  for (size_t i=0; i < static_cast<size_t>(moments2.size()); i++) {
    vec_ir_tensor_moments2.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     moments2[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments2;
  for (size_t i=0; i < vec_ir_tensor_moments2.size(); i++) {
    vec_meta_moments2.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_moments2[i]));
  }

  std::vector<const phi::MetaTensor*> meta_moments2;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments2.size()); i++) {
    meta_moments2.push_back(&vec_meta_moments2[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta1_pows;
  for (size_t i=0; i < static_cast<size_t>(beta1_pows.size()); i++) {
    vec_ir_tensor_beta1_pows.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta1_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pows;
  for (size_t i=0; i < vec_ir_tensor_beta1_pows.size(); i++) {
    vec_meta_beta1_pows.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta1_pows[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta1_pows;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pows.size()); i++) {
    meta_beta1_pows.push_back(&vec_meta_beta1_pows[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_beta2_pows;
  for (size_t i=0; i < static_cast<size_t>(beta2_pows.size()); i++) {
    vec_ir_tensor_beta2_pows.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     beta2_pows[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pows;
  for (size_t i=0; i < vec_ir_tensor_beta2_pows.size(); i++) {
    vec_meta_beta2_pows.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_beta2_pows[i]));
  }

  std::vector<const phi::MetaTensor*> meta_beta2_pows;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pows.size()); i++) {
    meta_beta2_pows.push_back(&vec_meta_beta2_pows[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_params;
  if (master_params_.impl() != nullptr) {
    pir::VectorType master_params = master_params_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_params.size()); i++) {
        vec_ir_tensor_master_params.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_params[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_params;
  for (size_t i=0; i < vec_ir_tensor_master_params.size(); i++) {
    vec_meta_master_params.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_params[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_params;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_params.size()); i++) {
    meta_master_params.push_back(&vec_meta_master_params[i]);
  }


  paddle::dialect::IrMetaTensor meta_skip_update;
  paddle::dialect::IrTensor ir_tensor_skip_update;
  if (skip_update_.impl() != nullptr) {
    paddle::dialect::DenseTensorType skip_update = skip_update_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_skip_update";
    ir_tensor_skip_update = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(skip_update.dtype()),
                                                        skip_update.dims(),
                                                        skip_update.data_layout(),
                                                        skip_update.lod(),
                                                        skip_update.offset());
    VLOG(4) << "Builder construction  meta_skip_update";
    meta_skip_update = paddle::dialect::IrMetaTensor(&ir_tensor_skip_update);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_params_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_params_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_params_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_params_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_params_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_params_out.size()); i++) {
    meta_params_out.push_back(&vec_meta_params_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moments1_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments1_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_moments1_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moments1_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moments1_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments1_out.size()); i++) {
    meta_moments1_out.push_back(&vec_meta_moments1_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_moments2_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_moments2_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_moments2_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_moments2_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_moments2_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_moments2_out.size()); i++) {
    meta_moments2_out.push_back(&vec_meta_moments2_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta1_pows_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta1_pows_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_beta1_pows_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta1_pows_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta1_pows_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta1_pows_out.size()); i++) {
    meta_beta1_pows_out.push_back(&vec_meta_beta1_pows_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_beta2_pows_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_beta2_pows_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_beta2_pows_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_beta2_pows_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_beta2_pows_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_beta2_pows_out.size()); i++) {
    meta_beta2_pows_out.push_back(&vec_meta_beta2_pows_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_params_out((params.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_params_out;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    vec_meta_master_params_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_params_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_params_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_params_out.size()); i++) {
    meta_master_params_out.push_back(&vec_meta_master_params_out[i]);
  }

  phi::FusedAdamInferMeta(meta_params, meta_grads, meta_learning_rate, meta_moments1, meta_moments2, meta_beta1_pows, meta_beta2_pows, meta_master_params, meta_skip_update, beta1, beta2, epsilon, chunk_size, weight_decay, use_adamw, multi_precision, use_global_beta_pow, meta_params_out, meta_moments1_out, meta_moments2_out, meta_beta1_pows_out, meta_beta2_pows_out, meta_master_params_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> params_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    params_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_params_out[i].dtype()), vec_dense_params_out[i].dims(), vec_dense_params_out[i].layout(), vec_dense_params_out[i].lod(), vec_dense_params_out[i].offset()));
  }
  pir::Type params_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), params_out_types);
  argument_outputs.push_back(params_out_vector_type);

  std::vector<pir::Type> moments1_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    moments1_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moments1_out[i].dtype()), vec_dense_moments1_out[i].dims(), vec_dense_moments1_out[i].layout(), vec_dense_moments1_out[i].lod(), vec_dense_moments1_out[i].offset()));
  }
  pir::Type moments1_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moments1_out_types);
  argument_outputs.push_back(moments1_out_vector_type);

  std::vector<pir::Type> moments2_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    moments2_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_moments2_out[i].dtype()), vec_dense_moments2_out[i].dims(), vec_dense_moments2_out[i].layout(), vec_dense_moments2_out[i].lod(), vec_dense_moments2_out[i].offset()));
  }
  pir::Type moments2_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), moments2_out_types);
  argument_outputs.push_back(moments2_out_vector_type);

  std::vector<pir::Type> beta1_pows_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    beta1_pows_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta1_pows_out[i].dtype()), vec_dense_beta1_pows_out[i].dims(), vec_dense_beta1_pows_out[i].layout(), vec_dense_beta1_pows_out[i].lod(), vec_dense_beta1_pows_out[i].offset()));
  }
  pir::Type beta1_pows_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta1_pows_out_types);
  argument_outputs.push_back(beta1_pows_out_vector_type);

  std::vector<pir::Type> beta2_pows_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    beta2_pows_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_beta2_pows_out[i].dtype()), vec_dense_beta2_pows_out[i].dims(), vec_dense_beta2_pows_out[i].layout(), vec_dense_beta2_pows_out[i].lod(), vec_dense_beta2_pows_out[i].offset()));
  }
  pir::Type beta2_pows_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), beta2_pows_out_types);
  argument_outputs.push_back(beta2_pows_out_vector_type);

  std::vector<pir::Type> master_params_out_types;
  for (size_t i=0; i < static_cast<size_t>(params.size()); i++) {
    master_params_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_params_out[i].dtype()), vec_dense_master_params_out[i].dims(), vec_dense_master_params_out[i].layout(), vec_dense_master_params_out[i].lod(), vec_dense_master_params_out[i].offset()));
  }
  pir::Type master_params_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_params_out_types);
  argument_outputs.push_back(master_params_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedAdam_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedAdam_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 9u,
                    "The size %d of inputs must be equal to 9.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto vec_type = (*this)->operand_source(5).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto vec_type = (*this)->operand_source(6).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val =  (*this)->operand(7)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
    }
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("beta1")>0,
                 "beta1 does not exist.");
  IR_ENFORCE(attributes.at("beta1").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: beta1 is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("beta2")>0,
                 "beta2 does not exist.");
  IR_ENFORCE(attributes.at("beta2").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: beta2 is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<paddle::dialect::ScalarAttribute>(),
                 "Type of attribute: epsilon is not paddle::dialect::ScalarAttribute.");

  IR_ENFORCE(attributes.count("chunk_size")>0,
                 "chunk_size does not exist.");
  IR_ENFORCE(attributes.at("chunk_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: chunk_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("weight_decay")>0,
                 "weight_decay does not exist.");
  IR_ENFORCE(attributes.at("weight_decay").isa<pir::FloatAttribute>(),
                 "Type of attribute: weight_decay is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_adamw")>0,
                 "use_adamw does not exist.");
  IR_ENFORCE(attributes.at("use_adamw").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_adamw is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("use_global_beta_pow")>0,
                 "use_global_beta_pow does not exist.");
  IR_ENFORCE(attributes.at("use_global_beta_pow").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_beta_pow is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  auto output_2_type = (*this)->result(2).type();
  if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  else {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  auto output_3_type = (*this)->result(3).type();
  if (auto vec_type = output_3_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 3th output.");
    }
  }
  else {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  auto output_4_type = (*this)->result(4).type();
  if (auto vec_type = output_4_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 4th output.");
    }
  }
  else {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  auto output_5_type = (*this)->result(5).type();
  if (auto vec_type = output_5_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 5th output.");
    }
  }
  else {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedAdam_Op.";
}

void FusedAdam_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedAdamInferMeta);
  fn(infer_meta);
}

phi::DataType FusedAdam_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedAdam_Op";
  


  return expected_kernel_dtype;
}

const char *FusedBatchNormActOp::attributes_name[3] = { "momentum", "epsilon", "act_type" };

OpInfoTuple FusedBatchNormActOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBatchNormActInferMeta", {"x", "scale", "bias", "mean", "variance"}, "fused_batch_norm_act", {"x", "scale", "bias", "mean", "variance", "momentum", "epsilon", "act_type"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_batch_norm_act");
}

void FusedBatchNormActOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, float momentum, float epsilon, const std::string& act_type) {
  VLOG(4) << "Start build FusedBatchNormActOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBatchNormActOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBatchNormActOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedBatchNormActOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBatchNormActOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FusedBatchNormActOp. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBatchNormActOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBatchNormActOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::StrAttribute>(),
                 "Type of attribute: act_type is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  }
  VLOG(4) << "End Verifying for: FusedBatchNormActOp.";
}

void FusedBatchNormActOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBatchNormActInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBatchNormActOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBatchNormActOp";
  


  return expected_kernel_dtype;
}

const char *FusedBatchNormAct_Op::attributes_name[3] = { "momentum", "epsilon", "act_type" };

OpInfoTuple FusedBatchNormAct_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBatchNormActInferMeta", {"x", "scale", "bias", "mean", "variance"}, "fused_batch_norm_act", {"x", "scale", "bias", "mean", "variance", "momentum", "epsilon", "act_type"}, {"x"}, {}, {}, {{"mean_out", "mean"},{"variance_out", "variance"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_batch_norm_act");
}

void FusedBatchNormAct_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, float momentum, float epsilon, const std::string& act_type) {
  VLOG(4) << "Start build FusedBatchNormAct_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBatchNormAct_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBatchNormAct_Op";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedBatchNormAct_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBatchNormAct_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FusedBatchNormAct_Op. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBatchNormAct_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBatchNormAct_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::StrAttribute>(),
                 "Type of attribute: act_type is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  }
  VLOG(4) << "End Verifying for: FusedBatchNormAct_Op.";
}

void FusedBatchNormAct_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBatchNormActInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBatchNormAct_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBatchNormAct_Op";
  


  return expected_kernel_dtype;
}

const char *FusedBnAddActivationOp::attributes_name[3] = { "momentum", "epsilon", "act_type" };

OpInfoTuple FusedBnAddActivationOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("z", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBatchNormActInferMeta", {"x", "scale", "bias", "mean", "variance"}, "fused_bn_add_activation", {"x", "z", "scale", "bias", "mean", "variance", "momentum", "epsilon", "act_type"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bn_add_activation");
}

void FusedBnAddActivationOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value z_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, float momentum, float epsilon, const std::string& act_type) {
  VLOG(4) << "Start build FusedBnAddActivationOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, z_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType z = z_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)z;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBnAddActivationOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value z_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBnAddActivationOp";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedBnAddActivationOp. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBnAddActivationOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FusedBnAddActivationOp. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, z_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType z = z_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)z;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBnAddActivationOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBnAddActivationOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::StrAttribute>(),
                 "Type of attribute: act_type is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  }
  VLOG(4) << "End Verifying for: FusedBnAddActivationOp.";
}

void FusedBnAddActivationOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBatchNormActInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBnAddActivationOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBnAddActivationOp";
  


  return expected_kernel_dtype;
}

const char *FusedBnAddActivation_Op::attributes_name[3] = { "momentum", "epsilon", "act_type" };

OpInfoTuple FusedBnAddActivation_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("z", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_type", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedBatchNormActInferMeta", {"x", "scale", "bias", "mean", "variance"}, "fused_bn_add_activation", {"x", "z", "scale", "bias", "mean", "variance", "momentum", "epsilon", "act_type"}, {"x"}, {}, {}, {{"mean_out", "mean"},{"variance_out", "variance"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_bn_add_activation");
}

void FusedBnAddActivation_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value z_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, float momentum, float epsilon, const std::string& act_type) {
  VLOG(4) << "Start build FusedBnAddActivation_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, z_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType z = z_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)z;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBnAddActivation_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value z_, pir::Value scale_, pir::Value bias_, pir::Value mean_, pir::Value variance_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedBnAddActivation_Op";


  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for FusedBnAddActivation_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedBnAddActivation_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_type") != attributes.end(),
          "'act_type' Attribute is expected for FusedBnAddActivation_Op. ");
  std::string act_type = attributes.at("act_type").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, z_, scale_, bias_, mean_, variance_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_act_type = pir::StrAttribute::get(pir::IrContext::Instance(), act_type);
  argument.AddAttribute("act_type", attr_act_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType z = z_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)z;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::FusedBatchNormActInferMeta(meta_x, meta_scale, meta_bias, meta_mean, meta_variance, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedBnAddActivation_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedBnAddActivation_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_type")>0,
                 "act_type does not exist.");
  IR_ENFORCE(attributes.at("act_type").isa<pir::StrAttribute>(),
                 "Type of attribute: act_type is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  }
  VLOG(4) << "End Verifying for: FusedBnAddActivation_Op.";
}

void FusedBnAddActivation_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedBatchNormActInferMeta);
  fn(infer_meta);
}

phi::DataType FusedBnAddActivation_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedBnAddActivation_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple FusedSoftmaxMaskUpperTriangleOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("X", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("Out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"X"}, "fused_softmax_mask_upper_triangle", {"X"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_softmax_mask_upper_triangle");
}

void FusedSoftmaxMaskUpperTriangleOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value X_) {
  VLOG(4) << "Start build FusedSoftmaxMaskUpperTriangleOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {X_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType X = X_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)X;

  VLOG(4) << "Builder construction  dense_X";
  paddle::dialect::IrTensor ir_tensor_X(paddle::dialect::TransToPhiDataType(X.dtype()),
                                                      X.dims(),
                                                      X.data_layout(),
                                                      X.lod(),
                                                      X.offset());
  VLOG(4) << "Builder construction  meta_X";
  paddle::dialect::IrMetaTensor meta_X(&ir_tensor_X);
  paddle::dialect::IrTensor dense_Out;
  paddle::dialect::IrMetaTensor meta_Out(&dense_Out);

  phi::UnchangedInferMeta(meta_X, &meta_Out);

  std::vector<pir::Type> argument_outputs;
  pir::Type Out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_Out.dtype()), dense_Out.dims(), dense_Out.layout(), dense_Out.lod(), dense_Out.offset());
  argument_outputs.push_back(Out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedSoftmaxMaskUpperTriangleOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedSoftmaxMaskUpperTriangleOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: FusedSoftmaxMaskUpperTriangleOp.";
}

void FusedSoftmaxMaskUpperTriangleOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType FusedSoftmaxMaskUpperTriangleOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedSoftmaxMaskUpperTriangleOp";
  


  return expected_kernel_dtype;
}

const char *GaussianOp::attributes_name[5] = { "mean", "std", "seed", "dtype", "place" };

OpInfoTuple GaussianOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mean", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("std", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("GaussianInferMeta", {"shape", "mean", "std", "seed", "dtype"}, "gaussian", {"shape", "mean", "std", "seed", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "gaussian");
}

void GaussianOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& shape, float mean, float std, int seed, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build GaussianOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GaussianInferMeta(shape, mean, std, seed, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build GaussianOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for GaussianOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("mean") != attributes.end(),
          "'mean' Attribute is expected for GaussianOp. ");
  float mean = attributes.at("mean").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("std") != attributes.end(),
          "'std' Attribute is expected for GaussianOp. ");
  float std = attributes.at("std").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for GaussianOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for GaussianOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for GaussianOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GaussianInferMeta(shape, mean, std, seed, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shape_, float mean, float std, int seed, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build GaussianOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::GaussianInferMeta(shape, mean, std, seed, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GaussianOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GaussianOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mean")>0,
                 "mean does not exist.");
  IR_ENFORCE(attributes.at("mean").isa<pir::FloatAttribute>(),
                 "Type of attribute: mean is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("std")>0,
                 "std does not exist.");
  IR_ENFORCE(attributes.at("std").isa<pir::FloatAttribute>(),
                 "Type of attribute: std is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GaussianOp.";
}

void GaussianOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::GaussianInferMeta);
  fn(infer_meta);
}

phi::DataType GaussianOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GaussianOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GetTensorFromSelectedRowsOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "get_tensor_from_selected_rows", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "get_tensor_from_selected_rows");
}

void GetTensorFromSelectedRowsOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build GetTensorFromSelectedRowsOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GetTensorFromSelectedRowsOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GetTensorFromSelectedRowsOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GetTensorFromSelectedRowsOp.";
}

void GetTensorFromSelectedRowsOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType GetTensorFromSelectedRowsOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GetTensorFromSelectedRowsOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GreaterEqualOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "greater_equal", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "greater_equal");
}

void GreaterEqualOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build GreaterEqualOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GreaterEqualOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GreaterEqualOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GreaterEqualOp.";
}

void GreaterEqualOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType GreaterEqualOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GreaterEqualOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GreaterEqual_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "greater_equal", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "greater_equal");
}

void GreaterEqual_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build GreaterEqual_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GreaterEqual_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GreaterEqual_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GreaterEqual_Op.";
}

void GreaterEqual_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType GreaterEqual_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GreaterEqual_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple GreaterThanOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "greater_than", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "greater_than");
}

void GreaterThanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build GreaterThanOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GreaterThanOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GreaterThanOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GreaterThanOp.";
}

void GreaterThanOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType GreaterThanOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GreaterThanOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple GreaterThan_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "greater_than", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "greater_than");
}

void GreaterThan_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build GreaterThan_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void GreaterThan_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: GreaterThan_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: GreaterThan_Op.";
}

void GreaterThan_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType GreaterThan_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: GreaterThan_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple HardswishOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "hardswish", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hardswish");
}

void HardswishOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build HardswishOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HardswishOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HardswishOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: HardswishOp.";
}

void HardswishOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType HardswishOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HardswishOp";
  


  return expected_kernel_dtype;
}

const char *HsigmoidLossOp::attributes_name[2] = { "num_classes", "is_sparse" };

OpInfoTuple HsigmoidLossOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("path", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("code", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_classes", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_sparse", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("pre_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("w_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("HSigmoidLossInferMeta", {"x", "label", "w", "bias", "path", "code", "num_classes", "is_sparse"}, "hsigmoid_loss", {"x", "label", "w", "bias", "path", "code", "num_classes", "is_sparse"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "hsigmoid_loss");
}

void HsigmoidLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value w_, pir::Value bias_, pir::Value path_, pir::Value code_, int num_classes, bool is_sparse) {
  VLOG(4) << "Start build HsigmoidLossOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, w_, bias_, path_, code_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_classes);
  argument.AddAttribute("num_classes", attr_num_classes);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_path;
  paddle::dialect::IrTensor ir_tensor_path;
  if (path_.impl() != nullptr) {
    paddle::dialect::DenseTensorType path = path_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_path";
    ir_tensor_path = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(path.dtype()),
                                                        path.dims(),
                                                        path.data_layout(),
                                                        path.lod(),
                                                        path.offset());
    VLOG(4) << "Builder construction  meta_path";
    meta_path = paddle::dialect::IrMetaTensor(&ir_tensor_path);
  }


  paddle::dialect::IrMetaTensor meta_code;
  paddle::dialect::IrTensor ir_tensor_code;
  if (code_.impl() != nullptr) {
    paddle::dialect::DenseTensorType code = code_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_code";
    ir_tensor_code = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(code.dtype()),
                                                        code.dims(),
                                                        code.data_layout(),
                                                        code.lod(),
                                                        code.offset());
    VLOG(4) << "Builder construction  meta_code";
    meta_code = paddle::dialect::IrMetaTensor(&ir_tensor_code);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_pre_out;
  paddle::dialect::IrMetaTensor meta_pre_out(&dense_pre_out);
  paddle::dialect::IrTensor dense_w_out;
  paddle::dialect::IrMetaTensor meta_w_out(&dense_w_out);

  phi::HSigmoidLossInferMeta(meta_x, meta_label, meta_w, meta_bias, meta_path, meta_code, num_classes, is_sparse, &meta_out, &meta_pre_out, &meta_w_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type pre_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pre_out.dtype()), dense_pre_out.dims(), dense_pre_out.layout(), dense_pre_out.lod(), dense_pre_out.offset());
  argument_outputs.push_back(pre_out_dense_tensor_type);

  pir::Type w_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_w_out.dtype()), dense_w_out.dims(), dense_w_out.layout(), dense_w_out.lod(), dense_w_out.offset());
  argument_outputs.push_back(w_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HsigmoidLossOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value label_, pir::Value w_, pir::Value bias_, pir::Value path_, pir::Value code_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build HsigmoidLossOp";


  IR_ENFORCE(
      attributes.find("num_classes") != attributes.end(),
          "'num_classes' Attribute is expected for HsigmoidLossOp. ");
  int num_classes = attributes.at("num_classes").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_sparse") != attributes.end(),
          "'is_sparse' Attribute is expected for HsigmoidLossOp. ");
  bool is_sparse = attributes.at("is_sparse").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, label_, w_, bias_, path_, code_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_classes);
  argument.AddAttribute("num_classes", attr_num_classes);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_path;
  paddle::dialect::IrTensor ir_tensor_path;
  if (path_.impl() != nullptr) {
    paddle::dialect::DenseTensorType path = path_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_path";
    ir_tensor_path = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(path.dtype()),
                                                        path.dims(),
                                                        path.data_layout(),
                                                        path.lod(),
                                                        path.offset());
    VLOG(4) << "Builder construction  meta_path";
    meta_path = paddle::dialect::IrMetaTensor(&ir_tensor_path);
  }


  paddle::dialect::IrMetaTensor meta_code;
  paddle::dialect::IrTensor ir_tensor_code;
  if (code_.impl() != nullptr) {
    paddle::dialect::DenseTensorType code = code_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_code";
    ir_tensor_code = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(code.dtype()),
                                                        code.dims(),
                                                        code.data_layout(),
                                                        code.lod(),
                                                        code.offset());
    VLOG(4) << "Builder construction  meta_code";
    meta_code = paddle::dialect::IrMetaTensor(&ir_tensor_code);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_pre_out;
  paddle::dialect::IrMetaTensor meta_pre_out(&dense_pre_out);
  paddle::dialect::IrTensor dense_w_out;
  paddle::dialect::IrMetaTensor meta_w_out(&dense_w_out);

  phi::HSigmoidLossInferMeta(meta_x, meta_label, meta_w, meta_bias, meta_path, meta_code, num_classes, is_sparse, &meta_out, &meta_pre_out, &meta_w_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type pre_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_pre_out.dtype()), dense_pre_out.dims(), dense_pre_out.layout(), dense_pre_out.lod(), dense_pre_out.offset());
  argument_outputs.push_back(pre_out_dense_tensor_type);

  pir::Type w_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_w_out.dtype()), dense_w_out.dims(), dense_w_out.layout(), dense_w_out.lod(), dense_w_out.offset());
  argument_outputs.push_back(w_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void HsigmoidLossOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: HsigmoidLossOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 6u,
                    "The size %d of inputs must be equal to 6.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("num_classes")>0,
                 "num_classes does not exist.");
  IR_ENFORCE(attributes.at("num_classes").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_classes is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("is_sparse")>0,
                 "is_sparse does not exist.");
  IR_ENFORCE(attributes.at("is_sparse").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_sparse is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: HsigmoidLossOp.";
}

void HsigmoidLossOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::HSigmoidLossInferMeta);
  fn(infer_meta);
}

phi::DataType HsigmoidLossOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: HsigmoidLossOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LessEqualOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "less_equal", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "less_equal");
}

void LessEqualOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LessEqualOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LessEqualOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LessEqualOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LessEqualOp.";
}

void LessEqualOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType LessEqualOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LessEqualOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LessEqual_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "less_equal", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "less_equal");
}

void LessEqual_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LessEqual_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LessEqual_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LessEqual_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LessEqual_Op.";
}

void LessEqual_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType LessEqual_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LessEqual_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple LessThanOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "less_than", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "less_than");
}

void LessThanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LessThanOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LessThanOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LessThanOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LessThanOp.";
}

void LessThanOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType LessThanOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LessThanOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LessThan_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "less_than", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "less_than");
}

void LessThan_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build LessThan_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LessThan_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LessThan_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LessThan_Op.";
}

void LessThan_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType LessThan_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LessThan_Op";
  


  return expected_kernel_dtype;
}

const char *LinspaceOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple LinspaceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("start", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("stop", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("number", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LinspaceInferMeta", {"start", "stop", "number", "dtype"}, "linspace", {"start", "stop", "number", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "linspace");
}

void LinspaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value start_, pir::Value stop_, pir::Value number_, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build LinspaceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {start_, stop_, number_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType start = start_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)start;
  paddle::dialect::DenseTensorType stop = stop_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stop;
  paddle::dialect::DenseTensorType number = number_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)number;

  VLOG(4) << "Builder construction  dense_start";
  paddle::dialect::IrTensor ir_tensor_start(paddle::dialect::TransToPhiDataType(start.dtype()),
                                                      start.dims(),
                                                      start.data_layout(),
                                                      start.lod(),
                                                      start.offset());
  VLOG(4) << "Builder construction  meta_start";
  paddle::dialect::IrMetaTensor meta_start(&ir_tensor_start);

  VLOG(4) << "Builder construction  dense_stop";
  paddle::dialect::IrTensor ir_tensor_stop(paddle::dialect::TransToPhiDataType(stop.dtype()),
                                                      stop.dims(),
                                                      stop.data_layout(),
                                                      stop.lod(),
                                                      stop.offset());
  VLOG(4) << "Builder construction  meta_stop";
  paddle::dialect::IrMetaTensor meta_stop(&ir_tensor_stop);

  VLOG(4) << "Builder construction  dense_number";
  paddle::dialect::IrTensor ir_tensor_number(paddle::dialect::TransToPhiDataType(number.dtype()),
                                                      number.dims(),
                                                      number.data_layout(),
                                                      number.lod(),
                                                      number.offset());
  VLOG(4) << "Builder construction  meta_number";
  paddle::dialect::IrMetaTensor meta_number(&ir_tensor_number);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LinspaceInferMeta(meta_start, meta_stop, meta_number, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LinspaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value start_, pir::Value stop_, pir::Value number_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LinspaceOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for LinspaceOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for LinspaceOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {start_, stop_, number_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType start = start_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)start;
  paddle::dialect::DenseTensorType stop = stop_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stop;
  paddle::dialect::DenseTensorType number = number_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)number;

  VLOG(4) << "Builder construction  dense_start";
  paddle::dialect::IrTensor ir_tensor_start(paddle::dialect::TransToPhiDataType(start.dtype()),
                                                      start.dims(),
                                                      start.data_layout(),
                                                      start.lod(),
                                                      start.offset());
  VLOG(4) << "Builder construction  meta_start";
  paddle::dialect::IrMetaTensor meta_start(&ir_tensor_start);

  VLOG(4) << "Builder construction  dense_stop";
  paddle::dialect::IrTensor ir_tensor_stop(paddle::dialect::TransToPhiDataType(stop.dtype()),
                                                      stop.dims(),
                                                      stop.data_layout(),
                                                      stop.lod(),
                                                      stop.offset());
  VLOG(4) << "Builder construction  meta_stop";
  paddle::dialect::IrMetaTensor meta_stop(&ir_tensor_stop);

  VLOG(4) << "Builder construction  dense_number";
  paddle::dialect::IrTensor ir_tensor_number(paddle::dialect::TransToPhiDataType(number.dtype()),
                                                      number.dims(),
                                                      number.data_layout(),
                                                      number.lod(),
                                                      number.offset());
  VLOG(4) << "Builder construction  meta_number";
  paddle::dialect::IrMetaTensor meta_number(&ir_tensor_number);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LinspaceInferMeta(meta_start, meta_stop, meta_number, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LinspaceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LinspaceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LinspaceOp.";
}

void LinspaceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LinspaceInferMeta);
  fn(infer_meta);
}

phi::DataType LinspaceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LinspaceOp";
  


  return expected_kernel_dtype;
}

const char *LoadCombineOp::attributes_name[3] = { "file_path", "load_as_fp16", "model_from_memory" };

OpInfoTuple LoadCombineOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("file_path", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("load_as_fp16", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("model_from_memory", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("Out", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "load_combine", {"file_path", "load_as_fp16", "model_from_memory"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "load_combine");
}

void LoadCombineOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LoadCombineOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("file_path")>0,
                 "file_path does not exist.");
  IR_ENFORCE(attributes.at("file_path").isa<pir::StrAttribute>(),
                 "Type of attribute: file_path is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("load_as_fp16")>0,
                 "load_as_fp16 does not exist.");
  IR_ENFORCE(attributes.at("load_as_fp16").isa<pir::BoolAttribute>(),
                 "Type of attribute: load_as_fp16 is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("model_from_memory")>0,
                 "model_from_memory does not exist.");
  IR_ENFORCE(attributes.at("model_from_memory").isa<pir::BoolAttribute>(),
                 "Type of attribute: model_from_memory is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  if (auto output_0_type = (*this)->result(0).type()) {
    if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th output.");
      }
    }
    else {
      IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  }
  VLOG(4) << "End Verifying for: LoadCombineOp.";
}

phi::DataType LoadCombineOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LoadCombineOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple LodArrayLengthOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lod_array_length");
}

void LodArrayLengthOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LodArrayLengthOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LodArrayLengthOp.";
}

phi::DataType LodArrayLengthOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LodArrayLengthOp";
  


  return expected_kernel_dtype;
}

const char *LogspaceOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple LogspaceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("start", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("stop", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("num", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("base", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogspaceInferMeta", {"start", "stop", "num", "base", "dtype"}, "logspace", {"start", "stop", "num", "base", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logspace");
}

void LogspaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value start_, pir::Value stop_, pir::Value num_, pir::Value base_, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build LogspaceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {start_, stop_, num_, base_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType start = start_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)start;
  paddle::dialect::DenseTensorType stop = stop_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stop;
  paddle::dialect::DenseTensorType num = num_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)num;
  paddle::dialect::DenseTensorType base = base_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)base;

  VLOG(4) << "Builder construction  dense_start";
  paddle::dialect::IrTensor ir_tensor_start(paddle::dialect::TransToPhiDataType(start.dtype()),
                                                      start.dims(),
                                                      start.data_layout(),
                                                      start.lod(),
                                                      start.offset());
  VLOG(4) << "Builder construction  meta_start";
  paddle::dialect::IrMetaTensor meta_start(&ir_tensor_start);

  VLOG(4) << "Builder construction  dense_stop";
  paddle::dialect::IrTensor ir_tensor_stop(paddle::dialect::TransToPhiDataType(stop.dtype()),
                                                      stop.dims(),
                                                      stop.data_layout(),
                                                      stop.lod(),
                                                      stop.offset());
  VLOG(4) << "Builder construction  meta_stop";
  paddle::dialect::IrMetaTensor meta_stop(&ir_tensor_stop);

  VLOG(4) << "Builder construction  dense_num";
  paddle::dialect::IrTensor ir_tensor_num(paddle::dialect::TransToPhiDataType(num.dtype()),
                                                      num.dims(),
                                                      num.data_layout(),
                                                      num.lod(),
                                                      num.offset());
  VLOG(4) << "Builder construction  meta_num";
  paddle::dialect::IrMetaTensor meta_num(&ir_tensor_num);

  VLOG(4) << "Builder construction  dense_base";
  paddle::dialect::IrTensor ir_tensor_base(paddle::dialect::TransToPhiDataType(base.dtype()),
                                                      base.dims(),
                                                      base.data_layout(),
                                                      base.lod(),
                                                      base.offset());
  VLOG(4) << "Builder construction  meta_base";
  paddle::dialect::IrMetaTensor meta_base(&ir_tensor_base);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogspaceInferMeta(meta_start, meta_stop, meta_num, meta_base, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogspaceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value start_, pir::Value stop_, pir::Value num_, pir::Value base_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogspaceOp";


  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for LogspaceOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for LogspaceOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {start_, stop_, num_, base_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType start = start_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)start;
  paddle::dialect::DenseTensorType stop = stop_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)stop;
  paddle::dialect::DenseTensorType num = num_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)num;
  paddle::dialect::DenseTensorType base = base_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)base;

  VLOG(4) << "Builder construction  dense_start";
  paddle::dialect::IrTensor ir_tensor_start(paddle::dialect::TransToPhiDataType(start.dtype()),
                                                      start.dims(),
                                                      start.data_layout(),
                                                      start.lod(),
                                                      start.offset());
  VLOG(4) << "Builder construction  meta_start";
  paddle::dialect::IrMetaTensor meta_start(&ir_tensor_start);

  VLOG(4) << "Builder construction  dense_stop";
  paddle::dialect::IrTensor ir_tensor_stop(paddle::dialect::TransToPhiDataType(stop.dtype()),
                                                      stop.dims(),
                                                      stop.data_layout(),
                                                      stop.lod(),
                                                      stop.offset());
  VLOG(4) << "Builder construction  meta_stop";
  paddle::dialect::IrMetaTensor meta_stop(&ir_tensor_stop);

  VLOG(4) << "Builder construction  dense_num";
  paddle::dialect::IrTensor ir_tensor_num(paddle::dialect::TransToPhiDataType(num.dtype()),
                                                      num.dims(),
                                                      num.data_layout(),
                                                      num.lod(),
                                                      num.offset());
  VLOG(4) << "Builder construction  meta_num";
  paddle::dialect::IrMetaTensor meta_num(&ir_tensor_num);

  VLOG(4) << "Builder construction  dense_base";
  paddle::dialect::IrTensor ir_tensor_base(paddle::dialect::TransToPhiDataType(base.dtype()),
                                                      base.dims(),
                                                      base.data_layout(),
                                                      base.lod(),
                                                      base.offset());
  VLOG(4) << "Builder construction  meta_base";
  paddle::dialect::IrMetaTensor meta_base(&ir_tensor_base);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogspaceInferMeta(meta_start, meta_stop, meta_num, meta_base, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogspaceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogspaceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogspaceOp.";
}

void LogspaceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogspaceInferMeta);
  fn(infer_meta);
}

phi::DataType LogspaceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogspaceOp";
  


  return expected_kernel_dtype;
}

const char *LogsumexpOp::attributes_name[3] = { "axis", "keepdim", "reduce_all" };

OpInfoTuple LogsumexpOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LogsumexpInferMeta", {"x", "axis", "keepdim", "reduce_all"}, "logsumexp", {"x", "axis", "keepdim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "logsumexp");
}

void LogsumexpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all) {
  VLOG(4) << "Start build LogsumexpOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogsumexpInferMeta(meta_x, axis, keepdim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsumexpOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LogsumexpOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for LogsumexpOp. ");
  std::vector<int64_t> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for LogsumexpOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for LogsumexpOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::LogsumexpInferMeta(meta_x, axis, keepdim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LogsumexpOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LogsumexpOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("reduce_all")>0,
                 "reduce_all does not exist.");
  IR_ENFORCE(attributes.at("reduce_all").isa<pir::BoolAttribute>(),
                 "Type of attribute: reduce_all is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: LogsumexpOp.";
}

void LogsumexpOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LogsumexpInferMeta);
  fn(infer_meta);
}

phi::DataType LogsumexpOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LogsumexpOp";
  


  return expected_kernel_dtype;
}

const char *MatmulOp::attributes_name[2] = { "transpose_x", "transpose_y" };

OpInfoTuple MatmulOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("transpose_x", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("transpose_y", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatmulInferMeta", {"x", "y", "transpose_x", "transpose_y"}, "matmul", {"x", "y", "transpose_x", "transpose_y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matmul");
}

void MatmulOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, bool transpose_x, bool transpose_y) {
  VLOG(4) << "Start build MatmulOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_transpose_y = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_y);
  argument.AddAttribute("transpose_y", attr_transpose_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatmulInferMeta(meta_x, meta_y, transpose_x, transpose_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatmulOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatmulOp";


  IR_ENFORCE(
      attributes.find("transpose_x") != attributes.end(),
          "'transpose_x' Attribute is expected for MatmulOp. ");
  bool transpose_x = attributes.at("transpose_x").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_y") != attributes.end(),
          "'transpose_y' Attribute is expected for MatmulOp. ");
  bool transpose_y = attributes.at("transpose_y").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_transpose_x = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_x);
  argument.AddAttribute("transpose_x", attr_transpose_x);
  pir::Attribute attr_transpose_y = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_y);
  argument.AddAttribute("transpose_y", attr_transpose_y);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatmulInferMeta(meta_x, meta_y, transpose_x, transpose_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatmulOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MatmulOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("transpose_x")>0,
                 "transpose_x does not exist.");
  IR_ENFORCE(attributes.at("transpose_x").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_x is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("transpose_y")>0,
                 "transpose_y does not exist.");
  IR_ENFORCE(attributes.at("transpose_y").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_y is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MatmulOp.";
}

void MatmulOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatmulInferMeta);
  fn(infer_meta);
}

phi::DataType MatmulOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatmulOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MatrixRankOp::attributes_name[3] = { "tol", "use_default_tol", "hermitian" };

OpInfoTuple MatrixRankOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("tol", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_default_tol", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("hermitian", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatrixRankInferMeta", {"x", "use_default_tol", "hermitian"}, "matrix_rank", {"x", "tol", "use_default_tol", "hermitian"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matrix_rank");
}

void MatrixRankOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float tol, bool use_default_tol, bool hermitian) {
  VLOG(4) << "Start build MatrixRankOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_tol = pir::FloatAttribute::get(pir::IrContext::Instance(), tol);
  argument.AddAttribute("tol", attr_tol);
  pir::Attribute attr_use_default_tol = pir::BoolAttribute::get(pir::IrContext::Instance(), use_default_tol);
  argument.AddAttribute("use_default_tol", attr_use_default_tol);
  pir::Attribute attr_hermitian = pir::BoolAttribute::get(pir::IrContext::Instance(), hermitian);
  argument.AddAttribute("hermitian", attr_hermitian);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatrixRankInferMeta(meta_x, use_default_tol, hermitian, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixRankOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatrixRankOp";


  IR_ENFORCE(
      attributes.find("tol") != attributes.end(),
          "'tol' Attribute is expected for MatrixRankOp. ");
  float tol = attributes.at("tol").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("use_default_tol") != attributes.end(),
          "'use_default_tol' Attribute is expected for MatrixRankOp. ");
  bool use_default_tol = attributes.at("use_default_tol").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("hermitian") != attributes.end(),
          "'hermitian' Attribute is expected for MatrixRankOp. ");
  bool hermitian = attributes.at("hermitian").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_tol = pir::FloatAttribute::get(pir::IrContext::Instance(), tol);
  argument.AddAttribute("tol", attr_tol);
  pir::Attribute attr_use_default_tol = pir::BoolAttribute::get(pir::IrContext::Instance(), use_default_tol);
  argument.AddAttribute("use_default_tol", attr_use_default_tol);
  pir::Attribute attr_hermitian = pir::BoolAttribute::get(pir::IrContext::Instance(), hermitian);
  argument.AddAttribute("hermitian", attr_hermitian);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatrixRankInferMeta(meta_x, use_default_tol, hermitian, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixRankOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MatrixRankOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("tol")>0,
                 "tol does not exist.");
  IR_ENFORCE(attributes.at("tol").isa<pir::FloatAttribute>(),
                 "Type of attribute: tol is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_default_tol")>0,
                 "use_default_tol does not exist.");
  IR_ENFORCE(attributes.at("use_default_tol").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_default_tol is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("hermitian")>0,
                 "hermitian does not exist.");
  IR_ENFORCE(attributes.at("hermitian").isa<pir::BoolAttribute>(),
                 "Type of attribute: hermitian is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MatrixRankOp.";
}

void MatrixRankOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatrixRankInferMeta);
  fn(infer_meta);
}

phi::DataType MatrixRankOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatrixRankOp";
  


  return expected_kernel_dtype;
}

const char *MatrixRankTolOp::attributes_name[2] = { "use_default_tol", "hermitian" };

OpInfoTuple MatrixRankTolOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("atol_tensor", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("use_default_tol", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("hermitian", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatrixRankTolInferMeta", {"x", "atol_tensor", "use_default_tol", "hermitian"}, "matrix_rank_tol", {"x", "atol_tensor", "use_default_tol", "hermitian"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "matrix_rank_tol");
}

void MatrixRankTolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value atol_tensor_, bool use_default_tol, bool hermitian) {
  VLOG(4) << "Start build MatrixRankTolOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, atol_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_use_default_tol = pir::BoolAttribute::get(pir::IrContext::Instance(), use_default_tol);
  argument.AddAttribute("use_default_tol", attr_use_default_tol);
  pir::Attribute attr_hermitian = pir::BoolAttribute::get(pir::IrContext::Instance(), hermitian);
  argument.AddAttribute("hermitian", attr_hermitian);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType atol_tensor = atol_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)atol_tensor;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_atol_tensor";
  paddle::dialect::IrTensor ir_tensor_atol_tensor(paddle::dialect::TransToPhiDataType(atol_tensor.dtype()),
                                                      atol_tensor.dims(),
                                                      atol_tensor.data_layout(),
                                                      atol_tensor.lod(),
                                                      atol_tensor.offset());
  VLOG(4) << "Builder construction  meta_atol_tensor";
  paddle::dialect::IrMetaTensor meta_atol_tensor(&ir_tensor_atol_tensor);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatrixRankTolInferMeta(meta_x, meta_atol_tensor, use_default_tol, hermitian, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixRankTolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value atol_tensor_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatrixRankTolOp";


  IR_ENFORCE(
      attributes.find("use_default_tol") != attributes.end(),
          "'use_default_tol' Attribute is expected for MatrixRankTolOp. ");
  bool use_default_tol = attributes.at("use_default_tol").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("hermitian") != attributes.end(),
          "'hermitian' Attribute is expected for MatrixRankTolOp. ");
  bool hermitian = attributes.at("hermitian").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, atol_tensor_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_use_default_tol = pir::BoolAttribute::get(pir::IrContext::Instance(), use_default_tol);
  argument.AddAttribute("use_default_tol", attr_use_default_tol);
  pir::Attribute attr_hermitian = pir::BoolAttribute::get(pir::IrContext::Instance(), hermitian);
  argument.AddAttribute("hermitian", attr_hermitian);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType atol_tensor = atol_tensor_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)atol_tensor;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_atol_tensor";
  paddle::dialect::IrTensor ir_tensor_atol_tensor(paddle::dialect::TransToPhiDataType(atol_tensor.dtype()),
                                                      atol_tensor.dims(),
                                                      atol_tensor.data_layout(),
                                                      atol_tensor.lod(),
                                                      atol_tensor.offset());
  VLOG(4) << "Builder construction  meta_atol_tensor";
  paddle::dialect::IrMetaTensor meta_atol_tensor(&ir_tensor_atol_tensor);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::MatrixRankTolInferMeta(meta_x, meta_atol_tensor, use_default_tol, hermitian, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatrixRankTolOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MatrixRankTolOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("use_default_tol")>0,
                 "use_default_tol does not exist.");
  IR_ENFORCE(attributes.at("use_default_tol").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_default_tol is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("hermitian")>0,
                 "hermitian does not exist.");
  IR_ENFORCE(attributes.at("hermitian").isa<pir::BoolAttribute>(),
                 "Type of attribute: hermitian is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MatrixRankTolOp.";
}

void MatrixRankTolOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatrixRankTolInferMeta);
  fn(infer_meta);
}

phi::DataType MatrixRankTolOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatrixRankTolOp";
  


  return expected_kernel_dtype;
}

const char *MaxOp::attributes_name[1] = { "keepdim" };

OpInfoTuple MaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceIntArrayAxisInferMeta", {"x", "axis", "keepdim"}, "max", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "max");
}

void MaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build MaxOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MaxOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MaxOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for MaxOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool keepdim) {
  VLOG(4) << "Start build MaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MaxOp.";
}

void MaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceIntArrayAxisInferMeta);
  fn(infer_meta);
}

phi::DataType MaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaxOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MaximumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "maximum", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "maximum");
}

void MaximumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build MaximumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MaximumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MaximumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MaximumOp.";
}

void MaximumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType MaximumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MaximumOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MeanOp::attributes_name[2] = { "axis", "keepdim" };

OpInfoTuple MeanOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceIntArrayAxisInferMeta", {"x", "axis", "keepdim"}, "mean", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mean");
}

void MeanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build MeanOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeanOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MeanOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MeanOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for MeanOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = paddle::dialect::IntArrayAttribute::get(pir::IrContext::Instance(), phi::IntArray(axis));
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MeanOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MeanOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: axis is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MeanOp.";
}

void MeanOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceIntArrayAxisInferMeta);
  fn(infer_meta);
}

phi::DataType MeanOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MeanOp";
  


  return expected_kernel_dtype;
}

const char *MemcpyOp::attributes_name[1] = { "dst_place_type" };

OpInfoTuple MemcpyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dst_place_type", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "memcpy", {"x", "dst_place_type"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "memcpy");
}

void MemcpyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int dst_place_type) {
  VLOG(4) << "Start build MemcpyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_place_type = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_place_type);
  argument.AddAttribute("dst_place_type", attr_dst_place_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemcpyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MemcpyOp";


  IR_ENFORCE(
      attributes.find("dst_place_type") != attributes.end(),
          "'dst_place_type' Attribute is expected for MemcpyOp. ");
  int dst_place_type = attributes.at("dst_place_type").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_place_type = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_place_type);
  argument.AddAttribute("dst_place_type", attr_dst_place_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemcpyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MemcpyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dst_place_type")>0,
                 "dst_place_type does not exist.");
  IR_ENFORCE(attributes.at("dst_place_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: dst_place_type is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MemcpyOp.";
}

void MemcpyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MemcpyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MemcpyOp";
  


  return expected_kernel_dtype;
}

const char *MemcpyD2hOp::attributes_name[1] = { "dst_place_type" };

OpInfoTuple MemcpyD2hOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dst_place_type", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "memcpy_d2h", {"x", "dst_place_type"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "memcpy_d2h");
}

void MemcpyD2hOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int dst_place_type) {
  VLOG(4) << "Start build MemcpyD2hOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_place_type = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_place_type);
  argument.AddAttribute("dst_place_type", attr_dst_place_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemcpyD2hOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MemcpyD2hOp";


  IR_ENFORCE(
      attributes.find("dst_place_type") != attributes.end(),
          "'dst_place_type' Attribute is expected for MemcpyD2hOp. ");
  int dst_place_type = attributes.at("dst_place_type").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_place_type = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_place_type);
  argument.AddAttribute("dst_place_type", attr_dst_place_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemcpyD2hOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MemcpyD2hOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dst_place_type")>0,
                 "dst_place_type does not exist.");
  IR_ENFORCE(attributes.at("dst_place_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: dst_place_type is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MemcpyD2hOp.";
}

void MemcpyD2hOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MemcpyD2hOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MemcpyD2hOp";
  


  return expected_kernel_dtype;
}

const char *MemcpyH2dOp::attributes_name[1] = { "dst_place_type" };

OpInfoTuple MemcpyH2dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dst_place_type", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "memcpy_h2d", {"x", "dst_place_type"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "memcpy_h2d");
}

void MemcpyH2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int dst_place_type) {
  VLOG(4) << "Start build MemcpyH2dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_place_type = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_place_type);
  argument.AddAttribute("dst_place_type", attr_dst_place_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemcpyH2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MemcpyH2dOp";


  IR_ENFORCE(
      attributes.find("dst_place_type") != attributes.end(),
          "'dst_place_type' Attribute is expected for MemcpyH2dOp. ");
  int dst_place_type = attributes.at("dst_place_type").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_place_type = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_place_type);
  argument.AddAttribute("dst_place_type", attr_dst_place_type);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MemcpyH2dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MemcpyH2dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dst_place_type")>0,
                 "dst_place_type does not exist.");
  IR_ENFORCE(attributes.at("dst_place_type").isa<pir::Int32Attribute>(),
                 "Type of attribute: dst_place_type is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MemcpyH2dOp.";
}

void MemcpyH2dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MemcpyH2dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MemcpyH2dOp";
  


  return expected_kernel_dtype;
}

const char *MinOp::attributes_name[1] = { "keepdim" };

OpInfoTuple MinOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceIntArrayAxisInferMeta", {"x", "axis", "keepdim"}, "min", {"x", "axis", "keepdim"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "min");
}

void MinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, bool keepdim) {
  VLOG(4) << "Start build MinOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MinOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for MinOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for MinOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, bool keepdim) {
  VLOG(4) << "Start build MinOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMeta(meta_x, axis, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MinOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MinOp.";
}

void MinOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceIntArrayAxisInferMeta);
  fn(infer_meta);
}

phi::DataType MinOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MinOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MinimumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "minimum", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "minimum");
}

void MinimumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build MinimumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MinimumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MinimumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MinimumOp.";
}

void MinimumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType MinimumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MinimumOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *MishOp::attributes_name[1] = { "lambda" };

OpInfoTuple MishOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lambda", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "mish", {"x", "lambda"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "mish");
}

void MishOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float lambda) {
  VLOG(4) << "Start build MishOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), lambda);
  argument.AddAttribute("lambda", attr_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MishOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MishOp";


  IR_ENFORCE(
      attributes.find("lambda") != attributes.end(),
          "'lambda' Attribute is expected for MishOp. ");
  float lambda = attributes.at("lambda").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lambda = pir::FloatAttribute::get(pir::IrContext::Instance(), lambda);
  argument.AddAttribute("lambda", attr_lambda);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MishOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MishOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("lambda")>0,
                 "lambda does not exist.");
  IR_ENFORCE(attributes.at("lambda").isa<pir::FloatAttribute>(),
                 "Type of attribute: lambda is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MishOp.";
}

void MishOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType MishOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MishOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiplyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "multiply", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply");
}

void MultiplyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build MultiplyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiplyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultiplyOp.";
}

void MultiplyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplyOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiplySrOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "multiply_sr", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply");
}

void MultiplySrOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build MultiplySrOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplySrOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiplySrOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultiplySrOp.";
}

void MultiplySrOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplySrOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplySrOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple Multiply_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "multiply", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply");
}

void Multiply_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Multiply_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Multiply_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Multiply_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Multiply_Op.";
}

void Multiply_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType Multiply_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Multiply_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple MultiplySr_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::SelectedRowsType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::SelectedRowsType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "multiply_sr", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "multiply");
}

void MultiplySr_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build MultiplySr_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::SelectedRowsType x = x_.type().dyn_cast<paddle::dialect::SelectedRowsType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrSelectedRows dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::SelectedRowsType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MultiplySr_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MultiplySr_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::SelectedRowsType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::SelectedRowsType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: MultiplySr_Op.";
}

void MultiplySr_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType MultiplySr_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MultiplySr_Op";
  


  return expected_kernel_dtype;
}

const char *NormOp::attributes_name[3] = { "axis", "epsilon", "is_test" };

OpInfoTuple NormOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("norm", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NormInferMeta", {"x", "axis", "epsilon", "is_test"}, "norm", {"x", "axis", "epsilon", "is_test"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "norm");
}

void NormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis, float epsilon, bool is_test) {
  VLOG(4) << "Start build NormOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_norm;
  paddle::dialect::IrMetaTensor meta_norm(&dense_norm);

  phi::NormInferMeta(meta_x, axis, epsilon, is_test, &meta_out, &meta_norm);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type norm_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_norm.dtype()), dense_norm.dims(), dense_norm.layout(), dense_norm.lod(), dense_norm.offset());
  argument_outputs.push_back(norm_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NormOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NormOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for NormOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for NormOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for NormOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_norm;
  paddle::dialect::IrMetaTensor meta_norm(&dense_norm);

  phi::NormInferMeta(meta_x, axis, epsilon, is_test, &meta_out, &meta_norm);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type norm_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_norm.dtype()), dense_norm.dims(), dense_norm.layout(), dense_norm.lod(), dense_norm.offset());
  argument_outputs.push_back(norm_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NormOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NormOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: NormOp.";
}

void NormOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NormInferMeta);
  fn(infer_meta);
}

phi::DataType NormOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NormOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple NotEqualOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "not_equal", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "not_equal");
}

void NotEqualOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build NotEqualOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NotEqualOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NotEqualOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NotEqualOp.";
}

void NotEqualOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType NotEqualOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NotEqualOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple NotEqual_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CompareInferMeta", {"x", "y"}, "not_equal", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "not_equal");
}

void NotEqual_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build NotEqual_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::CompareInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NotEqual_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NotEqual_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NotEqual_Op.";
}

void NotEqual_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CompareInferMeta);
  fn(infer_meta);
}

phi::DataType NotEqual_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NotEqual_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple OneHotOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("num_classes", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("OneHotInferMeta", {"x", "num_classes"}, "one_hot", {"x", "num_classes"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "one_hot");
}

void OneHotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int num_classes) {
  VLOG(4) << "Start build OneHotOp";


  // Generate scalar mutable attribute: num_classes
  paddle::dialect::FullOp full_num_classes_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_classes, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult num_classes_ = full_num_classes_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, num_classes_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::OneHotInferMeta(meta_x, num_classes, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OneHotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build OneHotOp";


  IR_ENFORCE(
      attributes.find("num_classes") != attributes.end(),
          "'num_classes' Attribute is expected for OneHotOp. ");
  int num_classes = attributes.at("num_classes").dyn_cast<pir::Int32Attribute>().data();

  // Generate scalar mutable attribute: num_classes
  paddle::dialect::FullOp full_num_classes_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, num_classes, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult num_classes_ = full_num_classes_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, num_classes_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::OneHotInferMeta(meta_x, num_classes, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OneHotOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value num_classes_) {
  VLOG(4) << "Start build OneHotOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, num_classes_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar num_classes;
  if (num_classes_.dyn_cast<pir::OpResult>() && num_classes_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    num_classes = std::move(phi::Scalar(num_classes_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    num_classes = std::move(phi::Scalar(-1));
    num_classes.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::OneHotInferMeta(meta_x, num_classes, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OneHotOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: OneHotOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: OneHotOp.";
}

void OneHotOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::OneHotInferMeta);
  fn(infer_meta);
}

phi::DataType OneHotOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: OneHotOp";
  


  return expected_kernel_dtype;
}

const char *OnesOp::attributes_name[3] = { "shape", "dtype", "place" };

OpInfoTuple OnesOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ones");
}

void OnesOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& shape, phi::DataType dtype, const Place& place) {
  FullOp::Build(builder, argument, shape, 1, dtype, place);
}

void OnesOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: OnesOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: shape is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: OnesOp.";
}

void OnesOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateInferMeta);
  fn(infer_meta);
}

phi::DataType OnesOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: OnesOp";
  


  return expected_kernel_dtype;
}

const char *OnesLikeOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple OnesLikeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ones_like");
}

void OnesLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType dtype, const Place& place) {
  FullLikeOp::Build(builder, argument, x_, 1, dtype, place);
}

void OnesLikeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: OnesLikeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: OnesLikeOp.";
}

void OnesLikeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateLikeInferMeta);
  fn(infer_meta);
}

phi::DataType OnesLikeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: OnesLikeOp";
  


  return expected_kernel_dtype;
}

const char *PadOp::attributes_name[1] = { "paddings" };

OpInfoTuple PadOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("pad_value", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PadInferMeta", {"x", "paddings", "pad_value"}, "pad", {"x", "paddings", "pad_value"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pad");
}

void PadOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& paddings, float pad_value) {
  VLOG(4) << "Start build PadOp";


  // Generate scalar mutable attribute: pad_value
  paddle::dialect::FullOp full_pad_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, pad_value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult pad_value_ = full_pad_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PadInferMeta(meta_x, paddings, pad_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PadOp";


  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for PadOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("pad_value") != attributes.end(),
          "'pad_value' Attribute is expected for PadOp. ");
  float pad_value = attributes.at("pad_value").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  // Generate scalar mutable attribute: pad_value
  paddle::dialect::FullOp full_pad_value_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, pad_value, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult pad_value_ = full_pad_value_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PadInferMeta(meta_x, paddings, pad_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pad_value_, const std::vector<int>& paddings) {
  VLOG(4) << "Start build PadOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pad_value_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar pad_value;
  if (pad_value_.dyn_cast<pir::OpResult>() && pad_value_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    pad_value = std::move(phi::Scalar(pad_value_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    pad_value = std::move(phi::Scalar(-1));
    pad_value.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PadInferMeta(meta_x, paddings, pad_value, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PadOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PadOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PadOp.";
}

void PadOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PadInferMeta);
  fn(infer_meta);
}

phi::DataType PadOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PadOp";
  


  return expected_kernel_dtype;
}

const char *Pool2dOp::attributes_name[9] = { "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm" };

OpInfoTuple Pool2dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("kernel_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("ceil_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pooling_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("Pool2DInferMeta", {"x", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, "pool2d", {"x", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pool2d");
}

void Pool2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool2dOp";


  // Generate int_array mutable attribute: kernel_size
  paddle::dialect::FullIntArrayOp full_kernel_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(kernel_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult kernel_size_ = full_kernel_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Pool2DInferMeta(meta_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pool2dOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for Pool2dOp. ");
  std::vector<int64_t> kernel_size = attributes.at("kernel_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Pool2dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pool2dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("ceil_mode") != attributes.end(),
          "'ceil_mode' Attribute is expected for Pool2dOp. ");
  bool ceil_mode = attributes.at("ceil_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for Pool2dOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pool2dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pooling_type") != attributes.end(),
          "'pooling_type' Attribute is expected for Pool2dOp. ");
  std::string pooling_type = attributes.at("pooling_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for Pool2dOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for Pool2dOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Pool2dOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: kernel_size
  paddle::dialect::FullIntArrayOp full_kernel_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(kernel_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult kernel_size_ = full_kernel_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Pool2DInferMeta(meta_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value kernel_size_, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool2dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, kernel_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray kernel_size;
  if (kernel_size_.dyn_cast<pir::OpResult>() && kernel_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    kernel_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          kernel_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (kernel_size_.type().isa<pir::VectorType>()) {
    size_t kernel_size_size = kernel_size_.type().dyn_cast<pir::VectorType>().size();
    kernel_size = std::move(phi::IntArray(std::vector<int64_t>(kernel_size_size, -1)));
    kernel_size.SetFromTensor(true);
  } else if (kernel_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim kernel_size_dim = kernel_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t kernel_size_size = common::product(kernel_size_dim);
    if (common::contain_unknown_dim(kernel_size_dim)) {
      kernel_size_size = 1;
    }
    kernel_size = std::move(phi::IntArray(std::vector<int64_t>(kernel_size_size, -1)));
    kernel_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::Pool2DInferMeta(meta_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool2dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Pool2dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("ceil_mode")>0,
                 "ceil_mode does not exist.");
  IR_ENFORCE(attributes.at("ceil_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: ceil_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exclusive")>0,
                 "exclusive does not exist.");
  IR_ENFORCE(attributes.at("exclusive").isa<pir::BoolAttribute>(),
                 "Type of attribute: exclusive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("pooling_type")>0,
                 "pooling_type does not exist.");
  IR_ENFORCE(attributes.at("pooling_type").isa<pir::StrAttribute>(),
                 "Type of attribute: pooling_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("global_pooling")>0,
                 "global_pooling does not exist.");
  IR_ENFORCE(attributes.at("global_pooling").isa<pir::BoolAttribute>(),
                 "Type of attribute: global_pooling is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("adaptive")>0,
                 "adaptive does not exist.");
  IR_ENFORCE(attributes.at("adaptive").isa<pir::BoolAttribute>(),
                 "Type of attribute: adaptive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Pool2dOp.";
}

void Pool2dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::Pool2DInferMeta);
  fn(infer_meta);
}

phi::DataType Pool2dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pool2dOp";
  


  return expected_kernel_dtype;
}

const char *Pool3dOp::attributes_name[10] = { "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm" };

OpInfoTuple Pool3dOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("kernel_size", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("paddings", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("ceil_mode", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("exclusive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("pooling_type", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("global_pooling", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("adaptive", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("padding_algorithm", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("PoolInferMeta", {"x", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, "pool3d", {"x", "kernel_size", "strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling", "adaptive", "padding_algorithm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "pool3d");
}

void Pool3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm) {
  VLOG(4) << "Start build Pool3dOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PoolInferMeta(meta_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool3dOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Pool3dOp";


  IR_ENFORCE(
      attributes.find("kernel_size") != attributes.end(),
          "'kernel_size' Attribute is expected for Pool3dOp. ");
  std::vector<int> kernel_size;
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    kernel_size.push_back(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for Pool3dOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("paddings") != attributes.end(),
          "'paddings' Attribute is expected for Pool3dOp. ");
  std::vector<int> paddings;
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    paddings.push_back(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("ceil_mode") != attributes.end(),
          "'ceil_mode' Attribute is expected for Pool3dOp. ");
  bool ceil_mode = attributes.at("ceil_mode").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("exclusive") != attributes.end(),
          "'exclusive' Attribute is expected for Pool3dOp. ");
  bool exclusive = attributes.at("exclusive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for Pool3dOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("pooling_type") != attributes.end(),
          "'pooling_type' Attribute is expected for Pool3dOp. ");
  std::string pooling_type = attributes.at("pooling_type").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("global_pooling") != attributes.end(),
          "'global_pooling' Attribute is expected for Pool3dOp. ");
  bool global_pooling = attributes.at("global_pooling").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("adaptive") != attributes.end(),
          "'adaptive' Attribute is expected for Pool3dOp. ");
  bool adaptive = attributes.at("adaptive").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("padding_algorithm") != attributes.end(),
          "'padding_algorithm' Attribute is expected for Pool3dOp. ");
  std::string padding_algorithm = attributes.at("padding_algorithm").dyn_cast<pir::StrAttribute>().AsString();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_kernel_size;
  for (size_t i = 0; i < static_cast<size_t>(kernel_size.size()); i++) {
      pir::Attribute attr_kernel_size = pir::Int32Attribute::get(pir::IrContext::Instance(), kernel_size[i]);

    vec_kernel_size.push_back(attr_kernel_size);
  }
  pir::Attribute attr_kernel_size = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_kernel_size);
  argument.AddAttribute("kernel_size", attr_kernel_size);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_paddings;
  for (size_t i = 0; i < static_cast<size_t>(paddings.size()); i++) {
      pir::Attribute attr_paddings = pir::Int32Attribute::get(pir::IrContext::Instance(), paddings[i]);

    vec_paddings.push_back(attr_paddings);
  }
  pir::Attribute attr_paddings = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_paddings);
  argument.AddAttribute("paddings", attr_paddings);
  pir::Attribute attr_ceil_mode = pir::BoolAttribute::get(pir::IrContext::Instance(), ceil_mode);
  argument.AddAttribute("ceil_mode", attr_ceil_mode);
  pir::Attribute attr_exclusive = pir::BoolAttribute::get(pir::IrContext::Instance(), exclusive);
  argument.AddAttribute("exclusive", attr_exclusive);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);
  pir::Attribute attr_pooling_type = pir::StrAttribute::get(pir::IrContext::Instance(), pooling_type);
  argument.AddAttribute("pooling_type", attr_pooling_type);
  pir::Attribute attr_global_pooling = pir::BoolAttribute::get(pir::IrContext::Instance(), global_pooling);
  argument.AddAttribute("global_pooling", attr_global_pooling);
  pir::Attribute attr_adaptive = pir::BoolAttribute::get(pir::IrContext::Instance(), adaptive);
  argument.AddAttribute("adaptive", attr_adaptive);
  pir::Attribute attr_padding_algorithm = pir::StrAttribute::get(pir::IrContext::Instance(), padding_algorithm);
  argument.AddAttribute("padding_algorithm", attr_padding_algorithm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::PoolInferMeta(meta_x, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Pool3dOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Pool3dOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("kernel_size")>0,
                 "kernel_size does not exist.");
  IR_ENFORCE(attributes.at("kernel_size").isa<pir::ArrayAttribute>(),
                 "Type of attribute: kernel_size is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("kernel_size").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: kernel_size is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("paddings")>0,
                 "paddings does not exist.");
  IR_ENFORCE(attributes.at("paddings").isa<pir::ArrayAttribute>(),
                 "Type of attribute: paddings is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("paddings").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: paddings is not right.");
  }
  IR_ENFORCE(attributes.count("ceil_mode")>0,
                 "ceil_mode does not exist.");
  IR_ENFORCE(attributes.at("ceil_mode").isa<pir::BoolAttribute>(),
                 "Type of attribute: ceil_mode is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("exclusive")>0,
                 "exclusive does not exist.");
  IR_ENFORCE(attributes.at("exclusive").isa<pir::BoolAttribute>(),
                 "Type of attribute: exclusive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("pooling_type")>0,
                 "pooling_type does not exist.");
  IR_ENFORCE(attributes.at("pooling_type").isa<pir::StrAttribute>(),
                 "Type of attribute: pooling_type is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("global_pooling")>0,
                 "global_pooling does not exist.");
  IR_ENFORCE(attributes.at("global_pooling").isa<pir::BoolAttribute>(),
                 "Type of attribute: global_pooling is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("adaptive")>0,
                 "adaptive does not exist.");
  IR_ENFORCE(attributes.at("adaptive").isa<pir::BoolAttribute>(),
                 "Type of attribute: adaptive is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("padding_algorithm")>0,
                 "padding_algorithm does not exist.");
  IR_ENFORCE(attributes.at("padding_algorithm").isa<pir::StrAttribute>(),
                 "Type of attribute: padding_algorithm is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Pool3dOp.";
}

void Pool3dOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::PoolInferMeta);
  fn(infer_meta);
}

phi::DataType Pool3dOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Pool3dOp";
  


  return expected_kernel_dtype;
}

const char *PrintOp::attributes_name[10] = { "first_n", "message", "summarize", "print_tensor_name", "print_tensor_type", "print_tensor_shape", "print_tensor_layout", "print_tensor_lod", "print_phase", "is_forward" };

OpInfoTuple PrintOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("in", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("first_n", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("message", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("summarize", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("print_tensor_name", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("print_tensor_type", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("print_tensor_shape", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("print_tensor_layout", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("print_tensor_lod", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("print_phase", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_forward", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"in"}, "print_kernel", {"in", "first_n", "message", "summarize", "print_tensor_name", "print_tensor_type", "print_tensor_shape", "print_tensor_layout", "print_tensor_lod", "print_phase", "is_forward"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "print");
}

void PrintOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value in_, int first_n, const std::string& message, int summarize, bool print_tensor_name, bool print_tensor_type, bool print_tensor_shape, bool print_tensor_layout, bool print_tensor_lod, const std::string& print_phase, bool is_forward) {
  VLOG(4) << "Start build PrintOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_first_n = pir::Int32Attribute::get(pir::IrContext::Instance(), first_n);
  argument.AddAttribute("first_n", attr_first_n);
  pir::Attribute attr_message = pir::StrAttribute::get(pir::IrContext::Instance(), message);
  argument.AddAttribute("message", attr_message);
  pir::Attribute attr_summarize = pir::Int32Attribute::get(pir::IrContext::Instance(), summarize);
  argument.AddAttribute("summarize", attr_summarize);
  pir::Attribute attr_print_tensor_name = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_name);
  argument.AddAttribute("print_tensor_name", attr_print_tensor_name);
  pir::Attribute attr_print_tensor_type = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_type);
  argument.AddAttribute("print_tensor_type", attr_print_tensor_type);
  pir::Attribute attr_print_tensor_shape = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_shape);
  argument.AddAttribute("print_tensor_shape", attr_print_tensor_shape);
  pir::Attribute attr_print_tensor_layout = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_layout);
  argument.AddAttribute("print_tensor_layout", attr_print_tensor_layout);
  pir::Attribute attr_print_tensor_lod = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_lod);
  argument.AddAttribute("print_tensor_lod", attr_print_tensor_lod);
  pir::Attribute attr_print_phase = pir::StrAttribute::get(pir::IrContext::Instance(), print_phase);
  argument.AddAttribute("print_phase", attr_print_phase);
  pir::Attribute attr_is_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), is_forward);
  argument.AddAttribute("is_forward", attr_is_forward);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType in = in_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in;

  VLOG(4) << "Builder construction  dense_in";
  paddle::dialect::IrTensor ir_tensor_in(paddle::dialect::TransToPhiDataType(in.dtype()),
                                                      in.dims(),
                                                      in.data_layout(),
                                                      in.lod(),
                                                      in.offset());
  VLOG(4) << "Builder construction  meta_in";
  paddle::dialect::IrMetaTensor meta_in(&ir_tensor_in);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_in, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PrintOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value in_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build PrintOp";


  IR_ENFORCE(
      attributes.find("first_n") != attributes.end(),
          "'first_n' Attribute is expected for PrintOp. ");
  int first_n = attributes.at("first_n").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("message") != attributes.end(),
          "'message' Attribute is expected for PrintOp. ");
  std::string message = attributes.at("message").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("summarize") != attributes.end(),
          "'summarize' Attribute is expected for PrintOp. ");
  int summarize = attributes.at("summarize").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("print_tensor_name") != attributes.end(),
          "'print_tensor_name' Attribute is expected for PrintOp. ");
  bool print_tensor_name = attributes.at("print_tensor_name").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("print_tensor_type") != attributes.end(),
          "'print_tensor_type' Attribute is expected for PrintOp. ");
  bool print_tensor_type = attributes.at("print_tensor_type").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("print_tensor_shape") != attributes.end(),
          "'print_tensor_shape' Attribute is expected for PrintOp. ");
  bool print_tensor_shape = attributes.at("print_tensor_shape").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("print_tensor_layout") != attributes.end(),
          "'print_tensor_layout' Attribute is expected for PrintOp. ");
  bool print_tensor_layout = attributes.at("print_tensor_layout").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("print_tensor_lod") != attributes.end(),
          "'print_tensor_lod' Attribute is expected for PrintOp. ");
  bool print_tensor_lod = attributes.at("print_tensor_lod").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("print_phase") != attributes.end(),
          "'print_phase' Attribute is expected for PrintOp. ");
  std::string print_phase = attributes.at("print_phase").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_forward") != attributes.end(),
          "'is_forward' Attribute is expected for PrintOp. ");
  bool is_forward = attributes.at("is_forward").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_first_n = pir::Int32Attribute::get(pir::IrContext::Instance(), first_n);
  argument.AddAttribute("first_n", attr_first_n);
  pir::Attribute attr_message = pir::StrAttribute::get(pir::IrContext::Instance(), message);
  argument.AddAttribute("message", attr_message);
  pir::Attribute attr_summarize = pir::Int32Attribute::get(pir::IrContext::Instance(), summarize);
  argument.AddAttribute("summarize", attr_summarize);
  pir::Attribute attr_print_tensor_name = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_name);
  argument.AddAttribute("print_tensor_name", attr_print_tensor_name);
  pir::Attribute attr_print_tensor_type = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_type);
  argument.AddAttribute("print_tensor_type", attr_print_tensor_type);
  pir::Attribute attr_print_tensor_shape = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_shape);
  argument.AddAttribute("print_tensor_shape", attr_print_tensor_shape);
  pir::Attribute attr_print_tensor_layout = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_layout);
  argument.AddAttribute("print_tensor_layout", attr_print_tensor_layout);
  pir::Attribute attr_print_tensor_lod = pir::BoolAttribute::get(pir::IrContext::Instance(), print_tensor_lod);
  argument.AddAttribute("print_tensor_lod", attr_print_tensor_lod);
  pir::Attribute attr_print_phase = pir::StrAttribute::get(pir::IrContext::Instance(), print_phase);
  argument.AddAttribute("print_phase", attr_print_phase);
  pir::Attribute attr_is_forward = pir::BoolAttribute::get(pir::IrContext::Instance(), is_forward);
  argument.AddAttribute("is_forward", attr_is_forward);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType in = in_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)in;

  VLOG(4) << "Builder construction  dense_in";
  paddle::dialect::IrTensor ir_tensor_in(paddle::dialect::TransToPhiDataType(in.dtype()),
                                                      in.dims(),
                                                      in.data_layout(),
                                                      in.lod(),
                                                      in.offset());
  VLOG(4) << "Builder construction  meta_in";
  paddle::dialect::IrMetaTensor meta_in(&ir_tensor_in);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_in, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void PrintOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: PrintOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("first_n")>0,
                 "first_n does not exist.");
  IR_ENFORCE(attributes.at("first_n").isa<pir::Int32Attribute>(),
                 "Type of attribute: first_n is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("message")>0,
                 "message does not exist.");
  IR_ENFORCE(attributes.at("message").isa<pir::StrAttribute>(),
                 "Type of attribute: message is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("summarize")>0,
                 "summarize does not exist.");
  IR_ENFORCE(attributes.at("summarize").isa<pir::Int32Attribute>(),
                 "Type of attribute: summarize is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("print_tensor_name")>0,
                 "print_tensor_name does not exist.");
  IR_ENFORCE(attributes.at("print_tensor_name").isa<pir::BoolAttribute>(),
                 "Type of attribute: print_tensor_name is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("print_tensor_type")>0,
                 "print_tensor_type does not exist.");
  IR_ENFORCE(attributes.at("print_tensor_type").isa<pir::BoolAttribute>(),
                 "Type of attribute: print_tensor_type is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("print_tensor_shape")>0,
                 "print_tensor_shape does not exist.");
  IR_ENFORCE(attributes.at("print_tensor_shape").isa<pir::BoolAttribute>(),
                 "Type of attribute: print_tensor_shape is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("print_tensor_layout")>0,
                 "print_tensor_layout does not exist.");
  IR_ENFORCE(attributes.at("print_tensor_layout").isa<pir::BoolAttribute>(),
                 "Type of attribute: print_tensor_layout is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("print_tensor_lod")>0,
                 "print_tensor_lod does not exist.");
  IR_ENFORCE(attributes.at("print_tensor_lod").isa<pir::BoolAttribute>(),
                 "Type of attribute: print_tensor_lod is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("print_phase")>0,
                 "print_phase does not exist.");
  IR_ENFORCE(attributes.at("print_phase").isa<pir::StrAttribute>(),
                 "Type of attribute: print_phase is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("is_forward")>0,
                 "is_forward does not exist.");
  IR_ENFORCE(attributes.at("is_forward").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_forward is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: PrintOp.";
}

void PrintOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType PrintOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: PrintOp";
  


  return expected_kernel_dtype;
}

const char *ProdOp::attributes_name[2] = { "keep_dim", "reduce_all" };

OpInfoTuple ProdOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("dims", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("keep_dim", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("reduce_all", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReduceIntArrayAxisInferMetaBase", {"x", "dims", "keep_dim", "reduce_all"}, "prod", {"x", "dims", "keep_dim", "reduce_all"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "prod");
}

void ProdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& dims, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build ProdOp";


  // Generate int_array mutable attribute: dims
  paddle::dialect::FullIntArrayOp full_dims_op = builder.Build<paddle::dialect::FullIntArrayOp>(dims, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult dims_ = full_dims_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dims_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMetaBase(meta_x, dims, keep_dim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ProdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ProdOp";


  IR_ENFORCE(
      attributes.find("dims") != attributes.end(),
          "'dims' Attribute is expected for ProdOp. ");
  std::vector<int64_t> dims = attributes.at("dims").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("keep_dim") != attributes.end(),
          "'keep_dim' Attribute is expected for ProdOp. ");
  bool keep_dim = attributes.at("keep_dim").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("reduce_all") != attributes.end(),
          "'reduce_all' Attribute is expected for ProdOp. ");
  bool reduce_all = attributes.at("reduce_all").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: dims
  paddle::dialect::FullIntArrayOp full_dims_op = builder.Build<paddle::dialect::FullIntArrayOp>(dims, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult dims_ = full_dims_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dims_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMetaBase(meta_x, dims, keep_dim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ProdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value dims_, bool keep_dim, bool reduce_all) {
  VLOG(4) << "Start build ProdOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dims_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_keep_dim = pir::BoolAttribute::get(pir::IrContext::Instance(), keep_dim);
  argument.AddAttribute("keep_dim", attr_keep_dim);
  pir::Attribute attr_reduce_all = pir::BoolAttribute::get(pir::IrContext::Instance(), reduce_all);
  argument.AddAttribute("reduce_all", attr_reduce_all);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray dims;
  if (dims_.dyn_cast<pir::OpResult>() && dims_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    dims = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          dims_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (dims_.type().isa<pir::VectorType>()) {
    size_t dims_size = dims_.type().dyn_cast<pir::VectorType>().size();
    dims = std::move(phi::IntArray(std::vector<int64_t>(dims_size, -1)));
    dims.SetFromTensor(true);
  } else if (dims_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim dims_dim = dims_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t dims_size = common::product(dims_dim);
    if (common::contain_unknown_dim(dims_dim)) {
      dims_size = 1;
    }
    dims = std::move(phi::IntArray(std::vector<int64_t>(dims_size, -1)));
    dims.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReduceIntArrayAxisInferMetaBase(meta_x, dims, keep_dim, reduce_all, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ProdOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ProdOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("keep_dim")>0,
                 "keep_dim does not exist.");
  IR_ENFORCE(attributes.at("keep_dim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keep_dim is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("reduce_all")>0,
                 "reduce_all does not exist.");
  IR_ENFORCE(attributes.at("reduce_all").isa<pir::BoolAttribute>(),
                 "Type of attribute: reduce_all is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ProdOp.";
}

void ProdOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReduceIntArrayAxisInferMetaBase);
  fn(infer_meta);
}

phi::DataType ProdOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ProdOp";
  


  return expected_kernel_dtype;
}

const char *RandintOp::attributes_name[4] = { "low", "high", "dtype", "place" };

OpInfoTuple RandintOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("low", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("high", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RandintInferMeta", {"low", "high", "shape", "dtype"}, "randint", {"low", "high", "shape", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "randint");
}

void RandintOp::Build(pir::Builder &builder, pir::OperationArgument &argument, int low, int high, const std::vector<int64_t>& shape, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build RandintOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_low = pir::Int32Attribute::get(pir::IrContext::Instance(), low);
  argument.AddAttribute("low", attr_low);
  pir::Attribute attr_high = pir::Int32Attribute::get(pir::IrContext::Instance(), high);
  argument.AddAttribute("high", attr_high);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RandintInferMeta(low, high, shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RandintOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RandintOp";


  IR_ENFORCE(
      attributes.find("low") != attributes.end(),
          "'low' Attribute is expected for RandintOp. ");
  int low = attributes.at("low").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("high") != attributes.end(),
          "'high' Attribute is expected for RandintOp. ");
  int high = attributes.at("high").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for RandintOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for RandintOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for RandintOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_low = pir::Int32Attribute::get(pir::IrContext::Instance(), low);
  argument.AddAttribute("low", attr_low);
  pir::Attribute attr_high = pir::Int32Attribute::get(pir::IrContext::Instance(), high);
  argument.AddAttribute("high", attr_high);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RandintInferMeta(low, high, shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RandintOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shape_, int low, int high, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build RandintOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_low = pir::Int32Attribute::get(pir::IrContext::Instance(), low);
  argument.AddAttribute("low", attr_low);
  pir::Attribute attr_high = pir::Int32Attribute::get(pir::IrContext::Instance(), high);
  argument.AddAttribute("high", attr_high);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RandintInferMeta(low, high, shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RandintOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RandintOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("low")>0,
                 "low does not exist.");
  IR_ENFORCE(attributes.at("low").isa<pir::Int32Attribute>(),
                 "Type of attribute: low is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("high")>0,
                 "high does not exist.");
  IR_ENFORCE(attributes.at("high").isa<pir::Int32Attribute>(),
                 "Type of attribute: high is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RandintOp.";
}

void RandintOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RandintInferMeta);
  fn(infer_meta);
}

phi::DataType RandintOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RandintOp";
  


  return expected_kernel_dtype;
}

const char *RandpermOp::attributes_name[3] = { "n", "dtype", "place" };

OpInfoTuple RandpermOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("n", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RandpermInferMeta", {"n", "dtype"}, "randperm", {"n", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "randperm");
}

void RandpermOp::Build(pir::Builder &builder, pir::OperationArgument &argument, int n, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build RandpermOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RandpermInferMeta(n, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RandpermOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RandpermOp";


  IR_ENFORCE(
      attributes.find("n") != attributes.end(),
          "'n' Attribute is expected for RandpermOp. ");
  int n = attributes.at("n").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for RandpermOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for RandpermOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_n = pir::Int32Attribute::get(pir::IrContext::Instance(), n);
  argument.AddAttribute("n", attr_n);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RandpermInferMeta(n, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RandpermOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RandpermOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("n")>0,
                 "n does not exist.");
  IR_ENFORCE(attributes.at("n").isa<pir::Int32Attribute>(),
                 "Type of attribute: n is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RandpermOp.";
}

void RandpermOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RandpermInferMeta);
  fn(infer_meta);
}

phi::DataType RandpermOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RandpermOp";
  


  return expected_kernel_dtype;
}

const char *ReadFileOp::attributes_name[3] = { "filename", "dtype", "place" };

OpInfoTuple ReadFileOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("filename", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReadFileInferMeta", {"filename"}, "read_file", {"filename"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "read_file");
}

void ReadFileOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::string& filename, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build ReadFileOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_filename = pir::StrAttribute::get(pir::IrContext::Instance(), filename);
  argument.AddAttribute("filename", attr_filename);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReadFileInferMeta(filename, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReadFileOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ReadFileOp";


  IR_ENFORCE(
      attributes.find("filename") != attributes.end(),
          "'filename' Attribute is expected for ReadFileOp. ");
  std::string filename = attributes.at("filename").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for ReadFileOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for ReadFileOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_filename = pir::StrAttribute::get(pir::IrContext::Instance(), filename);
  argument.AddAttribute("filename", attr_filename);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ReadFileInferMeta(filename, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReadFileOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ReadFileOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("filename")>0,
                 "filename does not exist.");
  IR_ENFORCE(attributes.at("filename").isa<pir::StrAttribute>(),
                 "Type of attribute: filename is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ReadFileOp.";
}

void ReadFileOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReadFileInferMeta);
  fn(infer_meta);
}

phi::DataType ReadFileOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReadFileOp";
  


  return expected_kernel_dtype;
}

const char *RecvV2Op::attributes_name[6] = { "out_shape", "dtype", "peer", "ring_id", "use_calc_stream", "dynamic_shape" };

OpInfoTuple RecvV2Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("out_shape", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("peer", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dynamic_shape", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RecvV2InferMeta", {"ring_id", "dynamic_shape", "peer", "out_shape", "dtype"}, "recv_v2", {"ring_id", "dynamic_shape", "peer", "out_shape", "dtype", "use_calc_stream"}, {"dtype"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "recv_v2");
}

void RecvV2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int>& out_shape, phi::DataType dtype, int peer, int ring_id, bool use_calc_stream, bool dynamic_shape) {
  VLOG(4) << "Start build RecvV2Op";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_out_shape;
  for (size_t i = 0; i < static_cast<size_t>(out_shape.size()); i++) {
      pir::Attribute attr_out_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), out_shape[i]);

    vec_out_shape.push_back(attr_out_shape);
  }
  pir::Attribute attr_out_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_out_shape);
  argument.AddAttribute("out_shape", attr_out_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_peer = pir::Int32Attribute::get(pir::IrContext::Instance(), peer);
  argument.AddAttribute("peer", attr_peer);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_dynamic_shape = pir::BoolAttribute::get(pir::IrContext::Instance(), dynamic_shape);
  argument.AddAttribute("dynamic_shape", attr_dynamic_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RecvV2InferMeta(ring_id, dynamic_shape, peer, out_shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RecvV2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RecvV2Op";


  IR_ENFORCE(
      attributes.find("out_shape") != attributes.end(),
          "'out_shape' Attribute is expected for RecvV2Op. ");
  std::vector<int> out_shape;
  for (size_t i = 0; i < attributes.at("out_shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    out_shape.push_back(attributes.at("out_shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for RecvV2Op. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("peer") != attributes.end(),
          "'peer' Attribute is expected for RecvV2Op. ");
  int peer = attributes.at("peer").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for RecvV2Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for RecvV2Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dynamic_shape") != attributes.end(),
          "'dynamic_shape' Attribute is expected for RecvV2Op. ");
  bool dynamic_shape = attributes.at("dynamic_shape").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_out_shape;
  for (size_t i = 0; i < static_cast<size_t>(out_shape.size()); i++) {
      pir::Attribute attr_out_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), out_shape[i]);

    vec_out_shape.push_back(attr_out_shape);
  }
  pir::Attribute attr_out_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_out_shape);
  argument.AddAttribute("out_shape", attr_out_shape);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_peer = pir::Int32Attribute::get(pir::IrContext::Instance(), peer);
  argument.AddAttribute("peer", attr_peer);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_dynamic_shape = pir::BoolAttribute::get(pir::IrContext::Instance(), dynamic_shape);
  argument.AddAttribute("dynamic_shape", attr_dynamic_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RecvV2InferMeta(ring_id, dynamic_shape, peer, out_shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RecvV2Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RecvV2Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("out_shape")>0,
                 "out_shape does not exist.");
  IR_ENFORCE(attributes.at("out_shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: out_shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("out_shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("out_shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: out_shape is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("peer")>0,
                 "peer does not exist.");
  IR_ENFORCE(attributes.at("peer").isa<pir::Int32Attribute>(),
                 "Type of attribute: peer is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dynamic_shape")>0,
                 "dynamic_shape does not exist.");
  IR_ENFORCE(attributes.at("dynamic_shape").isa<pir::BoolAttribute>(),
                 "Type of attribute: dynamic_shape is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RecvV2Op.";
}

void RecvV2Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RecvV2InferMeta);
  fn(infer_meta);
}

phi::DataType RecvV2Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RecvV2Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple RemainderOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "remainder", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "remainder");
}

void RemainderOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build RemainderOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RemainderOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RemainderOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RemainderOp.";
}

void RemainderOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType RemainderOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RemainderOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

OpInfoTuple Remainder_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "remainder", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "remainder");
}

void Remainder_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Remainder_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Remainder_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Remainder_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Remainder_Op.";
}

void Remainder_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType Remainder_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Remainder_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

const char *RepeatInterleaveOp::attributes_name[2] = { "repeats", "axis" };

OpInfoTuple RepeatInterleaveOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("repeats", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RepeatInterleaveInferMeta", {"x", "repeats", "axis"}, "repeat_interleave", {"x", "repeats", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "repeat_interleave");
}

void RepeatInterleaveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int repeats, int axis) {
  VLOG(4) << "Start build RepeatInterleaveOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_repeats = pir::Int32Attribute::get(pir::IrContext::Instance(), repeats);
  argument.AddAttribute("repeats", attr_repeats);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RepeatInterleaveInferMeta(meta_x, repeats, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RepeatInterleaveOp";


  IR_ENFORCE(
      attributes.find("repeats") != attributes.end(),
          "'repeats' Attribute is expected for RepeatInterleaveOp. ");
  int repeats = attributes.at("repeats").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RepeatInterleaveOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_repeats = pir::Int32Attribute::get(pir::IrContext::Instance(), repeats);
  argument.AddAttribute("repeats", attr_repeats);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RepeatInterleaveInferMeta(meta_x, repeats, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RepeatInterleaveOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("repeats")>0,
                 "repeats does not exist.");
  IR_ENFORCE(attributes.at("repeats").isa<pir::Int32Attribute>(),
                 "Type of attribute: repeats is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RepeatInterleaveOp.";
}

void RepeatInterleaveOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RepeatInterleaveInferMeta);
  fn(infer_meta);
}

phi::DataType RepeatInterleaveOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RepeatInterleaveOp";
  


  return expected_kernel_dtype;
}

const char *RepeatInterleaveWithTensorIndexOp::attributes_name[1] = { "axis" };

OpInfoTuple RepeatInterleaveWithTensorIndexOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("repeats", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RepeatInterleaveWithTensorIndexInferMeta", {"x", "repeats", "axis"}, "repeat_interleave_with_tensor_index", {"x", "repeats", "axis"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "repeat_interleave_with_tensor_index");
}

void RepeatInterleaveWithTensorIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value repeats_, int axis) {
  VLOG(4) << "Start build RepeatInterleaveWithTensorIndexOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeats_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType repeats = repeats_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)repeats;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_repeats";
  paddle::dialect::IrTensor ir_tensor_repeats(paddle::dialect::TransToPhiDataType(repeats.dtype()),
                                                      repeats.dims(),
                                                      repeats.data_layout(),
                                                      repeats.lod(),
                                                      repeats.offset());
  VLOG(4) << "Builder construction  meta_repeats";
  paddle::dialect::IrMetaTensor meta_repeats(&ir_tensor_repeats);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RepeatInterleaveWithTensorIndexInferMeta(meta_x, meta_repeats, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveWithTensorIndexOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value repeats_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RepeatInterleaveWithTensorIndexOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for RepeatInterleaveWithTensorIndexOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeats_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType repeats = repeats_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)repeats;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_repeats";
  paddle::dialect::IrTensor ir_tensor_repeats(paddle::dialect::TransToPhiDataType(repeats.dtype()),
                                                      repeats.dims(),
                                                      repeats.data_layout(),
                                                      repeats.lod(),
                                                      repeats.offset());
  VLOG(4) << "Builder construction  meta_repeats";
  paddle::dialect::IrMetaTensor meta_repeats(&ir_tensor_repeats);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RepeatInterleaveWithTensorIndexInferMeta(meta_x, meta_repeats, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RepeatInterleaveWithTensorIndexOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RepeatInterleaveWithTensorIndexOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RepeatInterleaveWithTensorIndexOp.";
}

void RepeatInterleaveWithTensorIndexOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RepeatInterleaveWithTensorIndexInferMeta);
  fn(infer_meta);
}

phi::DataType RepeatInterleaveWithTensorIndexOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RepeatInterleaveWithTensorIndexOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ReshapeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReshapeWithXShapeInferMeta", {"x", "shape"}, "reshape", {"x", "shape"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reshape");
}

void ReshapeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& shape) {
  VLOG(4) << "Start build ReshapeOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::ReshapeWithXShapeInferMeta(meta_x, shape, &meta_out, &meta_xshape, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ReshapeOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for ReshapeOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::ReshapeWithXShapeInferMeta(meta_x, shape, &meta_out, &meta_xshape, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value shape_) {
  VLOG(4) << "Start build ReshapeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::ReshapeWithXShapeInferMeta(meta_x, shape, &meta_out, &meta_xshape, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ReshapeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ReshapeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: ReshapeOp.";
}

void ReshapeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReshapeWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType ReshapeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ReshapeOp";
  


  return expected_kernel_dtype;
}

bool ReshapeOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: ReshapeOp";
  return ReshapeOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple Reshape_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("xshape", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ReshapeWithXShapeInferMeta", {"x", "shape"}, "reshape", {"x", "shape"}, {}, {}, {{"out", "x"}}, {{"out", "x"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "reshape");
}

void Reshape_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& shape) {
  VLOG(4) << "Start build Reshape_Op";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::ReshapeWithXShapeInferMeta(meta_x, shape, &meta_out, &meta_xshape, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Reshape_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Reshape_Op";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for Reshape_Op. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::ReshapeWithXShapeInferMeta(meta_x, shape, &meta_out, &meta_xshape, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Reshape_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value shape_) {
  VLOG(4) << "Start build Reshape_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, shape_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_xshape;
  paddle::dialect::IrMetaTensor meta_xshape(&dense_xshape);

  phi::ReshapeWithXShapeInferMeta(meta_x, shape, &meta_out, &meta_xshape, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type xshape_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_xshape.dtype()), dense_xshape.dims(), dense_xshape.layout(), dense_xshape.lod(), dense_xshape.offset());
  argument_outputs.push_back(xshape_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Reshape_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Reshape_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: Reshape_Op.";
}

void Reshape_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ReshapeWithXShapeInferMeta);
  fn(infer_meta);
}

phi::DataType Reshape_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Reshape_Op";
  


  return expected_kernel_dtype;
}

bool Reshape_Op::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: Reshape_Op";
  return Reshape_OpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *RnnOp::attributes_name[8] = { "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test" };

OpInfoTuple RnnOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("pre_state", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true), paddle::dialect::OpInputInfo("weight_list", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true), paddle::dialect::OpInputInfo("sequence_length", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dropout_state_in", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_bidirec", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("input_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("hidden_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("num_layers", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dropout_state_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("state", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("reserve", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RnnInferMeta", {"x", "pre_state", "weight_list", "sequence_length", "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test"}, "rnn", {"x", "pre_state", "weight_list", "sequence_length", "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rnn");
}

void RnnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pre_state_, pir::Value weight_list_, pir::Value sequence_length_, pir::Value dropout_state_in_, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, const std::string& mode, int seed, bool is_test) {
  VLOG(4) << "Start build RnnOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pre_state_, weight_list_, sequence_length_, dropout_state_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_prob);
  argument.AddAttribute("dropout_prob", attr_dropout_prob);
  pir::Attribute attr_is_bidirec = pir::BoolAttribute::get(pir::IrContext::Instance(), is_bidirec);
  argument.AddAttribute("is_bidirec", attr_is_bidirec);
  pir::Attribute attr_input_size = pir::Int32Attribute::get(pir::IrContext::Instance(), input_size);
  argument.AddAttribute("input_size", attr_input_size);
  pir::Attribute attr_hidden_size = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_size);
  argument.AddAttribute("hidden_size", attr_hidden_size);
  pir::Attribute attr_num_layers = pir::Int32Attribute::get(pir::IrContext::Instance(), num_layers);
  argument.AddAttribute("num_layers", attr_num_layers);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType pre_state = pre_state_.type().dyn_cast<pir::VectorType>(); (void)pre_state;
  pir::VectorType weight_list = weight_list_.type().dyn_cast<pir::VectorType>(); (void)weight_list;
  paddle::dialect::DenseTensorType dropout_state_in = dropout_state_in_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_state_in;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_ir_tensor_pre_state.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state;
  for (size_t i=0; i < vec_ir_tensor_pre_state.size(); i++) {
    vec_meta_pre_state.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_state[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state.size()); i++) {
    meta_pre_state.push_back(&vec_meta_pre_state[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_weight_list;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_ir_tensor_weight_list.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list;
  for (size_t i=0; i < vec_ir_tensor_weight_list.size(); i++) {
    vec_meta_weight_list.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_weight_list[i]));
  }

  std::vector<const phi::MetaTensor*> meta_weight_list;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list.size()); i++) {
    meta_weight_list.push_back(&vec_meta_weight_list[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_sequence_length;
  paddle::dialect::IrTensor ir_tensor_sequence_length;
  if (sequence_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sequence_length = sequence_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sequence_length";
    ir_tensor_sequence_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sequence_length.dtype()),
                                                        sequence_length.dims(),
                                                        sequence_length.data_layout(),
                                                        sequence_length.lod(),
                                                        sequence_length.offset());
    VLOG(4) << "Builder construction  meta_sequence_length";
    meta_sequence_length = paddle::dialect::IrMetaTensor(&ir_tensor_sequence_length);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dropout_state_out;
  paddle::dialect::IrMetaTensor meta_dropout_state_out(&dense_dropout_state_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_state((pre_state.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_meta_state.push_back(paddle::dialect::IrMetaTensor(&vec_dense_state[i]));
  }
  std::vector<phi::MetaTensor*> meta_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_state.size()); i++) {
    meta_state.push_back(&vec_meta_state[i]);
  }
  paddle::dialect::IrTensor dense_reserve;
  paddle::dialect::IrMetaTensor meta_reserve(&dense_reserve);

  phi::RnnInferMeta(meta_x, meta_pre_state, meta_weight_list, meta_sequence_length, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, &meta_out, &meta_dropout_state_out, meta_state, &meta_reserve);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dropout_state_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_state_out.dtype()), dense_dropout_state_out.dims(), dense_dropout_state_out.layout(), dense_dropout_state_out.lod(), dense_dropout_state_out.offset());
  argument_outputs.push_back(dropout_state_out_dense_tensor_type);

  std::vector<pir::Type> state_types;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    state_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_state[i].dtype()), vec_dense_state[i].dims(), vec_dense_state[i].layout(), vec_dense_state[i].lod(), vec_dense_state[i].offset()));
  }
  pir::Type state_vector_type = pir::VectorType::get(pir::IrContext::Instance(), state_types);
  argument_outputs.push_back(state_vector_type);

  pir::Type reserve_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve.dtype()), dense_reserve.dims(), dense_reserve.layout(), dense_reserve.lod(), dense_reserve.offset());
  argument_outputs.push_back(reserve_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RnnOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pre_state_, pir::Value weight_list_, pir::Value sequence_length_, pir::Value dropout_state_in_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RnnOp";


  IR_ENFORCE(
      attributes.find("dropout_prob") != attributes.end(),
          "'dropout_prob' Attribute is expected for RnnOp. ");
  float dropout_prob = attributes.at("dropout_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_bidirec") != attributes.end(),
          "'is_bidirec' Attribute is expected for RnnOp. ");
  bool is_bidirec = attributes.at("is_bidirec").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("input_size") != attributes.end(),
          "'input_size' Attribute is expected for RnnOp. ");
  int input_size = attributes.at("input_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("hidden_size") != attributes.end(),
          "'hidden_size' Attribute is expected for RnnOp. ");
  int hidden_size = attributes.at("hidden_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("num_layers") != attributes.end(),
          "'num_layers' Attribute is expected for RnnOp. ");
  int num_layers = attributes.at("num_layers").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for RnnOp. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for RnnOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for RnnOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pre_state_, weight_list_, sequence_length_, dropout_state_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_prob);
  argument.AddAttribute("dropout_prob", attr_dropout_prob);
  pir::Attribute attr_is_bidirec = pir::BoolAttribute::get(pir::IrContext::Instance(), is_bidirec);
  argument.AddAttribute("is_bidirec", attr_is_bidirec);
  pir::Attribute attr_input_size = pir::Int32Attribute::get(pir::IrContext::Instance(), input_size);
  argument.AddAttribute("input_size", attr_input_size);
  pir::Attribute attr_hidden_size = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_size);
  argument.AddAttribute("hidden_size", attr_hidden_size);
  pir::Attribute attr_num_layers = pir::Int32Attribute::get(pir::IrContext::Instance(), num_layers);
  argument.AddAttribute("num_layers", attr_num_layers);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType pre_state = pre_state_.type().dyn_cast<pir::VectorType>(); (void)pre_state;
  pir::VectorType weight_list = weight_list_.type().dyn_cast<pir::VectorType>(); (void)weight_list;
  paddle::dialect::DenseTensorType dropout_state_in = dropout_state_in_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_state_in;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_ir_tensor_pre_state.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state;
  for (size_t i=0; i < vec_ir_tensor_pre_state.size(); i++) {
    vec_meta_pre_state.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_state[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state.size()); i++) {
    meta_pre_state.push_back(&vec_meta_pre_state[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_weight_list;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_ir_tensor_weight_list.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list;
  for (size_t i=0; i < vec_ir_tensor_weight_list.size(); i++) {
    vec_meta_weight_list.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_weight_list[i]));
  }

  std::vector<const phi::MetaTensor*> meta_weight_list;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list.size()); i++) {
    meta_weight_list.push_back(&vec_meta_weight_list[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_sequence_length;
  paddle::dialect::IrTensor ir_tensor_sequence_length;
  if (sequence_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sequence_length = sequence_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sequence_length";
    ir_tensor_sequence_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sequence_length.dtype()),
                                                        sequence_length.dims(),
                                                        sequence_length.data_layout(),
                                                        sequence_length.lod(),
                                                        sequence_length.offset());
    VLOG(4) << "Builder construction  meta_sequence_length";
    meta_sequence_length = paddle::dialect::IrMetaTensor(&ir_tensor_sequence_length);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dropout_state_out;
  paddle::dialect::IrMetaTensor meta_dropout_state_out(&dense_dropout_state_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_state((pre_state.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_meta_state.push_back(paddle::dialect::IrMetaTensor(&vec_dense_state[i]));
  }
  std::vector<phi::MetaTensor*> meta_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_state.size()); i++) {
    meta_state.push_back(&vec_meta_state[i]);
  }
  paddle::dialect::IrTensor dense_reserve;
  paddle::dialect::IrMetaTensor meta_reserve(&dense_reserve);

  phi::RnnInferMeta(meta_x, meta_pre_state, meta_weight_list, meta_sequence_length, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, &meta_out, &meta_dropout_state_out, meta_state, &meta_reserve);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dropout_state_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_state_out.dtype()), dense_dropout_state_out.dims(), dense_dropout_state_out.layout(), dense_dropout_state_out.lod(), dense_dropout_state_out.offset());
  argument_outputs.push_back(dropout_state_out_dense_tensor_type);

  std::vector<pir::Type> state_types;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    state_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_state[i].dtype()), vec_dense_state[i].dims(), vec_dense_state[i].layout(), vec_dense_state[i].lod(), vec_dense_state[i].offset()));
  }
  pir::Type state_vector_type = pir::VectorType::get(pir::IrContext::Instance(), state_types);
  argument_outputs.push_back(state_vector_type);

  pir::Type reserve_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve.dtype()), dense_reserve.dims(), dense_reserve.layout(), dense_reserve.lod(), dense_reserve.offset());
  argument_outputs.push_back(reserve_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RnnOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RnnOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dropout_prob")>0,
                 "dropout_prob does not exist.");
  IR_ENFORCE(attributes.at("dropout_prob").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_prob is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_bidirec")>0,
                 "is_bidirec does not exist.");
  IR_ENFORCE(attributes.at("is_bidirec").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_bidirec is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("input_size")>0,
                 "input_size does not exist.");
  IR_ENFORCE(attributes.at("input_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: input_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("hidden_size")>0,
                 "hidden_size does not exist.");
  IR_ENFORCE(attributes.at("hidden_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: hidden_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("num_layers")>0,
                 "num_layers does not exist.");
  IR_ENFORCE(attributes.at("num_layers").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_layers is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  auto output_2_type = (*this)->result(2).type();
  if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  else {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: RnnOp.";
}

void RnnOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RnnInferMeta);
  fn(infer_meta);
}

phi::DataType RnnOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RnnOp";
  


  return expected_kernel_dtype;
}

const char *Rnn_Op::attributes_name[8] = { "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test" };

OpInfoTuple Rnn_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("pre_state", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true), paddle::dialect::OpInputInfo("weight_list", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, true), paddle::dialect::OpInputInfo("sequence_length", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dropout_state_in", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dropout_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_bidirec", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("input_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("hidden_size", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("num_layers", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("mode", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dropout_state_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("state", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("reserve", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RnnInferMeta", {"x", "pre_state", "weight_list", "sequence_length", "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test"}, "rnn", {"x", "pre_state", "weight_list", "sequence_length", "dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers", "mode", "seed", "is_test"}, {"x"}, {}, {}, {{"dropout_state_out", "dropout_state_in"}});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rnn");
}

void Rnn_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pre_state_, pir::Value weight_list_, pir::Value sequence_length_, pir::Value dropout_state_in_, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, const std::string& mode, int seed, bool is_test) {
  VLOG(4) << "Start build Rnn_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pre_state_, weight_list_, sequence_length_, dropout_state_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_prob);
  argument.AddAttribute("dropout_prob", attr_dropout_prob);
  pir::Attribute attr_is_bidirec = pir::BoolAttribute::get(pir::IrContext::Instance(), is_bidirec);
  argument.AddAttribute("is_bidirec", attr_is_bidirec);
  pir::Attribute attr_input_size = pir::Int32Attribute::get(pir::IrContext::Instance(), input_size);
  argument.AddAttribute("input_size", attr_input_size);
  pir::Attribute attr_hidden_size = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_size);
  argument.AddAttribute("hidden_size", attr_hidden_size);
  pir::Attribute attr_num_layers = pir::Int32Attribute::get(pir::IrContext::Instance(), num_layers);
  argument.AddAttribute("num_layers", attr_num_layers);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType pre_state = pre_state_.type().dyn_cast<pir::VectorType>(); (void)pre_state;
  pir::VectorType weight_list = weight_list_.type().dyn_cast<pir::VectorType>(); (void)weight_list;
  paddle::dialect::DenseTensorType dropout_state_in = dropout_state_in_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_state_in;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_ir_tensor_pre_state.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state;
  for (size_t i=0; i < vec_ir_tensor_pre_state.size(); i++) {
    vec_meta_pre_state.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_state[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state.size()); i++) {
    meta_pre_state.push_back(&vec_meta_pre_state[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_weight_list;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_ir_tensor_weight_list.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list;
  for (size_t i=0; i < vec_ir_tensor_weight_list.size(); i++) {
    vec_meta_weight_list.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_weight_list[i]));
  }

  std::vector<const phi::MetaTensor*> meta_weight_list;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list.size()); i++) {
    meta_weight_list.push_back(&vec_meta_weight_list[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_sequence_length;
  paddle::dialect::IrTensor ir_tensor_sequence_length;
  if (sequence_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sequence_length = sequence_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sequence_length";
    ir_tensor_sequence_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sequence_length.dtype()),
                                                        sequence_length.dims(),
                                                        sequence_length.data_layout(),
                                                        sequence_length.lod(),
                                                        sequence_length.offset());
    VLOG(4) << "Builder construction  meta_sequence_length";
    meta_sequence_length = paddle::dialect::IrMetaTensor(&ir_tensor_sequence_length);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dropout_state_out;
  paddle::dialect::IrMetaTensor meta_dropout_state_out(&dense_dropout_state_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_state((pre_state.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_meta_state.push_back(paddle::dialect::IrMetaTensor(&vec_dense_state[i]));
  }
  std::vector<phi::MetaTensor*> meta_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_state.size()); i++) {
    meta_state.push_back(&vec_meta_state[i]);
  }
  paddle::dialect::IrTensor dense_reserve;
  paddle::dialect::IrMetaTensor meta_reserve(&dense_reserve);

  phi::RnnInferMeta(meta_x, meta_pre_state, meta_weight_list, meta_sequence_length, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, &meta_out, &meta_dropout_state_out, meta_state, &meta_reserve);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dropout_state_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_state_out.dtype()), dense_dropout_state_out.dims(), dense_dropout_state_out.layout(), dense_dropout_state_out.lod(), dense_dropout_state_out.offset());
  argument_outputs.push_back(dropout_state_out_dense_tensor_type);

  std::vector<pir::Type> state_types;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    state_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_state[i].dtype()), vec_dense_state[i].dims(), vec_dense_state[i].layout(), vec_dense_state[i].lod(), vec_dense_state[i].offset()));
  }
  pir::Type state_vector_type = pir::VectorType::get(pir::IrContext::Instance(), state_types);
  argument_outputs.push_back(state_vector_type);

  pir::Type reserve_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve.dtype()), dense_reserve.dims(), dense_reserve.layout(), dense_reserve.lod(), dense_reserve.offset());
  argument_outputs.push_back(reserve_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rnn_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value pre_state_, pir::Value weight_list_, pir::Value sequence_length_, pir::Value dropout_state_in_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Rnn_Op";


  IR_ENFORCE(
      attributes.find("dropout_prob") != attributes.end(),
          "'dropout_prob' Attribute is expected for Rnn_Op. ");
  float dropout_prob = attributes.at("dropout_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_bidirec") != attributes.end(),
          "'is_bidirec' Attribute is expected for Rnn_Op. ");
  bool is_bidirec = attributes.at("is_bidirec").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("input_size") != attributes.end(),
          "'input_size' Attribute is expected for Rnn_Op. ");
  int input_size = attributes.at("input_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("hidden_size") != attributes.end(),
          "'hidden_size' Attribute is expected for Rnn_Op. ");
  int hidden_size = attributes.at("hidden_size").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("num_layers") != attributes.end(),
          "'num_layers' Attribute is expected for Rnn_Op. ");
  int num_layers = attributes.at("num_layers").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("mode") != attributes.end(),
          "'mode' Attribute is expected for Rnn_Op. ");
  std::string mode = attributes.at("mode").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for Rnn_Op. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for Rnn_Op. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, pre_state_, weight_list_, sequence_length_, dropout_state_in_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dropout_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_prob);
  argument.AddAttribute("dropout_prob", attr_dropout_prob);
  pir::Attribute attr_is_bidirec = pir::BoolAttribute::get(pir::IrContext::Instance(), is_bidirec);
  argument.AddAttribute("is_bidirec", attr_is_bidirec);
  pir::Attribute attr_input_size = pir::Int32Attribute::get(pir::IrContext::Instance(), input_size);
  argument.AddAttribute("input_size", attr_input_size);
  pir::Attribute attr_hidden_size = pir::Int32Attribute::get(pir::IrContext::Instance(), hidden_size);
  argument.AddAttribute("hidden_size", attr_hidden_size);
  pir::Attribute attr_num_layers = pir::Int32Attribute::get(pir::IrContext::Instance(), num_layers);
  argument.AddAttribute("num_layers", attr_num_layers);
  pir::Attribute attr_mode = pir::StrAttribute::get(pir::IrContext::Instance(), mode);
  argument.AddAttribute("mode", attr_mode);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  pir::VectorType pre_state = pre_state_.type().dyn_cast<pir::VectorType>(); (void)pre_state;
  pir::VectorType weight_list = weight_list_.type().dyn_cast<pir::VectorType>(); (void)weight_list;
  paddle::dialect::DenseTensorType dropout_state_in = dropout_state_in_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)dropout_state_in;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_pre_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_ir_tensor_pre_state.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     pre_state[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_pre_state;
  for (size_t i=0; i < vec_ir_tensor_pre_state.size(); i++) {
    vec_meta_pre_state.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_pre_state[i]));
  }

  std::vector<const phi::MetaTensor*> meta_pre_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_pre_state.size()); i++) {
    meta_pre_state.push_back(&vec_meta_pre_state[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_weight_list;
  for (size_t i=0; i < static_cast<size_t>(weight_list.size()); i++) {
    vec_ir_tensor_weight_list.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     weight_list[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_weight_list;
  for (size_t i=0; i < vec_ir_tensor_weight_list.size(); i++) {
    vec_meta_weight_list.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_weight_list[i]));
  }

  std::vector<const phi::MetaTensor*> meta_weight_list;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_weight_list.size()); i++) {
    meta_weight_list.push_back(&vec_meta_weight_list[i]);
  }
 
  paddle::dialect::IrMetaTensor meta_sequence_length;
  paddle::dialect::IrTensor ir_tensor_sequence_length;
  if (sequence_length_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sequence_length = sequence_length_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sequence_length";
    ir_tensor_sequence_length = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sequence_length.dtype()),
                                                        sequence_length.dims(),
                                                        sequence_length.data_layout(),
                                                        sequence_length.lod(),
                                                        sequence_length.offset());
    VLOG(4) << "Builder construction  meta_sequence_length";
    meta_sequence_length = paddle::dialect::IrMetaTensor(&ir_tensor_sequence_length);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dropout_state_out;
  paddle::dialect::IrMetaTensor meta_dropout_state_out(&dense_dropout_state_out);
  std::vector<paddle::dialect::IrTensor> vec_dense_state((pre_state.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_state;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    vec_meta_state.push_back(paddle::dialect::IrMetaTensor(&vec_dense_state[i]));
  }
  std::vector<phi::MetaTensor*> meta_state;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_state.size()); i++) {
    meta_state.push_back(&vec_meta_state[i]);
  }
  paddle::dialect::IrTensor dense_reserve;
  paddle::dialect::IrMetaTensor meta_reserve(&dense_reserve);

  phi::RnnInferMeta(meta_x, meta_pre_state, meta_weight_list, meta_sequence_length, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, &meta_out, &meta_dropout_state_out, meta_state, &meta_reserve);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dropout_state_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_state_out.dtype()), dense_dropout_state_out.dims(), dense_dropout_state_out.layout(), dense_dropout_state_out.lod(), dense_dropout_state_out.offset());
  argument_outputs.push_back(dropout_state_out_dense_tensor_type);

  std::vector<pir::Type> state_types;
  for (size_t i=0; i < static_cast<size_t>(pre_state.size()); i++) {
    state_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_state[i].dtype()), vec_dense_state[i].dims(), vec_dense_state[i].layout(), vec_dense_state[i].lod(), vec_dense_state[i].offset()));
  }
  pir::Type state_vector_type = pir::VectorType::get(pir::IrContext::Instance(), state_types);
  argument_outputs.push_back(state_vector_type);

  pir::Type reserve_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve.dtype()), dense_reserve.dims(), dense_reserve.layout(), dense_reserve.lod(), dense_reserve.offset());
  argument_outputs.push_back(reserve_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Rnn_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Rnn_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dropout_prob")>0,
                 "dropout_prob does not exist.");
  IR_ENFORCE(attributes.at("dropout_prob").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_prob is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_bidirec")>0,
                 "is_bidirec does not exist.");
  IR_ENFORCE(attributes.at("is_bidirec").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_bidirec is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("input_size")>0,
                 "input_size does not exist.");
  IR_ENFORCE(attributes.at("input_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: input_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("hidden_size")>0,
                 "hidden_size does not exist.");
  IR_ENFORCE(attributes.at("hidden_size").isa<pir::Int32Attribute>(),
                 "Type of attribute: hidden_size is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("num_layers")>0,
                 "num_layers does not exist.");
  IR_ENFORCE(attributes.at("num_layers").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_layers is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("mode")>0,
                 "mode does not exist.");
  IR_ENFORCE(attributes.at("mode").isa<pir::StrAttribute>(),
                 "Type of attribute: mode is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  auto output_2_type = (*this)->result(2).type();
  if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  else {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: Rnn_Op.";
}

void Rnn_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RnnInferMeta);
  fn(infer_meta);
}

phi::DataType Rnn_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Rnn_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple RowConvOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("filter", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RowConvInferMeta", {"x", "filter"}, "row_conv", {"x", "filter"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "row_conv");
}

void RowConvOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value filter_) {
  VLOG(4) << "Start build RowConvOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, filter_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType filter = filter_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)filter;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_filter";
  paddle::dialect::IrTensor ir_tensor_filter(paddle::dialect::TransToPhiDataType(filter.dtype()),
                                                      filter.dims(),
                                                      filter.data_layout(),
                                                      filter.lod(),
                                                      filter.offset());
  VLOG(4) << "Builder construction  meta_filter";
  paddle::dialect::IrMetaTensor meta_filter(&ir_tensor_filter);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::RowConvInferMeta(meta_x, meta_filter, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RowConvOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RowConvOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: RowConvOp.";
}

void RowConvOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RowConvInferMeta);
  fn(infer_meta);
}

phi::DataType RowConvOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RowConvOp";
  


  return expected_kernel_dtype;
}

const char *RreluOp::attributes_name[3] = { "lower", "upper", "is_test" };

OpInfoTuple RreluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("lower", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("upper", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("noise", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("RReluInferMeta", {"x", "lower", "upper", "is_test"}, "rrelu", {"x", "lower", "upper", "is_test"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "rrelu");
}

void RreluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float lower, float upper, bool is_test) {
  VLOG(4) << "Start build RreluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lower = pir::FloatAttribute::get(pir::IrContext::Instance(), lower);
  argument.AddAttribute("lower", attr_lower);
  pir::Attribute attr_upper = pir::FloatAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_noise;
  paddle::dialect::IrMetaTensor meta_noise(&dense_noise);

  phi::RReluInferMeta(meta_x, lower, upper, is_test, &meta_out, &meta_noise);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type noise_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_noise.dtype()), dense_noise.dims(), dense_noise.layout(), dense_noise.lod(), dense_noise.offset());
  argument_outputs.push_back(noise_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RreluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build RreluOp";


  IR_ENFORCE(
      attributes.find("lower") != attributes.end(),
          "'lower' Attribute is expected for RreluOp. ");
  float lower = attributes.at("lower").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("upper") != attributes.end(),
          "'upper' Attribute is expected for RreluOp. ");
  float upper = attributes.at("upper").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for RreluOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_lower = pir::FloatAttribute::get(pir::IrContext::Instance(), lower);
  argument.AddAttribute("lower", attr_lower);
  pir::Attribute attr_upper = pir::FloatAttribute::get(pir::IrContext::Instance(), upper);
  argument.AddAttribute("upper", attr_upper);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_noise;
  paddle::dialect::IrMetaTensor meta_noise(&dense_noise);

  phi::RReluInferMeta(meta_x, lower, upper, is_test, &meta_out, &meta_noise);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type noise_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_noise.dtype()), dense_noise.dims(), dense_noise.layout(), dense_noise.lod(), dense_noise.offset());
  argument_outputs.push_back(noise_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void RreluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: RreluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("lower")>0,
                 "lower does not exist.");
  IR_ENFORCE(attributes.at("lower").isa<pir::FloatAttribute>(),
                 "Type of attribute: lower is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("upper")>0,
                 "upper does not exist.");
  IR_ENFORCE(attributes.at("upper").isa<pir::FloatAttribute>(),
                 "Type of attribute: upper is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: RreluOp.";
}

void RreluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::RReluInferMeta);
  fn(infer_meta);
}

phi::DataType RreluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: RreluOp";
  


  return expected_kernel_dtype;
}

const char *SaveCombineOp::attributes_name[4] = { "file_path", "overwrite", "save_as_fp16", "save_to_memory" };

OpInfoTuple SaveCombineOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("file_path", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("overwrite", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("save_as_fp16", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("save_to_memory", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "save_combine_tensor", {"x", "file_path", "overwrite", "save_as_fp16", "save_to_memory"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "save_combine");
}

void SaveCombineOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SaveCombineOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("file_path")>0,
                 "file_path does not exist.");
  IR_ENFORCE(attributes.at("file_path").isa<pir::StrAttribute>(),
                 "Type of attribute: file_path is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("overwrite")>0,
                 "overwrite does not exist.");
  IR_ENFORCE(attributes.at("overwrite").isa<pir::BoolAttribute>(),
                 "Type of attribute: overwrite is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("save_as_fp16")>0,
                 "save_as_fp16 does not exist.");
  IR_ENFORCE(attributes.at("save_as_fp16").isa<pir::BoolAttribute>(),
                 "Type of attribute: save_as_fp16 is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("save_to_memory")>0,
                 "save_to_memory does not exist.");
  IR_ENFORCE(attributes.at("save_to_memory").isa<pir::BoolAttribute>(),
                 "Type of attribute: save_to_memory is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  if (auto output_0_type = (*this)->result(0).type()) {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: SaveCombineOp.";
}

phi::DataType SaveCombineOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SaveCombineOp";
  


  return expected_kernel_dtype;
}

const char *SeedOp::attributes_name[4] = { "seed", "deterministic", "rng_name", "force_cpu" };

OpInfoTuple SeedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("deterministic", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rng_name", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("force_cpu", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SeedInferMeta", {"seed"}, "seed", {"seed", "deterministic", "rng_name", "force_cpu"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "seed");
}

void SeedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, int seed, bool deterministic, const std::string& rng_name, bool force_cpu) {
  VLOG(4) << "Start build SeedOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_deterministic = pir::BoolAttribute::get(pir::IrContext::Instance(), deterministic);
  argument.AddAttribute("deterministic", attr_deterministic);
  pir::Attribute attr_rng_name = pir::StrAttribute::get(pir::IrContext::Instance(), rng_name);
  argument.AddAttribute("rng_name", attr_rng_name);
  pir::Attribute attr_force_cpu = pir::BoolAttribute::get(pir::IrContext::Instance(), force_cpu);
  argument.AddAttribute("force_cpu", attr_force_cpu);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SeedInferMeta(seed, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SeedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SeedOp";


  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for SeedOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("deterministic") != attributes.end(),
          "'deterministic' Attribute is expected for SeedOp. ");
  bool deterministic = attributes.at("deterministic").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rng_name") != attributes.end(),
          "'rng_name' Attribute is expected for SeedOp. ");
  std::string rng_name = attributes.at("rng_name").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("force_cpu") != attributes.end(),
          "'force_cpu' Attribute is expected for SeedOp. ");
  bool force_cpu = attributes.at("force_cpu").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_deterministic = pir::BoolAttribute::get(pir::IrContext::Instance(), deterministic);
  argument.AddAttribute("deterministic", attr_deterministic);
  pir::Attribute attr_rng_name = pir::StrAttribute::get(pir::IrContext::Instance(), rng_name);
  argument.AddAttribute("rng_name", attr_rng_name);
  pir::Attribute attr_force_cpu = pir::BoolAttribute::get(pir::IrContext::Instance(), force_cpu);
  argument.AddAttribute("force_cpu", attr_force_cpu);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SeedInferMeta(seed, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SeedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SeedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("deterministic")>0,
                 "deterministic does not exist.");
  IR_ENFORCE(attributes.at("deterministic").isa<pir::BoolAttribute>(),
                 "Type of attribute: deterministic is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rng_name")>0,
                 "rng_name does not exist.");
  IR_ENFORCE(attributes.at("rng_name").isa<pir::StrAttribute>(),
                 "Type of attribute: rng_name is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("force_cpu")>0,
                 "force_cpu does not exist.");
  IR_ENFORCE(attributes.at("force_cpu").isa<pir::BoolAttribute>(),
                 "Type of attribute: force_cpu is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SeedOp.";
}

void SeedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SeedInferMeta);
  fn(infer_meta);
}

phi::DataType SeedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SeedOp";
  


  return expected_kernel_dtype;
}

const char *SendV2Op::attributes_name[4] = { "ring_id", "peer", "use_calc_stream", "dynamic_shape" };

OpInfoTuple SendV2Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("peer", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("use_calc_stream", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dynamic_shape", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = {  };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SendV2InferMeta", {"peer", "ring_id"}, "send_v2", {"x", "ring_id", "dynamic_shape", "peer", "use_calc_stream"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "send_v2");
}

void SendV2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int ring_id, int peer, bool use_calc_stream, bool dynamic_shape) {
  VLOG(4) << "Start build SendV2Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_peer = pir::Int32Attribute::get(pir::IrContext::Instance(), peer);
  argument.AddAttribute("peer", attr_peer);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_dynamic_shape = pir::BoolAttribute::get(pir::IrContext::Instance(), dynamic_shape);
  argument.AddAttribute("dynamic_shape", attr_dynamic_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  phi::SendV2InferMeta(peer, ring_id);

  std::vector<pir::Type> argument_outputs;  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendV2Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SendV2Op";


  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for SendV2Op. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("peer") != attributes.end(),
          "'peer' Attribute is expected for SendV2Op. ");
  int peer = attributes.at("peer").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("use_calc_stream") != attributes.end(),
          "'use_calc_stream' Attribute is expected for SendV2Op. ");
  bool use_calc_stream = attributes.at("use_calc_stream").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dynamic_shape") != attributes.end(),
          "'dynamic_shape' Attribute is expected for SendV2Op. ");
  bool dynamic_shape = attributes.at("dynamic_shape").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_peer = pir::Int32Attribute::get(pir::IrContext::Instance(), peer);
  argument.AddAttribute("peer", attr_peer);
  pir::Attribute attr_use_calc_stream = pir::BoolAttribute::get(pir::IrContext::Instance(), use_calc_stream);
  argument.AddAttribute("use_calc_stream", attr_use_calc_stream);
  pir::Attribute attr_dynamic_shape = pir::BoolAttribute::get(pir::IrContext::Instance(), dynamic_shape);
  argument.AddAttribute("dynamic_shape", attr_dynamic_shape);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  phi::SendV2InferMeta(peer, ring_id);

  std::vector<pir::Type> argument_outputs;  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SendV2Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SendV2Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("peer")>0,
                 "peer does not exist.");
  IR_ENFORCE(attributes.at("peer").isa<pir::Int32Attribute>(),
                 "Type of attribute: peer is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("use_calc_stream")>0,
                 "use_calc_stream does not exist.");
  IR_ENFORCE(attributes.at("use_calc_stream").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_calc_stream is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dynamic_shape")>0,
                 "dynamic_shape does not exist.");
  IR_ENFORCE(attributes.at("dynamic_shape").isa<pir::BoolAttribute>(),
                 "Type of attribute: dynamic_shape is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 0u,
                    "The size %d of outputs must be equal to 0.", output_size);
  // Outputs num is 0, not need to check outputs type.
  }
  VLOG(4) << "End Verifying for: SendV2Op.";
}

void SendV2Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SendV2InferMeta);
  fn(infer_meta);
}

phi::DataType SendV2Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SendV2Op";
  


  return expected_kernel_dtype;
}

const char *SetValueOp::attributes_name[5] = { "axes", "decrease_axes", "none_axes", "shape", "values" };

OpInfoTuple SetValueOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("steps", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("none_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("values", "pir::ArrayAttribute<paddle::dialect::ScalarAttribute>", "std::vector<Scalar>") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SetValueInferMeta", {"x"}, "set_value", {"x", "starts", "ends", "steps", "axes", "decrease_axes", "none_axes", "shape", "values"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "set_value");
}

void SetValueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, const std::vector<int64_t>& shape, std::vector<phi::Scalar> values) {
  VLOG(4) << "Start build SetValueOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int64Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SetValueOp";


  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("steps") != attributes.end(),
          "'steps' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> steps = attributes.at("steps").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axes") != attributes.end(),
          "'decrease_axes' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> decrease_axes;
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axes.push_back(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("none_axes") != attributes.end(),
          "'none_axes' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> none_axes;
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    none_axes.push_back(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for SetValueOp. ");
  std::vector<int64_t> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("values") != attributes.end(),
          "'values' Attribute is expected for SetValueOp. ");
  std::vector<phi::Scalar> values;
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    values.push_back(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<paddle::dialect::ScalarAttribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int64Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value starts_, pir::Value ends_, pir::Value steps_, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, const std::vector<int64_t>& shape, std::vector<phi::Scalar> values) {
  VLOG(4) << "Start build SetValueOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int64Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray steps;
  if (steps_.dyn_cast<pir::OpResult>() && steps_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    steps = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          steps_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (steps_.type().isa<pir::VectorType>()) {
    size_t steps_size = steps_.type().dyn_cast<pir::VectorType>().size();
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else if (steps_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim steps_dim = steps_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t steps_size = common::product(steps_dim);
    if (common::contain_unknown_dim(steps_dim)) {
      steps_size = 1;
    }
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SetValueOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("decrease_axes")>0,
                 "decrease_axes does not exist.");
  IR_ENFORCE(attributes.at("decrease_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: decrease_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: decrease_axes is not right.");
  }
  IR_ENFORCE(attributes.count("none_axes")>0,
                 "none_axes does not exist.");
  IR_ENFORCE(attributes.at("none_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: none_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: none_axes is not right.");
  }
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("values")>0,
                 "values does not exist.");
  IR_ENFORCE(attributes.at("values").isa<pir::ArrayAttribute>(),
                 "Type of attribute: values is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).isa<paddle::dialect::ScalarAttribute>(),
                   "Type of attribute: values is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SetValueOp.";
}

void SetValueOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SetValueInferMeta);
  fn(infer_meta);
}

phi::DataType SetValueOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SetValueOp";
  


  return expected_kernel_dtype;
}

const char *SetValue_Op::attributes_name[5] = { "axes", "decrease_axes", "none_axes", "shape", "values" };

OpInfoTuple SetValue_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("steps", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("none_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("values", "pir::ArrayAttribute<paddle::dialect::ScalarAttribute>", "std::vector<Scalar>") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SetValueInferMeta", {"x"}, "set_value", {"x", "starts", "ends", "steps", "axes", "decrease_axes", "none_axes", "shape", "values"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "set_value");
}

void SetValue_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, const std::vector<int64_t>& shape, std::vector<phi::Scalar> values) {
  VLOG(4) << "Start build SetValue_Op";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int64Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValue_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SetValue_Op";


  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("steps") != attributes.end(),
          "'steps' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> steps = attributes.at("steps").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axes") != attributes.end(),
          "'decrease_axes' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> decrease_axes;
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axes.push_back(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("none_axes") != attributes.end(),
          "'none_axes' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> none_axes;
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    none_axes.push_back(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for SetValue_Op. ");
  std::vector<int64_t> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("values") != attributes.end(),
          "'values' Attribute is expected for SetValue_Op. ");
  std::vector<phi::Scalar> values;
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    values.push_back(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<paddle::dialect::ScalarAttribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int64Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValue_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value starts_, pir::Value ends_, pir::Value steps_, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, const std::vector<int64_t>& shape, std::vector<phi::Scalar> values) {
  VLOG(4) << "Start build SetValue_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int64Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  std::vector<pir::Attribute> vec_values;
  for (size_t i = 0; i < static_cast<size_t>(values.size()); i++) {
      pir::Attribute attr_values = paddle::dialect::TransToIrAttribute(values[i], pir::IrContext::Instance());

    vec_values.push_back(attr_values);
  }
  pir::Attribute attr_values = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_values);
  argument.AddAttribute("values", attr_values);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray steps;
  if (steps_.dyn_cast<pir::OpResult>() && steps_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    steps = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          steps_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (steps_.type().isa<pir::VectorType>()) {
    size_t steps_size = steps_.type().dyn_cast<pir::VectorType>().size();
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else if (steps_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim steps_dim = steps_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t steps_size = common::product(steps_dim);
    if (common::contain_unknown_dim(steps_dim)) {
      steps_size = 1;
    }
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValue_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SetValue_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("decrease_axes")>0,
                 "decrease_axes does not exist.");
  IR_ENFORCE(attributes.at("decrease_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: decrease_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: decrease_axes is not right.");
  }
  IR_ENFORCE(attributes.count("none_axes")>0,
                 "none_axes does not exist.");
  IR_ENFORCE(attributes.at("none_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: none_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: none_axes is not right.");
  }
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("values")>0,
                 "values does not exist.");
  IR_ENFORCE(attributes.at("values").isa<pir::ArrayAttribute>(),
                 "Type of attribute: values is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("values").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("values").dyn_cast<pir::ArrayAttribute>().at(i).isa<paddle::dialect::ScalarAttribute>(),
                   "Type of attribute: values is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SetValue_Op.";
}

void SetValue_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SetValueInferMeta);
  fn(infer_meta);
}

phi::DataType SetValue_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SetValue_Op";
  


  return expected_kernel_dtype;
}

const char *SetValueWithTensorOp::attributes_name[3] = { "axes", "decrease_axes", "none_axes" };

OpInfoTuple SetValueWithTensorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("steps", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("none_axes", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SetValueInferMeta", {"x"}, "set_value_with_tensor", {"x", "values", "starts", "ends", "steps", "axes", "decrease_axes", "none_axes"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "set_value_with_tensor");
}

void SetValueWithTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value values_, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {
  VLOG(4) << "Start build SetValueWithTensorOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, values_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value values_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SetValueWithTensorOp";


  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SetValueWithTensorOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SetValueWithTensorOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("steps") != attributes.end(),
          "'steps' Attribute is expected for SetValueWithTensorOp. ");
  std::vector<int64_t> steps = attributes.at("steps").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SetValueWithTensorOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axes") != attributes.end(),
          "'decrease_axes' Attribute is expected for SetValueWithTensorOp. ");
  std::vector<int64_t> decrease_axes;
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axes.push_back(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("none_axes") != attributes.end(),
          "'none_axes' Attribute is expected for SetValueWithTensorOp. ");
  std::vector<int64_t> none_axes;
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    none_axes.push_back(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, values_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value values_, pir::Value starts_, pir::Value ends_, pir::Value steps_, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {
  VLOG(4) << "Start build SetValueWithTensorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, values_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray steps;
  if (steps_.dyn_cast<pir::OpResult>() && steps_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    steps = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          steps_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (steps_.type().isa<pir::VectorType>()) {
    size_t steps_size = steps_.type().dyn_cast<pir::VectorType>().size();
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else if (steps_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim steps_dim = steps_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t steps_size = common::product(steps_dim);
    if (common::contain_unknown_dim(steps_dim)) {
      steps_size = 1;
    }
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SetValueWithTensorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("decrease_axes")>0,
                 "decrease_axes does not exist.");
  IR_ENFORCE(attributes.at("decrease_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: decrease_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: decrease_axes is not right.");
  }
  IR_ENFORCE(attributes.count("none_axes")>0,
                 "none_axes does not exist.");
  IR_ENFORCE(attributes.at("none_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: none_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: none_axes is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SetValueWithTensorOp.";
}

void SetValueWithTensorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SetValueInferMeta);
  fn(infer_meta);
}

phi::DataType SetValueWithTensorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SetValueWithTensorOp";
  


  return expected_kernel_dtype;
}

const char *SetValueWithTensor_Op::attributes_name[3] = { "axes", "decrease_axes", "none_axes" };

OpInfoTuple SetValueWithTensor_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("values", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("steps", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("none_axes", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SetValueInferMeta", {"x"}, "set_value_with_tensor", {"x", "values", "starts", "ends", "steps", "axes", "decrease_axes", "none_axes"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "set_value_with_tensor");
}

void SetValueWithTensor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value values_, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {
  VLOG(4) << "Start build SetValueWithTensor_Op";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, values_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value values_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SetValueWithTensor_Op";


  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SetValueWithTensor_Op. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SetValueWithTensor_Op. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("steps") != attributes.end(),
          "'steps' Attribute is expected for SetValueWithTensor_Op. ");
  std::vector<int64_t> steps = attributes.at("steps").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SetValueWithTensor_Op. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axes") != attributes.end(),
          "'decrease_axes' Attribute is expected for SetValueWithTensor_Op. ");
  std::vector<int64_t> decrease_axes;
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axes.push_back(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("none_axes") != attributes.end(),
          "'none_axes' Attribute is expected for SetValueWithTensor_Op. ");
  std::vector<int64_t> none_axes;
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    none_axes.push_back(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: steps
  paddle::dialect::FullIntArrayOp full_steps_op = builder.Build<paddle::dialect::FullIntArrayOp>(steps, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult steps_ = full_steps_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, values_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensor_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value values_, pir::Value starts_, pir::Value ends_, pir::Value steps_, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes) {
  VLOG(4) << "Start build SetValueWithTensor_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, values_, starts_, ends_, steps_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_decrease_axes;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axes.size()); i++) {
      pir::Attribute attr_decrease_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axes[i]);

    vec_decrease_axes.push_back(attr_decrease_axes);
  }
  pir::Attribute attr_decrease_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axes);
  argument.AddAttribute("decrease_axes", attr_decrease_axes);
  std::vector<pir::Attribute> vec_none_axes;
  for (size_t i = 0; i < static_cast<size_t>(none_axes.size()); i++) {
      pir::Attribute attr_none_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), none_axes[i]);

    vec_none_axes.push_back(attr_none_axes);
  }
  pir::Attribute attr_none_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_none_axes);
  argument.AddAttribute("none_axes", attr_none_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType values = values_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)values;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray steps;
  if (steps_.dyn_cast<pir::OpResult>() && steps_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    steps = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          steps_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (steps_.type().isa<pir::VectorType>()) {
    size_t steps_size = steps_.type().dyn_cast<pir::VectorType>().size();
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else if (steps_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim steps_dim = steps_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t steps_size = common::product(steps_dim);
    if (common::contain_unknown_dim(steps_dim)) {
      steps_size = 1;
    }
    steps = std::move(phi::IntArray(std::vector<int64_t>(steps_size, -1)));
    steps.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SetValueInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SetValueWithTensor_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SetValueWithTensor_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto vec_type = (*this)->operand_source(4).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("decrease_axes")>0,
                 "decrease_axes does not exist.");
  IR_ENFORCE(attributes.at("decrease_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: decrease_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("decrease_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: decrease_axes is not right.");
  }
  IR_ENFORCE(attributes.count("none_axes")>0,
                 "none_axes does not exist.");
  IR_ENFORCE(attributes.at("none_axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: none_axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("none_axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: none_axes is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SetValueWithTensor_Op.";
}

void SetValueWithTensor_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SetValueInferMeta);
  fn(infer_meta);
}

phi::DataType SetValueWithTensor_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SetValueWithTensor_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple ShadowFeedOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "shadow_feed", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "shadow_feed");
}

void ShadowFeedOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ShadowFeedOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShadowFeedOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ShadowFeedOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ShadowFeedOp.";
}

void ShadowFeedOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ShadowFeedOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShadowFeedOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple ShareDataOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "share_data", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "share_data");
}

void ShareDataOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build ShareDataOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShareDataOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ShareDataOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ShareDataOp.";
}

void ShareDataOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType ShareDataOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShareDataOp";
  


  return expected_kernel_dtype;
}

const char *ShuffleBatchOp::attributes_name[1] = { "startup_seed" };

OpInfoTuple ShuffleBatchOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("seed", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("startup_seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("shuffle_idx", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("seed_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ShuffleBatchInferMeta", {"x", "seed", "startup_seed"}, "shuffle_batch", {"x", "seed", "startup_seed"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "shuffle_batch");
}

void ShuffleBatchOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value seed_, int startup_seed) {
  VLOG(4) << "Start build ShuffleBatchOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, seed_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_startup_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), startup_seed);
  argument.AddAttribute("startup_seed", attr_startup_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType seed = seed_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_seed";
  paddle::dialect::IrTensor ir_tensor_seed(paddle::dialect::TransToPhiDataType(seed.dtype()),
                                                      seed.dims(),
                                                      seed.data_layout(),
                                                      seed.lod(),
                                                      seed.offset());
  VLOG(4) << "Builder construction  meta_seed";
  paddle::dialect::IrMetaTensor meta_seed(&ir_tensor_seed);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_shuffle_idx;
  paddle::dialect::IrMetaTensor meta_shuffle_idx(&dense_shuffle_idx);
  paddle::dialect::IrTensor dense_seed_out;
  paddle::dialect::IrMetaTensor meta_seed_out(&dense_seed_out);

  phi::ShuffleBatchInferMeta(meta_x, meta_seed, startup_seed, &meta_out, &meta_shuffle_idx, &meta_seed_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type shuffle_idx_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_shuffle_idx.dtype()), dense_shuffle_idx.dims(), dense_shuffle_idx.layout(), dense_shuffle_idx.lod(), dense_shuffle_idx.offset());
  argument_outputs.push_back(shuffle_idx_dense_tensor_type);

  pir::Type seed_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_out.dtype()), dense_seed_out.dims(), dense_seed_out.layout(), dense_seed_out.lod(), dense_seed_out.offset());
  argument_outputs.push_back(seed_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShuffleBatchOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value seed_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build ShuffleBatchOp";


  IR_ENFORCE(
      attributes.find("startup_seed") != attributes.end(),
          "'startup_seed' Attribute is expected for ShuffleBatchOp. ");
  int startup_seed = attributes.at("startup_seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, seed_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_startup_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), startup_seed);
  argument.AddAttribute("startup_seed", attr_startup_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType seed = seed_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)seed;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_seed";
  paddle::dialect::IrTensor ir_tensor_seed(paddle::dialect::TransToPhiDataType(seed.dtype()),
                                                      seed.dims(),
                                                      seed.data_layout(),
                                                      seed.lod(),
                                                      seed.offset());
  VLOG(4) << "Builder construction  meta_seed";
  paddle::dialect::IrMetaTensor meta_seed(&ir_tensor_seed);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_shuffle_idx;
  paddle::dialect::IrMetaTensor meta_shuffle_idx(&dense_shuffle_idx);
  paddle::dialect::IrTensor dense_seed_out;
  paddle::dialect::IrMetaTensor meta_seed_out(&dense_seed_out);

  phi::ShuffleBatchInferMeta(meta_x, meta_seed, startup_seed, &meta_out, &meta_shuffle_idx, &meta_seed_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type shuffle_idx_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_shuffle_idx.dtype()), dense_shuffle_idx.dims(), dense_shuffle_idx.layout(), dense_shuffle_idx.lod(), dense_shuffle_idx.offset());
  argument_outputs.push_back(shuffle_idx_dense_tensor_type);

  pir::Type seed_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_seed_out.dtype()), dense_seed_out.dims(), dense_seed_out.layout(), dense_seed_out.lod(), dense_seed_out.offset());
  argument_outputs.push_back(seed_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void ShuffleBatchOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ShuffleBatchOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("startup_seed")>0,
                 "startup_seed does not exist.");
  IR_ENFORCE(attributes.at("startup_seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: startup_seed is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: ShuffleBatchOp.";
}

void ShuffleBatchOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ShuffleBatchInferMeta);
  fn(infer_meta);
}

phi::DataType ShuffleBatchOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ShuffleBatchOp";
  


  return expected_kernel_dtype;
}

const char *SliceOp::attributes_name[3] = { "axes", "infer_flags", "decrease_axis" };

OpInfoTuple SliceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("infer_flags", "pir::ArrayAttribute<pir::Int64Attribute>", ""), paddle::dialect::OpAttributeInfo("decrease_axis", "pir::ArrayAttribute<pir::Int64Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SliceRawInferMeta", {"input", "axes", "starts", "ends", "infer_flags", "decrease_axis"}, "slice", {"input", "axes", "starts", "ends", "infer_flags", "decrease_axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "slice");
}

void SliceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, const std::vector<int64_t>& axes, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& infer_flags, const std::vector<int64_t>& decrease_axis) {
  VLOG(4) << "Start build SliceOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, starts_, ends_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_infer_flags;
  for (size_t i = 0; i < static_cast<size_t>(infer_flags.size()); i++) {
      pir::Attribute attr_infer_flags = pir::Int64Attribute::get(pir::IrContext::Instance(), infer_flags[i]);

    vec_infer_flags.push_back(attr_infer_flags);
  }
  pir::Attribute attr_infer_flags = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_infer_flags);
  argument.AddAttribute("infer_flags", attr_infer_flags);
  std::vector<pir::Attribute> vec_decrease_axis;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axis.size()); i++) {
      pir::Attribute attr_decrease_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axis[i]);

    vec_decrease_axis.push_back(attr_decrease_axis);
  }
  pir::Attribute attr_decrease_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axis);
  argument.AddAttribute("decrease_axis", attr_decrease_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SliceRawInferMeta(meta_input, axes, starts, ends, infer_flags, decrease_axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SliceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SliceOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for SliceOp. ");
  std::vector<int64_t> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for SliceOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for SliceOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("infer_flags") != attributes.end(),
          "'infer_flags' Attribute is expected for SliceOp. ");
  std::vector<int64_t> infer_flags;
  for (size_t i = 0; i < attributes.at("infer_flags").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    infer_flags.push_back(attributes.at("infer_flags").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("decrease_axis") != attributes.end(),
          "'decrease_axis' Attribute is expected for SliceOp. ");
  std::vector<int64_t> decrease_axis;
  for (size_t i = 0; i < attributes.at("decrease_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    decrease_axis.push_back(attributes.at("decrease_axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int64Attribute>().data());
  }

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, starts_, ends_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_infer_flags;
  for (size_t i = 0; i < static_cast<size_t>(infer_flags.size()); i++) {
      pir::Attribute attr_infer_flags = pir::Int64Attribute::get(pir::IrContext::Instance(), infer_flags[i]);

    vec_infer_flags.push_back(attr_infer_flags);
  }
  pir::Attribute attr_infer_flags = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_infer_flags);
  argument.AddAttribute("infer_flags", attr_infer_flags);
  std::vector<pir::Attribute> vec_decrease_axis;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axis.size()); i++) {
      pir::Attribute attr_decrease_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axis[i]);

    vec_decrease_axis.push_back(attr_decrease_axis);
  }
  pir::Attribute attr_decrease_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axis);
  argument.AddAttribute("decrease_axis", attr_decrease_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SliceRawInferMeta(meta_input, axes, starts, ends, infer_flags, decrease_axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SliceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value starts_, pir::Value ends_, const std::vector<int64_t>& axes, const std::vector<int64_t>& infer_flags, const std::vector<int64_t>& decrease_axis) {
  VLOG(4) << "Start build SliceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, starts_, ends_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int64Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);
  std::vector<pir::Attribute> vec_infer_flags;
  for (size_t i = 0; i < static_cast<size_t>(infer_flags.size()); i++) {
      pir::Attribute attr_infer_flags = pir::Int64Attribute::get(pir::IrContext::Instance(), infer_flags[i]);

    vec_infer_flags.push_back(attr_infer_flags);
  }
  pir::Attribute attr_infer_flags = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_infer_flags);
  argument.AddAttribute("infer_flags", attr_infer_flags);
  std::vector<pir::Attribute> vec_decrease_axis;
  for (size_t i = 0; i < static_cast<size_t>(decrease_axis.size()); i++) {
      pir::Attribute attr_decrease_axis = pir::Int64Attribute::get(pir::IrContext::Instance(), decrease_axis[i]);

    vec_decrease_axis.push_back(attr_decrease_axis);
  }
  pir::Attribute attr_decrease_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_decrease_axis);
  argument.AddAttribute("decrease_axis", attr_decrease_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SliceRawInferMeta(meta_input, axes, starts, ends, infer_flags, decrease_axis, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SliceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SliceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  IR_ENFORCE(attributes.count("infer_flags")>0,
                 "infer_flags does not exist.");
  IR_ENFORCE(attributes.at("infer_flags").isa<pir::ArrayAttribute>(),
                 "Type of attribute: infer_flags is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("infer_flags").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("infer_flags").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: infer_flags is not right.");
  }
  IR_ENFORCE(attributes.count("decrease_axis")>0,
                 "decrease_axis does not exist.");
  IR_ENFORCE(attributes.at("decrease_axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: decrease_axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("decrease_axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("decrease_axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int64Attribute>(),
                   "Type of attribute: decrease_axis is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SliceOp.";
}

void SliceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SliceRawInferMeta);
  fn(infer_meta);
}

phi::DataType SliceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SliceOp";
  


  return expected_kernel_dtype;
}

bool SliceOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: SliceOp";
  return SliceOpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *SoftReluOp::attributes_name[1] = { "threshold" };

OpInfoTuple SoftReluOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("threshold", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "soft_relu", {"x", "threshold"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "soft_relu");
}

void SoftReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, float threshold) {
  VLOG(4) << "Start build SoftReluOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftReluOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftReluOp";


  IR_ENFORCE(
      attributes.find("threshold") != attributes.end(),
          "'threshold' Attribute is expected for SoftReluOp. ");
  float threshold = attributes.at("threshold").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_threshold = pir::FloatAttribute::get(pir::IrContext::Instance(), threshold);
  argument.AddAttribute("threshold", attr_threshold);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftReluOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SoftReluOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("threshold")>0,
                 "threshold does not exist.");
  IR_ENFORCE(attributes.at("threshold").isa<pir::FloatAttribute>(),
                 "Type of attribute: threshold is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SoftReluOp.";
}

void SoftReluOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SoftReluOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftReluOp";
  


  return expected_kernel_dtype;
}

const char *SoftmaxOp::attributes_name[1] = { "axis" };

OpInfoTuple SoftmaxOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SoftmaxInferMeta", {"x", "axis"}, "softmax", {"x", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softmax");
}

void SoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis) {
  VLOG(4) << "Start build SoftmaxOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SoftmaxInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftmaxOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SoftmaxOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SoftmaxOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SoftmaxInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SoftmaxOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SoftmaxOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SoftmaxOp.";
}

void SoftmaxOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SoftmaxInferMeta);
  fn(infer_meta);
}

phi::DataType SoftmaxOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SoftmaxOp";
  


  return expected_kernel_dtype;
}

const char *Softmax_Op::attributes_name[1] = { "axis" };

OpInfoTuple Softmax_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SoftmaxInferMeta", {"x", "axis"}, "softmax", {"x", "axis"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "softmax");
}

void Softmax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int axis) {
  VLOG(4) << "Start build Softmax_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SoftmaxInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Softmax_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Softmax_Op";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for Softmax_Op. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SoftmaxInferMeta(meta_x, axis, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Softmax_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Softmax_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Softmax_Op.";
}

void Softmax_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SoftmaxInferMeta);
  fn(infer_meta);
}

phi::DataType Softmax_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Softmax_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple SplitOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("sections", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SplitInferMeta", {"x", "sections", "axis"}, "split", {"x", "sections", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "split");
}

void SplitOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& sections, int axis) {
  VLOG(4) << "Start build SplitOp";


  // Generate int_array mutable attribute: sections
  paddle::dialect::FullIntArrayOp full_sections_op = builder.Build<paddle::dialect::FullIntArrayOp>(sections, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult sections_ = full_sections_op->result(0);
      // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, sections_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((sections.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(sections.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::SplitInferMeta(meta_x, sections, axis, meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(sections.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SplitOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SplitOp";


  IR_ENFORCE(
      attributes.find("sections") != attributes.end(),
          "'sections' Attribute is expected for SplitOp. ");
  std::vector<int64_t> sections = attributes.at("sections").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SplitOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  // Generate int_array mutable attribute: sections
  paddle::dialect::FullIntArrayOp full_sections_op = builder.Build<paddle::dialect::FullIntArrayOp>(sections, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult sections_ = full_sections_op->result(0);
      // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, sections_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((sections.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(sections.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::SplitInferMeta(meta_x, sections, axis, meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(sections.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SplitOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value sections_, pir::Value axis_) {
  VLOG(4) << "Start build SplitOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, sections_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray sections;
  if (sections_.dyn_cast<pir::OpResult>() && sections_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    sections = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          sections_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (sections_.type().isa<pir::VectorType>()) {
    size_t sections_size = sections_.type().dyn_cast<pir::VectorType>().size();
    sections = std::move(phi::IntArray(std::vector<int64_t>(sections_size, -1)));
    sections.SetFromTensor(true);
  } else if (sections_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim sections_dim = sections_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t sections_size = common::product(sections_dim);
    if (common::contain_unknown_dim(sections_dim)) {
      sections_size = 1;
    }
    sections = std::move(phi::IntArray(std::vector<int64_t>(sections_size, -1)));
    sections.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((sections.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(sections.size()); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::SplitInferMeta(meta_x, sections, axis, meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(sections.size()); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SplitOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SplitOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: SplitOp.";
}

void SplitOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SplitInferMeta);
  fn(infer_meta);
}

phi::DataType SplitOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SplitOp";
  


  return expected_kernel_dtype;
}

const char *SplitWithNumOp::attributes_name[1] = { "num" };

OpInfoTuple SplitWithNumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SplitWithNumInferMeta", {"x", "num", "axis"}, "split_with_num", {"x", "num", "axis"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "split_with_num");
}

void SplitWithNumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int num, int axis) {
  VLOG(4) << "Start build SplitWithNumOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num = pir::Int32Attribute::get(pir::IrContext::Instance(), num);
  argument.AddAttribute("num", attr_num);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((num), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::SplitWithNumInferMeta(meta_x, num, axis, meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SplitWithNumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SplitWithNumOp";


  IR_ENFORCE(
      attributes.find("num") != attributes.end(),
          "'num' Attribute is expected for SplitWithNumOp. ");
  int num = attributes.at("num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SplitWithNumOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::INT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num = pir::Int32Attribute::get(pir::IrContext::Instance(), num);
  argument.AddAttribute("num", attr_num);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((num), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::SplitWithNumInferMeta(meta_x, num, axis, meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SplitWithNumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, int num) {
  VLOG(4) << "Start build SplitWithNumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num = pir::Int32Attribute::get(pir::IrContext::Instance(), num);
  argument.AddAttribute("num", attr_num);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  std::vector<paddle::dialect::IrTensor> vec_dense_out((num), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_out;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    vec_meta_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_out.size()); i++) {
    meta_out.push_back(&vec_meta_out[i]);
  }

  phi::SplitWithNumInferMeta(meta_x, num, axis, meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> out_types;
  for (size_t i=0; i < static_cast<size_t>(num); i++) {
    out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_out[i].dtype()), vec_dense_out[i].dims(), vec_dense_out[i].layout(), vec_dense_out[i].lod(), vec_dense_out[i].offset()));
  }
  pir::Type out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), out_types);
  argument_outputs.push_back(out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SplitWithNumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SplitWithNumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("num")>0,
                 "num does not exist.");
  IR_ENFORCE(attributes.at("num").isa<pir::Int32Attribute>(),
                 "Type of attribute: num is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: SplitWithNumOp.";
}

void SplitWithNumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SplitWithNumInferMeta);
  fn(infer_meta);
}

phi::DataType SplitWithNumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SplitWithNumOp";
  


  return expected_kernel_dtype;
}

const char *StridedSliceOp::attributes_name[1] = { "axes" };

OpInfoTuple StridedSliceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("starts", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("ends", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("strides", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("axes", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("StridedSliceInferMeta", {"x", "axes", "starts", "ends", "strides"}, "strided_slice", {"x", "axes", "starts", "ends", "strides"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "strided_slice");
}

void StridedSliceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& axes, const std::vector<int64_t>& starts, const std::vector<int64_t>& ends, const std::vector<int64_t>& strides) {
  VLOG(4) << "Start build StridedSliceOp";


  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: strides
  paddle::dialect::FullIntArrayOp full_strides_op = builder.Build<paddle::dialect::FullIntArrayOp>(strides, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult strides_ = full_strides_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, strides_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int32Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedSliceInferMeta(meta_x, axes, starts, ends, strides, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StridedSliceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build StridedSliceOp";


  IR_ENFORCE(
      attributes.find("axes") != attributes.end(),
          "'axes' Attribute is expected for StridedSliceOp. ");
  std::vector<int> axes;
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axes.push_back(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("starts") != attributes.end(),
          "'starts' Attribute is expected for StridedSliceOp. ");
  std::vector<int64_t> starts = attributes.at("starts").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("ends") != attributes.end(),
          "'ends' Attribute is expected for StridedSliceOp. ");
  std::vector<int64_t> ends = attributes.at("ends").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for StridedSliceOp. ");
  std::vector<int64_t> strides = attributes.at("strides").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: starts
  paddle::dialect::FullIntArrayOp full_starts_op = builder.Build<paddle::dialect::FullIntArrayOp>(starts, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult starts_ = full_starts_op->result(0);
      // Generate int_array mutable attribute: ends
  paddle::dialect::FullIntArrayOp full_ends_op = builder.Build<paddle::dialect::FullIntArrayOp>(ends, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult ends_ = full_ends_op->result(0);
      // Generate int_array mutable attribute: strides
  paddle::dialect::FullIntArrayOp full_strides_op = builder.Build<paddle::dialect::FullIntArrayOp>(strides, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult strides_ = full_strides_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, strides_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int32Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedSliceInferMeta(meta_x, axes, starts, ends, strides, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StridedSliceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value starts_, pir::Value ends_, pir::Value strides_, const std::vector<int>& axes) {
  VLOG(4) << "Start build StridedSliceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, starts_, ends_, strides_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_axes;
  for (size_t i = 0; i < static_cast<size_t>(axes.size()); i++) {
      pir::Attribute attr_axes = pir::Int32Attribute::get(pir::IrContext::Instance(), axes[i]);

    vec_axes.push_back(attr_axes);
  }
  pir::Attribute attr_axes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axes);
  argument.AddAttribute("axes", attr_axes);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray starts;
  if (starts_.dyn_cast<pir::OpResult>() && starts_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    starts = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          starts_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (starts_.type().isa<pir::VectorType>()) {
    size_t starts_size = starts_.type().dyn_cast<pir::VectorType>().size();
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else if (starts_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim starts_dim = starts_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t starts_size = common::product(starts_dim);
    if (common::contain_unknown_dim(starts_dim)) {
      starts_size = 1;
    }
    starts = std::move(phi::IntArray(std::vector<int64_t>(starts_size, -1)));
    starts.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray ends;
  if (ends_.dyn_cast<pir::OpResult>() && ends_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    ends = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          ends_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (ends_.type().isa<pir::VectorType>()) {
    size_t ends_size = ends_.type().dyn_cast<pir::VectorType>().size();
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else if (ends_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim ends_dim = ends_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t ends_size = common::product(ends_dim);
    if (common::contain_unknown_dim(ends_dim)) {
      ends_size = 1;
    }
    ends = std::move(phi::IntArray(std::vector<int64_t>(ends_size, -1)));
    ends.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::IntArray strides;
  if (strides_.dyn_cast<pir::OpResult>() && strides_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    strides = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          strides_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (strides_.type().isa<pir::VectorType>()) {
    size_t strides_size = strides_.type().dyn_cast<pir::VectorType>().size();
    strides = std::move(phi::IntArray(std::vector<int64_t>(strides_size, -1)));
    strides.SetFromTensor(true);
  } else if (strides_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim strides_dim = strides_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t strides_size = common::product(strides_dim);
    if (common::contain_unknown_dim(strides_dim)) {
      strides_size = 1;
    }
    strides = std::move(phi::IntArray(std::vector<int64_t>(strides_size, -1)));
    strides.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::StridedSliceInferMeta(meta_x, axes, starts, ends, strides, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void StridedSliceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: StridedSliceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 4u,
                    "The size %d of inputs must be equal to 4.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("axes")>0,
                 "axes does not exist.");
  IR_ENFORCE(attributes.at("axes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: axes is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: StridedSliceOp.";
}

void StridedSliceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::StridedSliceInferMeta);
  fn(infer_meta);
}

phi::DataType StridedSliceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: StridedSliceOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SubtractOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "subtract", {"x", "y"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "subtract");
}

void SubtractOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build SubtractOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SubtractOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SubtractOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SubtractOp.";
}

void SubtractOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType SubtractOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SubtractOp";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

bool SubtractOp::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: SubtractOp";
  return SubtractOpInferSymbolicShape(this->operation(), shape_analysis);
}

OpInfoTuple Subtract_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("ElementwiseInferMeta", {"x", "y"}, "subtract", {"x", "y"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "subtract");
}

void Subtract_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_) {
  VLOG(4) << "Start build Subtract_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::ElementwiseInferMeta(meta_x, meta_y, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Subtract_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Subtract_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Subtract_Op.";
}

void Subtract_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::ElementwiseInferMeta);
  fn(infer_meta);
}

phi::DataType Subtract_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Subtract_Op";
  


  // deal complex_promote
  if (framework::IsComplexType(expected_kernel_dtype)) {{
    // only promote inputs’s types when contains complex input
    return tensor_dtype;
  }}

  return expected_kernel_dtype;
}

bool Subtract_Op::InferSymbolicShape(pir::ShapeConstraintIRAnalysis* shape_analysis) {
  VLOG(4) << "Infer symbolic shape for op: Subtract_Op";
  return Subtract_OpInferSymbolicShape(this->operation(), shape_analysis);
}

const char *SumOp::attributes_name[2] = { "dtype", "keepdim" };

OpInfoTuple SumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("axis", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("keepdim", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SumInferMeta", {"x", "axis", "dtype", "keepdim"}, "sum", {"x", "axis", "dtype", "keepdim"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sum");
}

void SumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& axis, phi::DataType dtype, bool keepdim) {
  VLOG(4) << "Start build SumOp";


  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SumInferMeta(meta_x, axis, dtype, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SumOp";


  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SumOp. ");
  std::vector<int64_t> axis = attributes.at("axis").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for SumOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("keepdim") != attributes.end(),
          "'keepdim' Attribute is expected for SumOp. ");
  bool keepdim = attributes.at("keepdim").dyn_cast<pir::BoolAttribute>().data();

  // Generate int_array mutable attribute: axis
  paddle::dialect::FullIntArrayOp full_axis_op = builder.Build<paddle::dialect::FullIntArrayOp>(axis, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SumInferMeta(meta_x, axis, dtype, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value axis_, phi::DataType dtype, bool keepdim) {
  VLOG(4) << "Start build SumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_keepdim = pir::BoolAttribute::get(pir::IrContext::Instance(), keepdim);
  argument.AddAttribute("keepdim", attr_keepdim);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    axis = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          axis_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (axis_.type().isa<pir::VectorType>()) {
    size_t axis_size = axis_.type().dyn_cast<pir::VectorType>().size();
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else if (axis_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim axis_dim = axis_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t axis_size = common::product(axis_dim);
    if (common::contain_unknown_dim(axis_dim)) {
      axis_size = 1;
    }
    axis = std::move(phi::IntArray(std::vector<int64_t>(axis_size, -1)));
    axis.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::SumInferMeta(meta_x, axis, dtype, keepdim, &meta_out, phi::MetaConfig(false, false));

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("keepdim")>0,
                 "keepdim does not exist.");
  IR_ENFORCE(attributes.at("keepdim").isa<pir::BoolAttribute>(),
                 "Type of attribute: keepdim is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SumOp.";
}

void SumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SumInferMeta);
  fn(infer_meta);
}

phi::DataType SumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SumOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple SwishOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "swish", {"x"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "swish");
}

void SwishOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_) {
  VLOG(4) << "Start build SwishOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SwishOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SwishOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: SwishOp.";
}

void SwishOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType SwishOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SwishOp";
  


  return expected_kernel_dtype;
}

const char *SyncBatchNorm_Op::attributes_name[6] = { "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics" };

OpInfoTuple SyncBatchNorm_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("mean", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("variance", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("scale", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("momentum", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("data_layout", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("use_global_stats", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("trainable_statistics", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("mean_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("variance_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("saved_variance", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("reserve_space", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BatchNormInferMeta", {"x", "mean", "variance", "scale", "bias", "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics"}, "sync_batch_norm", {"x", "mean", "variance", "scale", "bias", "is_test", "momentum", "epsilon", "data_layout", "use_global_stats", "trainable_statistics"}, {"x"}, {}, {{"mean_out", "mean"},{"variance_out", "variance"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sync_batch_norm_");
}

void SyncBatchNorm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, bool is_test, float momentum, float epsilon, const std::string& data_layout, bool use_global_stats, bool trainable_statistics) {
  VLOG(4) << "Start build SyncBatchNorm_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::BatchNormInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SyncBatchNorm_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value mean_, pir::Value variance_, pir::Value scale_, pir::Value bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SyncBatchNorm_Op";


  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for SyncBatchNorm_Op. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("momentum") != attributes.end(),
          "'momentum' Attribute is expected for SyncBatchNorm_Op. ");
  float momentum = attributes.at("momentum").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for SyncBatchNorm_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("data_layout") != attributes.end(),
          "'data_layout' Attribute is expected for SyncBatchNorm_Op. ");
  std::string data_layout = attributes.at("data_layout").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("use_global_stats") != attributes.end(),
          "'use_global_stats' Attribute is expected for SyncBatchNorm_Op. ");
  bool use_global_stats = attributes.at("use_global_stats").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("trainable_statistics") != attributes.end(),
          "'trainable_statistics' Attribute is expected for SyncBatchNorm_Op. ");
  bool trainable_statistics = attributes.at("trainable_statistics").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, mean_, variance_, scale_, bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_momentum = pir::FloatAttribute::get(pir::IrContext::Instance(), momentum);
  argument.AddAttribute("momentum", attr_momentum);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_data_layout = pir::StrAttribute::get(pir::IrContext::Instance(), data_layout);
  argument.AddAttribute("data_layout", attr_data_layout);
  pir::Attribute attr_use_global_stats = pir::BoolAttribute::get(pir::IrContext::Instance(), use_global_stats);
  argument.AddAttribute("use_global_stats", attr_use_global_stats);
  pir::Attribute attr_trainable_statistics = pir::BoolAttribute::get(pir::IrContext::Instance(), trainable_statistics);
  argument.AddAttribute("trainable_statistics", attr_trainable_statistics);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType mean = mean_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)mean;
  paddle::dialect::DenseTensorType variance = variance_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)variance;
  paddle::dialect::DenseTensorType scale = scale_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)scale;
  paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)bias;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_mean";
  paddle::dialect::IrTensor ir_tensor_mean(paddle::dialect::TransToPhiDataType(mean.dtype()),
                                                      mean.dims(),
                                                      mean.data_layout(),
                                                      mean.lod(),
                                                      mean.offset());
  VLOG(4) << "Builder construction  meta_mean";
  paddle::dialect::IrMetaTensor meta_mean(&ir_tensor_mean);

  VLOG(4) << "Builder construction  dense_variance";
  paddle::dialect::IrTensor ir_tensor_variance(paddle::dialect::TransToPhiDataType(variance.dtype()),
                                                      variance.dims(),
                                                      variance.data_layout(),
                                                      variance.lod(),
                                                      variance.offset());
  VLOG(4) << "Builder construction  meta_variance";
  paddle::dialect::IrMetaTensor meta_variance(&ir_tensor_variance);

  VLOG(4) << "Builder construction  dense_scale";
  paddle::dialect::IrTensor ir_tensor_scale(paddle::dialect::TransToPhiDataType(scale.dtype()),
                                                      scale.dims(),
                                                      scale.data_layout(),
                                                      scale.lod(),
                                                      scale.offset());
  VLOG(4) << "Builder construction  meta_scale";
  paddle::dialect::IrMetaTensor meta_scale(&ir_tensor_scale);

  VLOG(4) << "Builder construction  dense_bias";
  paddle::dialect::IrTensor ir_tensor_bias(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                      bias.dims(),
                                                      bias.data_layout(),
                                                      bias.lod(),
                                                      bias.offset());
  VLOG(4) << "Builder construction  meta_bias";
  paddle::dialect::IrMetaTensor meta_bias(&ir_tensor_bias);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_mean_out;
  paddle::dialect::IrMetaTensor meta_mean_out(&dense_mean_out);
  paddle::dialect::IrTensor dense_variance_out;
  paddle::dialect::IrMetaTensor meta_variance_out(&dense_variance_out);
  paddle::dialect::IrTensor dense_saved_mean;
  paddle::dialect::IrMetaTensor meta_saved_mean(&dense_saved_mean);
  paddle::dialect::IrTensor dense_saved_variance;
  paddle::dialect::IrMetaTensor meta_saved_variance(&dense_saved_variance);
  paddle::dialect::IrTensor dense_reserve_space;
  paddle::dialect::IrMetaTensor meta_reserve_space(&dense_reserve_space);

  phi::BatchNormInferMeta(meta_x, meta_mean, meta_variance, meta_scale, meta_bias, is_test, momentum, epsilon, data_layout, use_global_stats, trainable_statistics, &meta_out, &meta_mean_out, &meta_variance_out, &meta_saved_mean, &meta_saved_variance, &meta_reserve_space);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type mean_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_mean_out.dtype()), dense_mean_out.dims(), dense_mean_out.layout(), dense_mean_out.lod(), dense_mean_out.offset());
  argument_outputs.push_back(mean_out_dense_tensor_type);

  pir::Type variance_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_variance_out.dtype()), dense_variance_out.dims(), dense_variance_out.layout(), dense_variance_out.lod(), dense_variance_out.offset());
  argument_outputs.push_back(variance_out_dense_tensor_type);

  pir::Type saved_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_mean.dtype()), dense_saved_mean.dims(), dense_saved_mean.layout(), dense_saved_mean.lod(), dense_saved_mean.offset());
  argument_outputs.push_back(saved_mean_dense_tensor_type);

  pir::Type saved_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_saved_variance.dtype()), dense_saved_variance.dims(), dense_saved_variance.layout(), dense_saved_variance.lod(), dense_saved_variance.offset());
  argument_outputs.push_back(saved_variance_dense_tensor_type);

  pir::Type reserve_space_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_reserve_space.dtype()), dense_reserve_space.dims(), dense_reserve_space.layout(), dense_reserve_space.lod(), dense_reserve_space.offset());
  argument_outputs.push_back(reserve_space_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SyncBatchNorm_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SyncBatchNorm_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("momentum")>0,
                 "momentum does not exist.");
  IR_ENFORCE(attributes.at("momentum").isa<pir::FloatAttribute>(),
                 "Type of attribute: momentum is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("data_layout")>0,
                 "data_layout does not exist.");
  IR_ENFORCE(attributes.at("data_layout").isa<pir::StrAttribute>(),
                 "Type of attribute: data_layout is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("use_global_stats")>0,
                 "use_global_stats does not exist.");
  IR_ENFORCE(attributes.at("use_global_stats").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_global_stats is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("trainable_statistics")>0,
                 "trainable_statistics does not exist.");
  IR_ENFORCE(attributes.at("trainable_statistics").isa<pir::BoolAttribute>(),
                 "Type of attribute: trainable_statistics is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 6u,
                    "The size %d of outputs must be equal to 6.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  }
  VLOG(4) << "End Verifying for: SyncBatchNorm_Op.";
}

void SyncBatchNorm_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BatchNormInferMeta);
  fn(infer_meta);
}

phi::DataType SyncBatchNorm_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SyncBatchNorm_Op";
  


  return expected_kernel_dtype;
}

OpInfoTuple TileOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("repeat_times", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TileInferMeta", {"x", "repeat_times"}, "tile", {"x", "repeat_times"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tile");
}

void TileOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int64_t>& repeat_times) {
  VLOG(4) << "Start build TileOp";


  // Generate int_array mutable attribute: repeat_times
  paddle::dialect::FullIntArrayOp full_repeat_times_op = builder.Build<paddle::dialect::FullIntArrayOp>(repeat_times, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult repeat_times_ = full_repeat_times_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeat_times_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TileInferMeta(meta_x, repeat_times, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TileOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TileOp";


  IR_ENFORCE(
      attributes.find("repeat_times") != attributes.end(),
          "'repeat_times' Attribute is expected for TileOp. ");
  std::vector<int64_t> repeat_times = attributes.at("repeat_times").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  // Generate int_array mutable attribute: repeat_times
  paddle::dialect::FullIntArrayOp full_repeat_times_op = builder.Build<paddle::dialect::FullIntArrayOp>(repeat_times, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult repeat_times_ = full_repeat_times_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeat_times_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TileInferMeta(meta_x, repeat_times, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TileOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value repeat_times_) {
  VLOG(4) << "Start build TileOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, repeat_times_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  phi::IntArray repeat_times;
  if (repeat_times_.dyn_cast<pir::OpResult>() && repeat_times_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    repeat_times = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          repeat_times_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (repeat_times_.type().isa<pir::VectorType>()) {
    size_t repeat_times_size = repeat_times_.type().dyn_cast<pir::VectorType>().size();
    repeat_times = std::move(phi::IntArray(std::vector<int64_t>(repeat_times_size, -1)));
    repeat_times.SetFromTensor(true);
  } else if (repeat_times_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim repeat_times_dim = repeat_times_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t repeat_times_size = common::product(repeat_times_dim);
    if (common::contain_unknown_dim(repeat_times_dim)) {
      repeat_times_size = 1;
    }
    repeat_times = std::move(phi::IntArray(std::vector<int64_t>(repeat_times_size, -1)));
    repeat_times.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TileInferMeta(meta_x, repeat_times, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TileOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TileOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TileOp.";
}

void TileOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TileInferMeta);
  fn(infer_meta);
}

phi::DataType TileOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TileOp";
  


  return expected_kernel_dtype;
}

const char *TransLayoutOp::attributes_name[1] = { "perm" };

OpInfoTuple TransLayoutOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("perm", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TransposeInferMeta", {"x", "perm"}, "transpose", {"x", "perm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "trans_layout");
}

void TransLayoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& perm) {
  VLOG(4) << "Start build TransLayoutOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TransposeInferMeta(meta_x, perm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransLayoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TransLayoutOp";


  IR_ENFORCE(
      attributes.find("perm") != attributes.end(),
          "'perm' Attribute is expected for TransLayoutOp. ");
  std::vector<int> perm;
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    perm.push_back(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TransposeInferMeta(meta_x, perm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransLayoutOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TransLayoutOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("perm")>0,
                 "perm does not exist.");
  IR_ENFORCE(attributes.at("perm").isa<pir::ArrayAttribute>(),
                 "Type of attribute: perm is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: perm is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TransLayoutOp.";
}

void TransLayoutOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TransposeInferMeta);
  fn(infer_meta);
}

phi::DataType TransLayoutOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TransLayoutOp";
  


  return expected_kernel_dtype;
}

const char *TransposeOp::attributes_name[1] = { "perm" };

OpInfoTuple TransposeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("perm", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TransposeInferMeta", {"x", "perm"}, "transpose", {"x", "perm"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "transpose");
}

void TransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& perm) {
  VLOG(4) << "Start build TransposeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TransposeInferMeta(meta_x, perm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransposeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TransposeOp";


  IR_ENFORCE(
      attributes.find("perm") != attributes.end(),
          "'perm' Attribute is expected for TransposeOp. ");
  std::vector<int> perm;
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    perm.push_back(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TransposeInferMeta(meta_x, perm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TransposeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TransposeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("perm")>0,
                 "perm does not exist.");
  IR_ENFORCE(attributes.at("perm").isa<pir::ArrayAttribute>(),
                 "Type of attribute: perm is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: perm is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TransposeOp.";
}

void TransposeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TransposeInferMeta);
  fn(infer_meta);
}

phi::DataType TransposeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TransposeOp";
  


  return expected_kernel_dtype;
}

const char *Transpose_Op::attributes_name[1] = { "perm" };

OpInfoTuple Transpose_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("perm", "pir::ArrayAttribute<pir::Int32Attribute>", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TransposeInferMeta", {"x", "perm"}, "transpose", {"x", "perm"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "transpose");
}

void Transpose_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, const std::vector<int>& perm) {
  VLOG(4) << "Start build Transpose_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TransposeInferMeta(meta_x, perm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Transpose_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Transpose_Op";


  IR_ENFORCE(
      attributes.find("perm") != attributes.end(),
          "'perm' Attribute is expected for Transpose_Op. ");
  std::vector<int> perm;
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    perm.push_back(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_perm;
  for (size_t i = 0; i < static_cast<size_t>(perm.size()); i++) {
      pir::Attribute attr_perm = pir::Int32Attribute::get(pir::IrContext::Instance(), perm[i]);

    vec_perm.push_back(attr_perm);
  }
  pir::Attribute attr_perm = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_perm);
  argument.AddAttribute("perm", attr_perm);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TransposeInferMeta(meta_x, perm, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Transpose_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Transpose_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("perm")>0,
                 "perm does not exist.");
  IR_ENFORCE(attributes.at("perm").isa<pir::ArrayAttribute>(),
                 "Type of attribute: perm is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("perm").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("perm").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: perm is not right.");
  }
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Transpose_Op.";
}

void Transpose_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TransposeInferMeta);
  fn(infer_meta);
}

phi::DataType Transpose_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Transpose_Op";
  


  return expected_kernel_dtype;
}

const char *TrilOp::attributes_name[1] = { "diagonal" };

OpInfoTuple TrilOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("diagonal", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TrilInferMeta", {"x", "diagonal"}, "tril", {"x", "diagonal"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tril");
}

void TrilOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int diagonal) {
  VLOG(4) << "Start build TrilOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TrilInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TrilOp";


  IR_ENFORCE(
      attributes.find("diagonal") != attributes.end(),
          "'diagonal' Attribute is expected for TrilOp. ");
  int diagonal = attributes.at("diagonal").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TrilInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TrilOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("diagonal")>0,
                 "diagonal does not exist.");
  IR_ENFORCE(attributes.at("diagonal").isa<pir::Int32Attribute>(),
                 "Type of attribute: diagonal is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TrilOp.";
}

void TrilOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TrilInferMeta);
  fn(infer_meta);
}

phi::DataType TrilOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TrilOp";
  


  return expected_kernel_dtype;
}

const char *Tril_Op::attributes_name[1] = { "diagonal" };

OpInfoTuple Tril_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("diagonal", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TrilInferMeta", {"x", "diagonal"}, "tril", {"x", "diagonal"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tril");
}

void Tril_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int diagonal) {
  VLOG(4) << "Start build Tril_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TrilInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Tril_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Tril_Op";


  IR_ENFORCE(
      attributes.find("diagonal") != attributes.end(),
          "'diagonal' Attribute is expected for Tril_Op. ");
  int diagonal = attributes.at("diagonal").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TrilInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Tril_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Tril_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("diagonal")>0,
                 "diagonal does not exist.");
  IR_ENFORCE(attributes.at("diagonal").isa<pir::Int32Attribute>(),
                 "Type of attribute: diagonal is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Tril_Op.";
}

void Tril_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TrilInferMeta);
  fn(infer_meta);
}

phi::DataType Tril_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Tril_Op";
  


  return expected_kernel_dtype;
}

const char *TrilIndicesOp::attributes_name[5] = { "rows", "cols", "offset", "dtype", "place" };

OpInfoTuple TrilIndicesOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("rows", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("cols", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TrilIndicesInferMeta", {"rows", "cols", "offset", "dtype"}, "tril_indices", {"rows", "cols", "offset", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "tril_indices");
}

void TrilIndicesOp::Build(pir::Builder &builder, pir::OperationArgument &argument, int rows, int cols, int offset, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build TrilIndicesOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_rows = pir::Int32Attribute::get(pir::IrContext::Instance(), rows);
  argument.AddAttribute("rows", attr_rows);
  pir::Attribute attr_cols = pir::Int32Attribute::get(pir::IrContext::Instance(), cols);
  argument.AddAttribute("cols", attr_cols);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TrilIndicesInferMeta(rows, cols, offset, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilIndicesOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TrilIndicesOp";


  IR_ENFORCE(
      attributes.find("rows") != attributes.end(),
          "'rows' Attribute is expected for TrilIndicesOp. ");
  int rows = attributes.at("rows").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("cols") != attributes.end(),
          "'cols' Attribute is expected for TrilIndicesOp. ");
  int cols = attributes.at("cols").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for TrilIndicesOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for TrilIndicesOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for TrilIndicesOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_rows = pir::Int32Attribute::get(pir::IrContext::Instance(), rows);
  argument.AddAttribute("rows", attr_rows);
  pir::Attribute attr_cols = pir::Int32Attribute::get(pir::IrContext::Instance(), cols);
  argument.AddAttribute("cols", attr_cols);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TrilIndicesInferMeta(rows, cols, offset, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TrilIndicesOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TrilIndicesOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("rows")>0,
                 "rows does not exist.");
  IR_ENFORCE(attributes.at("rows").isa<pir::Int32Attribute>(),
                 "Type of attribute: rows is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("cols")>0,
                 "cols does not exist.");
  IR_ENFORCE(attributes.at("cols").isa<pir::Int32Attribute>(),
                 "Type of attribute: cols is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TrilIndicesOp.";
}

void TrilIndicesOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TrilIndicesInferMeta);
  fn(infer_meta);
}

phi::DataType TrilIndicesOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TrilIndicesOp";
  


  return expected_kernel_dtype;
}

const char *TriuOp::attributes_name[1] = { "diagonal" };

OpInfoTuple TriuOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("diagonal", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TriuInferMeta", {"x", "diagonal"}, "triu", {"x", "diagonal"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "triu");
}

void TriuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int diagonal) {
  VLOG(4) << "Start build TriuOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriuInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriuOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TriuOp";


  IR_ENFORCE(
      attributes.find("diagonal") != attributes.end(),
          "'diagonal' Attribute is expected for TriuOp. ");
  int diagonal = attributes.at("diagonal").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriuInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriuOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TriuOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("diagonal")>0,
                 "diagonal does not exist.");
  IR_ENFORCE(attributes.at("diagonal").isa<pir::Int32Attribute>(),
                 "Type of attribute: diagonal is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TriuOp.";
}

void TriuOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TriuInferMeta);
  fn(infer_meta);
}

phi::DataType TriuOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TriuOp";
  


  return expected_kernel_dtype;
}

const char *Triu_Op::attributes_name[1] = { "diagonal" };

OpInfoTuple Triu_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("diagonal", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TriuInferMeta", {"x", "diagonal"}, "triu", {"x", "diagonal"}, {}, {}, {{"out", "x"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "triu");
}

void Triu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int diagonal) {
  VLOG(4) << "Start build Triu_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriuInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Triu_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build Triu_Op";


  IR_ENFORCE(
      attributes.find("diagonal") != attributes.end(),
          "'diagonal' Attribute is expected for Triu_Op. ");
  int diagonal = attributes.at("diagonal").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_diagonal = pir::Int32Attribute::get(pir::IrContext::Instance(), diagonal);
  argument.AddAttribute("diagonal", attr_diagonal);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriuInferMeta(meta_x, diagonal, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void Triu_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: Triu_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("diagonal")>0,
                 "diagonal does not exist.");
  IR_ENFORCE(attributes.at("diagonal").isa<pir::Int32Attribute>(),
                 "Type of attribute: diagonal is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: Triu_Op.";
}

void Triu_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TriuInferMeta);
  fn(infer_meta);
}

phi::DataType Triu_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: Triu_Op";
  


  return expected_kernel_dtype;
}

const char *TriuIndicesOp::attributes_name[5] = { "row", "col", "offset", "dtype", "place" };

OpInfoTuple TriuIndicesOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("row", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("col", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("offset", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TriuIndicesInferMeta", {"row", "col", "offset", "dtype"}, "triu_indices", {"row", "col", "offset", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "triu_indices");
}

void TriuIndicesOp::Build(pir::Builder &builder, pir::OperationArgument &argument, int row, int col, int offset, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build TriuIndicesOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_row = pir::Int32Attribute::get(pir::IrContext::Instance(), row);
  argument.AddAttribute("row", attr_row);
  pir::Attribute attr_col = pir::Int32Attribute::get(pir::IrContext::Instance(), col);
  argument.AddAttribute("col", attr_col);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriuIndicesInferMeta(row, col, offset, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriuIndicesOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TriuIndicesOp";


  IR_ENFORCE(
      attributes.find("row") != attributes.end(),
          "'row' Attribute is expected for TriuIndicesOp. ");
  int row = attributes.at("row").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("col") != attributes.end(),
          "'col' Attribute is expected for TriuIndicesOp. ");
  int col = attributes.at("col").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("offset") != attributes.end(),
          "'offset' Attribute is expected for TriuIndicesOp. ");
  int offset = attributes.at("offset").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for TriuIndicesOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for TriuIndicesOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_row = pir::Int32Attribute::get(pir::IrContext::Instance(), row);
  argument.AddAttribute("row", attr_row);
  pir::Attribute attr_col = pir::Int32Attribute::get(pir::IrContext::Instance(), col);
  argument.AddAttribute("col", attr_col);
  pir::Attribute attr_offset = pir::Int32Attribute::get(pir::IrContext::Instance(), offset);
  argument.AddAttribute("offset", attr_offset);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TriuIndicesInferMeta(row, col, offset, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TriuIndicesOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TriuIndicesOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("row")>0,
                 "row does not exist.");
  IR_ENFORCE(attributes.at("row").isa<pir::Int32Attribute>(),
                 "Type of attribute: row is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("col")>0,
                 "col does not exist.");
  IR_ENFORCE(attributes.at("col").isa<pir::Int32Attribute>(),
                 "Type of attribute: col is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("offset")>0,
                 "offset does not exist.");
  IR_ENFORCE(attributes.at("offset").isa<pir::Int32Attribute>(),
                 "Type of attribute: offset is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TriuIndicesOp.";
}

void TriuIndicesOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TriuIndicesInferMeta);
  fn(infer_meta);
}

phi::DataType TriuIndicesOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TriuIndicesOp";
  


  return expected_kernel_dtype;
}

const char *TruncatedGaussianRandomOp::attributes_name[6] = { "shape", "mean", "std", "seed", "dtype", "place" };

OpInfoTuple TruncatedGaussianRandomOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("mean", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("std", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("TruncatedGaussianRandomInferMeta", {"shape", "mean", "std", "seed", "dtype"}, "truncated_gaussian_random", {"shape", "mean", "std", "seed", "dtype"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "truncated_gaussian_random");
}

void TruncatedGaussianRandomOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int>& shape, float mean, float std, int seed, phi::DataType dtype, const Place& place) {
  VLOG(4) << "Start build TruncatedGaussianRandomOp";



  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TruncatedGaussianRandomInferMeta(shape, mean, std, seed, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TruncatedGaussianRandomOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build TruncatedGaussianRandomOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for TruncatedGaussianRandomOp. ");
  std::vector<int> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("mean") != attributes.end(),
          "'mean' Attribute is expected for TruncatedGaussianRandomOp. ");
  float mean = attributes.at("mean").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("std") != attributes.end(),
          "'std' Attribute is expected for TruncatedGaussianRandomOp. ");
  float std = attributes.at("std").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for TruncatedGaussianRandomOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for TruncatedGaussianRandomOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for TruncatedGaussianRandomOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();


  VLOG(4) << "Builder construction inputs";

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_mean = pir::FloatAttribute::get(pir::IrContext::Instance(), mean);
  argument.AddAttribute("mean", attr_mean);
  pir::Attribute attr_std = pir::FloatAttribute::get(pir::IrContext::Instance(), std);
  argument.AddAttribute("std", attr_std);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::TruncatedGaussianRandomInferMeta(shape, mean, std, seed, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void TruncatedGaussianRandomOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: TruncatedGaussianRandomOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("mean")>0,
                 "mean does not exist.");
  IR_ENFORCE(attributes.at("mean").isa<pir::FloatAttribute>(),
                 "Type of attribute: mean is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("std")>0,
                 "std does not exist.");
  IR_ENFORCE(attributes.at("std").isa<pir::FloatAttribute>(),
                 "Type of attribute: std is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: TruncatedGaussianRandomOp.";
}

void TruncatedGaussianRandomOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::TruncatedGaussianRandomInferMeta);
  fn(infer_meta);
}

phi::DataType TruncatedGaussianRandomOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: TruncatedGaussianRandomOp";
  


  return expected_kernel_dtype;
}

const char *UniformOp::attributes_name[3] = { "dtype", "seed", "place" };

OpInfoTuple UniformOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("shape", "paddle::dialect::IntArrayAttribute", false, false, true, false), paddle::dialect::OpInputInfo("min", "paddle::dialect::ScalarAttribute", false, false, true, false), paddle::dialect::OpInputInfo("max", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniformRandomInferMeta", {"shape", "dtype"}, "uniform", {"shape", "dtype", "min", "max", "seed"}, {"dtype"}, {"place"}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "uniform");
}

void UniformOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& shape, phi::DataType dtype, float min, float max, int seed, const Place& place) {
  VLOG(4) << "Start build UniformOp";


  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
      // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniformOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for UniformOp. ");
  std::vector<int64_t> shape = attributes.at("shape").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for UniformOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for UniformOp. ");
  float min = attributes.at("min").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for UniformOp. ");
  float max = attributes.at("max").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for UniformOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("place") != attributes.end(),
          "'place' Attribute is expected for UniformOp. ");
  Place place = attributes.at("place").dyn_cast<paddle::dialect::PlaceAttribute>().data();

  // Generate int_array mutable attribute: shape
  paddle::dialect::FullIntArrayOp full_shape_op = builder.Build<paddle::dialect::FullIntArrayOp>(shape, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult shape_ = full_shape_op->result(0);
      // Generate scalar mutable attribute: min
  paddle::dialect::FullOp full_min_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, min, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult min_ = full_min_op->result(0);
      // Generate scalar mutable attribute: max
  paddle::dialect::FullOp full_max_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, max, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult max_ = full_max_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value shape_, pir::Value min_, pir::Value max_, phi::DataType dtype, int seed, const Place& place) {
  VLOG(4) << "Start build UniformOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {shape_, min_, max_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_place = paddle::dialect::PlaceAttribute::get(pir::IrContext::Instance(), place);
  argument.AddAttribute("place", attr_place);

  VLOG(4) << "Builder construction outputs";
  phi::IntArray shape;
  if (shape_.dyn_cast<pir::OpResult>() && shape_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    shape = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          shape_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (shape_.type().isa<pir::VectorType>()) {
    size_t shape_size = shape_.type().dyn_cast<pir::VectorType>().size();
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else if (shape_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim shape_dim = shape_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t shape_size = common::product(shape_dim);
    if (common::contain_unknown_dim(shape_dim)) {
      shape_size = 1;
    }
    shape = std::move(phi::IntArray(std::vector<int64_t>(shape_size, -1)));
    shape.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }
  phi::Scalar min;
  if (min_.dyn_cast<pir::OpResult>() && min_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    min = std::move(phi::Scalar(min_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    min = std::move(phi::Scalar(-1));
    min.SetFromTensor(true);
  }
  phi::Scalar max;
  if (max_.dyn_cast<pir::OpResult>() && max_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    max = std::move(phi::Scalar(max_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    max = std::move(phi::Scalar(-1));
    max.SetFromTensor(true);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UniformRandomInferMeta(shape, dtype, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UniformOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: UniformOp.";
}

void UniformOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniformRandomInferMeta);
  fn(infer_meta);
}

phi::DataType UniformOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniformOp";
  


  return expected_kernel_dtype;
}

const char *UniformRandomBatchSizeLikeOp::attributes_name[10] = { "shape", "input_dim_idx", "output_dim_idx", "min", "max", "seed", "diag_num", "diag_step", "diag_val", "dtype" };

OpInfoTuple UniformRandomBatchSizeLikeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("input_dim_idx", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("output_dim_idx", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("min", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("max", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_num", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_step", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("diag_val", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("BatchSizeLikeInferMeta", {"input", "shape", "input_dim_idx", "output_dim_idx"}, "uniform_random_batch_size_like", {"input", "shape", "input_dim_idx", "output_dim_idx", "min", "max", "seed", "diag_num", "diag_step", "diag_val", "dtype"}, {"dtype"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "uniform_random_batch_size_like");
}

void UniformRandomBatchSizeLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, const std::vector<int>& shape, int input_dim_idx, int output_dim_idx, float min, float max, int seed, int diag_num, int diag_step, float diag_val, phi::DataType dtype) {
  VLOG(4) << "Start build UniformRandomBatchSizeLikeOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_input_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), input_dim_idx);
  argument.AddAttribute("input_dim_idx", attr_input_dim_idx);
  pir::Attribute attr_output_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), output_dim_idx);
  argument.AddAttribute("output_dim_idx", attr_output_dim_idx);
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BatchSizeLikeInferMeta(meta_input, shape, input_dim_idx, output_dim_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformRandomBatchSizeLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniformRandomBatchSizeLikeOp";


  IR_ENFORCE(
      attributes.find("shape") != attributes.end(),
          "'shape' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  std::vector<int> shape;
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    shape.push_back(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("input_dim_idx") != attributes.end(),
          "'input_dim_idx' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  int input_dim_idx = attributes.at("input_dim_idx").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("output_dim_idx") != attributes.end(),
          "'output_dim_idx' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  int output_dim_idx = attributes.at("output_dim_idx").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("min") != attributes.end(),
          "'min' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  float min = attributes.at("min").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("max") != attributes.end(),
          "'max' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  float max = attributes.at("max").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_num") != attributes.end(),
          "'diag_num' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  int diag_num = attributes.at("diag_num").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_step") != attributes.end(),
          "'diag_step' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  int diag_step = attributes.at("diag_step").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("diag_val") != attributes.end(),
          "'diag_val' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  float diag_val = attributes.at("diag_val").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for UniformRandomBatchSizeLikeOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_shape;
  for (size_t i = 0; i < static_cast<size_t>(shape.size()); i++) {
      pir::Attribute attr_shape = pir::Int32Attribute::get(pir::IrContext::Instance(), shape[i]);

    vec_shape.push_back(attr_shape);
  }
  pir::Attribute attr_shape = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_shape);
  argument.AddAttribute("shape", attr_shape);
  pir::Attribute attr_input_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), input_dim_idx);
  argument.AddAttribute("input_dim_idx", attr_input_dim_idx);
  pir::Attribute attr_output_dim_idx = pir::Int32Attribute::get(pir::IrContext::Instance(), output_dim_idx);
  argument.AddAttribute("output_dim_idx", attr_output_dim_idx);
  pir::Attribute attr_min = pir::FloatAttribute::get(pir::IrContext::Instance(), min);
  argument.AddAttribute("min", attr_min);
  pir::Attribute attr_max = pir::FloatAttribute::get(pir::IrContext::Instance(), max);
  argument.AddAttribute("max", attr_max);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_diag_num = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_num);
  argument.AddAttribute("diag_num", attr_diag_num);
  pir::Attribute attr_diag_step = pir::Int32Attribute::get(pir::IrContext::Instance(), diag_step);
  argument.AddAttribute("diag_step", attr_diag_step);
  pir::Attribute attr_diag_val = pir::FloatAttribute::get(pir::IrContext::Instance(), diag_val);
  argument.AddAttribute("diag_val", attr_diag_val);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::BatchSizeLikeInferMeta(meta_input, shape, input_dim_idx, output_dim_idx, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniformRandomBatchSizeLikeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UniformRandomBatchSizeLikeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<pir::ArrayAttribute>(),
                 "Type of attribute: shape is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("shape").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("shape").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: shape is not right.");
  }
  IR_ENFORCE(attributes.count("input_dim_idx")>0,
                 "input_dim_idx does not exist.");
  IR_ENFORCE(attributes.at("input_dim_idx").isa<pir::Int32Attribute>(),
                 "Type of attribute: input_dim_idx is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("output_dim_idx")>0,
                 "output_dim_idx does not exist.");
  IR_ENFORCE(attributes.at("output_dim_idx").isa<pir::Int32Attribute>(),
                 "Type of attribute: output_dim_idx is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("min")>0,
                 "min does not exist.");
  IR_ENFORCE(attributes.at("min").isa<pir::FloatAttribute>(),
                 "Type of attribute: min is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("max")>0,
                 "max does not exist.");
  IR_ENFORCE(attributes.at("max").isa<pir::FloatAttribute>(),
                 "Type of attribute: max is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_num")>0,
                 "diag_num does not exist.");
  IR_ENFORCE(attributes.at("diag_num").isa<pir::Int32Attribute>(),
                 "Type of attribute: diag_num is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_step")>0,
                 "diag_step does not exist.");
  IR_ENFORCE(attributes.at("diag_step").isa<pir::Int32Attribute>(),
                 "Type of attribute: diag_step is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("diag_val")>0,
                 "diag_val does not exist.");
  IR_ENFORCE(attributes.at("diag_val").isa<pir::FloatAttribute>(),
                 "Type of attribute: diag_val is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: UniformRandomBatchSizeLikeOp.";
}

void UniformRandomBatchSizeLikeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::BatchSizeLikeInferMeta);
  fn(infer_meta);
}

phi::DataType UniformRandomBatchSizeLikeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniformRandomBatchSizeLikeOp";
  


  return expected_kernel_dtype;
}

const char *UniqueOp::attributes_name[6] = { "return_index", "return_inverse", "return_counts", "axis", "dtype", "is_sorted" };

OpInfoTuple UniqueOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("return_index", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("return_inverse", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("return_counts", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("is_sorted", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("indices", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("inverse", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("counts", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UniqueRawInferMeta", {"x", "return_index", "return_inverse", "return_counts", "axis", "dtype", "is_sorted"}, "unique", {"x", "return_index", "return_inverse", "return_counts", "axis", "dtype", "is_sorted"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unique");
}

void UniqueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, bool return_index, bool return_inverse, bool return_counts, const std::vector<int>& axis, phi::DataType dtype, bool is_sorted) {
  VLOG(4) << "Start build UniqueOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_index = pir::BoolAttribute::get(pir::IrContext::Instance(), return_index);
  argument.AddAttribute("return_index", attr_return_index);
  pir::Attribute attr_return_inverse = pir::BoolAttribute::get(pir::IrContext::Instance(), return_inverse);
  argument.AddAttribute("return_inverse", attr_return_inverse);
  pir::Attribute attr_return_counts = pir::BoolAttribute::get(pir::IrContext::Instance(), return_counts);
  argument.AddAttribute("return_counts", attr_return_counts);
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_is_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sorted);
  argument.AddAttribute("is_sorted", attr_is_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);
  paddle::dialect::IrTensor dense_inverse;
  paddle::dialect::IrMetaTensor meta_inverse(&dense_inverse);
  paddle::dialect::IrTensor dense_counts;
  paddle::dialect::IrMetaTensor meta_counts(&dense_counts);

  phi::UniqueRawInferMeta(meta_x, return_index, return_inverse, return_counts, axis, dtype, is_sorted, &meta_out, &meta_indices, &meta_inverse, &meta_counts);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);

  pir::Type inverse_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_inverse.dtype()), dense_inverse.dims(), dense_inverse.layout(), dense_inverse.lod(), dense_inverse.offset());
  argument_outputs.push_back(inverse_dense_tensor_type);

  pir::Type counts_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_counts.dtype()), dense_counts.dims(), dense_counts.layout(), dense_counts.lod(), dense_counts.offset());
  argument_outputs.push_back(counts_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniqueOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UniqueOp";


  IR_ENFORCE(
      attributes.find("return_index") != attributes.end(),
          "'return_index' Attribute is expected for UniqueOp. ");
  bool return_index = attributes.at("return_index").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("return_inverse") != attributes.end(),
          "'return_inverse' Attribute is expected for UniqueOp. ");
  bool return_inverse = attributes.at("return_inverse").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("return_counts") != attributes.end(),
          "'return_counts' Attribute is expected for UniqueOp. ");
  bool return_counts = attributes.at("return_counts").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for UniqueOp. ");
  std::vector<int> axis;
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    axis.push_back(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("dtype") != attributes.end(),
          "'dtype' Attribute is expected for UniqueOp. ");
  phi::DataType dtype = attributes.at("dtype").dyn_cast<paddle::dialect::DataTypeAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_sorted") != attributes.end(),
          "'is_sorted' Attribute is expected for UniqueOp. ");
  bool is_sorted = attributes.at("is_sorted").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_return_index = pir::BoolAttribute::get(pir::IrContext::Instance(), return_index);
  argument.AddAttribute("return_index", attr_return_index);
  pir::Attribute attr_return_inverse = pir::BoolAttribute::get(pir::IrContext::Instance(), return_inverse);
  argument.AddAttribute("return_inverse", attr_return_inverse);
  pir::Attribute attr_return_counts = pir::BoolAttribute::get(pir::IrContext::Instance(), return_counts);
  argument.AddAttribute("return_counts", attr_return_counts);
  std::vector<pir::Attribute> vec_axis;
  for (size_t i = 0; i < static_cast<size_t>(axis.size()); i++) {
      pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis[i]);

    vec_axis.push_back(attr_axis);
  }
  pir::Attribute attr_axis = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_dtype = paddle::dialect::DataTypeAttribute::get(pir::IrContext::Instance(), dtype);
  argument.AddAttribute("dtype", attr_dtype);
  pir::Attribute attr_is_sorted = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sorted);
  argument.AddAttribute("is_sorted", attr_is_sorted);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_indices;
  paddle::dialect::IrMetaTensor meta_indices(&dense_indices);
  paddle::dialect::IrTensor dense_inverse;
  paddle::dialect::IrMetaTensor meta_inverse(&dense_inverse);
  paddle::dialect::IrTensor dense_counts;
  paddle::dialect::IrMetaTensor meta_counts(&dense_counts);

  phi::UniqueRawInferMeta(meta_x, return_index, return_inverse, return_counts, axis, dtype, is_sorted, &meta_out, &meta_indices, &meta_inverse, &meta_counts);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type indices_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_indices.dtype()), dense_indices.dims(), dense_indices.layout(), dense_indices.lod(), dense_indices.offset());
  argument_outputs.push_back(indices_dense_tensor_type);

  pir::Type inverse_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_inverse.dtype()), dense_inverse.dims(), dense_inverse.layout(), dense_inverse.lod(), dense_inverse.offset());
  argument_outputs.push_back(inverse_dense_tensor_type);

  pir::Type counts_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_counts.dtype()), dense_counts.dims(), dense_counts.layout(), dense_counts.lod(), dense_counts.offset());
  argument_outputs.push_back(counts_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UniqueOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UniqueOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("return_index")>0,
                 "return_index does not exist.");
  IR_ENFORCE(attributes.at("return_index").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_index is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("return_inverse")>0,
                 "return_inverse does not exist.");
  IR_ENFORCE(attributes.at("return_inverse").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_inverse is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("return_counts")>0,
                 "return_counts does not exist.");
  IR_ENFORCE(attributes.at("return_counts").isa<pir::BoolAttribute>(),
                 "Type of attribute: return_counts is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::ArrayAttribute>(),
                 "Type of attribute: axis is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("axis").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("axis").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: axis is not right.");
  }
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("is_sorted")>0,
                 "is_sorted does not exist.");
  IR_ENFORCE(attributes.at("is_sorted").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_sorted is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 4u,
                    "The size %d of outputs must be equal to 4.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  }
  VLOG(4) << "End Verifying for: UniqueOp.";
}

void UniqueOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UniqueRawInferMeta);
  fn(infer_meta);
}

phi::DataType UniqueOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UniqueOp";
  


  return expected_kernel_dtype;
}

std::tuple<phi::DataType, phi::Backend> UniqueOp::ParseKernelKey(pir::Operation *op) {
  VLOG(4) << "Parse kernel key for op: UniqueOp";
  return UniqueOpParseKernelKey(op);
}

const char *UnpoolOp::attributes_name[4] = { "ksize", "strides", "padding", "data_format" };

OpInfoTuple UnpoolOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("indices", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("output_size", "paddle::dialect::IntArrayAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ksize", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("strides", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("padding", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("data_format", "pir::StrAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnpoolInferMeta", {"x", "indices", "ksize", "strides", "padding", "output_size", "data_format"}, "unpool", {"x", "indices", "ksize", "strides", "padding", "output_size", "data_format"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "unpool");
}

void UnpoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& padding, const std::vector<int64_t>& output_size, const std::string& data_format) {
  VLOG(4) << "Start build UnpoolOp";


  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_padding;
  for (size_t i = 0; i < static_cast<size_t>(padding.size()); i++) {
      pir::Attribute attr_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), padding[i]);

    vec_padding.push_back(attr_padding);
  }
  pir::Attribute attr_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_padding);
  argument.AddAttribute("padding", attr_padding);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnpoolInferMeta(meta_x, meta_indices, ksize, strides, padding, output_size, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnpoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build UnpoolOp";


  IR_ENFORCE(
      attributes.find("ksize") != attributes.end(),
          "'ksize' Attribute is expected for UnpoolOp. ");
  std::vector<int> ksize;
  for (size_t i = 0; i < attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    ksize.push_back(attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("strides") != attributes.end(),
          "'strides' Attribute is expected for UnpoolOp. ");
  std::vector<int> strides;
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    strides.push_back(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("padding") != attributes.end(),
          "'padding' Attribute is expected for UnpoolOp. ");
  std::vector<int> padding;
  for (size_t i = 0; i < attributes.at("padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    padding.push_back(attributes.at("padding").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("output_size") != attributes.end(),
          "'output_size' Attribute is expected for UnpoolOp. ");
  std::vector<int64_t> output_size = attributes.at("output_size").dyn_cast<paddle::dialect::IntArrayAttribute>().data().GetData();

  IR_ENFORCE(
      attributes.find("data_format") != attributes.end(),
          "'data_format' Attribute is expected for UnpoolOp. ");
  std::string data_format = attributes.at("data_format").dyn_cast<pir::StrAttribute>().AsString();

  // Generate int_array mutable attribute: output_size
  paddle::dialect::FullIntArrayOp full_output_size_op = builder.Build<paddle::dialect::FullIntArrayOp>(output_size, phi::DataType::INT64, phi::CPUPlace());
  pir::OpResult output_size_ = full_output_size_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_padding;
  for (size_t i = 0; i < static_cast<size_t>(padding.size()); i++) {
      pir::Attribute attr_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), padding[i]);

    vec_padding.push_back(attr_padding);
  }
  pir::Attribute attr_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_padding);
  argument.AddAttribute("padding", attr_padding);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnpoolInferMeta(meta_x, meta_indices, ksize, strides, padding, output_size, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnpoolOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value indices_, pir::Value output_size_, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& padding, const std::string& data_format) {
  VLOG(4) << "Start build UnpoolOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, indices_, output_size_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_ksize;
  for (size_t i = 0; i < static_cast<size_t>(ksize.size()); i++) {
      pir::Attribute attr_ksize = pir::Int32Attribute::get(pir::IrContext::Instance(), ksize[i]);

    vec_ksize.push_back(attr_ksize);
  }
  pir::Attribute attr_ksize = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_ksize);
  argument.AddAttribute("ksize", attr_ksize);
  std::vector<pir::Attribute> vec_strides;
  for (size_t i = 0; i < static_cast<size_t>(strides.size()); i++) {
      pir::Attribute attr_strides = pir::Int32Attribute::get(pir::IrContext::Instance(), strides[i]);

    vec_strides.push_back(attr_strides);
  }
  pir::Attribute attr_strides = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_strides);
  argument.AddAttribute("strides", attr_strides);
  std::vector<pir::Attribute> vec_padding;
  for (size_t i = 0; i < static_cast<size_t>(padding.size()); i++) {
      pir::Attribute attr_padding = pir::Int32Attribute::get(pir::IrContext::Instance(), padding[i]);

    vec_padding.push_back(attr_padding);
  }
  pir::Attribute attr_padding = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_padding);
  argument.AddAttribute("padding", attr_padding);
  pir::Attribute attr_data_format = pir::StrAttribute::get(pir::IrContext::Instance(), data_format);
  argument.AddAttribute("data_format", attr_data_format);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType indices = indices_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)indices;
  phi::IntArray output_size;
  if (output_size_.dyn_cast<pir::OpResult>() && output_size_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullIntArrayOp>()) {
    output_size = std::move(phi::IntArray(paddle::dialect::GetInt64Vector(
                          output_size_.dyn_cast<pir::OpResult>().owner()
                          ->dyn_cast<paddle::dialect::FullIntArrayOp>()
                          .attribute("value"))));
  } else if (output_size_.type().isa<pir::VectorType>()) {
    size_t output_size_size = output_size_.type().dyn_cast<pir::VectorType>().size();
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else if (output_size_.type().isa<paddle::dialect::DenseTensorType>()) {
    common::DDim output_size_dim = output_size_.type().dyn_cast<paddle::dialect::DenseTensorType>().dims();
    size_t output_size_size = common::product(output_size_dim);
    if (common::contain_unknown_dim(output_size_dim)) {
      output_size_size = 1;
    }
    output_size = std::move(phi::IntArray(std::vector<int64_t>(output_size_size, -1)));
    output_size.SetFromTensor(true);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented("Only support VectorType or DenseTensorType"));
  }


  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_indices";
  paddle::dialect::IrTensor ir_tensor_indices(paddle::dialect::TransToPhiDataType(indices.dtype()),
                                                      indices.dims(),
                                                      indices.data_layout(),
                                                      indices.lod(),
                                                      indices.offset());
  VLOG(4) << "Builder construction  meta_indices";
  paddle::dialect::IrMetaTensor meta_indices(&ir_tensor_indices);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnpoolInferMeta(meta_x, meta_indices, ksize, strides, padding, output_size, data_format, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void UnpoolOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: UnpoolOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ksize")>0,
                 "ksize does not exist.");
  IR_ENFORCE(attributes.at("ksize").isa<pir::ArrayAttribute>(),
                 "Type of attribute: ksize is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("ksize").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: ksize is not right.");
  }
  IR_ENFORCE(attributes.count("strides")>0,
                 "strides does not exist.");
  IR_ENFORCE(attributes.at("strides").isa<pir::ArrayAttribute>(),
                 "Type of attribute: strides is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("strides").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("strides").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: strides is not right.");
  }
  IR_ENFORCE(attributes.count("padding")>0,
                 "padding does not exist.");
  IR_ENFORCE(attributes.at("padding").isa<pir::ArrayAttribute>(),
                 "Type of attribute: padding is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("padding").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("padding").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: padding is not right.");
  }
  IR_ENFORCE(attributes.count("data_format")>0,
                 "data_format does not exist.");
  IR_ENFORCE(attributes.at("data_format").isa<pir::StrAttribute>(),
                 "Type of attribute: data_format is not pir::StrAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: UnpoolOp.";
}

void UnpoolOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnpoolInferMeta);
  fn(infer_meta);
}

phi::DataType UnpoolOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: UnpoolOp";
  


  return expected_kernel_dtype;
}

OpInfoTuple WriteToArrayOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("i", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = {  };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "write_to_array");
}

void WriteToArrayOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: WriteToArrayOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  // Attributes num is 0, not need to check attributes type.
  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  }
  VLOG(4) << "End Verifying for: WriteToArrayOp.";
}

phi::DataType WriteToArrayOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: WriteToArrayOp";
  


  return expected_kernel_dtype;
}

const char *ZerosOp::attributes_name[3] = { "shape", "dtype", "place" };

OpInfoTuple ZerosOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = {  };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("shape", "paddle::dialect::IntArrayAttribute", "std::vector<int64_t>"), paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "zeros");
}

void ZerosOp::Build(pir::Builder &builder, pir::OperationArgument &argument, const std::vector<int64_t>& shape, phi::DataType dtype, const Place& place) {
  FullOp::Build(builder, argument, shape, 0, dtype, place);
}

void ZerosOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ZerosOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 0u,
                    "The size %d of inputs must be equal to 0.", input_size);
  // Inputs num is 0, not need to check inputs type.
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("shape")>0,
                 "shape does not exist.");
  IR_ENFORCE(attributes.at("shape").isa<paddle::dialect::IntArrayAttribute>(),
                 "Type of attribute: shape is not paddle::dialect::IntArrayAttribute.");

  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ZerosOp.";
}

void ZerosOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateInferMeta);
  fn(infer_meta);
}

phi::DataType ZerosOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ZerosOp";
  


  return expected_kernel_dtype;
}

const char *ZerosLikeOp::attributes_name[2] = { "dtype", "place" };

OpInfoTuple ZerosLikeOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dtype", "paddle::dialect::DataTypeAttribute", ""), paddle::dialect::OpAttributeInfo("place", "paddle::dialect::PlaceAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("", {""}, "", {""}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "zeros_like");
}

void ZerosLikeOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, phi::DataType dtype, const Place& place) {
  FullLikeOp::Build(builder, argument, x_, 0, dtype, place);
}

void ZerosLikeOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: ZerosLikeOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dtype")>0,
                 "dtype does not exist.");
  IR_ENFORCE(attributes.at("dtype").isa<paddle::dialect::DataTypeAttribute>(),
                 "Type of attribute: dtype is not paddle::dialect::DataTypeAttribute.");

  IR_ENFORCE(attributes.count("place")>0,
                 "place does not exist.");
  IR_ENFORCE(attributes.at("place").isa<paddle::dialect::PlaceAttribute>(),
                 "Type of attribute: place is not paddle::dialect::PlaceAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: ZerosLikeOp.";
}

void ZerosLikeOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CreateLikeInferMeta);
  fn(infer_meta);
}

phi::DataType ZerosLikeOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: ZerosLikeOp";
  


  return expected_kernel_dtype;
}

const char *CSoftmaxWithCrossEntropyOp::attributes_name[4] = { "ignore_index", "ring_id", "rank", "nranks" };

OpInfoTuple CSoftmaxWithCrossEntropyOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("logits", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("ignore_index", "pir::Int64Attribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("rank", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("nranks", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("softmax", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("loss", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("CSoftmaxWithCrossEntropyInferMeta", {"logits", "label", "ignore_index", "ring_id", "rank", "nranks"}, "c_softmax_with_cross_entropy", {"logits", "label", "ignore_index", "ring_id", "rank", "nranks"}, {"logits"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "c_softmax_with_cross_entropy");
}

void CSoftmaxWithCrossEntropyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, int64_t ignore_index, int ring_id, int rank, int nranks) {
  VLOG(4) << "Start build CSoftmaxWithCrossEntropyOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::CSoftmaxWithCrossEntropyInferMeta(meta_logits, meta_label, ignore_index, ring_id, rank, nranks, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSoftmaxWithCrossEntropyOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value logits_, pir::Value label_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build CSoftmaxWithCrossEntropyOp";


  IR_ENFORCE(
      attributes.find("ignore_index") != attributes.end(),
          "'ignore_index' Attribute is expected for CSoftmaxWithCrossEntropyOp. ");
  int64_t ignore_index = attributes.at("ignore_index").dyn_cast<pir::Int64Attribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for CSoftmaxWithCrossEntropyOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("rank") != attributes.end(),
          "'rank' Attribute is expected for CSoftmaxWithCrossEntropyOp. ");
  int rank = attributes.at("rank").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("nranks") != attributes.end(),
          "'nranks' Attribute is expected for CSoftmaxWithCrossEntropyOp. ");
  int nranks = attributes.at("nranks").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {logits_, label_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_ignore_index = pir::Int64Attribute::get(pir::IrContext::Instance(), ignore_index);
  argument.AddAttribute("ignore_index", attr_ignore_index);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);
  pir::Attribute attr_rank = pir::Int32Attribute::get(pir::IrContext::Instance(), rank);
  argument.AddAttribute("rank", attr_rank);
  pir::Attribute attr_nranks = pir::Int32Attribute::get(pir::IrContext::Instance(), nranks);
  argument.AddAttribute("nranks", attr_nranks);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType logits = logits_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)logits;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;

  VLOG(4) << "Builder construction  dense_logits";
  paddle::dialect::IrTensor ir_tensor_logits(paddle::dialect::TransToPhiDataType(logits.dtype()),
                                                      logits.dims(),
                                                      logits.data_layout(),
                                                      logits.lod(),
                                                      logits.offset());
  VLOG(4) << "Builder construction  meta_logits";
  paddle::dialect::IrMetaTensor meta_logits(&ir_tensor_logits);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);
  paddle::dialect::IrTensor dense_softmax;
  paddle::dialect::IrMetaTensor meta_softmax(&dense_softmax);
  paddle::dialect::IrTensor dense_loss;
  paddle::dialect::IrMetaTensor meta_loss(&dense_loss);

  phi::CSoftmaxWithCrossEntropyInferMeta(meta_logits, meta_label, ignore_index, ring_id, rank, nranks, &meta_softmax, &meta_loss);

  std::vector<pir::Type> argument_outputs;
  pir::Type softmax_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax.dtype()), dense_softmax.dims(), dense_softmax.layout(), dense_softmax.lod(), dense_softmax.offset());
  argument_outputs.push_back(softmax_dense_tensor_type);

  pir::Type loss_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_loss.dtype()), dense_loss.dims(), dense_loss.layout(), dense_loss.lod(), dense_loss.offset());
  argument_outputs.push_back(loss_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void CSoftmaxWithCrossEntropyOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: CSoftmaxWithCrossEntropyOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("ignore_index")>0,
                 "ignore_index does not exist.");
  IR_ENFORCE(attributes.at("ignore_index").isa<pir::Int64Attribute>(),
                 "Type of attribute: ignore_index is not pir::Int64Attribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("rank")>0,
                 "rank does not exist.");
  IR_ENFORCE(attributes.at("rank").isa<pir::Int32Attribute>(),
                 "Type of attribute: rank is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("nranks")>0,
                 "nranks does not exist.");
  IR_ENFORCE(attributes.at("nranks").isa<pir::Int32Attribute>(),
                 "Type of attribute: nranks is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: CSoftmaxWithCrossEntropyOp.";
}

void CSoftmaxWithCrossEntropyOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::CSoftmaxWithCrossEntropyInferMeta);
  fn(infer_meta);
}

phi::DataType CSoftmaxWithCrossEntropyOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: CSoftmaxWithCrossEntropyOp";
  


  return expected_kernel_dtype;
}

const char *DpsgdOp::attributes_name[4] = { "clip", "batch_size", "sigma", "seed" };

OpInfoTuple DpsgdOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("clip", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("batch_size", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("sigma", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("DpsgdInferMeta", {"param", "grad", "learning_rate", "clip", "batch_size", "sigma", "seed"}, "dpsgd", {"param", "grad", "learning_rate", "clip", "batch_size", "sigma", "seed"}, {"param"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "dpsgd");
}

void DpsgdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, float clip, float batch_size, float sigma, int seed) {
  VLOG(4) << "Start build DpsgdOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_clip = pir::FloatAttribute::get(pir::IrContext::Instance(), clip);
  argument.AddAttribute("clip", attr_clip);
  pir::Attribute attr_batch_size = pir::FloatAttribute::get(pir::IrContext::Instance(), batch_size);
  argument.AddAttribute("batch_size", attr_batch_size);
  pir::Attribute attr_sigma = pir::FloatAttribute::get(pir::IrContext::Instance(), sigma);
  argument.AddAttribute("sigma", attr_sigma);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);

  phi::DpsgdInferMeta(meta_param, meta_grad, meta_learning_rate, clip, batch_size, sigma, seed, &meta_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DpsgdOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value learning_rate_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build DpsgdOp";


  IR_ENFORCE(
      attributes.find("clip") != attributes.end(),
          "'clip' Attribute is expected for DpsgdOp. ");
  float clip = attributes.at("clip").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("batch_size") != attributes.end(),
          "'batch_size' Attribute is expected for DpsgdOp. ");
  float batch_size = attributes.at("batch_size").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("sigma") != attributes.end(),
          "'sigma' Attribute is expected for DpsgdOp. ");
  float sigma = attributes.at("sigma").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for DpsgdOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, learning_rate_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_clip = pir::FloatAttribute::get(pir::IrContext::Instance(), clip);
  argument.AddAttribute("clip", attr_clip);
  pir::Attribute attr_batch_size = pir::FloatAttribute::get(pir::IrContext::Instance(), batch_size);
  argument.AddAttribute("batch_size", attr_batch_size);
  pir::Attribute attr_sigma = pir::FloatAttribute::get(pir::IrContext::Instance(), sigma);
  argument.AddAttribute("sigma", attr_sigma);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);

  phi::DpsgdInferMeta(meta_param, meta_grad, meta_learning_rate, clip, batch_size, sigma, seed, &meta_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void DpsgdOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: DpsgdOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("clip")>0,
                 "clip does not exist.");
  IR_ENFORCE(attributes.at("clip").isa<pir::FloatAttribute>(),
                 "Type of attribute: clip is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("batch_size")>0,
                 "batch_size does not exist.");
  IR_ENFORCE(attributes.at("batch_size").isa<pir::FloatAttribute>(),
                 "Type of attribute: batch_size is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("sigma")>0,
                 "sigma does not exist.");
  IR_ENFORCE(attributes.at("sigma").isa<pir::FloatAttribute>(),
                 "Type of attribute: sigma is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: DpsgdOp.";
}

void DpsgdOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::DpsgdInferMeta);
  fn(infer_meta);
}

phi::DataType DpsgdOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: DpsgdOp";
  


  return expected_kernel_dtype;
}

const char *FtrlOp::attributes_name[3] = { "l1", "l2", "lr_power" };

OpInfoTuple FtrlOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("squared_accumulator", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("linear_accumulator", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("l1", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("l2", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("lr_power", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("squared_accum_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("linear_accum_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FtrlInferMeta", {"param", "squared_accumulator", "linear_accumulator", "grad", "learning_rate", "l1", "l2", "lr_power"}, "ftrl", {"param", "squared_accumulator", "linear_accumulator", "grad", "learning_rate", "l1", "l2", "lr_power"}, {"param"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "ftrl");
}

void FtrlOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value squared_accumulator_, pir::Value linear_accumulator_, pir::Value grad_, pir::Value learning_rate_, float l1, float l2, float lr_power) {
  VLOG(4) << "Start build FtrlOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, squared_accumulator_, linear_accumulator_, grad_, learning_rate_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_l1 = pir::FloatAttribute::get(pir::IrContext::Instance(), l1);
  argument.AddAttribute("l1", attr_l1);
  pir::Attribute attr_l2 = pir::FloatAttribute::get(pir::IrContext::Instance(), l2);
  argument.AddAttribute("l2", attr_l2);
  pir::Attribute attr_lr_power = pir::FloatAttribute::get(pir::IrContext::Instance(), lr_power);
  argument.AddAttribute("lr_power", attr_lr_power);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType squared_accumulator = squared_accumulator_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)squared_accumulator;
  paddle::dialect::DenseTensorType linear_accumulator = linear_accumulator_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear_accumulator;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_squared_accumulator";
  paddle::dialect::IrTensor ir_tensor_squared_accumulator(paddle::dialect::TransToPhiDataType(squared_accumulator.dtype()),
                                                      squared_accumulator.dims(),
                                                      squared_accumulator.data_layout(),
                                                      squared_accumulator.lod(),
                                                      squared_accumulator.offset());
  VLOG(4) << "Builder construction  meta_squared_accumulator";
  paddle::dialect::IrMetaTensor meta_squared_accumulator(&ir_tensor_squared_accumulator);

  VLOG(4) << "Builder construction  dense_linear_accumulator";
  paddle::dialect::IrTensor ir_tensor_linear_accumulator(paddle::dialect::TransToPhiDataType(linear_accumulator.dtype()),
                                                      linear_accumulator.dims(),
                                                      linear_accumulator.data_layout(),
                                                      linear_accumulator.lod(),
                                                      linear_accumulator.offset());
  VLOG(4) << "Builder construction  meta_linear_accumulator";
  paddle::dialect::IrMetaTensor meta_linear_accumulator(&ir_tensor_linear_accumulator);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_squared_accum_out;
  paddle::dialect::IrMetaTensor meta_squared_accum_out(&dense_squared_accum_out);
  paddle::dialect::IrTensor dense_linear_accum_out;
  paddle::dialect::IrMetaTensor meta_linear_accum_out(&dense_linear_accum_out);

  phi::FtrlInferMeta(meta_param, meta_squared_accumulator, meta_linear_accumulator, meta_grad, meta_learning_rate, l1, l2, lr_power, &meta_param_out, &meta_squared_accum_out, &meta_linear_accum_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type squared_accum_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_accum_out.dtype()), dense_squared_accum_out.dims(), dense_squared_accum_out.layout(), dense_squared_accum_out.lod(), dense_squared_accum_out.offset());
  argument_outputs.push_back(squared_accum_out_dense_tensor_type);

  pir::Type linear_accum_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear_accum_out.dtype()), dense_linear_accum_out.dims(), dense_linear_accum_out.layout(), dense_linear_accum_out.lod(), dense_linear_accum_out.offset());
  argument_outputs.push_back(linear_accum_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FtrlOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value squared_accumulator_, pir::Value linear_accumulator_, pir::Value grad_, pir::Value learning_rate_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FtrlOp";


  IR_ENFORCE(
      attributes.find("l1") != attributes.end(),
          "'l1' Attribute is expected for FtrlOp. ");
  float l1 = attributes.at("l1").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("l2") != attributes.end(),
          "'l2' Attribute is expected for FtrlOp. ");
  float l2 = attributes.at("l2").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("lr_power") != attributes.end(),
          "'lr_power' Attribute is expected for FtrlOp. ");
  float lr_power = attributes.at("lr_power").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, squared_accumulator_, linear_accumulator_, grad_, learning_rate_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_l1 = pir::FloatAttribute::get(pir::IrContext::Instance(), l1);
  argument.AddAttribute("l1", attr_l1);
  pir::Attribute attr_l2 = pir::FloatAttribute::get(pir::IrContext::Instance(), l2);
  argument.AddAttribute("l2", attr_l2);
  pir::Attribute attr_lr_power = pir::FloatAttribute::get(pir::IrContext::Instance(), lr_power);
  argument.AddAttribute("lr_power", attr_lr_power);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType squared_accumulator = squared_accumulator_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)squared_accumulator;
  paddle::dialect::DenseTensorType linear_accumulator = linear_accumulator_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear_accumulator;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_squared_accumulator";
  paddle::dialect::IrTensor ir_tensor_squared_accumulator(paddle::dialect::TransToPhiDataType(squared_accumulator.dtype()),
                                                      squared_accumulator.dims(),
                                                      squared_accumulator.data_layout(),
                                                      squared_accumulator.lod(),
                                                      squared_accumulator.offset());
  VLOG(4) << "Builder construction  meta_squared_accumulator";
  paddle::dialect::IrMetaTensor meta_squared_accumulator(&ir_tensor_squared_accumulator);

  VLOG(4) << "Builder construction  dense_linear_accumulator";
  paddle::dialect::IrTensor ir_tensor_linear_accumulator(paddle::dialect::TransToPhiDataType(linear_accumulator.dtype()),
                                                      linear_accumulator.dims(),
                                                      linear_accumulator.data_layout(),
                                                      linear_accumulator.lod(),
                                                      linear_accumulator.offset());
  VLOG(4) << "Builder construction  meta_linear_accumulator";
  paddle::dialect::IrMetaTensor meta_linear_accumulator(&ir_tensor_linear_accumulator);

  VLOG(4) << "Builder construction  dense_grad";
  paddle::dialect::IrTensor ir_tensor_grad(paddle::dialect::TransToPhiDataType(grad.dtype()),
                                                      grad.dims(),
                                                      grad.data_layout(),
                                                      grad.lod(),
                                                      grad.offset());
  VLOG(4) << "Builder construction  meta_grad";
  paddle::dialect::IrMetaTensor meta_grad(&ir_tensor_grad);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_squared_accum_out;
  paddle::dialect::IrMetaTensor meta_squared_accum_out(&dense_squared_accum_out);
  paddle::dialect::IrTensor dense_linear_accum_out;
  paddle::dialect::IrMetaTensor meta_linear_accum_out(&dense_linear_accum_out);

  phi::FtrlInferMeta(meta_param, meta_squared_accumulator, meta_linear_accumulator, meta_grad, meta_learning_rate, l1, l2, lr_power, &meta_param_out, &meta_squared_accum_out, &meta_linear_accum_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type squared_accum_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_squared_accum_out.dtype()), dense_squared_accum_out.dims(), dense_squared_accum_out.layout(), dense_squared_accum_out.lod(), dense_squared_accum_out.offset());
  argument_outputs.push_back(squared_accum_out_dense_tensor_type);

  pir::Type linear_accum_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear_accum_out.dtype()), dense_linear_accum_out.dims(), dense_linear_accum_out.layout(), dense_linear_accum_out.lod(), dense_linear_accum_out.offset());
  argument_outputs.push_back(linear_accum_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FtrlOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FtrlOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("l1")>0,
                 "l1 does not exist.");
  IR_ENFORCE(attributes.at("l1").isa<pir::FloatAttribute>(),
                 "Type of attribute: l1 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("l2")>0,
                 "l2 does not exist.");
  IR_ENFORCE(attributes.at("l2").isa<pir::FloatAttribute>(),
                 "Type of attribute: l2 is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("lr_power")>0,
                 "lr_power does not exist.");
  IR_ENFORCE(attributes.at("lr_power").isa<pir::FloatAttribute>(),
                 "Type of attribute: lr_power is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: FtrlOp.";
}

void FtrlOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FtrlInferMeta);
  fn(infer_meta);
}

phi::DataType FtrlOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FtrlOp";
  


  return expected_kernel_dtype;
}

const char *FusedAttentionOp::attributes_name[16] = { "num_heads", "transpose_qkv_wb", "pre_layer_norm", "epsilon", "attn_dropout_rate", "is_test", "attn_dropout_fix_seed", "attn_dropout_seed", "attn_dropout_implementation", "dropout_rate", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon", "add_residual", "ring_id" };

OpInfoTuple FusedAttentionOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("ln_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("qkv_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("qkv_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("cache_kv", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("src_mask", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("out_linear_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("out_linear_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_scale_2", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln_bias_2", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_heads", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("transpose_qkv_wb", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("pre_layer_norm", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("attn_dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_rate", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout_seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dropout_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("ln_epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("add_residual", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("ln_mean", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln_var", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qkv_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qkv_bias_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("transpose_out_2", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qk_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("qktv_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("softmax_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("attn_dropout_mask_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("attn_dropout_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("src_mask_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("fmha_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("out_linear_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dropout_mask_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln_mean_2", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln_var_2", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("bias_dropout_residual_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("cache_kv_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedAttentionInferMeta", {"x", "ln_scale", "ln_bias", "qkv_weight", "qkv_bias", "cache_kv", "src_mask", "out_linear_weight", "out_linear_bias", "ln_scale_2", "ln_bias_2", "num_heads", "transpose_qkv_wb", "pre_layer_norm", "epsilon", "attn_dropout_rate", "is_test", "attn_dropout_fix_seed", "attn_dropout_seed", "attn_dropout_implementation", "dropout_rate", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon", "add_residual", "ring_id"}, "fused_attention", {"x", "ln_scale", "ln_bias", "qkv_weight", "qkv_bias", "cache_kv", "src_mask", "out_linear_weight", "out_linear_bias", "ln_scale_2", "ln_bias_2", "num_heads", "transpose_qkv_wb", "pre_layer_norm", "epsilon", "attn_dropout_rate", "is_test", "attn_dropout_fix_seed", "attn_dropout_seed", "attn_dropout_implementation", "dropout_rate", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon", "add_residual", "ring_id"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_attention");
}

void FusedAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value qkv_weight_, pir::Value qkv_bias_, pir::Value cache_kv_, pir::Value src_mask_, pir::Value out_linear_weight_, pir::Value out_linear_bias_, pir::Value ln_scale_2_, pir::Value ln_bias_2_, int num_heads, bool transpose_qkv_wb, bool pre_layer_norm, float epsilon, float attn_dropout_rate, bool is_test, bool attn_dropout_fix_seed, int attn_dropout_seed, const std::string& attn_dropout_implementation, float dropout_rate, bool dropout_fix_seed, int dropout_seed, const std::string& dropout_implementation, float ln_epsilon, bool add_residual, int ring_id) {
  VLOG(4) << "Start build FusedAttentionOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ln_scale_, ln_bias_, qkv_weight_, qkv_bias_, cache_kv_, src_mask_, out_linear_weight_, out_linear_bias_, ln_scale_2_, ln_bias_2_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_heads = pir::Int32Attribute::get(pir::IrContext::Instance(), num_heads);
  argument.AddAttribute("num_heads", attr_num_heads);
  pir::Attribute attr_transpose_qkv_wb = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_qkv_wb);
  argument.AddAttribute("transpose_qkv_wb", attr_transpose_qkv_wb);
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_attn_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), attn_dropout_rate);
  argument.AddAttribute("attn_dropout_rate", attr_attn_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_attn_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), attn_dropout_fix_seed);
  argument.AddAttribute("attn_dropout_fix_seed", attr_attn_dropout_fix_seed);
  pir::Attribute attr_attn_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), attn_dropout_seed);
  argument.AddAttribute("attn_dropout_seed", attr_attn_dropout_seed);
  pir::Attribute attr_attn_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), attn_dropout_implementation);
  argument.AddAttribute("attn_dropout_implementation", attr_attn_dropout_implementation);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType qkv_weight = qkv_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv_weight;
  paddle::dialect::DenseTensorType out_linear_weight = out_linear_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_linear_weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }


  VLOG(4) << "Builder construction  dense_qkv_weight";
  paddle::dialect::IrTensor ir_tensor_qkv_weight(paddle::dialect::TransToPhiDataType(qkv_weight.dtype()),
                                                      qkv_weight.dims(),
                                                      qkv_weight.data_layout(),
                                                      qkv_weight.lod(),
                                                      qkv_weight.offset());
  VLOG(4) << "Builder construction  meta_qkv_weight";
  paddle::dialect::IrMetaTensor meta_qkv_weight(&ir_tensor_qkv_weight);

  paddle::dialect::IrMetaTensor meta_qkv_bias;
  paddle::dialect::IrTensor ir_tensor_qkv_bias;
  if (qkv_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias = qkv_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias";
    ir_tensor_qkv_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias.dtype()),
                                                        qkv_bias.dims(),
                                                        qkv_bias.data_layout(),
                                                        qkv_bias.lod(),
                                                        qkv_bias.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias";
    meta_qkv_bias = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias);
  }


  paddle::dialect::IrMetaTensor meta_cache_kv;
  paddle::dialect::IrTensor ir_tensor_cache_kv;
  if (cache_kv_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_kv = cache_kv_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_kv";
    ir_tensor_cache_kv = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_kv.dtype()),
                                                        cache_kv.dims(),
                                                        cache_kv.data_layout(),
                                                        cache_kv.lod(),
                                                        cache_kv.offset());
    VLOG(4) << "Builder construction  meta_cache_kv";
    meta_cache_kv = paddle::dialect::IrMetaTensor(&ir_tensor_cache_kv);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  VLOG(4) << "Builder construction  dense_out_linear_weight";
  paddle::dialect::IrTensor ir_tensor_out_linear_weight(paddle::dialect::TransToPhiDataType(out_linear_weight.dtype()),
                                                      out_linear_weight.dims(),
                                                      out_linear_weight.data_layout(),
                                                      out_linear_weight.lod(),
                                                      out_linear_weight.offset());
  VLOG(4) << "Builder construction  meta_out_linear_weight";
  paddle::dialect::IrMetaTensor meta_out_linear_weight(&ir_tensor_out_linear_weight);

  paddle::dialect::IrMetaTensor meta_out_linear_bias;
  paddle::dialect::IrTensor ir_tensor_out_linear_bias;
  if (out_linear_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_linear_bias = out_linear_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_linear_bias";
    ir_tensor_out_linear_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias.dtype()),
                                                        out_linear_bias.dims(),
                                                        out_linear_bias.data_layout(),
                                                        out_linear_bias.lod(),
                                                        out_linear_bias.offset());
    VLOG(4) << "Builder construction  meta_out_linear_bias";
    meta_out_linear_bias = paddle::dialect::IrMetaTensor(&ir_tensor_out_linear_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale_2;
  paddle::dialect::IrTensor ir_tensor_ln_scale_2;
  if (ln_scale_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale_2 = ln_scale_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale_2";
    ir_tensor_ln_scale_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale_2.dtype()),
                                                        ln_scale_2.dims(),
                                                        ln_scale_2.data_layout(),
                                                        ln_scale_2.lod(),
                                                        ln_scale_2.offset());
    VLOG(4) << "Builder construction  meta_ln_scale_2";
    meta_ln_scale_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias_2;
  paddle::dialect::IrTensor ir_tensor_ln_bias_2;
  if (ln_bias_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias_2 = ln_bias_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias_2";
    ir_tensor_ln_bias_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias_2.dtype()),
                                                        ln_bias_2.dims(),
                                                        ln_bias_2.data_layout(),
                                                        ln_bias_2.lod(),
                                                        ln_bias_2.offset());
    VLOG(4) << "Builder construction  meta_ln_bias_2";
    meta_ln_bias_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias_2);
  }

  paddle::dialect::IrTensor dense_ln_mean;
  paddle::dialect::IrMetaTensor meta_ln_mean(&dense_ln_mean);
  paddle::dialect::IrTensor dense_ln_var;
  paddle::dialect::IrMetaTensor meta_ln_var(&dense_ln_var);
  paddle::dialect::IrTensor dense_ln_out;
  paddle::dialect::IrMetaTensor meta_ln_out(&dense_ln_out);
  paddle::dialect::IrTensor dense_qkv_out;
  paddle::dialect::IrMetaTensor meta_qkv_out(&dense_qkv_out);
  paddle::dialect::IrTensor dense_qkv_bias_out;
  paddle::dialect::IrMetaTensor meta_qkv_bias_out(&dense_qkv_bias_out);
  paddle::dialect::IrTensor dense_transpose_out_2;
  paddle::dialect::IrMetaTensor meta_transpose_out_2(&dense_transpose_out_2);
  paddle::dialect::IrTensor dense_qk_out;
  paddle::dialect::IrMetaTensor meta_qk_out(&dense_qk_out);
  paddle::dialect::IrTensor dense_qktv_out;
  paddle::dialect::IrMetaTensor meta_qktv_out(&dense_qktv_out);
  paddle::dialect::IrTensor dense_softmax_out;
  paddle::dialect::IrMetaTensor meta_softmax_out(&dense_softmax_out);
  paddle::dialect::IrTensor dense_attn_dropout_mask_out;
  paddle::dialect::IrMetaTensor meta_attn_dropout_mask_out(&dense_attn_dropout_mask_out);
  paddle::dialect::IrTensor dense_attn_dropout_out;
  paddle::dialect::IrMetaTensor meta_attn_dropout_out(&dense_attn_dropout_out);
  paddle::dialect::IrTensor dense_src_mask_out;
  paddle::dialect::IrMetaTensor meta_src_mask_out(&dense_src_mask_out);
  paddle::dialect::IrTensor dense_fmha_out;
  paddle::dialect::IrMetaTensor meta_fmha_out(&dense_fmha_out);
  paddle::dialect::IrTensor dense_out_linear_out;
  paddle::dialect::IrMetaTensor meta_out_linear_out(&dense_out_linear_out);
  paddle::dialect::IrTensor dense_dropout_mask_out;
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&dense_dropout_mask_out);
  paddle::dialect::IrTensor dense_ln_mean_2;
  paddle::dialect::IrMetaTensor meta_ln_mean_2(&dense_ln_mean_2);
  paddle::dialect::IrTensor dense_ln_var_2;
  paddle::dialect::IrMetaTensor meta_ln_var_2(&dense_ln_var_2);
  paddle::dialect::IrTensor dense_bias_dropout_residual_out;
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out(&dense_bias_dropout_residual_out);
  paddle::dialect::IrTensor dense_cache_kv_out;
  paddle::dialect::IrMetaTensor meta_cache_kv_out(&dense_cache_kv_out);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedAttentionInferMeta(meta_x, meta_ln_scale, meta_ln_bias, meta_qkv_weight, meta_qkv_bias, meta_cache_kv, meta_src_mask, meta_out_linear_weight, meta_out_linear_bias, meta_ln_scale_2, meta_ln_bias_2, num_heads, transpose_qkv_wb, pre_layer_norm, epsilon, attn_dropout_rate, is_test, attn_dropout_fix_seed, attn_dropout_seed, attn_dropout_implementation, dropout_rate, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, add_residual, ring_id, &meta_ln_mean, &meta_ln_var, &meta_ln_out, &meta_qkv_out, &meta_qkv_bias_out, &meta_transpose_out_2, &meta_qk_out, &meta_qktv_out, &meta_softmax_out, &meta_attn_dropout_mask_out, &meta_attn_dropout_out, &meta_src_mask_out, &meta_fmha_out, &meta_out_linear_out, &meta_dropout_mask_out, &meta_ln_mean_2, &meta_ln_var_2, &meta_bias_dropout_residual_out, &meta_cache_kv_out, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type ln_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_mean.dtype()), dense_ln_mean.dims(), dense_ln_mean.layout(), dense_ln_mean.lod(), dense_ln_mean.offset());
  argument_outputs.push_back(ln_mean_dense_tensor_type);

  pir::Type ln_var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_var.dtype()), dense_ln_var.dims(), dense_ln_var.layout(), dense_ln_var.lod(), dense_ln_var.offset());
  argument_outputs.push_back(ln_var_dense_tensor_type);

  pir::Type ln_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_out.dtype()), dense_ln_out.dims(), dense_ln_out.layout(), dense_ln_out.lod(), dense_ln_out.offset());
  argument_outputs.push_back(ln_out_dense_tensor_type);

  pir::Type qkv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_out.dtype()), dense_qkv_out.dims(), dense_qkv_out.layout(), dense_qkv_out.lod(), dense_qkv_out.offset());
  argument_outputs.push_back(qkv_out_dense_tensor_type);

  pir::Type qkv_bias_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_bias_out.dtype()), dense_qkv_bias_out.dims(), dense_qkv_bias_out.layout(), dense_qkv_bias_out.lod(), dense_qkv_bias_out.offset());
  argument_outputs.push_back(qkv_bias_out_dense_tensor_type);

  pir::Type transpose_out_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_transpose_out_2.dtype()), dense_transpose_out_2.dims(), dense_transpose_out_2.layout(), dense_transpose_out_2.lod(), dense_transpose_out_2.offset());
  argument_outputs.push_back(transpose_out_2_dense_tensor_type);

  pir::Type qk_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qk_out.dtype()), dense_qk_out.dims(), dense_qk_out.layout(), dense_qk_out.lod(), dense_qk_out.offset());
  argument_outputs.push_back(qk_out_dense_tensor_type);

  pir::Type qktv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qktv_out.dtype()), dense_qktv_out.dims(), dense_qktv_out.layout(), dense_qktv_out.lod(), dense_qktv_out.offset());
  argument_outputs.push_back(qktv_out_dense_tensor_type);

  pir::Type softmax_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_out.dtype()), dense_softmax_out.dims(), dense_softmax_out.layout(), dense_softmax_out.lod(), dense_softmax_out.offset());
  argument_outputs.push_back(softmax_out_dense_tensor_type);

  pir::Type attn_dropout_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_attn_dropout_mask_out.dtype()), dense_attn_dropout_mask_out.dims(), dense_attn_dropout_mask_out.layout(), dense_attn_dropout_mask_out.lod(), dense_attn_dropout_mask_out.offset());
  argument_outputs.push_back(attn_dropout_mask_out_dense_tensor_type);

  pir::Type attn_dropout_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_attn_dropout_out.dtype()), dense_attn_dropout_out.dims(), dense_attn_dropout_out.layout(), dense_attn_dropout_out.lod(), dense_attn_dropout_out.offset());
  argument_outputs.push_back(attn_dropout_out_dense_tensor_type);

  pir::Type src_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_src_mask_out.dtype()), dense_src_mask_out.dims(), dense_src_mask_out.layout(), dense_src_mask_out.lod(), dense_src_mask_out.offset());
  argument_outputs.push_back(src_mask_out_dense_tensor_type);

  pir::Type fmha_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fmha_out.dtype()), dense_fmha_out.dims(), dense_fmha_out.layout(), dense_fmha_out.lod(), dense_fmha_out.offset());
  argument_outputs.push_back(fmha_out_dense_tensor_type);

  pir::Type out_linear_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_out.dtype()), dense_out_linear_out.dims(), dense_out_linear_out.layout(), dense_out_linear_out.lod(), dense_out_linear_out.offset());
  argument_outputs.push_back(out_linear_out_dense_tensor_type);

  pir::Type dropout_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_mask_out.dtype()), dense_dropout_mask_out.dims(), dense_dropout_mask_out.layout(), dense_dropout_mask_out.lod(), dense_dropout_mask_out.offset());
  argument_outputs.push_back(dropout_mask_out_dense_tensor_type);

  pir::Type ln_mean_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_mean_2.dtype()), dense_ln_mean_2.dims(), dense_ln_mean_2.layout(), dense_ln_mean_2.lod(), dense_ln_mean_2.offset());
  argument_outputs.push_back(ln_mean_2_dense_tensor_type);

  pir::Type ln_var_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_var_2.dtype()), dense_ln_var_2.dims(), dense_ln_var_2.layout(), dense_ln_var_2.lod(), dense_ln_var_2.offset());
  argument_outputs.push_back(ln_var_2_dense_tensor_type);

  pir::Type bias_dropout_residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_dropout_residual_out.dtype()), dense_bias_dropout_residual_out.dims(), dense_bias_dropout_residual_out.layout(), dense_bias_dropout_residual_out.lod(), dense_bias_dropout_residual_out.offset());
  argument_outputs.push_back(bias_dropout_residual_out_dense_tensor_type);

  pir::Type cache_kv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_cache_kv_out.dtype()), dense_cache_kv_out.dims(), dense_cache_kv_out.layout(), dense_cache_kv_out.lod(), dense_cache_kv_out.offset());
  argument_outputs.push_back(cache_kv_out_dense_tensor_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedAttentionOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value ln_scale_, pir::Value ln_bias_, pir::Value qkv_weight_, pir::Value qkv_bias_, pir::Value cache_kv_, pir::Value src_mask_, pir::Value out_linear_weight_, pir::Value out_linear_bias_, pir::Value ln_scale_2_, pir::Value ln_bias_2_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedAttentionOp";


  IR_ENFORCE(
      attributes.find("num_heads") != attributes.end(),
          "'num_heads' Attribute is expected for FusedAttentionOp. ");
  int num_heads = attributes.at("num_heads").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("transpose_qkv_wb") != attributes.end(),
          "'transpose_qkv_wb' Attribute is expected for FusedAttentionOp. ");
  bool transpose_qkv_wb = attributes.at("transpose_qkv_wb").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("pre_layer_norm") != attributes.end(),
          "'pre_layer_norm' Attribute is expected for FusedAttentionOp. ");
  bool pre_layer_norm = attributes.at("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for FusedAttentionOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_rate") != attributes.end(),
          "'attn_dropout_rate' Attribute is expected for FusedAttentionOp. ");
  float attn_dropout_rate = attributes.at("attn_dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedAttentionOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_fix_seed") != attributes.end(),
          "'attn_dropout_fix_seed' Attribute is expected for FusedAttentionOp. ");
  bool attn_dropout_fix_seed = attributes.at("attn_dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_seed") != attributes.end(),
          "'attn_dropout_seed' Attribute is expected for FusedAttentionOp. ");
  int attn_dropout_seed = attributes.at("attn_dropout_seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("attn_dropout_implementation") != attributes.end(),
          "'attn_dropout_implementation' Attribute is expected for FusedAttentionOp. ");
  std::string attn_dropout_implementation = attributes.at("attn_dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dropout_rate") != attributes.end(),
          "'dropout_rate' Attribute is expected for FusedAttentionOp. ");
  float dropout_rate = attributes.at("dropout_rate").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_fix_seed") != attributes.end(),
          "'dropout_fix_seed' Attribute is expected for FusedAttentionOp. ");
  bool dropout_fix_seed = attributes.at("dropout_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_seed") != attributes.end(),
          "'dropout_seed' Attribute is expected for FusedAttentionOp. ");
  int dropout_seed = attributes.at("dropout_seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dropout_implementation") != attributes.end(),
          "'dropout_implementation' Attribute is expected for FusedAttentionOp. ");
  std::string dropout_implementation = attributes.at("dropout_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("ln_epsilon") != attributes.end(),
          "'ln_epsilon' Attribute is expected for FusedAttentionOp. ");
  float ln_epsilon = attributes.at("ln_epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("add_residual") != attributes.end(),
          "'add_residual' Attribute is expected for FusedAttentionOp. ");
  bool add_residual = attributes.at("add_residual").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for FusedAttentionOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, ln_scale_, ln_bias_, qkv_weight_, qkv_bias_, cache_kv_, src_mask_, out_linear_weight_, out_linear_bias_, ln_scale_2_, ln_bias_2_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_heads = pir::Int32Attribute::get(pir::IrContext::Instance(), num_heads);
  argument.AddAttribute("num_heads", attr_num_heads);
  pir::Attribute attr_transpose_qkv_wb = pir::BoolAttribute::get(pir::IrContext::Instance(), transpose_qkv_wb);
  argument.AddAttribute("transpose_qkv_wb", attr_transpose_qkv_wb);
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_attn_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), attn_dropout_rate);
  argument.AddAttribute("attn_dropout_rate", attr_attn_dropout_rate);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_attn_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), attn_dropout_fix_seed);
  argument.AddAttribute("attn_dropout_fix_seed", attr_attn_dropout_fix_seed);
  pir::Attribute attr_attn_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), attn_dropout_seed);
  argument.AddAttribute("attn_dropout_seed", attr_attn_dropout_seed);
  pir::Attribute attr_attn_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), attn_dropout_implementation);
  argument.AddAttribute("attn_dropout_implementation", attr_attn_dropout_implementation);
  pir::Attribute attr_dropout_rate = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout_rate);
  argument.AddAttribute("dropout_rate", attr_dropout_rate);
  pir::Attribute attr_dropout_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout_fix_seed);
  argument.AddAttribute("dropout_fix_seed", attr_dropout_fix_seed);
  pir::Attribute attr_dropout_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout_seed);
  argument.AddAttribute("dropout_seed", attr_dropout_seed);
  pir::Attribute attr_dropout_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout_implementation);
  argument.AddAttribute("dropout_implementation", attr_dropout_implementation);
  pir::Attribute attr_ln_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln_epsilon);
  argument.AddAttribute("ln_epsilon", attr_ln_epsilon);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType qkv_weight = qkv_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)qkv_weight;
  paddle::dialect::DenseTensorType out_linear_weight = out_linear_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)out_linear_weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_ln_scale;
  paddle::dialect::IrTensor ir_tensor_ln_scale;
  if (ln_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale = ln_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale";
    ir_tensor_ln_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale.dtype()),
                                                        ln_scale.dims(),
                                                        ln_scale.data_layout(),
                                                        ln_scale.lod(),
                                                        ln_scale.offset());
    VLOG(4) << "Builder construction  meta_ln_scale";
    meta_ln_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias;
  paddle::dialect::IrTensor ir_tensor_ln_bias;
  if (ln_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias = ln_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias";
    ir_tensor_ln_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias.dtype()),
                                                        ln_bias.dims(),
                                                        ln_bias.data_layout(),
                                                        ln_bias.lod(),
                                                        ln_bias.offset());
    VLOG(4) << "Builder construction  meta_ln_bias";
    meta_ln_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias);
  }


  VLOG(4) << "Builder construction  dense_qkv_weight";
  paddle::dialect::IrTensor ir_tensor_qkv_weight(paddle::dialect::TransToPhiDataType(qkv_weight.dtype()),
                                                      qkv_weight.dims(),
                                                      qkv_weight.data_layout(),
                                                      qkv_weight.lod(),
                                                      qkv_weight.offset());
  VLOG(4) << "Builder construction  meta_qkv_weight";
  paddle::dialect::IrMetaTensor meta_qkv_weight(&ir_tensor_qkv_weight);

  paddle::dialect::IrMetaTensor meta_qkv_bias;
  paddle::dialect::IrTensor ir_tensor_qkv_bias;
  if (qkv_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType qkv_bias = qkv_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_qkv_bias";
    ir_tensor_qkv_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(qkv_bias.dtype()),
                                                        qkv_bias.dims(),
                                                        qkv_bias.data_layout(),
                                                        qkv_bias.lod(),
                                                        qkv_bias.offset());
    VLOG(4) << "Builder construction  meta_qkv_bias";
    meta_qkv_bias = paddle::dialect::IrMetaTensor(&ir_tensor_qkv_bias);
  }


  paddle::dialect::IrMetaTensor meta_cache_kv;
  paddle::dialect::IrTensor ir_tensor_cache_kv;
  if (cache_kv_.impl() != nullptr) {
    paddle::dialect::DenseTensorType cache_kv = cache_kv_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_cache_kv";
    ir_tensor_cache_kv = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(cache_kv.dtype()),
                                                        cache_kv.dims(),
                                                        cache_kv.data_layout(),
                                                        cache_kv.lod(),
                                                        cache_kv.offset());
    VLOG(4) << "Builder construction  meta_cache_kv";
    meta_cache_kv = paddle::dialect::IrMetaTensor(&ir_tensor_cache_kv);
  }


  paddle::dialect::IrMetaTensor meta_src_mask;
  paddle::dialect::IrTensor ir_tensor_src_mask;
  if (src_mask_.impl() != nullptr) {
    paddle::dialect::DenseTensorType src_mask = src_mask_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_src_mask";
    ir_tensor_src_mask = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(src_mask.dtype()),
                                                        src_mask.dims(),
                                                        src_mask.data_layout(),
                                                        src_mask.lod(),
                                                        src_mask.offset());
    VLOG(4) << "Builder construction  meta_src_mask";
    meta_src_mask = paddle::dialect::IrMetaTensor(&ir_tensor_src_mask);
  }


  VLOG(4) << "Builder construction  dense_out_linear_weight";
  paddle::dialect::IrTensor ir_tensor_out_linear_weight(paddle::dialect::TransToPhiDataType(out_linear_weight.dtype()),
                                                      out_linear_weight.dims(),
                                                      out_linear_weight.data_layout(),
                                                      out_linear_weight.lod(),
                                                      out_linear_weight.offset());
  VLOG(4) << "Builder construction  meta_out_linear_weight";
  paddle::dialect::IrMetaTensor meta_out_linear_weight(&ir_tensor_out_linear_weight);

  paddle::dialect::IrMetaTensor meta_out_linear_bias;
  paddle::dialect::IrTensor ir_tensor_out_linear_bias;
  if (out_linear_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType out_linear_bias = out_linear_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_out_linear_bias";
    ir_tensor_out_linear_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(out_linear_bias.dtype()),
                                                        out_linear_bias.dims(),
                                                        out_linear_bias.data_layout(),
                                                        out_linear_bias.lod(),
                                                        out_linear_bias.offset());
    VLOG(4) << "Builder construction  meta_out_linear_bias";
    meta_out_linear_bias = paddle::dialect::IrMetaTensor(&ir_tensor_out_linear_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln_scale_2;
  paddle::dialect::IrTensor ir_tensor_ln_scale_2;
  if (ln_scale_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_scale_2 = ln_scale_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_scale_2";
    ir_tensor_ln_scale_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_scale_2.dtype()),
                                                        ln_scale_2.dims(),
                                                        ln_scale_2.data_layout(),
                                                        ln_scale_2.lod(),
                                                        ln_scale_2.offset());
    VLOG(4) << "Builder construction  meta_ln_scale_2";
    meta_ln_scale_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_scale_2);
  }


  paddle::dialect::IrMetaTensor meta_ln_bias_2;
  paddle::dialect::IrTensor ir_tensor_ln_bias_2;
  if (ln_bias_2_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln_bias_2 = ln_bias_2_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln_bias_2";
    ir_tensor_ln_bias_2 = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln_bias_2.dtype()),
                                                        ln_bias_2.dims(),
                                                        ln_bias_2.data_layout(),
                                                        ln_bias_2.lod(),
                                                        ln_bias_2.offset());
    VLOG(4) << "Builder construction  meta_ln_bias_2";
    meta_ln_bias_2 = paddle::dialect::IrMetaTensor(&ir_tensor_ln_bias_2);
  }

  paddle::dialect::IrTensor dense_ln_mean;
  paddle::dialect::IrMetaTensor meta_ln_mean(&dense_ln_mean);
  paddle::dialect::IrTensor dense_ln_var;
  paddle::dialect::IrMetaTensor meta_ln_var(&dense_ln_var);
  paddle::dialect::IrTensor dense_ln_out;
  paddle::dialect::IrMetaTensor meta_ln_out(&dense_ln_out);
  paddle::dialect::IrTensor dense_qkv_out;
  paddle::dialect::IrMetaTensor meta_qkv_out(&dense_qkv_out);
  paddle::dialect::IrTensor dense_qkv_bias_out;
  paddle::dialect::IrMetaTensor meta_qkv_bias_out(&dense_qkv_bias_out);
  paddle::dialect::IrTensor dense_transpose_out_2;
  paddle::dialect::IrMetaTensor meta_transpose_out_2(&dense_transpose_out_2);
  paddle::dialect::IrTensor dense_qk_out;
  paddle::dialect::IrMetaTensor meta_qk_out(&dense_qk_out);
  paddle::dialect::IrTensor dense_qktv_out;
  paddle::dialect::IrMetaTensor meta_qktv_out(&dense_qktv_out);
  paddle::dialect::IrTensor dense_softmax_out;
  paddle::dialect::IrMetaTensor meta_softmax_out(&dense_softmax_out);
  paddle::dialect::IrTensor dense_attn_dropout_mask_out;
  paddle::dialect::IrMetaTensor meta_attn_dropout_mask_out(&dense_attn_dropout_mask_out);
  paddle::dialect::IrTensor dense_attn_dropout_out;
  paddle::dialect::IrMetaTensor meta_attn_dropout_out(&dense_attn_dropout_out);
  paddle::dialect::IrTensor dense_src_mask_out;
  paddle::dialect::IrMetaTensor meta_src_mask_out(&dense_src_mask_out);
  paddle::dialect::IrTensor dense_fmha_out;
  paddle::dialect::IrMetaTensor meta_fmha_out(&dense_fmha_out);
  paddle::dialect::IrTensor dense_out_linear_out;
  paddle::dialect::IrMetaTensor meta_out_linear_out(&dense_out_linear_out);
  paddle::dialect::IrTensor dense_dropout_mask_out;
  paddle::dialect::IrMetaTensor meta_dropout_mask_out(&dense_dropout_mask_out);
  paddle::dialect::IrTensor dense_ln_mean_2;
  paddle::dialect::IrMetaTensor meta_ln_mean_2(&dense_ln_mean_2);
  paddle::dialect::IrTensor dense_ln_var_2;
  paddle::dialect::IrMetaTensor meta_ln_var_2(&dense_ln_var_2);
  paddle::dialect::IrTensor dense_bias_dropout_residual_out;
  paddle::dialect::IrMetaTensor meta_bias_dropout_residual_out(&dense_bias_dropout_residual_out);
  paddle::dialect::IrTensor dense_cache_kv_out;
  paddle::dialect::IrMetaTensor meta_cache_kv_out(&dense_cache_kv_out);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::FusedAttentionInferMeta(meta_x, meta_ln_scale, meta_ln_bias, meta_qkv_weight, meta_qkv_bias, meta_cache_kv, meta_src_mask, meta_out_linear_weight, meta_out_linear_bias, meta_ln_scale_2, meta_ln_bias_2, num_heads, transpose_qkv_wb, pre_layer_norm, epsilon, attn_dropout_rate, is_test, attn_dropout_fix_seed, attn_dropout_seed, attn_dropout_implementation, dropout_rate, dropout_fix_seed, dropout_seed, dropout_implementation, ln_epsilon, add_residual, ring_id, &meta_ln_mean, &meta_ln_var, &meta_ln_out, &meta_qkv_out, &meta_qkv_bias_out, &meta_transpose_out_2, &meta_qk_out, &meta_qktv_out, &meta_softmax_out, &meta_attn_dropout_mask_out, &meta_attn_dropout_out, &meta_src_mask_out, &meta_fmha_out, &meta_out_linear_out, &meta_dropout_mask_out, &meta_ln_mean_2, &meta_ln_var_2, &meta_bias_dropout_residual_out, &meta_cache_kv_out, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type ln_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_mean.dtype()), dense_ln_mean.dims(), dense_ln_mean.layout(), dense_ln_mean.lod(), dense_ln_mean.offset());
  argument_outputs.push_back(ln_mean_dense_tensor_type);

  pir::Type ln_var_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_var.dtype()), dense_ln_var.dims(), dense_ln_var.layout(), dense_ln_var.lod(), dense_ln_var.offset());
  argument_outputs.push_back(ln_var_dense_tensor_type);

  pir::Type ln_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_out.dtype()), dense_ln_out.dims(), dense_ln_out.layout(), dense_ln_out.lod(), dense_ln_out.offset());
  argument_outputs.push_back(ln_out_dense_tensor_type);

  pir::Type qkv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_out.dtype()), dense_qkv_out.dims(), dense_qkv_out.layout(), dense_qkv_out.lod(), dense_qkv_out.offset());
  argument_outputs.push_back(qkv_out_dense_tensor_type);

  pir::Type qkv_bias_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qkv_bias_out.dtype()), dense_qkv_bias_out.dims(), dense_qkv_bias_out.layout(), dense_qkv_bias_out.lod(), dense_qkv_bias_out.offset());
  argument_outputs.push_back(qkv_bias_out_dense_tensor_type);

  pir::Type transpose_out_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_transpose_out_2.dtype()), dense_transpose_out_2.dims(), dense_transpose_out_2.layout(), dense_transpose_out_2.lod(), dense_transpose_out_2.offset());
  argument_outputs.push_back(transpose_out_2_dense_tensor_type);

  pir::Type qk_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qk_out.dtype()), dense_qk_out.dims(), dense_qk_out.layout(), dense_qk_out.lod(), dense_qk_out.offset());
  argument_outputs.push_back(qk_out_dense_tensor_type);

  pir::Type qktv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_qktv_out.dtype()), dense_qktv_out.dims(), dense_qktv_out.layout(), dense_qktv_out.lod(), dense_qktv_out.offset());
  argument_outputs.push_back(qktv_out_dense_tensor_type);

  pir::Type softmax_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_softmax_out.dtype()), dense_softmax_out.dims(), dense_softmax_out.layout(), dense_softmax_out.lod(), dense_softmax_out.offset());
  argument_outputs.push_back(softmax_out_dense_tensor_type);

  pir::Type attn_dropout_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_attn_dropout_mask_out.dtype()), dense_attn_dropout_mask_out.dims(), dense_attn_dropout_mask_out.layout(), dense_attn_dropout_mask_out.lod(), dense_attn_dropout_mask_out.offset());
  argument_outputs.push_back(attn_dropout_mask_out_dense_tensor_type);

  pir::Type attn_dropout_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_attn_dropout_out.dtype()), dense_attn_dropout_out.dims(), dense_attn_dropout_out.layout(), dense_attn_dropout_out.lod(), dense_attn_dropout_out.offset());
  argument_outputs.push_back(attn_dropout_out_dense_tensor_type);

  pir::Type src_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_src_mask_out.dtype()), dense_src_mask_out.dims(), dense_src_mask_out.layout(), dense_src_mask_out.lod(), dense_src_mask_out.offset());
  argument_outputs.push_back(src_mask_out_dense_tensor_type);

  pir::Type fmha_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_fmha_out.dtype()), dense_fmha_out.dims(), dense_fmha_out.layout(), dense_fmha_out.lod(), dense_fmha_out.offset());
  argument_outputs.push_back(fmha_out_dense_tensor_type);

  pir::Type out_linear_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out_linear_out.dtype()), dense_out_linear_out.dims(), dense_out_linear_out.layout(), dense_out_linear_out.lod(), dense_out_linear_out.offset());
  argument_outputs.push_back(out_linear_out_dense_tensor_type);

  pir::Type dropout_mask_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout_mask_out.dtype()), dense_dropout_mask_out.dims(), dense_dropout_mask_out.layout(), dense_dropout_mask_out.lod(), dense_dropout_mask_out.offset());
  argument_outputs.push_back(dropout_mask_out_dense_tensor_type);

  pir::Type ln_mean_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_mean_2.dtype()), dense_ln_mean_2.dims(), dense_ln_mean_2.layout(), dense_ln_mean_2.lod(), dense_ln_mean_2.offset());
  argument_outputs.push_back(ln_mean_2_dense_tensor_type);

  pir::Type ln_var_2_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln_var_2.dtype()), dense_ln_var_2.dims(), dense_ln_var_2.layout(), dense_ln_var_2.lod(), dense_ln_var_2.offset());
  argument_outputs.push_back(ln_var_2_dense_tensor_type);

  pir::Type bias_dropout_residual_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_bias_dropout_residual_out.dtype()), dense_bias_dropout_residual_out.dims(), dense_bias_dropout_residual_out.layout(), dense_bias_dropout_residual_out.lod(), dense_bias_dropout_residual_out.offset());
  argument_outputs.push_back(bias_dropout_residual_out_dense_tensor_type);

  pir::Type cache_kv_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_cache_kv_out.dtype()), dense_cache_kv_out.dims(), dense_cache_kv_out.layout(), dense_cache_kv_out.lod(), dense_cache_kv_out.offset());
  argument_outputs.push_back(cache_kv_out_dense_tensor_type);

  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedAttentionOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedAttentionOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 11u,
                    "The size %d of inputs must be equal to 11.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  IR_ENFORCE((*this)->operand_source(7).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  if (auto val = (*this)->operand(9)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  }
  if (auto val = (*this)->operand(10)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("num_heads")>0,
                 "num_heads does not exist.");
  IR_ENFORCE(attributes.at("num_heads").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_heads is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("transpose_qkv_wb")>0,
                 "transpose_qkv_wb does not exist.");
  IR_ENFORCE(attributes.at("transpose_qkv_wb").isa<pir::BoolAttribute>(),
                 "Type of attribute: transpose_qkv_wb is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("pre_layer_norm")>0,
                 "pre_layer_norm does not exist.");
  IR_ENFORCE(attributes.at("pre_layer_norm").isa<pir::BoolAttribute>(),
                 "Type of attribute: pre_layer_norm is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("attn_dropout_rate")>0,
                 "attn_dropout_rate does not exist.");
  IR_ENFORCE(attributes.at("attn_dropout_rate").isa<pir::FloatAttribute>(),
                 "Type of attribute: attn_dropout_rate is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("attn_dropout_fix_seed")>0,
                 "attn_dropout_fix_seed does not exist.");
  IR_ENFORCE(attributes.at("attn_dropout_fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: attn_dropout_fix_seed is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("attn_dropout_seed")>0,
                 "attn_dropout_seed does not exist.");
  IR_ENFORCE(attributes.at("attn_dropout_seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: attn_dropout_seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("attn_dropout_implementation")>0,
                 "attn_dropout_implementation does not exist.");
  IR_ENFORCE(attributes.at("attn_dropout_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: attn_dropout_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dropout_rate")>0,
                 "dropout_rate does not exist.");
  IR_ENFORCE(attributes.at("dropout_rate").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout_rate is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout_fix_seed")>0,
                 "dropout_fix_seed does not exist.");
  IR_ENFORCE(attributes.at("dropout_fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: dropout_fix_seed is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout_seed")>0,
                 "dropout_seed does not exist.");
  IR_ENFORCE(attributes.at("dropout_seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: dropout_seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dropout_implementation")>0,
                 "dropout_implementation does not exist.");
  IR_ENFORCE(attributes.at("dropout_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: dropout_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("ln_epsilon")>0,
                 "ln_epsilon does not exist.");
  IR_ENFORCE(attributes.at("ln_epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: ln_epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("add_residual")>0,
                 "add_residual does not exist.");
  IR_ENFORCE(attributes.at("add_residual").isa<pir::BoolAttribute>(),
                 "Type of attribute: add_residual is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 20u,
                    "The size %d of outputs must be equal to 20.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  IR_ENFORCE((*this)->result(3).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 3th output.");
  IR_ENFORCE((*this)->result(4).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 4th output.");
  IR_ENFORCE((*this)->result(5).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 5th output.");
  IR_ENFORCE((*this)->result(6).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 6th output.");
  IR_ENFORCE((*this)->result(7).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 7th output.");
  IR_ENFORCE((*this)->result(8).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 8th output.");
  IR_ENFORCE((*this)->result(9).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 9th output.");
  IR_ENFORCE((*this)->result(10).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 10th output.");
  IR_ENFORCE((*this)->result(11).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 11th output.");
  IR_ENFORCE((*this)->result(12).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 12th output.");
  IR_ENFORCE((*this)->result(13).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 13th output.");
  IR_ENFORCE((*this)->result(14).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 14th output.");
  if (auto output_15_type = (*this)->result(15).type()) {
    IR_ENFORCE(output_15_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 15th output.");
  }
  if (auto output_16_type = (*this)->result(16).type()) {
    IR_ENFORCE(output_16_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 16th output.");
  }
  if (auto output_17_type = (*this)->result(17).type()) {
    IR_ENFORCE(output_17_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 17th output.");
  }
  if (auto output_18_type = (*this)->result(18).type()) {
    IR_ENFORCE(output_18_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 18th output.");
  }
  IR_ENFORCE((*this)->result(19).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 19th output.");
  }
  VLOG(4) << "End Verifying for: FusedAttentionOp.";
}

void FusedAttentionOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedAttentionInferMeta);
  fn(infer_meta);
}

phi::DataType FusedAttentionOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedAttentionOp";
  


  return expected_kernel_dtype;
}

const char *FusedElemwiseAddActivationOp::attributes_name[4] = { "functor_list", "scale", "axis", "save_intermediate_out" };

OpInfoTuple FusedElemwiseAddActivationOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("functor_list", "pir::ArrayAttribute<pir::StrAttribute>", ""), paddle::dialect::OpAttributeInfo("scale", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("axis", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("save_intermediate_out", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("intermediate_out", "paddle::dialect::DenseTensorType", true, true) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedElemwiseAddActivationInferMeta", {"x", "y", "functor_list", "scale", "axis", "save_intermediate_out"}, "fused_elemwise_add_activation", {"x", "y", "functor_list", "scale", "axis", "save_intermediate_out"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_elemwise_add_activation");
}

void FusedElemwiseAddActivationOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, const std::vector<std::string>& functor_list, float scale, int axis, bool save_intermediate_out) {
  VLOG(4) << "Start build FusedElemwiseAddActivationOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_functor_list;
  for (size_t i = 0; i < static_cast<size_t>(functor_list.size()); i++) {
      pir::Attribute attr_functor_list = pir::StrAttribute::get(pir::IrContext::Instance(), functor_list[i]);

    vec_functor_list.push_back(attr_functor_list);
  }
  pir::Attribute attr_functor_list = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_functor_list);
  argument.AddAttribute("functor_list", attr_functor_list);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_save_intermediate_out = pir::BoolAttribute::get(pir::IrContext::Instance(), save_intermediate_out);
  argument.AddAttribute("save_intermediate_out", attr_save_intermediate_out);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_intermediate_out;
  paddle::dialect::IrMetaTensor meta_intermediate_out(&dense_intermediate_out);

  phi::FusedElemwiseAddActivationInferMeta(meta_x, meta_y, functor_list, scale, axis, save_intermediate_out, &meta_out, &meta_intermediate_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type intermediate_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_intermediate_out.dtype()), dense_intermediate_out.dims(), dense_intermediate_out.layout(), dense_intermediate_out.lod(), dense_intermediate_out.offset());
  argument_outputs.push_back(intermediate_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedElemwiseAddActivationOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedElemwiseAddActivationOp";


  IR_ENFORCE(
      attributes.find("functor_list") != attributes.end(),
          "'functor_list' Attribute is expected for FusedElemwiseAddActivationOp. ");
  std::vector<std::string> functor_list;
  for (size_t i = 0; i < attributes.at("functor_list").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    functor_list.push_back(attributes.at("functor_list").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::StrAttribute>().AsString());
  }

  IR_ENFORCE(
      attributes.find("scale") != attributes.end(),
          "'scale' Attribute is expected for FusedElemwiseAddActivationOp. ");
  float scale = attributes.at("scale").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for FusedElemwiseAddActivationOp. ");
  int axis = attributes.at("axis").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("save_intermediate_out") != attributes.end(),
          "'save_intermediate_out' Attribute is expected for FusedElemwiseAddActivationOp. ");
  bool save_intermediate_out = attributes.at("save_intermediate_out").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  std::vector<pir::Attribute> vec_functor_list;
  for (size_t i = 0; i < static_cast<size_t>(functor_list.size()); i++) {
      pir::Attribute attr_functor_list = pir::StrAttribute::get(pir::IrContext::Instance(), functor_list[i]);

    vec_functor_list.push_back(attr_functor_list);
  }
  pir::Attribute attr_functor_list = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_functor_list);
  argument.AddAttribute("functor_list", attr_functor_list);
  pir::Attribute attr_scale = pir::FloatAttribute::get(pir::IrContext::Instance(), scale);
  argument.AddAttribute("scale", attr_scale);
  pir::Attribute attr_axis = pir::Int32Attribute::get(pir::IrContext::Instance(), axis);
  argument.AddAttribute("axis", attr_axis);
  pir::Attribute attr_save_intermediate_out = pir::BoolAttribute::get(pir::IrContext::Instance(), save_intermediate_out);
  argument.AddAttribute("save_intermediate_out", attr_save_intermediate_out);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_intermediate_out;
  paddle::dialect::IrMetaTensor meta_intermediate_out(&dense_intermediate_out);

  phi::FusedElemwiseAddActivationInferMeta(meta_x, meta_y, functor_list, scale, axis, save_intermediate_out, &meta_out, &meta_intermediate_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type intermediate_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_intermediate_out.dtype()), dense_intermediate_out.dims(), dense_intermediate_out.layout(), dense_intermediate_out.lod(), dense_intermediate_out.offset());
  argument_outputs.push_back(intermediate_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedElemwiseAddActivationOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedElemwiseAddActivationOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 2u,
                    "The size %d of inputs must be equal to 2.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("functor_list")>0,
                 "functor_list does not exist.");
  IR_ENFORCE(attributes.at("functor_list").isa<pir::ArrayAttribute>(),
                 "Type of attribute: functor_list is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("functor_list").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("functor_list").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::StrAttribute>(),
                   "Type of attribute: functor_list is not right.");
  }
  IR_ENFORCE(attributes.count("scale")>0,
                 "scale does not exist.");
  IR_ENFORCE(attributes.at("scale").isa<pir::FloatAttribute>(),
                 "Type of attribute: scale is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("axis")>0,
                 "axis does not exist.");
  IR_ENFORCE(attributes.at("axis").isa<pir::Int32Attribute>(),
                 "Type of attribute: axis is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("save_intermediate_out")>0,
                 "save_intermediate_out does not exist.");
  IR_ENFORCE(attributes.at("save_intermediate_out").isa<pir::BoolAttribute>(),
                 "Type of attribute: save_intermediate_out is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  if (auto output_1_type = (*this)->result(1).type()) {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  }
  VLOG(4) << "End Verifying for: FusedElemwiseAddActivationOp.";
}

void FusedElemwiseAddActivationOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedElemwiseAddActivationInferMeta);
  fn(infer_meta);
}

phi::DataType FusedElemwiseAddActivationOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedElemwiseAddActivationOp";
  


  return expected_kernel_dtype;
}

const char *FusedFeedforwardOp::attributes_name[15] = { "pre_layer_norm", "ln1_epsilon", "ln2_epsilon", "act_method", "dropout1_prob", "dropout2_prob", "dropout1_implementation", "dropout2_implementation", "is_test", "dropout1_fix_seed", "dropout2_fix_seed", "dropout1_seed_val", "dropout2_seed_val", "add_residual", "ring_id" };

OpInfoTuple FusedFeedforwardOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("dropout1_seed", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("dropout2_seed", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("linear1_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("linear1_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("linear2_weight", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("linear2_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln1_bias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln2_scale", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("ln2_bias", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("pre_layer_norm", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ln1_epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("ln2_epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("act_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout2_prob", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("dropout2_implementation", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout2_fix_seed", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("dropout1_seed_val", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("dropout2_seed_val", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("add_residual", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("ring_id", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dropout1_mask", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dropout2_mask", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln1_mean", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln1_variance", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln2_mean", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("ln2_variance", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("linear1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("ln1_out", "paddle::dialect::DenseTensorType", true, false), paddle::dialect::OpOutputInfo("dropout1_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("dropout2_out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("FusedFeedForwardInferMeta", {"x", "dropout1_seed", "dropout2_seed", "linear1_weight", "linear1_bias", "linear2_weight", "linear2_bias", "ln1_scale", "ln1_bias", "ln2_scale", "ln2_bias", "pre_layer_norm", "ln1_epsilon", "ln2_epsilon", "act_method", "dropout1_prob", "dropout2_prob", "dropout1_implementation", "dropout2_implementation", "is_test", "dropout1_fix_seed", "dropout2_fix_seed", "dropout1_seed_val", "dropout2_seed_val", "add_residual", "ring_id"}, "fused_feedforward", {"x", "dropout1_seed", "dropout2_seed", "linear1_weight", "linear1_bias", "linear2_weight", "linear2_bias", "ln1_scale", "ln1_bias", "ln2_scale", "ln2_bias", "pre_layer_norm", "ln1_epsilon", "ln2_epsilon", "act_method", "dropout1_prob", "dropout2_prob", "dropout1_implementation", "dropout2_implementation", "is_test", "dropout1_fix_seed", "dropout2_fix_seed", "dropout1_seed_val", "dropout2_seed_val", "add_residual", "ring_id"}, {"x"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "fused_feedforward");
}

void FusedFeedforwardOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value dropout1_seed_, pir::Value dropout2_seed_, pir::Value linear1_weight_, pir::Value linear1_bias_, pir::Value linear2_weight_, pir::Value linear2_bias_, pir::Value ln1_scale_, pir::Value ln1_bias_, pir::Value ln2_scale_, pir::Value ln2_bias_, bool pre_layer_norm, float ln1_epsilon, float ln2_epsilon, const std::string& act_method, float dropout1_prob, float dropout2_prob, const std::string& dropout1_implementation, const std::string& dropout2_implementation, bool is_test, bool dropout1_fix_seed, bool dropout2_fix_seed, int dropout1_seed_val, int dropout2_seed_val, bool add_residual, int ring_id) {
  VLOG(4) << "Start build FusedFeedforwardOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dropout1_seed_, dropout2_seed_, linear1_weight_, linear1_bias_, linear2_weight_, linear2_bias_, ln1_scale_, ln1_bias_, ln2_scale_, ln2_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_ln1_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln1_epsilon);
  argument.AddAttribute("ln1_epsilon", attr_ln1_epsilon);
  pir::Attribute attr_ln2_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln2_epsilon);
  argument.AddAttribute("ln2_epsilon", attr_ln2_epsilon);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_dropout1_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout1_prob);
  argument.AddAttribute("dropout1_prob", attr_dropout1_prob);
  pir::Attribute attr_dropout2_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout2_prob);
  argument.AddAttribute("dropout2_prob", attr_dropout2_prob);
  pir::Attribute attr_dropout1_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout1_implementation);
  argument.AddAttribute("dropout1_implementation", attr_dropout1_implementation);
  pir::Attribute attr_dropout2_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout2_implementation);
  argument.AddAttribute("dropout2_implementation", attr_dropout2_implementation);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout1_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout1_fix_seed);
  argument.AddAttribute("dropout1_fix_seed", attr_dropout1_fix_seed);
  pir::Attribute attr_dropout2_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout2_fix_seed);
  argument.AddAttribute("dropout2_fix_seed", attr_dropout2_fix_seed);
  pir::Attribute attr_dropout1_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout1_seed_val);
  argument.AddAttribute("dropout1_seed_val", attr_dropout1_seed_val);
  pir::Attribute attr_dropout2_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout2_seed_val);
  argument.AddAttribute("dropout2_seed_val", attr_dropout2_seed_val);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType linear1_weight = linear1_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear1_weight;
  paddle::dialect::DenseTensorType linear2_weight = linear2_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear2_weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_dropout1_seed;
  paddle::dialect::IrTensor ir_tensor_dropout1_seed;
  if (dropout1_seed_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dropout1_seed = dropout1_seed_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dropout1_seed";
    ir_tensor_dropout1_seed = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dropout1_seed.dtype()),
                                                        dropout1_seed.dims(),
                                                        dropout1_seed.data_layout(),
                                                        dropout1_seed.lod(),
                                                        dropout1_seed.offset());
    VLOG(4) << "Builder construction  meta_dropout1_seed";
    meta_dropout1_seed = paddle::dialect::IrMetaTensor(&ir_tensor_dropout1_seed);
  }


  paddle::dialect::IrMetaTensor meta_dropout2_seed;
  paddle::dialect::IrTensor ir_tensor_dropout2_seed;
  if (dropout2_seed_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dropout2_seed = dropout2_seed_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dropout2_seed";
    ir_tensor_dropout2_seed = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dropout2_seed.dtype()),
                                                        dropout2_seed.dims(),
                                                        dropout2_seed.data_layout(),
                                                        dropout2_seed.lod(),
                                                        dropout2_seed.offset());
    VLOG(4) << "Builder construction  meta_dropout2_seed";
    meta_dropout2_seed = paddle::dialect::IrMetaTensor(&ir_tensor_dropout2_seed);
  }


  VLOG(4) << "Builder construction  dense_linear1_weight";
  paddle::dialect::IrTensor ir_tensor_linear1_weight(paddle::dialect::TransToPhiDataType(linear1_weight.dtype()),
                                                      linear1_weight.dims(),
                                                      linear1_weight.data_layout(),
                                                      linear1_weight.lod(),
                                                      linear1_weight.offset());
  VLOG(4) << "Builder construction  meta_linear1_weight";
  paddle::dialect::IrMetaTensor meta_linear1_weight(&ir_tensor_linear1_weight);

  paddle::dialect::IrMetaTensor meta_linear1_bias;
  paddle::dialect::IrTensor ir_tensor_linear1_bias;
  if (linear1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear1_bias = linear1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear1_bias";
    ir_tensor_linear1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear1_bias.dtype()),
                                                        linear1_bias.dims(),
                                                        linear1_bias.data_layout(),
                                                        linear1_bias.lod(),
                                                        linear1_bias.offset());
    VLOG(4) << "Builder construction  meta_linear1_bias";
    meta_linear1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear1_bias);
  }


  VLOG(4) << "Builder construction  dense_linear2_weight";
  paddle::dialect::IrTensor ir_tensor_linear2_weight(paddle::dialect::TransToPhiDataType(linear2_weight.dtype()),
                                                      linear2_weight.dims(),
                                                      linear2_weight.data_layout(),
                                                      linear2_weight.lod(),
                                                      linear2_weight.offset());
  VLOG(4) << "Builder construction  meta_linear2_weight";
  paddle::dialect::IrMetaTensor meta_linear2_weight(&ir_tensor_linear2_weight);

  paddle::dialect::IrMetaTensor meta_linear2_bias;
  paddle::dialect::IrTensor ir_tensor_linear2_bias;
  if (linear2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear2_bias = linear2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear2_bias";
    ir_tensor_linear2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear2_bias.dtype()),
                                                        linear2_bias.dims(),
                                                        linear2_bias.data_layout(),
                                                        linear2_bias.lod(),
                                                        linear2_bias.offset());
    VLOG(4) << "Builder construction  meta_linear2_bias";
    meta_linear2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear2_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln1_scale;
  paddle::dialect::IrTensor ir_tensor_ln1_scale;
  if (ln1_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_scale = ln1_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_scale";
    ir_tensor_ln1_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_scale.dtype()),
                                                        ln1_scale.dims(),
                                                        ln1_scale.data_layout(),
                                                        ln1_scale.lod(),
                                                        ln1_scale.offset());
    VLOG(4) << "Builder construction  meta_ln1_scale";
    meta_ln1_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln1_bias;
  paddle::dialect::IrTensor ir_tensor_ln1_bias;
  if (ln1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_bias = ln1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_bias";
    ir_tensor_ln1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_bias.dtype()),
                                                        ln1_bias.dims(),
                                                        ln1_bias.data_layout(),
                                                        ln1_bias.lod(),
                                                        ln1_bias.offset());
    VLOG(4) << "Builder construction  meta_ln1_bias";
    meta_ln1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln2_scale;
  paddle::dialect::IrTensor ir_tensor_ln2_scale;
  if (ln2_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_scale = ln2_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_scale";
    ir_tensor_ln2_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_scale.dtype()),
                                                        ln2_scale.dims(),
                                                        ln2_scale.data_layout(),
                                                        ln2_scale.lod(),
                                                        ln2_scale.offset());
    VLOG(4) << "Builder construction  meta_ln2_scale";
    meta_ln2_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln2_bias;
  paddle::dialect::IrTensor ir_tensor_ln2_bias;
  if (ln2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_bias = ln2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_bias";
    ir_tensor_ln2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_bias.dtype()),
                                                        ln2_bias.dims(),
                                                        ln2_bias.data_layout(),
                                                        ln2_bias.lod(),
                                                        ln2_bias.offset());
    VLOG(4) << "Builder construction  meta_ln2_bias";
    meta_ln2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dropout1_mask;
  paddle::dialect::IrMetaTensor meta_dropout1_mask(&dense_dropout1_mask);
  paddle::dialect::IrTensor dense_dropout2_mask;
  paddle::dialect::IrMetaTensor meta_dropout2_mask(&dense_dropout2_mask);
  paddle::dialect::IrTensor dense_ln1_mean;
  paddle::dialect::IrMetaTensor meta_ln1_mean(&dense_ln1_mean);
  paddle::dialect::IrTensor dense_ln1_variance;
  paddle::dialect::IrMetaTensor meta_ln1_variance(&dense_ln1_variance);
  paddle::dialect::IrTensor dense_ln2_mean;
  paddle::dialect::IrMetaTensor meta_ln2_mean(&dense_ln2_mean);
  paddle::dialect::IrTensor dense_ln2_variance;
  paddle::dialect::IrMetaTensor meta_ln2_variance(&dense_ln2_variance);
  paddle::dialect::IrTensor dense_linear1_out;
  paddle::dialect::IrMetaTensor meta_linear1_out(&dense_linear1_out);
  paddle::dialect::IrTensor dense_ln1_out;
  paddle::dialect::IrMetaTensor meta_ln1_out(&dense_ln1_out);
  paddle::dialect::IrTensor dense_dropout1_out;
  paddle::dialect::IrMetaTensor meta_dropout1_out(&dense_dropout1_out);
  paddle::dialect::IrTensor dense_dropout2_out;
  paddle::dialect::IrMetaTensor meta_dropout2_out(&dense_dropout2_out);

  phi::FusedFeedForwardInferMeta(meta_x, meta_dropout1_seed, meta_dropout2_seed, meta_linear1_weight, meta_linear1_bias, meta_linear2_weight, meta_linear2_bias, meta_ln1_scale, meta_ln1_bias, meta_ln2_scale, meta_ln2_bias, pre_layer_norm, ln1_epsilon, ln2_epsilon, act_method, dropout1_prob, dropout2_prob, dropout1_implementation, dropout2_implementation, is_test, dropout1_fix_seed, dropout2_fix_seed, dropout1_seed_val, dropout2_seed_val, add_residual, ring_id, &meta_out, &meta_dropout1_mask, &meta_dropout2_mask, &meta_ln1_mean, &meta_ln1_variance, &meta_ln2_mean, &meta_ln2_variance, &meta_linear1_out, &meta_ln1_out, &meta_dropout1_out, &meta_dropout2_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dropout1_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout1_mask.dtype()), dense_dropout1_mask.dims(), dense_dropout1_mask.layout(), dense_dropout1_mask.lod(), dense_dropout1_mask.offset());
  argument_outputs.push_back(dropout1_mask_dense_tensor_type);

  pir::Type dropout2_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout2_mask.dtype()), dense_dropout2_mask.dims(), dense_dropout2_mask.layout(), dense_dropout2_mask.lod(), dense_dropout2_mask.offset());
  argument_outputs.push_back(dropout2_mask_dense_tensor_type);

  pir::Type ln1_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_mean.dtype()), dense_ln1_mean.dims(), dense_ln1_mean.layout(), dense_ln1_mean.lod(), dense_ln1_mean.offset());
  argument_outputs.push_back(ln1_mean_dense_tensor_type);

  pir::Type ln1_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_variance.dtype()), dense_ln1_variance.dims(), dense_ln1_variance.layout(), dense_ln1_variance.lod(), dense_ln1_variance.offset());
  argument_outputs.push_back(ln1_variance_dense_tensor_type);

  pir::Type ln2_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_mean.dtype()), dense_ln2_mean.dims(), dense_ln2_mean.layout(), dense_ln2_mean.lod(), dense_ln2_mean.offset());
  argument_outputs.push_back(ln2_mean_dense_tensor_type);

  pir::Type ln2_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_variance.dtype()), dense_ln2_variance.dims(), dense_ln2_variance.layout(), dense_ln2_variance.lod(), dense_ln2_variance.offset());
  argument_outputs.push_back(ln2_variance_dense_tensor_type);

  pir::Type linear1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear1_out.dtype()), dense_linear1_out.dims(), dense_linear1_out.layout(), dense_linear1_out.lod(), dense_linear1_out.offset());
  argument_outputs.push_back(linear1_out_dense_tensor_type);

  pir::Type ln1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_out.dtype()), dense_ln1_out.dims(), dense_ln1_out.layout(), dense_ln1_out.lod(), dense_ln1_out.offset());
  argument_outputs.push_back(ln1_out_dense_tensor_type);

  pir::Type dropout1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout1_out.dtype()), dense_dropout1_out.dims(), dense_dropout1_out.layout(), dense_dropout1_out.lod(), dense_dropout1_out.offset());
  argument_outputs.push_back(dropout1_out_dense_tensor_type);

  pir::Type dropout2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout2_out.dtype()), dense_dropout2_out.dims(), dense_dropout2_out.layout(), dense_dropout2_out.lod(), dense_dropout2_out.offset());
  argument_outputs.push_back(dropout2_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedFeedforwardOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value dropout1_seed_, pir::Value dropout2_seed_, pir::Value linear1_weight_, pir::Value linear1_bias_, pir::Value linear2_weight_, pir::Value linear2_bias_, pir::Value ln1_scale_, pir::Value ln1_bias_, pir::Value ln2_scale_, pir::Value ln2_bias_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build FusedFeedforwardOp";


  IR_ENFORCE(
      attributes.find("pre_layer_norm") != attributes.end(),
          "'pre_layer_norm' Attribute is expected for FusedFeedforwardOp. ");
  bool pre_layer_norm = attributes.at("pre_layer_norm").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ln1_epsilon") != attributes.end(),
          "'ln1_epsilon' Attribute is expected for FusedFeedforwardOp. ");
  float ln1_epsilon = attributes.at("ln1_epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("ln2_epsilon") != attributes.end(),
          "'ln2_epsilon' Attribute is expected for FusedFeedforwardOp. ");
  float ln2_epsilon = attributes.at("ln2_epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("act_method") != attributes.end(),
          "'act_method' Attribute is expected for FusedFeedforwardOp. ");
  std::string act_method = attributes.at("act_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dropout1_prob") != attributes.end(),
          "'dropout1_prob' Attribute is expected for FusedFeedforwardOp. ");
  float dropout1_prob = attributes.at("dropout1_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout2_prob") != attributes.end(),
          "'dropout2_prob' Attribute is expected for FusedFeedforwardOp. ");
  float dropout2_prob = attributes.at("dropout2_prob").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout1_implementation") != attributes.end(),
          "'dropout1_implementation' Attribute is expected for FusedFeedforwardOp. ");
  std::string dropout1_implementation = attributes.at("dropout1_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("dropout2_implementation") != attributes.end(),
          "'dropout2_implementation' Attribute is expected for FusedFeedforwardOp. ");
  std::string dropout2_implementation = attributes.at("dropout2_implementation").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for FusedFeedforwardOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout1_fix_seed") != attributes.end(),
          "'dropout1_fix_seed' Attribute is expected for FusedFeedforwardOp. ");
  bool dropout1_fix_seed = attributes.at("dropout1_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout2_fix_seed") != attributes.end(),
          "'dropout2_fix_seed' Attribute is expected for FusedFeedforwardOp. ");
  bool dropout2_fix_seed = attributes.at("dropout2_fix_seed").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("dropout1_seed_val") != attributes.end(),
          "'dropout1_seed_val' Attribute is expected for FusedFeedforwardOp. ");
  int dropout1_seed_val = attributes.at("dropout1_seed_val").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("dropout2_seed_val") != attributes.end(),
          "'dropout2_seed_val' Attribute is expected for FusedFeedforwardOp. ");
  int dropout2_seed_val = attributes.at("dropout2_seed_val").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("add_residual") != attributes.end(),
          "'add_residual' Attribute is expected for FusedFeedforwardOp. ");
  bool add_residual = attributes.at("add_residual").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("ring_id") != attributes.end(),
          "'ring_id' Attribute is expected for FusedFeedforwardOp. ");
  int ring_id = attributes.at("ring_id").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, dropout1_seed_, dropout2_seed_, linear1_weight_, linear1_bias_, linear2_weight_, linear2_bias_, ln1_scale_, ln1_bias_, ln2_scale_, ln2_bias_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_pre_layer_norm = pir::BoolAttribute::get(pir::IrContext::Instance(), pre_layer_norm);
  argument.AddAttribute("pre_layer_norm", attr_pre_layer_norm);
  pir::Attribute attr_ln1_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln1_epsilon);
  argument.AddAttribute("ln1_epsilon", attr_ln1_epsilon);
  pir::Attribute attr_ln2_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), ln2_epsilon);
  argument.AddAttribute("ln2_epsilon", attr_ln2_epsilon);
  pir::Attribute attr_act_method = pir::StrAttribute::get(pir::IrContext::Instance(), act_method);
  argument.AddAttribute("act_method", attr_act_method);
  pir::Attribute attr_dropout1_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout1_prob);
  argument.AddAttribute("dropout1_prob", attr_dropout1_prob);
  pir::Attribute attr_dropout2_prob = pir::FloatAttribute::get(pir::IrContext::Instance(), dropout2_prob);
  argument.AddAttribute("dropout2_prob", attr_dropout2_prob);
  pir::Attribute attr_dropout1_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout1_implementation);
  argument.AddAttribute("dropout1_implementation", attr_dropout1_implementation);
  pir::Attribute attr_dropout2_implementation = pir::StrAttribute::get(pir::IrContext::Instance(), dropout2_implementation);
  argument.AddAttribute("dropout2_implementation", attr_dropout2_implementation);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);
  pir::Attribute attr_dropout1_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout1_fix_seed);
  argument.AddAttribute("dropout1_fix_seed", attr_dropout1_fix_seed);
  pir::Attribute attr_dropout2_fix_seed = pir::BoolAttribute::get(pir::IrContext::Instance(), dropout2_fix_seed);
  argument.AddAttribute("dropout2_fix_seed", attr_dropout2_fix_seed);
  pir::Attribute attr_dropout1_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout1_seed_val);
  argument.AddAttribute("dropout1_seed_val", attr_dropout1_seed_val);
  pir::Attribute attr_dropout2_seed_val = pir::Int32Attribute::get(pir::IrContext::Instance(), dropout2_seed_val);
  argument.AddAttribute("dropout2_seed_val", attr_dropout2_seed_val);
  pir::Attribute attr_add_residual = pir::BoolAttribute::get(pir::IrContext::Instance(), add_residual);
  argument.AddAttribute("add_residual", attr_add_residual);
  pir::Attribute attr_ring_id = pir::Int32Attribute::get(pir::IrContext::Instance(), ring_id);
  argument.AddAttribute("ring_id", attr_ring_id);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType linear1_weight = linear1_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear1_weight;
  paddle::dialect::DenseTensorType linear2_weight = linear2_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)linear2_weight;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  paddle::dialect::IrMetaTensor meta_dropout1_seed;
  paddle::dialect::IrTensor ir_tensor_dropout1_seed;
  if (dropout1_seed_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dropout1_seed = dropout1_seed_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dropout1_seed";
    ir_tensor_dropout1_seed = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dropout1_seed.dtype()),
                                                        dropout1_seed.dims(),
                                                        dropout1_seed.data_layout(),
                                                        dropout1_seed.lod(),
                                                        dropout1_seed.offset());
    VLOG(4) << "Builder construction  meta_dropout1_seed";
    meta_dropout1_seed = paddle::dialect::IrMetaTensor(&ir_tensor_dropout1_seed);
  }


  paddle::dialect::IrMetaTensor meta_dropout2_seed;
  paddle::dialect::IrTensor ir_tensor_dropout2_seed;
  if (dropout2_seed_.impl() != nullptr) {
    paddle::dialect::DenseTensorType dropout2_seed = dropout2_seed_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_dropout2_seed";
    ir_tensor_dropout2_seed = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(dropout2_seed.dtype()),
                                                        dropout2_seed.dims(),
                                                        dropout2_seed.data_layout(),
                                                        dropout2_seed.lod(),
                                                        dropout2_seed.offset());
    VLOG(4) << "Builder construction  meta_dropout2_seed";
    meta_dropout2_seed = paddle::dialect::IrMetaTensor(&ir_tensor_dropout2_seed);
  }


  VLOG(4) << "Builder construction  dense_linear1_weight";
  paddle::dialect::IrTensor ir_tensor_linear1_weight(paddle::dialect::TransToPhiDataType(linear1_weight.dtype()),
                                                      linear1_weight.dims(),
                                                      linear1_weight.data_layout(),
                                                      linear1_weight.lod(),
                                                      linear1_weight.offset());
  VLOG(4) << "Builder construction  meta_linear1_weight";
  paddle::dialect::IrMetaTensor meta_linear1_weight(&ir_tensor_linear1_weight);

  paddle::dialect::IrMetaTensor meta_linear1_bias;
  paddle::dialect::IrTensor ir_tensor_linear1_bias;
  if (linear1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear1_bias = linear1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear1_bias";
    ir_tensor_linear1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear1_bias.dtype()),
                                                        linear1_bias.dims(),
                                                        linear1_bias.data_layout(),
                                                        linear1_bias.lod(),
                                                        linear1_bias.offset());
    VLOG(4) << "Builder construction  meta_linear1_bias";
    meta_linear1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear1_bias);
  }


  VLOG(4) << "Builder construction  dense_linear2_weight";
  paddle::dialect::IrTensor ir_tensor_linear2_weight(paddle::dialect::TransToPhiDataType(linear2_weight.dtype()),
                                                      linear2_weight.dims(),
                                                      linear2_weight.data_layout(),
                                                      linear2_weight.lod(),
                                                      linear2_weight.offset());
  VLOG(4) << "Builder construction  meta_linear2_weight";
  paddle::dialect::IrMetaTensor meta_linear2_weight(&ir_tensor_linear2_weight);

  paddle::dialect::IrMetaTensor meta_linear2_bias;
  paddle::dialect::IrTensor ir_tensor_linear2_bias;
  if (linear2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType linear2_bias = linear2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_linear2_bias";
    ir_tensor_linear2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(linear2_bias.dtype()),
                                                        linear2_bias.dims(),
                                                        linear2_bias.data_layout(),
                                                        linear2_bias.lod(),
                                                        linear2_bias.offset());
    VLOG(4) << "Builder construction  meta_linear2_bias";
    meta_linear2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_linear2_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln1_scale;
  paddle::dialect::IrTensor ir_tensor_ln1_scale;
  if (ln1_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_scale = ln1_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_scale";
    ir_tensor_ln1_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_scale.dtype()),
                                                        ln1_scale.dims(),
                                                        ln1_scale.data_layout(),
                                                        ln1_scale.lod(),
                                                        ln1_scale.offset());
    VLOG(4) << "Builder construction  meta_ln1_scale";
    meta_ln1_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln1_bias;
  paddle::dialect::IrTensor ir_tensor_ln1_bias;
  if (ln1_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln1_bias = ln1_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln1_bias";
    ir_tensor_ln1_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln1_bias.dtype()),
                                                        ln1_bias.dims(),
                                                        ln1_bias.data_layout(),
                                                        ln1_bias.lod(),
                                                        ln1_bias.offset());
    VLOG(4) << "Builder construction  meta_ln1_bias";
    meta_ln1_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln1_bias);
  }


  paddle::dialect::IrMetaTensor meta_ln2_scale;
  paddle::dialect::IrTensor ir_tensor_ln2_scale;
  if (ln2_scale_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_scale = ln2_scale_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_scale";
    ir_tensor_ln2_scale = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_scale.dtype()),
                                                        ln2_scale.dims(),
                                                        ln2_scale.data_layout(),
                                                        ln2_scale.lod(),
                                                        ln2_scale.offset());
    VLOG(4) << "Builder construction  meta_ln2_scale";
    meta_ln2_scale = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_scale);
  }


  paddle::dialect::IrMetaTensor meta_ln2_bias;
  paddle::dialect::IrTensor ir_tensor_ln2_bias;
  if (ln2_bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType ln2_bias = ln2_bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_ln2_bias";
    ir_tensor_ln2_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(ln2_bias.dtype()),
                                                        ln2_bias.dims(),
                                                        ln2_bias.data_layout(),
                                                        ln2_bias.lod(),
                                                        ln2_bias.offset());
    VLOG(4) << "Builder construction  meta_ln2_bias";
    meta_ln2_bias = paddle::dialect::IrMetaTensor(&ir_tensor_ln2_bias);
  }

  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_dropout1_mask;
  paddle::dialect::IrMetaTensor meta_dropout1_mask(&dense_dropout1_mask);
  paddle::dialect::IrTensor dense_dropout2_mask;
  paddle::dialect::IrMetaTensor meta_dropout2_mask(&dense_dropout2_mask);
  paddle::dialect::IrTensor dense_ln1_mean;
  paddle::dialect::IrMetaTensor meta_ln1_mean(&dense_ln1_mean);
  paddle::dialect::IrTensor dense_ln1_variance;
  paddle::dialect::IrMetaTensor meta_ln1_variance(&dense_ln1_variance);
  paddle::dialect::IrTensor dense_ln2_mean;
  paddle::dialect::IrMetaTensor meta_ln2_mean(&dense_ln2_mean);
  paddle::dialect::IrTensor dense_ln2_variance;
  paddle::dialect::IrMetaTensor meta_ln2_variance(&dense_ln2_variance);
  paddle::dialect::IrTensor dense_linear1_out;
  paddle::dialect::IrMetaTensor meta_linear1_out(&dense_linear1_out);
  paddle::dialect::IrTensor dense_ln1_out;
  paddle::dialect::IrMetaTensor meta_ln1_out(&dense_ln1_out);
  paddle::dialect::IrTensor dense_dropout1_out;
  paddle::dialect::IrMetaTensor meta_dropout1_out(&dense_dropout1_out);
  paddle::dialect::IrTensor dense_dropout2_out;
  paddle::dialect::IrMetaTensor meta_dropout2_out(&dense_dropout2_out);

  phi::FusedFeedForwardInferMeta(meta_x, meta_dropout1_seed, meta_dropout2_seed, meta_linear1_weight, meta_linear1_bias, meta_linear2_weight, meta_linear2_bias, meta_ln1_scale, meta_ln1_bias, meta_ln2_scale, meta_ln2_bias, pre_layer_norm, ln1_epsilon, ln2_epsilon, act_method, dropout1_prob, dropout2_prob, dropout1_implementation, dropout2_implementation, is_test, dropout1_fix_seed, dropout2_fix_seed, dropout1_seed_val, dropout2_seed_val, add_residual, ring_id, &meta_out, &meta_dropout1_mask, &meta_dropout2_mask, &meta_ln1_mean, &meta_ln1_variance, &meta_ln2_mean, &meta_ln2_variance, &meta_linear1_out, &meta_ln1_out, &meta_dropout1_out, &meta_dropout2_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type dropout1_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout1_mask.dtype()), dense_dropout1_mask.dims(), dense_dropout1_mask.layout(), dense_dropout1_mask.lod(), dense_dropout1_mask.offset());
  argument_outputs.push_back(dropout1_mask_dense_tensor_type);

  pir::Type dropout2_mask_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout2_mask.dtype()), dense_dropout2_mask.dims(), dense_dropout2_mask.layout(), dense_dropout2_mask.lod(), dense_dropout2_mask.offset());
  argument_outputs.push_back(dropout2_mask_dense_tensor_type);

  pir::Type ln1_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_mean.dtype()), dense_ln1_mean.dims(), dense_ln1_mean.layout(), dense_ln1_mean.lod(), dense_ln1_mean.offset());
  argument_outputs.push_back(ln1_mean_dense_tensor_type);

  pir::Type ln1_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_variance.dtype()), dense_ln1_variance.dims(), dense_ln1_variance.layout(), dense_ln1_variance.lod(), dense_ln1_variance.offset());
  argument_outputs.push_back(ln1_variance_dense_tensor_type);

  pir::Type ln2_mean_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_mean.dtype()), dense_ln2_mean.dims(), dense_ln2_mean.layout(), dense_ln2_mean.lod(), dense_ln2_mean.offset());
  argument_outputs.push_back(ln2_mean_dense_tensor_type);

  pir::Type ln2_variance_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln2_variance.dtype()), dense_ln2_variance.dims(), dense_ln2_variance.layout(), dense_ln2_variance.lod(), dense_ln2_variance.offset());
  argument_outputs.push_back(ln2_variance_dense_tensor_type);

  pir::Type linear1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_linear1_out.dtype()), dense_linear1_out.dims(), dense_linear1_out.layout(), dense_linear1_out.lod(), dense_linear1_out.offset());
  argument_outputs.push_back(linear1_out_dense_tensor_type);

  pir::Type ln1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_ln1_out.dtype()), dense_ln1_out.dims(), dense_ln1_out.layout(), dense_ln1_out.lod(), dense_ln1_out.offset());
  argument_outputs.push_back(ln1_out_dense_tensor_type);

  pir::Type dropout1_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout1_out.dtype()), dense_dropout1_out.dims(), dense_dropout1_out.layout(), dense_dropout1_out.lod(), dense_dropout1_out.offset());
  argument_outputs.push_back(dropout1_out_dense_tensor_type);

  pir::Type dropout2_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_dropout2_out.dtype()), dense_dropout2_out.dims(), dense_dropout2_out.layout(), dense_dropout2_out.lod(), dense_dropout2_out.offset());
  argument_outputs.push_back(dropout2_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void FusedFeedforwardOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: FusedFeedforwardOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 11u,
                    "The size %d of inputs must be equal to 11.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  if (auto val = (*this)->operand(1)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto val = (*this)->operand(2)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  IR_ENFORCE((*this)->operand_source(5).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  if (auto val = (*this)->operand(8)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th input, got %s.", (*this)->operand_source(8).type());
  }
  if (auto val = (*this)->operand(9)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 9th input, got %s.", (*this)->operand_source(9).type());
  }
  if (auto val = (*this)->operand(10)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 10th input, got %s.", (*this)->operand_source(10).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("pre_layer_norm")>0,
                 "pre_layer_norm does not exist.");
  IR_ENFORCE(attributes.at("pre_layer_norm").isa<pir::BoolAttribute>(),
                 "Type of attribute: pre_layer_norm is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ln1_epsilon")>0,
                 "ln1_epsilon does not exist.");
  IR_ENFORCE(attributes.at("ln1_epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: ln1_epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("ln2_epsilon")>0,
                 "ln2_epsilon does not exist.");
  IR_ENFORCE(attributes.at("ln2_epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: ln2_epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("act_method")>0,
                 "act_method does not exist.");
  IR_ENFORCE(attributes.at("act_method").isa<pir::StrAttribute>(),
                 "Type of attribute: act_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dropout1_prob")>0,
                 "dropout1_prob does not exist.");
  IR_ENFORCE(attributes.at("dropout1_prob").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout1_prob is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout2_prob")>0,
                 "dropout2_prob does not exist.");
  IR_ENFORCE(attributes.at("dropout2_prob").isa<pir::FloatAttribute>(),
                 "Type of attribute: dropout2_prob is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("dropout1_implementation")>0,
                 "dropout1_implementation does not exist.");
  IR_ENFORCE(attributes.at("dropout1_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: dropout1_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("dropout2_implementation")>0,
                 "dropout2_implementation does not exist.");
  IR_ENFORCE(attributes.at("dropout2_implementation").isa<pir::StrAttribute>(),
                 "Type of attribute: dropout2_implementation is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout1_fix_seed")>0,
                 "dropout1_fix_seed does not exist.");
  IR_ENFORCE(attributes.at("dropout1_fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: dropout1_fix_seed is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout2_fix_seed")>0,
                 "dropout2_fix_seed does not exist.");
  IR_ENFORCE(attributes.at("dropout2_fix_seed").isa<pir::BoolAttribute>(),
                 "Type of attribute: dropout2_fix_seed is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("dropout1_seed_val")>0,
                 "dropout1_seed_val does not exist.");
  IR_ENFORCE(attributes.at("dropout1_seed_val").isa<pir::Int32Attribute>(),
                 "Type of attribute: dropout1_seed_val is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("dropout2_seed_val")>0,
                 "dropout2_seed_val does not exist.");
  IR_ENFORCE(attributes.at("dropout2_seed_val").isa<pir::Int32Attribute>(),
                 "Type of attribute: dropout2_seed_val is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("add_residual")>0,
                 "add_residual does not exist.");
  IR_ENFORCE(attributes.at("add_residual").isa<pir::BoolAttribute>(),
                 "Type of attribute: add_residual is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("ring_id")>0,
                 "ring_id does not exist.");
  IR_ENFORCE(attributes.at("ring_id").isa<pir::Int32Attribute>(),
                 "Type of attribute: ring_id is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 11u,
                    "The size %d of outputs must be equal to 11.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  if (auto output_3_type = (*this)->result(3).type()) {
    IR_ENFORCE(output_3_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th output.");
  }
  if (auto output_4_type = (*this)->result(4).type()) {
    IR_ENFORCE(output_4_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th output.");
  }
  if (auto output_5_type = (*this)->result(5).type()) {
    IR_ENFORCE(output_5_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th output.");
  }
  if (auto output_6_type = (*this)->result(6).type()) {
    IR_ENFORCE(output_6_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th output.");
  }
  IR_ENFORCE((*this)->result(7).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 7th output.");
  if (auto output_8_type = (*this)->result(8).type()) {
    IR_ENFORCE(output_8_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 8th output.");
  }
  IR_ENFORCE((*this)->result(9).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 9th output.");
  IR_ENFORCE((*this)->result(10).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 10th output.");
  }
  VLOG(4) << "End Verifying for: FusedFeedforwardOp.";
}

void FusedFeedforwardOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::FusedFeedForwardInferMeta);
  fn(infer_meta);
}

phi::DataType FusedFeedforwardOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: FusedFeedforwardOp";
  


  return expected_kernel_dtype;
}

const char *NceOp::attributes_name[8] = { "num_total_classes", "custom_neg_classes", "num_neg_samples", "sampler", "seed", "is_sparse", "remote_prefetch", "is_test" };

OpInfoTuple NceOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("input", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("label", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("weight", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("bias", "paddle::dialect::DenseTensorType", true, false, false, true), paddle::dialect::OpInputInfo("sample_weight", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("custom_dist_probs", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("custom_dist_alias", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("custom_dist_alias_probs", "paddle::dialect::DenseTensorType", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("num_total_classes", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("custom_neg_classes", "pir::ArrayAttribute<pir::Int32Attribute>", ""), paddle::dialect::OpAttributeInfo("num_neg_samples", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("sampler", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("seed", "pir::Int32Attribute", ""), paddle::dialect::OpAttributeInfo("is_sparse", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("remote_prefetch", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("is_test", "pir::BoolAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("cost", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("sample_logits", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("sample_labels", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NceInferMeta", {"input", "label", "weight", "bias", "sample_weight", "custom_dist_probs", "custom_dist_alias", "custom_dist_alias_probs", "num_total_classes", "custom_neg_classes", "num_neg_samples", "sampler", "seed", "is_sparse", "remote_prefetch", "is_test"}, "nce", {"input", "label", "weight", "bias", "sample_weight", "custom_dist_probs", "custom_dist_alias", "custom_dist_alias_probs", "num_total_classes", "custom_neg_classes", "num_neg_samples", "sampler", "seed", "is_sparse", "remote_prefetch", "is_test"}, {"input"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "nce");
}

void NceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value weight_, pir::Value bias_, pir::Value sample_weight_, pir::Value custom_dist_probs_, pir::Value custom_dist_alias_, pir::Value custom_dist_alias_probs_, int num_total_classes, const std::vector<int>& custom_neg_classes, int num_neg_samples, int sampler, int seed, bool is_sparse, bool remote_prefetch, bool is_test) {
  VLOG(4) << "Start build NceOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, weight_, bias_, sample_weight_, custom_dist_probs_, custom_dist_alias_, custom_dist_alias_probs_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_total_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_total_classes);
  argument.AddAttribute("num_total_classes", attr_num_total_classes);
  std::vector<pir::Attribute> vec_custom_neg_classes;
  for (size_t i = 0; i < static_cast<size_t>(custom_neg_classes.size()); i++) {
      pir::Attribute attr_custom_neg_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), custom_neg_classes[i]);

    vec_custom_neg_classes.push_back(attr_custom_neg_classes);
  }
  pir::Attribute attr_custom_neg_classes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_custom_neg_classes);
  argument.AddAttribute("custom_neg_classes", attr_custom_neg_classes);
  pir::Attribute attr_num_neg_samples = pir::Int32Attribute::get(pir::IrContext::Instance(), num_neg_samples);
  argument.AddAttribute("num_neg_samples", attr_num_neg_samples);
  pir::Attribute attr_sampler = pir::Int32Attribute::get(pir::IrContext::Instance(), sampler);
  argument.AddAttribute("sampler", attr_sampler);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);
  pir::Attribute attr_remote_prefetch = pir::BoolAttribute::get(pir::IrContext::Instance(), remote_prefetch);
  argument.AddAttribute("remote_prefetch", attr_remote_prefetch);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_sample_weight;
  paddle::dialect::IrTensor ir_tensor_sample_weight;
  if (sample_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sample_weight = sample_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sample_weight";
    ir_tensor_sample_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sample_weight.dtype()),
                                                        sample_weight.dims(),
                                                        sample_weight.data_layout(),
                                                        sample_weight.lod(),
                                                        sample_weight.offset());
    VLOG(4) << "Builder construction  meta_sample_weight";
    meta_sample_weight = paddle::dialect::IrMetaTensor(&ir_tensor_sample_weight);
  }


  paddle::dialect::IrMetaTensor meta_custom_dist_probs;
  paddle::dialect::IrTensor ir_tensor_custom_dist_probs;
  if (custom_dist_probs_.impl() != nullptr) {
    paddle::dialect::DenseTensorType custom_dist_probs = custom_dist_probs_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_custom_dist_probs";
    ir_tensor_custom_dist_probs = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(custom_dist_probs.dtype()),
                                                        custom_dist_probs.dims(),
                                                        custom_dist_probs.data_layout(),
                                                        custom_dist_probs.lod(),
                                                        custom_dist_probs.offset());
    VLOG(4) << "Builder construction  meta_custom_dist_probs";
    meta_custom_dist_probs = paddle::dialect::IrMetaTensor(&ir_tensor_custom_dist_probs);
  }


  paddle::dialect::IrMetaTensor meta_custom_dist_alias;
  paddle::dialect::IrTensor ir_tensor_custom_dist_alias;
  if (custom_dist_alias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType custom_dist_alias = custom_dist_alias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_custom_dist_alias";
    ir_tensor_custom_dist_alias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(custom_dist_alias.dtype()),
                                                        custom_dist_alias.dims(),
                                                        custom_dist_alias.data_layout(),
                                                        custom_dist_alias.lod(),
                                                        custom_dist_alias.offset());
    VLOG(4) << "Builder construction  meta_custom_dist_alias";
    meta_custom_dist_alias = paddle::dialect::IrMetaTensor(&ir_tensor_custom_dist_alias);
  }


  paddle::dialect::IrMetaTensor meta_custom_dist_alias_probs;
  paddle::dialect::IrTensor ir_tensor_custom_dist_alias_probs;
  if (custom_dist_alias_probs_.impl() != nullptr) {
    paddle::dialect::DenseTensorType custom_dist_alias_probs = custom_dist_alias_probs_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_custom_dist_alias_probs";
    ir_tensor_custom_dist_alias_probs = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(custom_dist_alias_probs.dtype()),
                                                        custom_dist_alias_probs.dims(),
                                                        custom_dist_alias_probs.data_layout(),
                                                        custom_dist_alias_probs.lod(),
                                                        custom_dist_alias_probs.offset());
    VLOG(4) << "Builder construction  meta_custom_dist_alias_probs";
    meta_custom_dist_alias_probs = paddle::dialect::IrMetaTensor(&ir_tensor_custom_dist_alias_probs);
  }

  paddle::dialect::IrTensor dense_cost;
  paddle::dialect::IrMetaTensor meta_cost(&dense_cost);
  paddle::dialect::IrTensor dense_sample_logits;
  paddle::dialect::IrMetaTensor meta_sample_logits(&dense_sample_logits);
  paddle::dialect::IrTensor dense_sample_labels;
  paddle::dialect::IrMetaTensor meta_sample_labels(&dense_sample_labels);

  phi::NceInferMeta(meta_input, meta_label, meta_weight, meta_bias, meta_sample_weight, meta_custom_dist_probs, meta_custom_dist_alias, meta_custom_dist_alias_probs, num_total_classes, custom_neg_classes, num_neg_samples, sampler, seed, is_sparse, remote_prefetch, is_test, &meta_cost, &meta_sample_logits, &meta_sample_labels);

  std::vector<pir::Type> argument_outputs;
  pir::Type cost_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_cost.dtype()), dense_cost.dims(), dense_cost.layout(), dense_cost.lod(), dense_cost.offset());
  argument_outputs.push_back(cost_dense_tensor_type);

  pir::Type sample_logits_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sample_logits.dtype()), dense_sample_logits.dims(), dense_sample_logits.layout(), dense_sample_logits.lod(), dense_sample_logits.offset());
  argument_outputs.push_back(sample_logits_dense_tensor_type);

  pir::Type sample_labels_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sample_labels.dtype()), dense_sample_labels.dims(), dense_sample_labels.layout(), dense_sample_labels.lod(), dense_sample_labels.offset());
  argument_outputs.push_back(sample_labels_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NceOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value input_, pir::Value label_, pir::Value weight_, pir::Value bias_, pir::Value sample_weight_, pir::Value custom_dist_probs_, pir::Value custom_dist_alias_, pir::Value custom_dist_alias_probs_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NceOp";


  IR_ENFORCE(
      attributes.find("num_total_classes") != attributes.end(),
          "'num_total_classes' Attribute is expected for NceOp. ");
  int num_total_classes = attributes.at("num_total_classes").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("custom_neg_classes") != attributes.end(),
          "'custom_neg_classes' Attribute is expected for NceOp. ");
  std::vector<int> custom_neg_classes;
  for (size_t i = 0; i < attributes.at("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    custom_neg_classes.push_back(attributes.at("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::Int32Attribute>().data());
  }

  IR_ENFORCE(
      attributes.find("num_neg_samples") != attributes.end(),
          "'num_neg_samples' Attribute is expected for NceOp. ");
  int num_neg_samples = attributes.at("num_neg_samples").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("sampler") != attributes.end(),
          "'sampler' Attribute is expected for NceOp. ");
  int sampler = attributes.at("sampler").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("seed") != attributes.end(),
          "'seed' Attribute is expected for NceOp. ");
  int seed = attributes.at("seed").dyn_cast<pir::Int32Attribute>().data();

  IR_ENFORCE(
      attributes.find("is_sparse") != attributes.end(),
          "'is_sparse' Attribute is expected for NceOp. ");
  bool is_sparse = attributes.at("is_sparse").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("remote_prefetch") != attributes.end(),
          "'remote_prefetch' Attribute is expected for NceOp. ");
  bool remote_prefetch = attributes.at("remote_prefetch").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("is_test") != attributes.end(),
          "'is_test' Attribute is expected for NceOp. ");
  bool is_test = attributes.at("is_test").dyn_cast<pir::BoolAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {input_, label_, weight_, bias_, sample_weight_, custom_dist_probs_, custom_dist_alias_, custom_dist_alias_probs_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_num_total_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), num_total_classes);
  argument.AddAttribute("num_total_classes", attr_num_total_classes);
  std::vector<pir::Attribute> vec_custom_neg_classes;
  for (size_t i = 0; i < static_cast<size_t>(custom_neg_classes.size()); i++) {
      pir::Attribute attr_custom_neg_classes = pir::Int32Attribute::get(pir::IrContext::Instance(), custom_neg_classes[i]);

    vec_custom_neg_classes.push_back(attr_custom_neg_classes);
  }
  pir::Attribute attr_custom_neg_classes = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_custom_neg_classes);
  argument.AddAttribute("custom_neg_classes", attr_custom_neg_classes);
  pir::Attribute attr_num_neg_samples = pir::Int32Attribute::get(pir::IrContext::Instance(), num_neg_samples);
  argument.AddAttribute("num_neg_samples", attr_num_neg_samples);
  pir::Attribute attr_sampler = pir::Int32Attribute::get(pir::IrContext::Instance(), sampler);
  argument.AddAttribute("sampler", attr_sampler);
  pir::Attribute attr_seed = pir::Int32Attribute::get(pir::IrContext::Instance(), seed);
  argument.AddAttribute("seed", attr_seed);
  pir::Attribute attr_is_sparse = pir::BoolAttribute::get(pir::IrContext::Instance(), is_sparse);
  argument.AddAttribute("is_sparse", attr_is_sparse);
  pir::Attribute attr_remote_prefetch = pir::BoolAttribute::get(pir::IrContext::Instance(), remote_prefetch);
  argument.AddAttribute("remote_prefetch", attr_remote_prefetch);
  pir::Attribute attr_is_test = pir::BoolAttribute::get(pir::IrContext::Instance(), is_test);
  argument.AddAttribute("is_test", attr_is_test);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType input = input_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)input;
  paddle::dialect::DenseTensorType label = label_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)label;
  paddle::dialect::DenseTensorType weight = weight_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)weight;

  VLOG(4) << "Builder construction  dense_input";
  paddle::dialect::IrTensor ir_tensor_input(paddle::dialect::TransToPhiDataType(input.dtype()),
                                                      input.dims(),
                                                      input.data_layout(),
                                                      input.lod(),
                                                      input.offset());
  VLOG(4) << "Builder construction  meta_input";
  paddle::dialect::IrMetaTensor meta_input(&ir_tensor_input);

  VLOG(4) << "Builder construction  dense_label";
  paddle::dialect::IrTensor ir_tensor_label(paddle::dialect::TransToPhiDataType(label.dtype()),
                                                      label.dims(),
                                                      label.data_layout(),
                                                      label.lod(),
                                                      label.offset());
  VLOG(4) << "Builder construction  meta_label";
  paddle::dialect::IrMetaTensor meta_label(&ir_tensor_label);

  VLOG(4) << "Builder construction  dense_weight";
  paddle::dialect::IrTensor ir_tensor_weight(paddle::dialect::TransToPhiDataType(weight.dtype()),
                                                      weight.dims(),
                                                      weight.data_layout(),
                                                      weight.lod(),
                                                      weight.offset());
  VLOG(4) << "Builder construction  meta_weight";
  paddle::dialect::IrMetaTensor meta_weight(&ir_tensor_weight);

  paddle::dialect::IrMetaTensor meta_bias;
  paddle::dialect::IrTensor ir_tensor_bias;
  if (bias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType bias = bias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_bias";
    ir_tensor_bias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(bias.dtype()),
                                                        bias.dims(),
                                                        bias.data_layout(),
                                                        bias.lod(),
                                                        bias.offset());
    VLOG(4) << "Builder construction  meta_bias";
    meta_bias = paddle::dialect::IrMetaTensor(&ir_tensor_bias);
  }


  paddle::dialect::IrMetaTensor meta_sample_weight;
  paddle::dialect::IrTensor ir_tensor_sample_weight;
  if (sample_weight_.impl() != nullptr) {
    paddle::dialect::DenseTensorType sample_weight = sample_weight_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_sample_weight";
    ir_tensor_sample_weight = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(sample_weight.dtype()),
                                                        sample_weight.dims(),
                                                        sample_weight.data_layout(),
                                                        sample_weight.lod(),
                                                        sample_weight.offset());
    VLOG(4) << "Builder construction  meta_sample_weight";
    meta_sample_weight = paddle::dialect::IrMetaTensor(&ir_tensor_sample_weight);
  }


  paddle::dialect::IrMetaTensor meta_custom_dist_probs;
  paddle::dialect::IrTensor ir_tensor_custom_dist_probs;
  if (custom_dist_probs_.impl() != nullptr) {
    paddle::dialect::DenseTensorType custom_dist_probs = custom_dist_probs_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_custom_dist_probs";
    ir_tensor_custom_dist_probs = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(custom_dist_probs.dtype()),
                                                        custom_dist_probs.dims(),
                                                        custom_dist_probs.data_layout(),
                                                        custom_dist_probs.lod(),
                                                        custom_dist_probs.offset());
    VLOG(4) << "Builder construction  meta_custom_dist_probs";
    meta_custom_dist_probs = paddle::dialect::IrMetaTensor(&ir_tensor_custom_dist_probs);
  }


  paddle::dialect::IrMetaTensor meta_custom_dist_alias;
  paddle::dialect::IrTensor ir_tensor_custom_dist_alias;
  if (custom_dist_alias_.impl() != nullptr) {
    paddle::dialect::DenseTensorType custom_dist_alias = custom_dist_alias_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_custom_dist_alias";
    ir_tensor_custom_dist_alias = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(custom_dist_alias.dtype()),
                                                        custom_dist_alias.dims(),
                                                        custom_dist_alias.data_layout(),
                                                        custom_dist_alias.lod(),
                                                        custom_dist_alias.offset());
    VLOG(4) << "Builder construction  meta_custom_dist_alias";
    meta_custom_dist_alias = paddle::dialect::IrMetaTensor(&ir_tensor_custom_dist_alias);
  }


  paddle::dialect::IrMetaTensor meta_custom_dist_alias_probs;
  paddle::dialect::IrTensor ir_tensor_custom_dist_alias_probs;
  if (custom_dist_alias_probs_.impl() != nullptr) {
    paddle::dialect::DenseTensorType custom_dist_alias_probs = custom_dist_alias_probs_.type().dyn_cast<paddle::dialect::DenseTensorType>();
    VLOG(4) << "Builder construction  dense_custom_dist_alias_probs";
    ir_tensor_custom_dist_alias_probs = paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(custom_dist_alias_probs.dtype()),
                                                        custom_dist_alias_probs.dims(),
                                                        custom_dist_alias_probs.data_layout(),
                                                        custom_dist_alias_probs.lod(),
                                                        custom_dist_alias_probs.offset());
    VLOG(4) << "Builder construction  meta_custom_dist_alias_probs";
    meta_custom_dist_alias_probs = paddle::dialect::IrMetaTensor(&ir_tensor_custom_dist_alias_probs);
  }

  paddle::dialect::IrTensor dense_cost;
  paddle::dialect::IrMetaTensor meta_cost(&dense_cost);
  paddle::dialect::IrTensor dense_sample_logits;
  paddle::dialect::IrMetaTensor meta_sample_logits(&dense_sample_logits);
  paddle::dialect::IrTensor dense_sample_labels;
  paddle::dialect::IrMetaTensor meta_sample_labels(&dense_sample_labels);

  phi::NceInferMeta(meta_input, meta_label, meta_weight, meta_bias, meta_sample_weight, meta_custom_dist_probs, meta_custom_dist_alias, meta_custom_dist_alias_probs, num_total_classes, custom_neg_classes, num_neg_samples, sampler, seed, is_sparse, remote_prefetch, is_test, &meta_cost, &meta_sample_logits, &meta_sample_labels);

  std::vector<pir::Type> argument_outputs;
  pir::Type cost_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_cost.dtype()), dense_cost.dims(), dense_cost.layout(), dense_cost.lod(), dense_cost.offset());
  argument_outputs.push_back(cost_dense_tensor_type);

  pir::Type sample_logits_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sample_logits.dtype()), dense_sample_logits.dims(), dense_sample_logits.layout(), dense_sample_logits.lod(), dense_sample_logits.offset());
  argument_outputs.push_back(sample_logits_dense_tensor_type);

  pir::Type sample_labels_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_sample_labels.dtype()), dense_sample_labels.dims(), dense_sample_labels.layout(), dense_sample_labels.lod(), dense_sample_labels.offset());
  argument_outputs.push_back(sample_labels_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NceOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NceOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 8u,
                    "The size %d of inputs must be equal to 8.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  if (auto val = (*this)->operand(3)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val = (*this)->operand(4)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  }
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  if (auto val = (*this)->operand(6)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  if (auto val = (*this)->operand(7)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 7th input, got %s.", (*this)->operand_source(7).type());
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("num_total_classes")>0,
                 "num_total_classes does not exist.");
  IR_ENFORCE(attributes.at("num_total_classes").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_total_classes is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("custom_neg_classes")>0,
                 "custom_neg_classes does not exist.");
  IR_ENFORCE(attributes.at("custom_neg_classes").isa<pir::ArrayAttribute>(),
                 "Type of attribute: custom_neg_classes is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("custom_neg_classes").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::Int32Attribute>(),
                   "Type of attribute: custom_neg_classes is not right.");
  }
  IR_ENFORCE(attributes.count("num_neg_samples")>0,
                 "num_neg_samples does not exist.");
  IR_ENFORCE(attributes.at("num_neg_samples").isa<pir::Int32Attribute>(),
                 "Type of attribute: num_neg_samples is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("sampler")>0,
                 "sampler does not exist.");
  IR_ENFORCE(attributes.at("sampler").isa<pir::Int32Attribute>(),
                 "Type of attribute: sampler is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("seed")>0,
                 "seed does not exist.");
  IR_ENFORCE(attributes.at("seed").isa<pir::Int32Attribute>(),
                 "Type of attribute: seed is not pir::Int32Attribute.");

  IR_ENFORCE(attributes.count("is_sparse")>0,
                 "is_sparse does not exist.");
  IR_ENFORCE(attributes.at("is_sparse").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_sparse is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("remote_prefetch")>0,
                 "remote_prefetch does not exist.");
  IR_ENFORCE(attributes.at("remote_prefetch").isa<pir::BoolAttribute>(),
                 "Type of attribute: remote_prefetch is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("is_test")>0,
                 "is_test does not exist.");
  IR_ENFORCE(attributes.at("is_test").isa<pir::BoolAttribute>(),
                 "Type of attribute: is_test is not pir::BoolAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  IR_ENFORCE((*this)->result(2).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 2th output.");
  }
  VLOG(4) << "End Verifying for: NceOp.";
}

void NceOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NceInferMeta);
  fn(infer_meta);
}

phi::DataType NceOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NceOp";
  


  return expected_kernel_dtype;
}

const char *NumberCountOp::attributes_name[1] = { "upper_range" };

OpInfoTuple NumberCountOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("numbers", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("upper_range", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("NumberCountInferMeta", {"numbers", "upper_range"}, "number_count", {"numbers", "upper_range"}, {"numbers"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "number_count");
}

void NumberCountOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value numbers_, int upper_range) {
  VLOG(4) << "Start build NumberCountOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {numbers_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper_range = pir::Int32Attribute::get(pir::IrContext::Instance(), upper_range);
  argument.AddAttribute("upper_range", attr_upper_range);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType numbers = numbers_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)numbers;

  VLOG(4) << "Builder construction  dense_numbers";
  paddle::dialect::IrTensor ir_tensor_numbers(paddle::dialect::TransToPhiDataType(numbers.dtype()),
                                                      numbers.dims(),
                                                      numbers.data_layout(),
                                                      numbers.lod(),
                                                      numbers.offset());
  VLOG(4) << "Builder construction  meta_numbers";
  paddle::dialect::IrMetaTensor meta_numbers(&ir_tensor_numbers);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::NumberCountInferMeta(meta_numbers, upper_range, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NumberCountOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value numbers_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build NumberCountOp";


  IR_ENFORCE(
      attributes.find("upper_range") != attributes.end(),
          "'upper_range' Attribute is expected for NumberCountOp. ");
  int upper_range = attributes.at("upper_range").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {numbers_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_upper_range = pir::Int32Attribute::get(pir::IrContext::Instance(), upper_range);
  argument.AddAttribute("upper_range", attr_upper_range);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType numbers = numbers_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)numbers;

  VLOG(4) << "Builder construction  dense_numbers";
  paddle::dialect::IrTensor ir_tensor_numbers(paddle::dialect::TransToPhiDataType(numbers.dtype()),
                                                      numbers.dims(),
                                                      numbers.data_layout(),
                                                      numbers.lod(),
                                                      numbers.offset());
  VLOG(4) << "Builder construction  meta_numbers";
  paddle::dialect::IrMetaTensor meta_numbers(&ir_tensor_numbers);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::NumberCountInferMeta(meta_numbers, upper_range, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void NumberCountOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: NumberCountOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("upper_range")>0,
                 "upper_range does not exist.");
  IR_ENFORCE(attributes.at("upper_range").isa<pir::Int32Attribute>(),
                 "Type of attribute: upper_range is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: NumberCountOp.";
}

void NumberCountOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::NumberCountInferMeta);
  fn(infer_meta);
}

phi::DataType NumberCountOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: NumberCountOp";
  


  return expected_kernel_dtype;
}

const char *OnednnToPaddleLayoutOp::attributes_name[1] = { "dst_layout" };

OpInfoTuple OnednnToPaddleLayoutOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dst_layout", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("UnchangedInferMeta", {"x"}, "onednn_to_paddle_layout", {"x", "dst_layout"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "onednn_to_paddle_layout");
}

void OnednnToPaddleLayoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, int dst_layout) {
  VLOG(4) << "Start build OnednnToPaddleLayoutOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_layout = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_layout);
  argument.AddAttribute("dst_layout", attr_dst_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OnednnToPaddleLayoutOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build OnednnToPaddleLayoutOp";


  IR_ENFORCE(
      attributes.find("dst_layout") != attributes.end(),
          "'dst_layout' Attribute is expected for OnednnToPaddleLayoutOp. ");
  int dst_layout = attributes.at("dst_layout").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dst_layout = pir::Int32Attribute::get(pir::IrContext::Instance(), dst_layout);
  argument.AddAttribute("dst_layout", attr_dst_layout);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);

  phi::UnchangedInferMeta(meta_x, &meta_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void OnednnToPaddleLayoutOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: OnednnToPaddleLayoutOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 1u,
                    "The size %d of inputs must be equal to 1.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dst_layout")>0,
                 "dst_layout does not exist.");
  IR_ENFORCE(attributes.at("dst_layout").isa<pir::Int32Attribute>(),
                 "Type of attribute: dst_layout is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 1u,
                    "The size %d of outputs must be equal to 1.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  }
  VLOG(4) << "End Verifying for: OnednnToPaddleLayoutOp.";
}

void OnednnToPaddleLayoutOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::UnchangedInferMeta);
  fn(infer_meta);
}

phi::DataType OnednnToPaddleLayoutOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: OnednnToPaddleLayoutOp";
  


  return expected_kernel_dtype;
}

const char *SparseMomentumOp::attributes_name[6] = { "mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad" };

OpInfoTuple SparseMomentumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("grad", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("velocity", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("index", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "paddle::dialect::DenseTensorType", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "paddle::dialect::DenseTensorType", true, false, false, false), paddle::dialect::OpInputInfo("axis", "paddle::dialect::ScalarAttribute", false, false, true, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mu", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("use_nesterov", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_method", "pir::StrAttribute", ""), paddle::dialect::OpAttributeInfo("regularization_coeff", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rescale_grad", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("velocity_out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("master_param_out", "paddle::dialect::DenseTensorType", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("SparseMomentumInferMeta", {"param", "learning_rate", "velocity"}, "sparse_momentum", {"param", "grad", "velocity", "index", "learning_rate", "master_param", "mu", "axis", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"param"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "sparse_momentum");
}

void SparseMomentumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value index_, pir::Value learning_rate_, pir::Value master_param_, float mu, float axis, bool use_nesterov, const std::string& regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build SparseMomentumOp";


  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, index_, learning_rate_, master_param_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SparseMomentumInferMeta(meta_param, meta_learning_rate, meta_velocity, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
  argument_outputs.push_back(master_param_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseMomentumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value index_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build SparseMomentumOp";


  IR_ENFORCE(
      attributes.find("mu") != attributes.end(),
          "'mu' Attribute is expected for SparseMomentumOp. ");
  float mu = attributes.at("mu").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("axis") != attributes.end(),
          "'axis' Attribute is expected for SparseMomentumOp. ");
  float axis = attributes.at("axis").dyn_cast<paddle::dialect::ScalarAttribute>().data().to<float>();

  IR_ENFORCE(
      attributes.find("use_nesterov") != attributes.end(),
          "'use_nesterov' Attribute is expected for SparseMomentumOp. ");
  bool use_nesterov = attributes.at("use_nesterov").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("regularization_method") != attributes.end(),
          "'regularization_method' Attribute is expected for SparseMomentumOp. ");
  std::string regularization_method = attributes.at("regularization_method").dyn_cast<pir::StrAttribute>().AsString();

  IR_ENFORCE(
      attributes.find("regularization_coeff") != attributes.end(),
          "'regularization_coeff' Attribute is expected for SparseMomentumOp. ");
  float regularization_coeff = attributes.at("regularization_coeff").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for SparseMomentumOp. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rescale_grad") != attributes.end(),
          "'rescale_grad' Attribute is expected for SparseMomentumOp. ");
  float rescale_grad = attributes.at("rescale_grad").dyn_cast<pir::FloatAttribute>().data();

  // Generate scalar mutable attribute: axis
  paddle::dialect::FullOp full_axis_op = builder.Build<paddle::dialect::FullOp>(std::vector<int64_t>{1}, axis, phi::DataType::FLOAT32, phi::CPUPlace());
  pir::OpResult axis_ = full_axis_op->result(0);
    
  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, index_, learning_rate_, master_param_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;

  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SparseMomentumInferMeta(meta_param, meta_learning_rate, meta_velocity, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
  argument_outputs.push_back(master_param_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseMomentumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value index_, pir::Value learning_rate_, pir::Value master_param_, pir::Value axis_, float mu, bool use_nesterov, const std::string& regularization_method, float regularization_coeff, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build SparseMomentumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, index_, learning_rate_, master_param_, axis_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_use_nesterov = pir::BoolAttribute::get(pir::IrContext::Instance(), use_nesterov);
  argument.AddAttribute("use_nesterov", attr_use_nesterov);
  pir::Attribute attr_regularization_method = pir::StrAttribute::get(pir::IrContext::Instance(), regularization_method);
  argument.AddAttribute("regularization_method", attr_regularization_method);
  pir::Attribute attr_regularization_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), regularization_coeff);
  argument.AddAttribute("regularization_coeff", attr_regularization_coeff);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType param = param_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)param;
  paddle::dialect::DenseTensorType grad = grad_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)grad;
  paddle::dialect::DenseTensorType velocity = velocity_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)velocity;
  paddle::dialect::DenseTensorType index = index_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)index;
  paddle::dialect::DenseTensorType learning_rate = learning_rate_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)learning_rate;
  phi::Scalar axis;
  if (axis_.dyn_cast<pir::OpResult>() && axis_.dyn_cast<pir::OpResult>().owner()->isa<paddle::dialect::FullOp>()) {
    axis = std::move(phi::Scalar(axis_.dyn_cast<pir::OpResult>().owner()
                                  ->dyn_cast<paddle::dialect::FullOp>()
                                  .attribute("value")
                                  .dyn_cast<paddle::dialect::ScalarAttribute>()
                                  .data()
                                  .to<int>()));
  }
  else {
    axis = std::move(phi::Scalar(-1));
    axis.SetFromTensor(true);
  }


  VLOG(4) << "Builder construction  dense_param";
  paddle::dialect::IrTensor ir_tensor_param(paddle::dialect::TransToPhiDataType(param.dtype()),
                                                      param.dims(),
                                                      param.data_layout(),
                                                      param.lod(),
                                                      param.offset());
  VLOG(4) << "Builder construction  meta_param";
  paddle::dialect::IrMetaTensor meta_param(&ir_tensor_param);

  VLOG(4) << "Builder construction  dense_learning_rate";
  paddle::dialect::IrTensor ir_tensor_learning_rate(paddle::dialect::TransToPhiDataType(learning_rate.dtype()),
                                                      learning_rate.dims(),
                                                      learning_rate.data_layout(),
                                                      learning_rate.lod(),
                                                      learning_rate.offset());
  VLOG(4) << "Builder construction  meta_learning_rate";
  paddle::dialect::IrMetaTensor meta_learning_rate(&ir_tensor_learning_rate);

  VLOG(4) << "Builder construction  dense_velocity";
  paddle::dialect::IrTensor ir_tensor_velocity(paddle::dialect::TransToPhiDataType(velocity.dtype()),
                                                      velocity.dims(),
                                                      velocity.data_layout(),
                                                      velocity.lod(),
                                                      velocity.offset());
  VLOG(4) << "Builder construction  meta_velocity";
  paddle::dialect::IrMetaTensor meta_velocity(&ir_tensor_velocity);
  paddle::dialect::IrTensor dense_param_out;
  paddle::dialect::IrMetaTensor meta_param_out(&dense_param_out);
  paddle::dialect::IrTensor dense_velocity_out;
  paddle::dialect::IrMetaTensor meta_velocity_out(&dense_velocity_out);
  paddle::dialect::IrTensor dense_master_param_out;
  paddle::dialect::IrMetaTensor meta_master_param_out(&dense_master_param_out);

  phi::SparseMomentumInferMeta(meta_param, meta_learning_rate, meta_velocity, &meta_param_out, &meta_velocity_out, &meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  pir::Type param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_param_out.dtype()), dense_param_out.dims(), dense_param_out.layout(), dense_param_out.lod(), dense_param_out.offset());
  argument_outputs.push_back(param_out_dense_tensor_type);

  pir::Type velocity_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_velocity_out.dtype()), dense_velocity_out.dims(), dense_velocity_out.layout(), dense_velocity_out.lod(), dense_velocity_out.offset());
  argument_outputs.push_back(velocity_out_dense_tensor_type);

  pir::Type master_param_out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_master_param_out.dtype()), dense_master_param_out.dims(), dense_master_param_out.layout(), dense_master_param_out.lod(), dense_master_param_out.offset());
  argument_outputs.push_back(master_param_out_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void SparseMomentumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: SparseMomentumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 7u,
                    "The size %d of inputs must be equal to 7.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  IR_ENFORCE((*this)->operand_source(4).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
  if (auto val = (*this)->operand(5)) {
    IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 5th input, got %s.", (*this)->operand_source(5).type());
  }
  IR_ENFORCE((*this)->operand_source(6).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 6th input, got %s.", (*this)->operand_source(6).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mu")>0,
                 "mu does not exist.");
  IR_ENFORCE(attributes.at("mu").isa<pir::FloatAttribute>(),
                 "Type of attribute: mu is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("use_nesterov")>0,
                 "use_nesterov does not exist.");
  IR_ENFORCE(attributes.at("use_nesterov").isa<pir::BoolAttribute>(),
                 "Type of attribute: use_nesterov is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("regularization_method")>0,
                 "regularization_method does not exist.");
  IR_ENFORCE(attributes.at("regularization_method").isa<pir::StrAttribute>(),
                 "Type of attribute: regularization_method is not pir::StrAttribute.");

  IR_ENFORCE(attributes.count("regularization_coeff")>0,
                 "regularization_coeff does not exist.");
  IR_ENFORCE(attributes.at("regularization_coeff").isa<pir::FloatAttribute>(),
                 "Type of attribute: regularization_coeff is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rescale_grad")>0,
                 "rescale_grad does not exist.");
  IR_ENFORCE(attributes.at("rescale_grad").isa<pir::FloatAttribute>(),
                 "Type of attribute: rescale_grad is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  if (auto output_2_type = (*this)->result(2).type()) {
    IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th output.");
  }
  }
  VLOG(4) << "End Verifying for: SparseMomentumOp.";
}

void SparseMomentumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::SparseMomentumInferMeta);
  fn(infer_meta);
}

phi::DataType SparseMomentumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: SparseMomentumOp";
  


  return expected_kernel_dtype;
}

const char *MatchMatrixTensorOp::attributes_name[1] = { "dim_t" };

OpInfoTuple MatchMatrixTensorOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("x", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("y", "paddle::dialect::DenseTensorType", false, false, false, true), paddle::dialect::OpInputInfo("w", "paddle::dialect::DenseTensorType", false, false, false, true) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("dim_t", "pir::Int32Attribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("out", "paddle::dialect::DenseTensorType", false, false), paddle::dialect::OpOutputInfo("tmp", "paddle::dialect::DenseTensorType", false, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("MatchMatrixTensorInferMeta", {"x", "y", "w", "dim_t"}, "match_matrix_tensor", {"x", "y", "w", "dim_t"}, {}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "match_matrix_tensor");
}

void MatchMatrixTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value w_, int dim_t) {
  VLOG(4) << "Start build MatchMatrixTensorOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, w_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim_t = pir::Int32Attribute::get(pir::IrContext::Instance(), dim_t);
  argument.AddAttribute("dim_t", attr_dim_t);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_tmp;
  paddle::dialect::IrMetaTensor meta_tmp(&dense_tmp);

  phi::MatchMatrixTensorInferMeta(meta_x, meta_y, meta_w, dim_t, &meta_out, &meta_tmp);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type tmp_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_tmp.dtype()), dense_tmp.dims(), dense_tmp.layout(), dense_tmp.lod(), dense_tmp.offset());
  argument_outputs.push_back(tmp_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatchMatrixTensorOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value x_, pir::Value y_, pir::Value w_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build MatchMatrixTensorOp";


  IR_ENFORCE(
      attributes.find("dim_t") != attributes.end(),
          "'dim_t' Attribute is expected for MatchMatrixTensorOp. ");
  int dim_t = attributes.at("dim_t").dyn_cast<pir::Int32Attribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {x_, y_, w_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_dim_t = pir::Int32Attribute::get(pir::IrContext::Instance(), dim_t);
  argument.AddAttribute("dim_t", attr_dim_t);

  VLOG(4) << "Builder construction outputs";
  paddle::dialect::DenseTensorType x = x_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)x;
  paddle::dialect::DenseTensorType y = y_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)y;
  paddle::dialect::DenseTensorType w = w_.type().dyn_cast<paddle::dialect::DenseTensorType>(); (void)w;

  VLOG(4) << "Builder construction  dense_x";
  paddle::dialect::IrTensor ir_tensor_x(paddle::dialect::TransToPhiDataType(x.dtype()),
                                                      x.dims(),
                                                      x.data_layout(),
                                                      x.lod(),
                                                      x.offset());
  VLOG(4) << "Builder construction  meta_x";
  paddle::dialect::IrMetaTensor meta_x(&ir_tensor_x);

  VLOG(4) << "Builder construction  dense_y";
  paddle::dialect::IrTensor ir_tensor_y(paddle::dialect::TransToPhiDataType(y.dtype()),
                                                      y.dims(),
                                                      y.data_layout(),
                                                      y.lod(),
                                                      y.offset());
  VLOG(4) << "Builder construction  meta_y";
  paddle::dialect::IrMetaTensor meta_y(&ir_tensor_y);

  VLOG(4) << "Builder construction  dense_w";
  paddle::dialect::IrTensor ir_tensor_w(paddle::dialect::TransToPhiDataType(w.dtype()),
                                                      w.dims(),
                                                      w.data_layout(),
                                                      w.lod(),
                                                      w.offset());
  VLOG(4) << "Builder construction  meta_w";
  paddle::dialect::IrMetaTensor meta_w(&ir_tensor_w);
  paddle::dialect::IrTensor dense_out;
  paddle::dialect::IrMetaTensor meta_out(&dense_out);
  paddle::dialect::IrTensor dense_tmp;
  paddle::dialect::IrMetaTensor meta_tmp(&dense_tmp);

  phi::MatchMatrixTensorInferMeta(meta_x, meta_y, meta_w, dim_t, &meta_out, &meta_tmp);

  std::vector<pir::Type> argument_outputs;
  pir::Type out_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_out.dtype()), dense_out.dims(), dense_out.layout(), dense_out.lod(), dense_out.offset());
  argument_outputs.push_back(out_dense_tensor_type);

  pir::Type tmp_dense_tensor_type = paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(dense_tmp.dtype()), dense_tmp.dims(), dense_tmp.layout(), dense_tmp.lod(), dense_tmp.offset());
  argument_outputs.push_back(tmp_dense_tensor_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void MatchMatrixTensorOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: MatchMatrixTensorOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 3u,
                    "The size %d of inputs must be equal to 3.", input_size);
  IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                  "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("dim_t")>0,
                 "dim_t does not exist.");
  IR_ENFORCE(attributes.at("dim_t").isa<pir::Int32Attribute>(),
                 "Type of attribute: dim_t is not pir::Int32Attribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 2u,
                    "The size %d of outputs must be equal to 2.", output_size);
  IR_ENFORCE((*this)->result(0).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 0th output.");
  IR_ENFORCE((*this)->result(1).type().isa<paddle::dialect::DenseTensorType>(),
                 "Type validation failed for the 1th output.");
  }
  VLOG(4) << "End Verifying for: MatchMatrixTensorOp.";
}

void MatchMatrixTensorOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::MatchMatrixTensorInferMeta);
  fn(infer_meta);
}

phi::DataType MatchMatrixTensorOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: MatchMatrixTensorOp";
  


  return expected_kernel_dtype;
}

const char *LarsMomentumOp::attributes_name[6] = { "mu", "lars_coeff", "lars_weight_decay", "epsilon", "multi_precision", "rescale_grad" };

OpInfoTuple LarsMomentumOp::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("velocity", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mu", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("lars_coeff", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("lars_weight_decay", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rescale_grad", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("velocity_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("master_param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LarsMomentumInferMeta", {"param", "velocity", "learning_rate", "grad", "master_param", "lars_weight_decay", "mu", "lars_coeff", "epsilon", "multi_precision", "rescale_grad"}, "lars_momentum", {"param", "velocity", "learning_rate", "grad", "master_param", "lars_weight_decay", "mu", "lars_coeff", "epsilon", "multi_precision", "rescale_grad"}, {"param"}, {}, {}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lars_momentum");
}

void LarsMomentumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, float mu, float lars_coeff, const std::vector<float>& lars_weight_decay, float epsilon, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build LarsMomentumOp";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_lars_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_coeff);
  argument.AddAttribute("lars_coeff", attr_lars_coeff);
  std::vector<pir::Attribute> vec_lars_weight_decay;
  for (size_t i = 0; i < static_cast<size_t>(lars_weight_decay.size()); i++) {
      pir::Attribute attr_lars_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_weight_decay[i]);

    vec_lars_weight_decay.push_back(attr_lars_weight_decay);
  }
  pir::Attribute attr_lars_weight_decay = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_lars_weight_decay);
  argument.AddAttribute("lars_weight_decay", attr_lars_weight_decay);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType velocity = velocity_.type().dyn_cast<pir::VectorType>(); (void)velocity;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_velocity;
  for (size_t i=0; i < static_cast<size_t>(velocity.size()); i++) {
    vec_ir_tensor_velocity.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity;
  for (size_t i=0; i < vec_ir_tensor_velocity.size(); i++) {
    vec_meta_velocity.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_velocity[i]));
  }

  std::vector<const phi::MetaTensor*> meta_velocity;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity.size()); i++) {
    meta_velocity.push_back(&vec_meta_velocity[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_velocity_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_velocity_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_velocity_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity_out.size()); i++) {
    meta_velocity_out.push_back(&vec_meta_velocity_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::LarsMomentumInferMeta(meta_param, meta_velocity, meta_learning_rate, meta_grad, meta_master_param, lars_weight_decay, mu, lars_coeff, epsilon, multi_precision, rescale_grad, meta_param_out, meta_velocity_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> velocity_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    velocity_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_velocity_out[i].dtype()), vec_dense_velocity_out[i].dims(), vec_dense_velocity_out[i].layout(), vec_dense_velocity_out[i].lod(), vec_dense_velocity_out[i].offset()));
  }
  pir::Type velocity_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), velocity_out_types);
  argument_outputs.push_back(velocity_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LarsMomentumOp::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LarsMomentumOp";


  IR_ENFORCE(
      attributes.find("mu") != attributes.end(),
          "'mu' Attribute is expected for LarsMomentumOp. ");
  float mu = attributes.at("mu").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("lars_coeff") != attributes.end(),
          "'lars_coeff' Attribute is expected for LarsMomentumOp. ");
  float lars_coeff = attributes.at("lars_coeff").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("lars_weight_decay") != attributes.end(),
          "'lars_weight_decay' Attribute is expected for LarsMomentumOp. ");
  std::vector<float> lars_weight_decay;
  for (size_t i = 0; i < attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    lars_weight_decay.push_back(attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LarsMomentumOp. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for LarsMomentumOp. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rescale_grad") != attributes.end(),
          "'rescale_grad' Attribute is expected for LarsMomentumOp. ");
  float rescale_grad = attributes.at("rescale_grad").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_lars_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_coeff);
  argument.AddAttribute("lars_coeff", attr_lars_coeff);
  std::vector<pir::Attribute> vec_lars_weight_decay;
  for (size_t i = 0; i < static_cast<size_t>(lars_weight_decay.size()); i++) {
      pir::Attribute attr_lars_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_weight_decay[i]);

    vec_lars_weight_decay.push_back(attr_lars_weight_decay);
  }
  pir::Attribute attr_lars_weight_decay = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_lars_weight_decay);
  argument.AddAttribute("lars_weight_decay", attr_lars_weight_decay);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType velocity = velocity_.type().dyn_cast<pir::VectorType>(); (void)velocity;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_velocity;
  for (size_t i=0; i < static_cast<size_t>(velocity.size()); i++) {
    vec_ir_tensor_velocity.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity;
  for (size_t i=0; i < vec_ir_tensor_velocity.size(); i++) {
    vec_meta_velocity.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_velocity[i]));
  }

  std::vector<const phi::MetaTensor*> meta_velocity;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity.size()); i++) {
    meta_velocity.push_back(&vec_meta_velocity[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_velocity_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_velocity_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_velocity_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity_out.size()); i++) {
    meta_velocity_out.push_back(&vec_meta_velocity_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::LarsMomentumInferMeta(meta_param, meta_velocity, meta_learning_rate, meta_grad, meta_master_param, lars_weight_decay, mu, lars_coeff, epsilon, multi_precision, rescale_grad, meta_param_out, meta_velocity_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> velocity_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    velocity_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_velocity_out[i].dtype()), vec_dense_velocity_out[i].dims(), vec_dense_velocity_out[i].layout(), vec_dense_velocity_out[i].lod(), vec_dense_velocity_out[i].offset()));
  }
  pir::Type velocity_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), velocity_out_types);
  argument_outputs.push_back(velocity_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LarsMomentumOp::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LarsMomentumOp.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val =  (*this)->operand(4)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
    }
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mu")>0,
                 "mu does not exist.");
  IR_ENFORCE(attributes.at("mu").isa<pir::FloatAttribute>(),
                 "Type of attribute: mu is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("lars_coeff")>0,
                 "lars_coeff does not exist.");
  IR_ENFORCE(attributes.at("lars_coeff").isa<pir::FloatAttribute>(),
                 "Type of attribute: lars_coeff is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("lars_weight_decay")>0,
                 "lars_weight_decay does not exist.");
  IR_ENFORCE(attributes.at("lars_weight_decay").isa<pir::ArrayAttribute>(),
                 "Type of attribute: lars_weight_decay is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: lars_weight_decay is not right.");
  }
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rescale_grad")>0,
                 "rescale_grad does not exist.");
  IR_ENFORCE(attributes.at("rescale_grad").isa<pir::FloatAttribute>(),
                 "Type of attribute: rescale_grad is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th output.");
      }
    }
    else {
      IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  }
  VLOG(4) << "End Verifying for: LarsMomentumOp.";
}

void LarsMomentumOp::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LarsMomentumInferMeta);
  fn(infer_meta);
}

phi::DataType LarsMomentumOp::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LarsMomentumOp";
  


  return expected_kernel_dtype;
}

const char *LarsMomentum_Op::attributes_name[6] = { "mu", "lars_coeff", "lars_weight_decay", "epsilon", "multi_precision", "rescale_grad" };

OpInfoTuple LarsMomentum_Op::GetOpInfo() {
  std::vector<paddle::dialect::OpInputInfo> inputs = { paddle::dialect::OpInputInfo("param", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("grad", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("velocity", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("learning_rate", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false, false, false), paddle::dialect::OpInputInfo("master_param", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false, false, false) };
  std::vector<paddle::dialect::OpAttributeInfo> attributes = { paddle::dialect::OpAttributeInfo("mu", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("lars_coeff", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("lars_weight_decay", "pir::ArrayAttribute<pir::FloatAttribute>", ""), paddle::dialect::OpAttributeInfo("epsilon", "pir::FloatAttribute", ""), paddle::dialect::OpAttributeInfo("multi_precision", "pir::BoolAttribute", ""), paddle::dialect::OpAttributeInfo("rescale_grad", "pir::FloatAttribute", "") };
  std::vector<paddle::dialect::OpOutputInfo> outputs = { paddle::dialect::OpOutputInfo("param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("velocity_out", "pir::VectorType<paddle::dialect::DenseTensorType>", false, false), paddle::dialect::OpOutputInfo("master_param_out", "pir::VectorType<paddle::dialect::DenseTensorType>", true, false) };
  paddle::dialect::OpRunTimeInfo run_time_info = paddle::dialect::OpRunTimeInfo("LarsMomentumInferMeta", {"param", "velocity", "learning_rate", "grad", "master_param", "lars_weight_decay", "mu", "lars_coeff", "epsilon", "multi_precision", "rescale_grad"}, "lars_momentum", {"param", "velocity", "learning_rate", "grad", "master_param", "lars_weight_decay", "mu", "lars_coeff", "epsilon", "multi_precision", "rescale_grad"}, {"param"}, {}, {{"master_param_out", "master_param"}}, {});
  return std::make_tuple(inputs, attributes, outputs, run_time_info, "lars_momentum");
}

void LarsMomentum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, float mu, float lars_coeff, const std::vector<float>& lars_weight_decay, float epsilon, bool multi_precision, float rescale_grad) {
  VLOG(4) << "Start build LarsMomentum_Op";



  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_lars_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_coeff);
  argument.AddAttribute("lars_coeff", attr_lars_coeff);
  std::vector<pir::Attribute> vec_lars_weight_decay;
  for (size_t i = 0; i < static_cast<size_t>(lars_weight_decay.size()); i++) {
      pir::Attribute attr_lars_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_weight_decay[i]);

    vec_lars_weight_decay.push_back(attr_lars_weight_decay);
  }
  pir::Attribute attr_lars_weight_decay = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_lars_weight_decay);
  argument.AddAttribute("lars_weight_decay", attr_lars_weight_decay);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType velocity = velocity_.type().dyn_cast<pir::VectorType>(); (void)velocity;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_velocity;
  for (size_t i=0; i < static_cast<size_t>(velocity.size()); i++) {
    vec_ir_tensor_velocity.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity;
  for (size_t i=0; i < vec_ir_tensor_velocity.size(); i++) {
    vec_meta_velocity.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_velocity[i]));
  }

  std::vector<const phi::MetaTensor*> meta_velocity;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity.size()); i++) {
    meta_velocity.push_back(&vec_meta_velocity[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_velocity_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_velocity_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_velocity_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity_out.size()); i++) {
    meta_velocity_out.push_back(&vec_meta_velocity_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::LarsMomentumInferMeta(meta_param, meta_velocity, meta_learning_rate, meta_grad, meta_master_param, lars_weight_decay, mu, lars_coeff, epsilon, multi_precision, rescale_grad, meta_param_out, meta_velocity_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> velocity_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    velocity_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_velocity_out[i].dtype()), vec_dense_velocity_out[i].dims(), vec_dense_velocity_out[i].layout(), vec_dense_velocity_out[i].lod(), vec_dense_velocity_out[i].offset()));
  }
  pir::Type velocity_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), velocity_out_types);
  argument_outputs.push_back(velocity_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LarsMomentum_Op::Build(pir::Builder &builder, pir::OperationArgument &argument, pir::Value param_, pir::Value grad_, pir::Value velocity_, pir::Value learning_rate_, pir::Value master_param_, pir::AttributeMap attributes) {
  VLOG(4) << "Start build LarsMomentum_Op";


  IR_ENFORCE(
      attributes.find("mu") != attributes.end(),
          "'mu' Attribute is expected for LarsMomentum_Op. ");
  float mu = attributes.at("mu").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("lars_coeff") != attributes.end(),
          "'lars_coeff' Attribute is expected for LarsMomentum_Op. ");
  float lars_coeff = attributes.at("lars_coeff").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("lars_weight_decay") != attributes.end(),
          "'lars_weight_decay' Attribute is expected for LarsMomentum_Op. ");
  std::vector<float> lars_weight_decay;
  for (size_t i = 0; i < attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    lars_weight_decay.push_back(attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().at(i).dyn_cast<pir::FloatAttribute>().data());
  }

  IR_ENFORCE(
      attributes.find("epsilon") != attributes.end(),
          "'epsilon' Attribute is expected for LarsMomentum_Op. ");
  float epsilon = attributes.at("epsilon").dyn_cast<pir::FloatAttribute>().data();

  IR_ENFORCE(
      attributes.find("multi_precision") != attributes.end(),
          "'multi_precision' Attribute is expected for LarsMomentum_Op. ");
  bool multi_precision = attributes.at("multi_precision").dyn_cast<pir::BoolAttribute>().data();

  IR_ENFORCE(
      attributes.find("rescale_grad") != attributes.end(),
          "'rescale_grad' Attribute is expected for LarsMomentum_Op. ");
  float rescale_grad = attributes.at("rescale_grad").dyn_cast<pir::FloatAttribute>().data();


  VLOG(4) << "Builder construction inputs";
  std::vector<pir::Value> argument_inputs = {param_, grad_, velocity_, learning_rate_, master_param_};
  argument.AddInputs(argument_inputs);

  VLOG(4) << "Builder construction attributes";
  pir::Attribute attr_mu = pir::FloatAttribute::get(pir::IrContext::Instance(), mu);
  argument.AddAttribute("mu", attr_mu);
  pir::Attribute attr_lars_coeff = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_coeff);
  argument.AddAttribute("lars_coeff", attr_lars_coeff);
  std::vector<pir::Attribute> vec_lars_weight_decay;
  for (size_t i = 0; i < static_cast<size_t>(lars_weight_decay.size()); i++) {
      pir::Attribute attr_lars_weight_decay = pir::FloatAttribute::get(pir::IrContext::Instance(), lars_weight_decay[i]);

    vec_lars_weight_decay.push_back(attr_lars_weight_decay);
  }
  pir::Attribute attr_lars_weight_decay = pir::ArrayAttribute::get(pir::IrContext::Instance(), vec_lars_weight_decay);
  argument.AddAttribute("lars_weight_decay", attr_lars_weight_decay);
  pir::Attribute attr_epsilon = pir::FloatAttribute::get(pir::IrContext::Instance(), epsilon);
  argument.AddAttribute("epsilon", attr_epsilon);
  pir::Attribute attr_multi_precision = pir::BoolAttribute::get(pir::IrContext::Instance(), multi_precision);
  argument.AddAttribute("multi_precision", attr_multi_precision);
  pir::Attribute attr_rescale_grad = pir::FloatAttribute::get(pir::IrContext::Instance(), rescale_grad);
  argument.AddAttribute("rescale_grad", attr_rescale_grad);

  VLOG(4) << "Builder construction outputs";
  pir::VectorType param = param_.type().dyn_cast<pir::VectorType>(); (void)param;
  pir::VectorType grad = grad_.type().dyn_cast<pir::VectorType>(); (void)grad;
  pir::VectorType velocity = velocity_.type().dyn_cast<pir::VectorType>(); (void)velocity;
  pir::VectorType learning_rate = learning_rate_.type().dyn_cast<pir::VectorType>(); (void)learning_rate;
  std::vector<paddle::dialect::IrTensor> vec_ir_tensor_param;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_ir_tensor_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param;
  for (size_t i=0; i < vec_ir_tensor_param.size(); i++) {
    vec_meta_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param.size()); i++) {
    meta_param.push_back(&vec_meta_param[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_velocity;
  for (size_t i=0; i < static_cast<size_t>(velocity.size()); i++) {
    vec_ir_tensor_velocity.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     velocity[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity;
  for (size_t i=0; i < vec_ir_tensor_velocity.size(); i++) {
    vec_meta_velocity.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_velocity[i]));
  }

  std::vector<const phi::MetaTensor*> meta_velocity;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity.size()); i++) {
    meta_velocity.push_back(&vec_meta_velocity[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(learning_rate.size()); i++) {
    vec_ir_tensor_learning_rate.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     learning_rate[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_learning_rate;
  for (size_t i=0; i < vec_ir_tensor_learning_rate.size(); i++) {
    vec_meta_learning_rate.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_learning_rate[i]));
  }

  std::vector<const phi::MetaTensor*> meta_learning_rate;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_learning_rate.size()); i++) {
    meta_learning_rate.push_back(&vec_meta_learning_rate[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_grad;
  for (size_t i=0; i < static_cast<size_t>(grad.size()); i++) {
    vec_ir_tensor_grad.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                     grad[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
  }
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_grad;
  for (size_t i=0; i < vec_ir_tensor_grad.size(); i++) {
    vec_meta_grad.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_grad[i]));
  }

  std::vector<const phi::MetaTensor*> meta_grad;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_grad.size()); i++) {
    meta_grad.push_back(&vec_meta_grad[i]);
  }
   std::vector<paddle::dialect::IrTensor> vec_ir_tensor_master_param;
  if (master_param_.impl() != nullptr) {
    pir::VectorType master_param = master_param_.type().dyn_cast<pir::VectorType>();
    for (size_t i=0; i < static_cast<size_t>(master_param.size()); i++) {
        vec_ir_tensor_master_param.push_back(paddle::dialect::IrTensor(paddle::dialect::TransToPhiDataType(master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dtype()),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().dims(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().data_layout(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().lod(),
                                                                        master_param[i].dyn_cast<paddle::dialect::DenseTensorType>().offset()));
    }
  }

  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param;
  for (size_t i=0; i < vec_ir_tensor_master_param.size(); i++) {
    vec_meta_master_param.push_back(paddle::dialect::IrMetaTensor(&vec_ir_tensor_master_param[i]));
  }

  std::vector<const phi::MetaTensor*> meta_master_param;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param.size()); i++) {
    meta_master_param.push_back(&vec_meta_master_param[i]);
  }

  std::vector<paddle::dialect::IrTensor> vec_dense_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_param_out.size()); i++) {
    meta_param_out.push_back(&vec_meta_param_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_velocity_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_velocity_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_velocity_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_velocity_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_velocity_out.size()); i++) {
    meta_velocity_out.push_back(&vec_meta_velocity_out[i]);
  }
  std::vector<paddle::dialect::IrTensor> vec_dense_master_param_out((param.size()), paddle::dialect::IrTensor());
  std::vector<paddle::dialect::IrMetaTensor> vec_meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    vec_meta_master_param_out.push_back(paddle::dialect::IrMetaTensor(&vec_dense_master_param_out[i]));
  }
  std::vector<phi::MetaTensor*> meta_master_param_out;
  for (size_t i=0; i < static_cast<size_t>(vec_meta_master_param_out.size()); i++) {
    meta_master_param_out.push_back(&vec_meta_master_param_out[i]);
  }

  phi::LarsMomentumInferMeta(meta_param, meta_velocity, meta_learning_rate, meta_grad, meta_master_param, lars_weight_decay, mu, lars_coeff, epsilon, multi_precision, rescale_grad, meta_param_out, meta_velocity_out, meta_master_param_out);

  std::vector<pir::Type> argument_outputs;
  std::vector<pir::Type> param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_param_out[i].dtype()), vec_dense_param_out[i].dims(), vec_dense_param_out[i].layout(), vec_dense_param_out[i].lod(), vec_dense_param_out[i].offset()));
  }
  pir::Type param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), param_out_types);
  argument_outputs.push_back(param_out_vector_type);

  std::vector<pir::Type> velocity_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    velocity_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_velocity_out[i].dtype()), vec_dense_velocity_out[i].dims(), vec_dense_velocity_out[i].layout(), vec_dense_velocity_out[i].lod(), vec_dense_velocity_out[i].offset()));
  }
  pir::Type velocity_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), velocity_out_types);
  argument_outputs.push_back(velocity_out_vector_type);

  std::vector<pir::Type> master_param_out_types;
  for (size_t i=0; i < static_cast<size_t>(param.size()); i++) {
    master_param_out_types.push_back(paddle::dialect::DenseTensorType::get(pir::IrContext::Instance(), paddle::dialect::TransToIrDataType(vec_dense_master_param_out[i].dtype()), vec_dense_master_param_out[i].dims(), vec_dense_master_param_out[i].layout(), vec_dense_master_param_out[i].lod(), vec_dense_master_param_out[i].offset()));
  }
  pir::Type master_param_out_vector_type = pir::VectorType::get(pir::IrContext::Instance(), master_param_out_types);
  argument_outputs.push_back(master_param_out_vector_type);
  argument.AddOutputs(argument_outputs.begin(), argument_outputs.end());
  ::pir::PassStopGradientsDefaultly(argument);

}

void LarsMomentum_Op::VerifySig() {
  VLOG(4) << "Start Verifying inputs, outputs and attributes for: LarsMomentum_Op.";
  VLOG(4) << "Verifying inputs:";
  {
  auto input_size = num_operands();
  IR_ENFORCE(input_size == 5u,
                    "The size %d of inputs must be equal to 5.", input_size);
  if (auto vec_type = (*this)->operand_source(0).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(0).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th input, got %s.", (*this)->operand_source(0).type());
  }
  if (auto vec_type = (*this)->operand_source(1).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(1).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th input, got %s.", (*this)->operand_source(1).type());
  }
  if (auto vec_type = (*this)->operand_source(2).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(2).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 2th input, got %s.", (*this)->operand_source(2).type());
  }
  if (auto vec_type = (*this)->operand_source(3).type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
      }
  }
  else {
    IR_ENFORCE((*this)->operand_source(3).type().isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 3th input, got %s.", (*this)->operand_source(3).type());
  }
  if (auto val =  (*this)->operand(4)) {
    if (auto vec_type = val.type().dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); i++) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                          "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
      }
    }
    else {
      IR_ENFORCE(val.type().isa<paddle::dialect::DenseTensorType>(),
                        "Type validation failed for the 4th input, got %s.", (*this)->operand_source(4).type());
    }
  }
  }
  VLOG(4) << "Verifying attributes:";
  {
  auto& attributes = this->attributes();
  IR_ENFORCE(attributes.count("mu")>0,
                 "mu does not exist.");
  IR_ENFORCE(attributes.at("mu").isa<pir::FloatAttribute>(),
                 "Type of attribute: mu is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("lars_coeff")>0,
                 "lars_coeff does not exist.");
  IR_ENFORCE(attributes.at("lars_coeff").isa<pir::FloatAttribute>(),
                 "Type of attribute: lars_coeff is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("lars_weight_decay")>0,
                 "lars_weight_decay does not exist.");
  IR_ENFORCE(attributes.at("lars_weight_decay").isa<pir::ArrayAttribute>(),
                 "Type of attribute: lars_weight_decay is not pir::ArrayAttribute.");
  for (size_t i = 0; i < attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().size(); i++) {
    IR_ENFORCE(attributes.at("lars_weight_decay").dyn_cast<pir::ArrayAttribute>().at(i).isa<pir::FloatAttribute>(),
                   "Type of attribute: lars_weight_decay is not right.");
  }
  IR_ENFORCE(attributes.count("epsilon")>0,
                 "epsilon does not exist.");
  IR_ENFORCE(attributes.at("epsilon").isa<pir::FloatAttribute>(),
                 "Type of attribute: epsilon is not pir::FloatAttribute.");

  IR_ENFORCE(attributes.count("multi_precision")>0,
                 "multi_precision does not exist.");
  IR_ENFORCE(attributes.at("multi_precision").isa<pir::BoolAttribute>(),
                 "Type of attribute: multi_precision is not pir::BoolAttribute.");

  IR_ENFORCE(attributes.count("rescale_grad")>0,
                 "rescale_grad does not exist.");
  IR_ENFORCE(attributes.at("rescale_grad").isa<pir::FloatAttribute>(),
                 "Type of attribute: rescale_grad is not pir::FloatAttribute.");

  }
  VLOG(4) << "Verifying outputs:";
  {
  auto output_size = num_results();
  IR_ENFORCE(output_size == 3u,
                    "The size %d of outputs must be equal to 3.", output_size);
  auto output_0_type = (*this)->result(0).type();
  if (auto vec_type = output_0_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 0th output.");
    }
  }
  else {
    IR_ENFORCE(output_0_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 0th output.");
  }
  auto output_1_type = (*this)->result(1).type();
  if (auto vec_type = output_1_type.dyn_cast<pir::VectorType>()) {
    for (size_t i = 0; i < vec_type.size(); i++) {
      IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 1th output.");
    }
  }
  else {
    IR_ENFORCE(output_1_type.isa<paddle::dialect::DenseTensorType>(),
                   "Type validation failed for the 1th output.");
  }
  if (auto output_2_type = (*this)->result(2).type()) {
    if (auto vec_type = output_2_type.dyn_cast<pir::VectorType>()) {
      for (size_t i = 0; i < vec_type.size(); ++i) {
        IR_ENFORCE(vec_type[i].isa<paddle::dialect::DenseTensorType>(),
                       "Type validation failed for the 2th output.");
      }
    }
    else {
      IR_ENFORCE(output_2_type.isa<paddle::dialect::DenseTensorType>(),
                     "Type validation failed for the 2th output.");
    }
  }
  }
  VLOG(4) << "End Verifying for: LarsMomentum_Op.";
}

void LarsMomentum_Op::InferMeta( phi::InferMetaContext *infer_meta ) {
  auto fn = PD_INFER_META(phi::LarsMomentumInferMeta);
  fn(infer_meta);
}

phi::DataType LarsMomentum_Op::GetKernelTypeForVar(
    const std::string& var_name,
    const phi::DataType& tensor_dtype,
    const phi::DataType& expected_kernel_dtype) {
  VLOG(4) << "Get KernelType for Var of op: LarsMomentum_Op";
  


  return expected_kernel_dtype;
}

} // namespace dialect
} // namespace paddle


IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Adadelta_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AddOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Add_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AllOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AminOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AnyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Assign_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignOut_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignValueOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::AssignValue_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BatchNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::BatchNorm_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CAllgatherOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CAllreduceMaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CAllreduceMax_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CAllreduceSumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CAllreduceSum_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CBroadcastOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CBroadcast_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CConcatOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CEmbeddingOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CIdentityOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CIdentity_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CReduceMinOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CReduceMin_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CReduceSumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CReduceSum_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CReducescatterOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CSyncCalcStreamOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CSyncCalcStream_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CSyncCommStreamOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CSyncCommStream_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CastOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Cast_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ChannelShuffleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Conv2dTransposeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CopyToOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DecayedAdagradOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DecodeJpegOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DeformableConvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DepthwiseConv2dTransposeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DisableCheckModelNanInfOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DistributeFpnProposalsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DivideOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Divide_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DropoutOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EinsumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ElementwisePowOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EmbeddingOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SparseWeightEmbeddingOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EmptyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EmptyLikeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EnableCheckModelNanInfOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EqualOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Equal_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Exponential_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::EyeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FeedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FetchOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FloorDivideOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FloorDivide_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FrobeniusNormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FullOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Full_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FullBatchSizeLikeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FullLikeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FullWithTensorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedAdam_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBatchNormActOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBatchNormAct_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBnAddActivationOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedBnAddActivation_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedSoftmaxMaskUpperTriangleOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GaussianOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GetTensorFromSelectedRowsOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GreaterEqualOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GreaterEqual_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GreaterThanOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::GreaterThan_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HardswishOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::HsigmoidLossOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LessEqualOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LessEqual_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LessThanOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LessThan_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LinspaceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LoadCombineOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LodArrayLengthOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogspaceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LogsumexpOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatmulOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatrixRankOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatrixRankTolOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MaximumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MeanOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MemcpyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MemcpyD2hOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MemcpyH2dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MinOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MinimumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MishOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplySrOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Multiply_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MultiplySr_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NormOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NotEqualOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NotEqual_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::OneHotOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::OnesOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::OnesLikeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PadOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pool2dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Pool3dOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::PrintOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ProdOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RandintOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RandpermOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReadFileOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RecvV2Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RemainderOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Remainder_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RepeatInterleaveOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RepeatInterleaveWithTensorIndexOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ReshapeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Reshape_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RnnOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Rnn_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RowConvOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::RreluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SaveCombineOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SeedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SendV2Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SetValueOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SetValue_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SetValueWithTensorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SetValueWithTensor_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShadowFeedOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShareDataOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ShuffleBatchOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SliceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftReluOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SoftmaxOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Softmax_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SplitOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SplitWithNumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::StridedSliceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SubtractOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Subtract_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SwishOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SyncBatchNorm_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TileOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TransLayoutOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TransposeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Transpose_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TrilOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Tril_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TrilIndicesOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TriuOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::Triu_Op)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TriuIndicesOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::TruncatedGaussianRandomOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniformOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniformRandomBatchSizeLikeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UniqueOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::UnpoolOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::WriteToArrayOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ZerosOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::ZerosLikeOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::CSoftmaxWithCrossEntropyOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::DpsgdOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FtrlOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedAttentionOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedElemwiseAddActivationOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::FusedFeedforwardOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NceOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::NumberCountOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::OnednnToPaddleLayoutOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::SparseMomentumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::MatchMatrixTensorOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LarsMomentumOp)

IR_DEFINE_EXPLICIT_TYPE_ID(paddle::dialect::LarsMomentum_Op)

