/* Copyright (c) 2016 PaddlePaddle Authors. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. */

#include "paddle/fluid/operators/activation_op.cu.h"

namespace paddle {
namespace operators {

template <typename T>
using CudaBReluFunctor = phi::funcs::CudaHardTanhFunctor<T>;
template <typename T>
using CudaBReluGradFunctor = phi::funcs::CudaHardTanhGradFunctor<T>;

USE_PHI_FUNCTOR(CudaCos)
USE_PHI_FUNCTOR(CudaTan)
USE_PHI_FUNCTOR(CudaAcos)
USE_PHI_FUNCTOR(CudaSin)
USE_PHI_FUNCTOR(CudaAsin)
USE_PHI_FUNCTOR(CudaAtan)
USE_PHI_FUNCTOR(CudaSinh)
USE_PHI_FUNCTOR(CudaCosh)
USE_PHI_FUNCTOR(CudaAsinh)
USE_PHI_FUNCTOR(CudaAcosh)
USE_PHI_FUNCTOR(CudaAtanh)
USE_PHI_FUNCTOR(CudaTanh)
USE_PHI_FUNCTOR(CudaLeakyRelu)
USE_PHI_FUNCTOR(CudaThresholdedRelu)
USE_PHI_FUNCTOR(CudaRelu6)
USE_PHI_FUNCTOR(CudaHardShrink)
USE_PHI_FUNCTOR(CudaSoftShrink)
USE_PHI_FUNCTOR(CudaTanhShrink)
USE_PHI_FUNCTOR(CudaSilu)
USE_PHI_FUNCTOR(CudaELU)
USE_PHI_FUNCTOR(CudaSoftsign)
USE_PHI_FUNCTOR(CudaSigmoid)
USE_PHI_FUNCTOR(CudaLogSigmoid)
USE_PHI_FUNCTOR(CudaHardSigmoid)
USE_PHI_FUNCTOR(CudaLog)
USE_PHI_FUNCTOR(CudaLog2)
USE_PHI_FUNCTOR(CudaLog10)
USE_PHI_FUNCTOR(CudaLog1p)
USE_PHI_FUNCTOR(CudaSwish)
USE_PHI_FUNCTOR(CudaHardSwish)

template <typename T>
using CudaRoundFunctor = phi::funcs::CudaRoundFunctor<T>;

template <typename T>
using CudaFloorFunctor = phi::funcs::CudaFloorFunctor<T>;

template <typename T>
using CudaCeilFunctor = phi::funcs::CudaCeilFunctor<T>;

template <typename T>
using CudaZeroGradFunctor = phi::funcs::CudaZeroGradFunctor<T>;

USE_PHI_FUNCTOR(CudaExp)
USE_PHI_FUNCTOR(CudaExpm1)
USE_PHI_FUNCTOR(CudaMish)
USE_PHI_FUNCTOR(CudaSTanh)
USE_PHI_FUNCTOR(CudaReciprocal)
USE_PHI_FUNCTOR(CudaSquare)
USE_PHI_FUNCTOR(CudaSqrt)
USE_PHI_FUNCTOR(CudaRsqrt)
USE_PHI_FUNCTOR(CudaSoftplus)

template <typename T>
using CudaELUGradNegativeAlphaFunctor =
    phi::funcs::CudaELUGradNegativeAlphaFunctor<T>;

}  // namespace operators
}  // namespace paddle

namespace ops = paddle::operators;

#ifdef PADDLE_WITH_XPU_KP
REGISTER_OP_KERNEL(
    brelu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              phi::funcs::CudaHardTanhFunctor<float>>);
REGISTER_OP_KERNEL(
    brelu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  phi::funcs::CudaHardTanhGradFunctor<float>>);

REGISTER_OP_KERNEL(
    ceil,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaCeilFunctor<float>>);
REGISTER_OP_KERNEL(
    ceil_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaZeroGradFunctor<float>>);

REGISTER_OP_KERNEL(
    celu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              phi::funcs::CudaCELUFunctor<float>>);
REGISTER_OP_KERNEL(
    celu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  phi::funcs::CudaCELUGradFunctor<float>>);

REGISTER_OP_KERNEL(
    elu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaELUFunctor<float>>);
REGISTER_OP_KERNEL(
    elu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaELUGradFunctor<float>>);

REGISTER_OP_KERNEL(
    exp,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaExpFunctor<float>>);
REGISTER_OP_KERNEL(
    exp_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaExpGradFunctor<float>>);

REGISTER_OP_KERNEL(
    floor,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaFloorFunctor<float>>);
REGISTER_OP_KERNEL(
    floor_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaZeroGradFunctor<float>>);

REGISTER_OP_KERNEL(
    hard_shrink,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              ops::CudaHardShrinkFunctor<float>>);
REGISTER_OP_KERNEL(
    hard_shrink_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaHardShrinkGradFunctor<float>>);

REGISTER_OP_KERNEL(
    hard_sigmoid,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              ops::CudaHardSigmoidFunctor<float>>);
REGISTER_OP_KERNEL(
    hard_sigmoid_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaHardSigmoidGradFunctor<float>>);

REGISTER_OP_KERNEL(hard_swish,
                   KP,
                   phi::XPUPlace,
                   ops::ActivationCudaKernel<phi::XPUContext,
                                             ops::CudaHardSwishFunctor<float>>);
REGISTER_OP_KERNEL(
    hard_swish_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaHardSwishGradFunctor<float>>);

REGISTER_OP_KERNEL(
    leaky_relu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              phi::funcs::CudaLeakyReluFunctor<float>>);
REGISTER_OP_KERNEL(
    leaky_relu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  phi::funcs::CudaLeakyReluGradFunctor<float>>);

REGISTER_OP_KERNEL(
    log,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaLogFunctor<float>>);
REGISTER_OP_KERNEL(
    log_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaLogGradFunctor<float>>);

REGISTER_OP_KERNEL(
    log1p,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaLog1pFunctor<float>>);
REGISTER_OP_KERNEL(
    log1p_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaLog1pGradFunctor<float>>);

REGISTER_OP_KERNEL(
    logsigmoid,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              ops::CudaLogSigmoidFunctor<float>>);
REGISTER_OP_KERNEL(
    logsigmoid_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaLogSigmoidGradFunctor<float>>);

REGISTER_OP_KERNEL(
    reciprocal,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              ops::CudaReciprocalFunctor<float>>);
REGISTER_OP_KERNEL(
    reciprocal_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaReciprocalGradFunctor<float>>);

REGISTER_OP_KERNEL(
    relu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              phi::funcs::CudaReluFunctor<float>>);
REGISTER_OP_KERNEL(
    relu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  phi::funcs::CudaReluGradFunctor<float>>);

REGISTER_OP_KERNEL(
    relu6,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaRelu6Functor<float>>);
REGISTER_OP_KERNEL(
    relu6_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaRelu6GradFunctor<float>>);

REGISTER_OP_KERNEL(
    sigmoid,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaSigmoidFunctor<float>>);
REGISTER_OP_KERNEL(
    sigmoid_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSigmoidGradFunctor<float>>);

REGISTER_OP_KERNEL(
    silu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaSiluFunctor<float>>);
REGISTER_OP_KERNEL(
    silu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSiluGradFunctor<float>>);

REGISTER_OP_KERNEL(soft_relu,
                   KP,
                   phi::XPUPlace,
                   ops::ActivationCudaKernel<phi::XPUContext,
                                             ops::CudaSoftReluFunctor<float>>);
REGISTER_OP_KERNEL(
    soft_relu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSoftReluGradFunctor<float>>);

REGISTER_OP_KERNEL(softplus,
                   KP,
                   phi::XPUPlace,
                   ops::ActivationCudaKernel<phi::XPUContext,
                                             ops::CudaSoftplusFunctor<float>>);
REGISTER_OP_KERNEL(
    softplus_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSoftplusGradFunctor<float>>);

REGISTER_OP_KERNEL(
    softshrink,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              ops::CudaSoftShrinkFunctor<float>>);
REGISTER_OP_KERNEL(
    softshrink_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSoftShrinkGradFunctor<float>>);

REGISTER_OP_KERNEL(softsign,
                   KP,
                   phi::XPUPlace,
                   ops::ActivationCudaKernel<phi::XPUContext,
                                             ops::CudaSoftsignFunctor<float>>);
REGISTER_OP_KERNEL(
    softsign_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSoftsignGradFunctor<float>>);

REGISTER_OP_KERNEL(
    sqrt,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaSqrtFunctor<float>>);
REGISTER_OP_KERNEL(
    sqrt_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSqrtGradFunctor<float>>);

REGISTER_OP_KERNEL(
    square,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaSquareFunctor<float>>);
REGISTER_OP_KERNEL(
    square_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSquareGradFunctor<float>>);

REGISTER_OP_KERNEL(
    swish,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext, ops::CudaSwishFunctor<float>>);
REGISTER_OP_KERNEL(
    swish_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaSwishGradFunctor<float>>);

REGISTER_OP_KERNEL(
    thresholded_relu,
    KP,
    phi::XPUPlace,
    ops::ActivationCudaKernel<phi::XPUContext,
                              ops::CudaThresholdedReluFunctor<float>>);
REGISTER_OP_KERNEL(
    thresholded_relu_grad,
    KP,
    phi::XPUPlace,
    ops::ActivationGradCudaKernel<phi::XPUContext,
                                  ops::CudaThresholdedReluGradFunctor<float>>);

#endif  // PADDLE_WITH_XPU_KP
