// this file is generated by paddle/phi/api/yaml/generator/generate_op.py, do not edit.
#include <string>
#include "paddle/fluid/framework/convert_utils.h"
#include "paddle/fluid/framework/infershape_utils.h"
#include "paddle/fluid/framework/op_registry.h"
#include "paddle/fluid/framework/op_version_registry.h"
#include "paddle/fluid/prim/api/composite_backward/composite_backward_api.h"
#include "paddle/fluid/prim/utils/static/composite_grad_desc_maker.h"
#include "paddle/fluid/prim/utils/static/desc_tensor.h"
#include "paddle/fluid/operators/generator/get_expected_kernel_func.h"
#include "paddle/phi/core/infermeta_utils.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"

namespace paddle {
namespace operators {

using paddle::framework::GradVarName;


class DirichletOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Alpha", "(Tensor), input 0 of dirichlet op.");
    AddOutput("Out", "(Tensor), output 0 of dirichlet op.");
    AddComment(R"DOC(
TODO: Documentation of dirichlet op.
)DOC");
  }
};


class DirichletOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(dirichlet, DirichletInferShapeFunctor,
                            PD_INFER_META(phi::DirichletInferMeta));



class DistOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of dist op.");
    AddInput("Y", "(Tensor), input 1 of dist op.");
    AddOutput("Out", "(Tensor), output 0 of dist op.");
    AddAttr<float>("p", "(float), attribute 0 for dist op.")
        .SetDefault(2.0);
    AddComment(R"DOC(
TODO: Documentation of dist op.
)DOC");
  }
};


class DistOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(dist, DistInferShapeFunctor,
                            PD_INFER_META(phi::DistInferMeta));



class DotOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of dot op.");
    AddInput("Y", "(Tensor), input 1 of dot op.");
    AddOutput("Out", "(Tensor), output 0 of dot op.");
    AddComment(R"DOC(
TODO: Documentation of dot op.
)DOC");
  }
};


class DotOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(dot, DotInferShapeFunctor,
                            PD_INFER_META(phi::DotInferMeta));



class EditDistanceOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Hyps", "(Tensor), input 0 of edit_distance op.");
    AddInput("Refs", "(Tensor), input 1 of edit_distance op.");
    AddInput("HypsLength", "(Tensor), input 2 of edit_distance op.")
        .AsDispensable();
    AddInput("RefsLength", "(Tensor), input 3 of edit_distance op.")
        .AsDispensable();
    AddOutput("SequenceNum", "(Tensor), output 0 of edit_distance op.");
    AddOutput("Out", "(Tensor), output 1 of edit_distance op.");
    AddAttr<bool>("normalized", "(bool), attribute 0 for edit_distance op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of edit_distance op.
)DOC");
  }
};


class EditDistanceOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::TransToProtoVarType(phi::DataType::FLOAT32);
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(edit_distance, EditDistanceInferShapeFunctor,
                            PD_INFER_META(phi::EditDistanceInferMeta));



class EigOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of eig op.");
    AddOutput("Eigenvalues", "(Tensor), output 0 of eig op.");
    AddOutput("Eigenvectors", "(Tensor), output 1 of eig op.");
    AddComment(R"DOC(
TODO: Documentation of eig op.
)DOC");
  }
};


class EigOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(eig, EigInferShapeFunctor,
                            PD_INFER_META(phi::EigInferMeta));



class EighOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of eigh op.");
    AddOutput("Eigenvalues", "(Tensor), output 0 of eigh op.");
    AddOutput("Eigenvectors", "(Tensor), output 1 of eigh op.");
    AddAttr<std::string>("UPLO", "(std::string), attribute 0 for eigh op.")
        .SetDefault("L");
    AddComment(R"DOC(
TODO: Documentation of eigh op.
)DOC");
  }
};


class EighOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(eigh, EighInferShapeFunctor,
                            PD_INFER_META(phi::EighInferMeta));



class EigvalsOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of eigvals op.");
    AddOutput("Out", "(Tensor), output 0 of eigvals op.");
    AddComment(R"DOC(
TODO: Documentation of eigvals op.
)DOC");
  }
};


class EigvalsOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(eigvals, EigvalsInferShapeFunctor,
                            PD_INFER_META(phi::EigvalsInferMeta));



class EigvalshOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of eigvalsh op.");
    AddOutput("Eigenvalues", "(Tensor), output 0 of eigvalsh op.");
    AddOutput("Eigenvectors", "(Tensor), output 1 of eigvalsh op.");
    AddAttr<std::string>("UPLO", "(std::string), attribute 0 for eigvalsh op.")
        .SetDefault("L");
    AddAttr<bool>("is_test", "(bool), attribute 1 for eigvalsh op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of eigvalsh op.
)DOC");
  }
};


class EigvalshOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(eigvalsh, EigvalshInferShapeFunctor,
                            PD_INFER_META(phi::EigvalshInferMeta));



class EluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elu op.");
    AddOutput("Out", "(Tensor), output 0 of elu op.");
    AddAttr<float>("alpha", "(float), attribute 0 for elu op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of elu op.
)DOC");
  }
};


class EluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(elu, EluInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(EluInplaceInferer,
                           {"X", "Out"});



class EqualAllOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of equal_all op.");
    AddInput("Y", "(Tensor), input 1 of equal_all op.");
    AddOutput("Out", "(Tensor), output 0 of equal_all op.");
    AddComment(R"DOC(
TODO: Documentation of equal_all op.
)DOC");
  }
};


class EqualAllOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(equal_all, EqualAllInferShapeFunctor,
                            PD_INFER_META(phi::CompareAllInferMeta));



class ErfOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of erf op.");
    AddOutput("Out", "(Tensor), output 0 of erf op.");
    AddComment(R"DOC(
TODO: Documentation of erf op.
)DOC");
  }
};


class ErfOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(erf, ErfInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ErfInplaceInferer,
                           {"X", "Out"});



class ErfinvOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of erfinv op.");
    AddOutput("Out", "(Tensor), output 0 of erfinv op.");
    AddComment(R"DOC(
TODO: Documentation of erfinv op.
)DOC");
  }
};


class ErfinvOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(erfinv, ErfinvInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ErfinvInplaceInferer,
                           {"X", "Out"});



class ExpOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of exp op.");
    AddOutput("Out", "(Tensor), output 0 of exp op.");
    AddComment(R"DOC(
TODO: Documentation of exp op.
)DOC");
  }
};


class ExpOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(exp, ExpInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ExpInplaceInferer,
                           {"X", "Out"});



class ExpandV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of expand_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of expand_v2 op.");
    AddInput("Shape", "attribute 0 for expand_v2 op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("expand_shapes_tensor", "attribute 0 for expand_v2 op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int>>("shape", "(std::vector<int>), attribute 0 for expand_v2 op.")
        .SetDefault({});
    AddComment(R"DOC(
TODO: Documentation of expand_v2 op.
)DOC");
  }
};


class ExpandV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(expand_v2, ExpandV2InferShapeFunctor,
                            PD_INFER_META(phi::ExpandInferMeta));



class ExpandAsV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of expand_as_v2 op.");
    AddInput("Y", "(Tensor), input 1 of expand_as_v2 op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of expand_as_v2 op.");
    AddAttr<std::vector<int>>("target_shape", "(std::vector<int>), attribute 0 for expand_as_v2 op.")
        .SetDefault({});
    AddComment(R"DOC(
TODO: Documentation of expand_as_v2 op.
)DOC");
  }
};


class ExpandAsV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(expand_as_v2, ExpandAsV2InferShapeFunctor,
                            PD_INFER_META(phi::ExpandAsInferMeta));



class Expm1OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of expm1 op.");
    AddOutput("Out", "(Tensor), output 0 of expm1 op.");
    AddComment(R"DOC(
TODO: Documentation of expm1 op.
)DOC");
  }
};


class Expm1Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(expm1, Expm1InferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Expm1InplaceInferer,
                           {"X", "Out"});



class FftC2cOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fft_c2c op.");
    AddOutput("Out", "(Tensor), output 0 of fft_c2c op.");
    AddAttr<std::vector<int64_t>>("axes", "(std::vector<int64_t>), attribute 0 for fft_c2c op.")
    ;
    AddAttr<std::string>("normalization", "(std::string), attribute 1 for fft_c2c op.")
    ;
    AddAttr<bool>("forward", "(bool), attribute 2 for fft_c2c op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fft_c2c op.
)DOC");
  }
};


class FftC2cOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fft_c2c, FftC2cInferShapeFunctor,
                            PD_INFER_META(phi::FFTC2CInferMeta));



class FftC2rOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fft_c2r op.");
    AddOutput("Out", "(Tensor), output 0 of fft_c2r op.");
    AddAttr<std::vector<int64_t>>("axes", "(std::vector<int64_t>), attribute 0 for fft_c2r op.")
    ;
    AddAttr<std::string>("normalization", "(std::string), attribute 1 for fft_c2r op.")
    ;
    AddAttr<bool>("forward", "(bool), attribute 2 for fft_c2r op.")
    ;
    AddAttr<int64_t>("last_dim_size", "(int64_t), attribute 3 for fft_c2r op.")
        .SetDefault(0L);
    AddComment(R"DOC(
TODO: Documentation of fft_c2r op.
)DOC");
  }
};


class FftC2rOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fft_c2r, FftC2rInferShapeFunctor,
                            PD_INFER_META(phi::FFTC2RInferMeta));



class FftR2cOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fft_r2c op.");
    AddOutput("Out", "(Tensor), output 0 of fft_r2c op.");
    AddAttr<std::vector<int64_t>>("axes", "(std::vector<int64_t>), attribute 0 for fft_r2c op.")
    ;
    AddAttr<std::string>("normalization", "(std::string), attribute 1 for fft_r2c op.")
    ;
    AddAttr<bool>("forward", "(bool), attribute 2 for fft_r2c op.")
    ;
    AddAttr<bool>("onesided", "(bool), attribute 3 for fft_r2c op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fft_r2c op.
)DOC");
  }
};


class FftR2cOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fft_r2c, FftR2cInferShapeFunctor,
                            PD_INFER_META(phi::FFTR2CInferMeta));



class FillAnyOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fill_any op.");
    AddOutput("Out", "(Tensor), output 0 of fill_any op.");
    AddAttr<float>("value", "(float), attribute 0 for fill_any op.")
        .SetDefault(0)
        .SupportTensor();
    AddComment(R"DOC(
TODO: Documentation of fill_any op.
)DOC");
  }
};


class FillAnyOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fill_any, FillAnyInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(FillAnyInplaceInferer,
                           {"X", "Out"});



class FillDiagonalOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fill_diagonal op.");
    AddOutput("Out", "(Tensor), output 0 of fill_diagonal op.");
    AddAttr<float>("value", "(float), attribute 0 for fill_diagonal op.")
        .SetDefault(0);
    AddAttr<int>("offset", "(int), attribute 1 for fill_diagonal op.")
        .SetDefault(0);
    AddAttr<bool>("wrap", "(bool), attribute 2 for fill_diagonal op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of fill_diagonal op.
)DOC");
  }
};


class FillDiagonalOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fill_diagonal, FillDiagonalInferShapeFunctor,
                            PD_INFER_META(phi::FillDiagonalInferMeta));
DECLARE_INPLACE_OP_INFERER(FillDiagonalInplaceInferer,
                           {"X", "Out"});



class FillDiagonalTensorOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fill_diagonal_tensor op.");
    AddInput("Y", "(Tensor), input 1 of fill_diagonal_tensor op.");
    AddOutput("Out", "(Tensor), output 0 of fill_diagonal_tensor op.");
    AddAttr<int64_t>("offset", "(int64_t), attribute 0 for fill_diagonal_tensor op.")
        .SetDefault(0);
    AddAttr<int>("dim1", "(int), attribute 1 for fill_diagonal_tensor op.")
        .SetDefault(0);
    AddAttr<int>("dim2", "(int), attribute 2 for fill_diagonal_tensor op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of fill_diagonal_tensor op.
)DOC");
  }
};


class FillDiagonalTensorOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fill_diagonal_tensor, FillDiagonalTensorInferShapeFunctor,
                            PD_INFER_META(phi::FillDiagonalTensorInferMeta));
DECLARE_INPLACE_OP_INFERER(FillDiagonalTensorInplaceInferer,
                           {"X", "Out"});



class FlashAttnOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("q", "(Tensor), input 0 of flash_attn op.");
    AddInput("k", "(Tensor), input 1 of flash_attn op.");
    AddInput("v", "(Tensor), input 2 of flash_attn op.");
    AddInput("fixed_seed_offset", "(Tensor), input 3 of flash_attn op.")
        .AsDispensable();
    AddInput("attn_mask", "(Tensor), input 4 of flash_attn op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of flash_attn op.");
    AddOutput("softmax", "(Tensor), output 1 of flash_attn op.");
    AddOutput("softmax_lse", "(Tensor), output 2 of flash_attn op.")
        .AsIntermediate();
    AddOutput("seed_offset", "(Tensor), output 3 of flash_attn op.")
        .AsIntermediate();
    AddAttr<float>("dropout", "(float), attribute 0 for flash_attn op.")
        .SetDefault(0.0);
    AddAttr<bool>("causal", "(bool), attribute 1 for flash_attn op.")
        .SetDefault(false);
    AddAttr<bool>("return_softmax", "(bool), attribute 2 for flash_attn op.")
        .SetDefault(false);
    AddAttr<bool>("is_test", "(bool), attribute 3 for flash_attn op.")
        .SetDefault(false);
    AddAttr<std::string>("rng_name", "(std::string), attribute 4 for flash_attn op.")
        .SetDefault("");
    AddComment(R"DOC(
TODO: Documentation of flash_attn op.
)DOC");
  }
};


class FlashAttnOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(flash_attn, FlashAttnInferShapeFunctor,
                            PD_INFER_META(phi::FlashAttnInferMeta));



class FlashAttnUnpaddedOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("q", "(Tensor), input 0 of flash_attn_unpadded op.");
    AddInput("k", "(Tensor), input 1 of flash_attn_unpadded op.");
    AddInput("v", "(Tensor), input 2 of flash_attn_unpadded op.");
    AddInput("cu_seqlens_q", "(Tensor), input 3 of flash_attn_unpadded op.");
    AddInput("cu_seqlens_k", "(Tensor), input 4 of flash_attn_unpadded op.");
    AddInput("fixed_seed_offset", "(Tensor), input 5 of flash_attn_unpadded op.")
        .AsDispensable();
    AddInput("attn_mask", "(Tensor), input 6 of flash_attn_unpadded op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of flash_attn_unpadded op.");
    AddOutput("softmax", "(Tensor), output 1 of flash_attn_unpadded op.");
    AddOutput("softmax_lse", "(Tensor), output 2 of flash_attn_unpadded op.")
        .AsIntermediate();
    AddOutput("seed_offset", "(Tensor), output 3 of flash_attn_unpadded op.")
        .AsIntermediate();
    AddAttr<int64_t>("max_seqlen_q", "(int64_t), attribute 0 for flash_attn_unpadded op.")
    ;
    AddAttr<int64_t>("max_seqlen_k", "(int64_t), attribute 1 for flash_attn_unpadded op.")
    ;
    AddAttr<float>("scale", "(float), attribute 2 for flash_attn_unpadded op.")
    ;
    AddAttr<float>("dropout", "(float), attribute 3 for flash_attn_unpadded op.")
        .SetDefault(0.0);
    AddAttr<bool>("causal", "(bool), attribute 4 for flash_attn_unpadded op.")
        .SetDefault(false);
    AddAttr<bool>("return_softmax", "(bool), attribute 5 for flash_attn_unpadded op.")
        .SetDefault(false);
    AddAttr<bool>("is_test", "(bool), attribute 6 for flash_attn_unpadded op.")
        .SetDefault(false);
    AddAttr<std::string>("rng_name", "(std::string), attribute 7 for flash_attn_unpadded op.")
        .SetDefault("");
    AddComment(R"DOC(
TODO: Documentation of flash_attn_unpadded op.
)DOC");
  }
};


class FlashAttnUnpaddedOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(flash_attn_unpadded, FlashAttnUnpaddedInferShapeFunctor,
                            PD_INFER_META(phi::FlashAttnInferMeta));



class FlattenContiguousRangeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of flatten_contiguous_range op.");
    AddOutput("Out", "(Tensor), output 0 of flatten_contiguous_range op.");
    AddOutput("XShape", "(Tensor), output 1 of flatten_contiguous_range op.")
        .AsIntermediate()
        .AsExtra();
    AddAttr<int>("start_axis", "(int), attribute 0 for flatten_contiguous_range op.")
        .SetDefault(1);
    AddAttr<int>("stop_axis", "(int), attribute 1 for flatten_contiguous_range op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of flatten_contiguous_range op.
)DOC");
  }
};


class FlattenContiguousRangeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(flatten_contiguous_range, FlattenContiguousRangeInferShapeFunctor,
                            PD_INFER_META(phi::FlattenWithXShapeInferMeta));
DECLARE_INPLACE_OP_INFERER(FlattenContiguousRangeInplaceInferer,
                           {"X", "Out"});



class FlipOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of flip op.");
    AddOutput("Out", "(Tensor), output 0 of flip op.");
    AddAttr<std::vector<int>>("axis", "(std::vector<int>), attribute 0 for flip op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of flip op.
)DOC");
  }
};


class FlipOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(flip, FlipInferShapeFunctor,
                            PD_INFER_META(phi::FlipInferMeta));



class FloorOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of floor op.");
    AddOutput("Out", "(Tensor), output 0 of floor op.");
    AddComment(R"DOC(
TODO: Documentation of floor op.
)DOC");
  }
};


class FloorOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(floor, FloorInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(FloorInplaceInferer,
                           {"X", "Out"});



class ElementwiseFmaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_fmax op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_fmax op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_fmax op.");
    AddComment(R"DOC(
TODO: Documentation of elementwise_fmax op.
)DOC");
  }
};


class ElementwiseFmaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_fmax, ElementwiseFmaxInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseInferMeta));



class ElementwiseFminOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_fmin op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_fmin op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_fmin op.");
    AddComment(R"DOC(
TODO: Documentation of elementwise_fmin op.
)DOC");
  }
};


class ElementwiseFminOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_fmin, ElementwiseFminInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseInferMeta));



class FoldOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fold op.");
    AddOutput("Y", "(Tensor), output 0 of fold op.");
    AddAttr<std::vector<int>>("output_sizes", "(std::vector<int>), attribute 0 for fold op.")
    ;
    AddAttr<std::vector<int>>("kernel_sizes", "(std::vector<int>), attribute 1 for fold op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 2 for fold op.")
    ;
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 3 for fold op.")
    ;
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 4 for fold op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fold op.
)DOC");
  }
};


class FoldOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fold, FoldInferShapeFunctor,
                            PD_INFER_META(phi::FoldInferMeta));



class FrameOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of frame op.");
    AddOutput("Out", "(Tensor), output 0 of frame op.");
    AddAttr<int>("frame_length", "(int), attribute 0 for frame op.")
    ;
    AddAttr<int>("hop_length", "(int), attribute 1 for frame op.")
    ;
    AddAttr<int>("axis", "(int), attribute 2 for frame op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of frame op.
)DOC");
  }
};


class FrameOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(frame, FrameInferShapeFunctor,
                            PD_INFER_META(phi::FrameInferMeta));



class FullIntArrayOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("out", "(Tensor), output 0 of full_int_array op.");
    AddInput("ValueTensor", "attribute 0 for full_int_array op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("ValueTensorList", "attribute 0 for full_int_array op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("value", "(std::vector<int64_t>), attribute 0 for full_int_array op.")
    ;
    AddAttr<int>("dtype", "(int), attribute 1 for full_int_array op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddAttr<int>("place", "(int), attribute 2 for full_int_array op.")
        .SetDefault(static_cast<int>(phi::Place(phi::CPUPlace()).GetType()));
    AddComment(R"DOC(
TODO: Documentation of full_int_array op.
)DOC");
  }
};


class FullIntArrayOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("place")->place()));
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(full_int_array, FullIntArrayInferShapeFunctor,
                            PD_INFER_META(phi::CreateIntArrayInferMeta));



class FusedBiasResidualLayernormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_bias_residual_layernorm op.");
    AddInput("bias", "(Tensor), input 1 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddInput("residual", "(Tensor), input 2 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddInput("norm_weight", "(Tensor), input 3 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddInput("norm_bias", "(Tensor), input 4 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of fused_bias_residual_layernorm op.");
    AddOutput("residual_out", "(Tensor), output 1 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddOutput("mean", "(Tensor), output 2 of fused_bias_residual_layernorm op.");
    AddOutput("variance", "(Tensor), output 3 of fused_bias_residual_layernorm op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("residual_alpha", "(float), attribute 1 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<int>("begin_norm_axis", "(int), attribute 2 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("quant_scale", "(float), attribute 3 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<int>("quant_round_type", "(int), attribute 4 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("quant_max_bound", "(float), attribute 5 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("quant_min_bound", "(float), attribute 6 for fused_bias_residual_layernorm op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fused_bias_residual_layernorm op.
)DOC");
  }
};


class FusedBiasResidualLayernormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_bias_residual_layernorm, FusedBiasResidualLayernormInferShapeFunctor,
                            PD_INFER_META(phi::FusedLayerNormInferMeta));



class GatherOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of gather op.");
    AddInput("Index", "(Tensor), input 1 of gather op.");
    AddOutput("Out", "(Tensor), output 0 of gather op.");
    AddInput("Axis", "attribute 0 for gather op from 0D Tensor.")
        .AsDispensable();
    AddAttr<int>("axis", "(int), attribute 0 for gather op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of gather op.
)DOC");
  }
};


class GatherOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(gather, GatherInferShapeFunctor,
                            PD_INFER_META(phi::GatherInferMeta));



class GatherNdOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of gather_nd op.");
    AddInput("Index", "(Tensor), input 1 of gather_nd op.");
    AddOutput("Out", "(Tensor), output 0 of gather_nd op.");
    AddComment(R"DOC(
TODO: Documentation of gather_nd op.
)DOC");
  }
};


class GatherNdOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(gather_nd, GatherNdInferShapeFunctor,
                            PD_INFER_META(phi::GatherNdInferMeta));



class GatherTreeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Ids", "(Tensor), input 0 of gather_tree op.");
    AddInput("Parents", "(Tensor), input 1 of gather_tree op.");
    AddOutput("Out", "(Tensor), output 0 of gather_tree op.");
    AddComment(R"DOC(
TODO: Documentation of gather_tree op.
)DOC");
  }
};


class GatherTreeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Ids");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(gather_tree, GatherTreeInferShapeFunctor,
                            PD_INFER_META(phi::GatherTreeMeta));



class GeluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of gelu op.");
    AddOutput("Out", "(Tensor), output 0 of gelu op.");
    AddAttr<bool>("approximate", "(bool), attribute 0 for gelu op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of gelu op.
)DOC");
  }
};


class GeluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(gelu, GeluInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class GenerateProposalsV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Scores", "(Tensor), input 0 of generate_proposals_v2 op.");
    AddInput("BboxDeltas", "(Tensor), input 1 of generate_proposals_v2 op.");
    AddInput("ImShape", "(Tensor), input 2 of generate_proposals_v2 op.");
    AddInput("Anchors", "(Tensor), input 3 of generate_proposals_v2 op.");
    AddInput("Variances", "(Tensor), input 4 of generate_proposals_v2 op.");
    AddOutput("RpnRois", "(Tensor), output 0 of generate_proposals_v2 op.");
    AddOutput("RpnRoiProbs", "(Tensor), output 1 of generate_proposals_v2 op.");
    AddOutput("RpnRoisNum", "(Tensor), output 2 of generate_proposals_v2 op.")
        .AsDispensable();
    AddAttr<int>("pre_nms_topN", "(int), attribute 0 for generate_proposals_v2 op.")
    ;
    AddAttr<int>("post_nms_topN", "(int), attribute 1 for generate_proposals_v2 op.")
    ;
    AddAttr<float>("nms_thresh", "(float), attribute 2 for generate_proposals_v2 op.")
    ;
    AddAttr<float>("min_size", "(float), attribute 3 for generate_proposals_v2 op.")
    ;
    AddAttr<float>("eta", "(float), attribute 4 for generate_proposals_v2 op.")
    ;
    AddAttr<bool>("pixel_offset", "(bool), attribute 5 for generate_proposals_v2 op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of generate_proposals_v2 op.
)DOC");
  }
};


class GenerateProposalsV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Anchors");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(generate_proposals_v2, GenerateProposalsV2InferShapeFunctor,
                            PD_INFER_META(phi::GenerateProposalsV2InferMeta));



class GridSamplerOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of grid_sampler op.");
    AddInput("Grid", "(Tensor), input 1 of grid_sampler op.");
    AddOutput("Output", "(Tensor), output 0 of grid_sampler op.");
    AddAttr<std::string>("mode", "(std::string), attribute 0 for grid_sampler op.")
        .SetDefault("bilinear");
    AddAttr<std::string>("padding_mode", "(std::string), attribute 1 for grid_sampler op.")
        .SetDefault("zeros");
    AddAttr<bool>("align_corners", "(bool), attribute 2 for grid_sampler op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of grid_sampler op.
)DOC");
  }
};


class GridSamplerOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(grid_sampler, GridSamplerInferShapeFunctor,
                            PD_INFER_META(phi::GridSampleBaseInferMeta));



class GroupNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of group_norm op.");
    AddInput("Scale", "(Tensor), input 1 of group_norm op.")
        .AsDispensable();
    AddInput("Bias", "(Tensor), input 2 of group_norm op.")
        .AsDispensable();
    AddOutput("Y", "(Tensor), output 0 of group_norm op.");
    AddOutput("Mean", "(Tensor), output 1 of group_norm op.")
        .AsIntermediate();
    AddOutput("Variance", "(Tensor), output 2 of group_norm op.")
        .AsIntermediate();
    AddAttr<float>("epsilon", "(float), attribute 0 for group_norm op.")
        .SetDefault(1e-5);
    AddAttr<int>("groups", "(int), attribute 1 for group_norm op.")
        .SetDefault(-1);
    AddAttr<std::string>("data_layout", "(std::string), attribute 2 for group_norm op.")
        .SetDefault("NCHW");
    AddComment(R"DOC(
TODO: Documentation of group_norm op.
)DOC");
  }
};


class GroupNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(group_norm, GroupNormInferShapeFunctor,
                            PD_INFER_META(phi::GroupNormInferMeta));



class GumbelSoftmaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of gumbel_softmax op.");
    AddOutput("Out", "(Tensor), output 0 of gumbel_softmax op.");
    AddAttr<float>("temperature", "(float), attribute 0 for gumbel_softmax op.")
        .SetDefault(1.0);
    AddAttr<bool>("hard", "(bool), attribute 1 for gumbel_softmax op.")
        .SetDefault(false);
    AddAttr<int>("axis", "(int), attribute 2 for gumbel_softmax op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of gumbel_softmax op.
)DOC");
  }
};


class GumbelSoftmaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(gumbel_softmax, GumbelSoftmaxInferShapeFunctor,
                            PD_INFER_META(phi::GumbelSoftmaxInferMeta));



class HardShrinkOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of hard_shrink op.");
    AddOutput("Out", "(Tensor), output 0 of hard_shrink op.");
    AddAttr<float>("threshold", "(float), attribute 0 for hard_shrink op.")
        .SetDefault(0.5);
    AddComment(R"DOC(
TODO: Documentation of hard_shrink op.
)DOC");
  }
};


class HardShrinkOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(hard_shrink, HardShrinkInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class HardSigmoidOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of hard_sigmoid op.");
    AddOutput("Out", "(Tensor), output 0 of hard_sigmoid op.");
    AddAttr<float>("slope", "(float), attribute 0 for hard_sigmoid op.")
        .SetDefault(0.2);
    AddAttr<float>("offset", "(float), attribute 1 for hard_sigmoid op.")
        .SetDefault(0.5);
    AddComment(R"DOC(
TODO: Documentation of hard_sigmoid op.
)DOC");
  }
};


class HardSigmoidOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(hard_sigmoid, HardSigmoidInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class BreluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of brelu op.");
    AddOutput("Out", "(Tensor), output 0 of brelu op.");
    AddAttr<float>("t_min", "(float), attribute 0 for brelu op.")
        .SetDefault(0);
    AddAttr<float>("t_max", "(float), attribute 1 for brelu op.")
        .SetDefault(24);
    AddComment(R"DOC(
TODO: Documentation of brelu op.
)DOC");
  }
};


class BreluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(brelu, BreluInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(BreluInplaceInferer,
                           {"X", "Out"});



class ElementwiseHeavisideOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_heaviside op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_heaviside op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_heaviside op.");
    AddComment(R"DOC(
TODO: Documentation of elementwise_heaviside op.
)DOC");
  }
};


class ElementwiseHeavisideOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_heaviside, ElementwiseHeavisideInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseInferMeta));



class HistogramOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of histogram op.");
    AddOutput("Out", "(Tensor), output 0 of histogram op.");
    AddAttr<int64_t>("bins", "(int64_t), attribute 0 for histogram op.")
        .SetDefault(100);
    AddAttr<int>("min", "(int), attribute 1 for histogram op.")
        .SetDefault(0);
    AddAttr<int>("max", "(int), attribute 2 for histogram op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of histogram op.
)DOC");
  }
};


class HistogramOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(histogram, HistogramInferShapeFunctor,
                            PD_INFER_META(phi::HistogramInferMeta));



class HuberLossOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of huber_loss op.");
    AddInput("Y", "(Tensor), input 1 of huber_loss op.");
    AddOutput("Out", "(Tensor), output 0 of huber_loss op.");
    AddOutput("Residual", "(Tensor), output 1 of huber_loss op.")
        .AsIntermediate();
    AddAttr<float>("delta", "(float), attribute 0 for huber_loss op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of huber_loss op.
)DOC");
  }
};


class HuberLossOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(huber_loss, HuberLossInferShapeFunctor,
                            PD_INFER_META(phi::HuberLossInferMeta));



class I0OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of i0 op.");
    AddOutput("out", "(Tensor), output 0 of i0 op.");
    AddComment(R"DOC(
TODO: Documentation of i0 op.
)DOC");
  }
};


class I0Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i0, I0InferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(I0InplaceInferer,
                           {"x", "out"});



class I0eOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of i0e op.");
    AddOutput("out", "(Tensor), output 0 of i0e op.");
    AddComment(R"DOC(
TODO: Documentation of i0e op.
)DOC");
  }
};


class I0eOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i0e, I0eInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class I1OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of i1 op.");
    AddOutput("out", "(Tensor), output 0 of i1 op.");
    AddComment(R"DOC(
TODO: Documentation of i1 op.
)DOC");
  }
};


class I1Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i1, I1InferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class I1eOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of i1e op.");
    AddOutput("out", "(Tensor), output 0 of i1e op.");
    AddComment(R"DOC(
TODO: Documentation of i1e op.
)DOC");
  }
};


class I1eOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i1e, I1eInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class ImagOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of imag op.");
    AddOutput("Out", "(Tensor), output 0 of imag op.");
    AddComment(R"DOC(
TODO: Documentation of imag op.
)DOC");
  }
};


class ImagOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(imag, ImagInferShapeFunctor,
                            PD_INFER_META(phi::RealAndImagInferMeta));



class IndexAddOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of index_add op.");
    AddInput("Index", "(Tensor), input 1 of index_add op.");
    AddInput("AddValue", "(Tensor), input 2 of index_add op.");
    AddOutput("Out", "(Tensor), output 0 of index_add op.");
    AddAttr<int>("axis", "(int), attribute 0 for index_add op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of index_add op.
)DOC");
  }
};


class IndexAddOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_add, IndexAddInferShapeFunctor,
                            PD_INFER_META(phi::IndexAddInferMeta));
DECLARE_INPLACE_OP_INFERER(IndexAddInplaceInferer,
                           {"X", "Out"});



class IndexPutOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of index_put op.");
    AddInput("indices", "(Tensor[]), input 1 of index_put op.")
        .AsDuplicable();
    AddInput("value", "(Tensor), input 2 of index_put op.");
    AddOutput("out", "(Tensor), output 0 of index_put op.");
    AddAttr<bool>("accumulate", "(bool), attribute 0 for index_put op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of index_put op.
)DOC");
  }
};


class IndexPutOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_put, IndexPutInferShapeFunctor,
                            PD_INFER_META(phi::IndexPutInferMeta));
DECLARE_INPLACE_OP_INFERER(IndexPutInplaceInferer,
                           {"x", "out"});



class IndexSampleOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of index_sample op.");
    AddInput("Index", "(Tensor), input 1 of index_sample op.");
    AddOutput("Out", "(Tensor), output 0 of index_sample op.");
    AddComment(R"DOC(
TODO: Documentation of index_sample op.
)DOC");
  }
};


class IndexSampleOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_sample, IndexSampleInferShapeFunctor,
                            PD_INFER_META(phi::IndexSampleInferMeta));



class IndexSelectOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of index_select op.");
    AddInput("Index", "(Tensor), input 1 of index_select op.");
    AddOutput("Out", "(Tensor), output 0 of index_select op.");
    AddAttr<int>("dim", "(int), attribute 0 for index_select op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of index_select op.
)DOC");
  }
};


class IndexSelectOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_select, IndexSelectInferShapeFunctor,
                            PD_INFER_META(phi::IndexSelectInferMeta));



class IndexSelectStridedOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of index_select_strided op.");
    AddOutput("out", "(Tensor), output 0 of index_select_strided op.");
    AddAttr<int64_t>("index", "(int64_t), attribute 0 for index_select_strided op.")
    ;
    AddAttr<int>("axis", "(int), attribute 1 for index_select_strided op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of index_select_strided op.
)DOC");
  }
};


class IndexSelectStridedOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_select_strided, IndexSelectStridedInferShapeFunctor,
                            PD_INFER_META(phi::IndexSelectStridedInferMeta));



class InstanceNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of instance_norm op.");
    AddInput("Scale", "(Tensor), input 1 of instance_norm op.")
        .AsDispensable();
    AddInput("Bias", "(Tensor), input 2 of instance_norm op.")
        .AsDispensable();
    AddOutput("Y", "(Tensor), output 0 of instance_norm op.");
    AddOutput("SavedMean", "(Tensor), output 1 of instance_norm op.")
        .AsIntermediate()
        .AsExtra();
    AddOutput("SavedVariance", "(Tensor), output 2 of instance_norm op.")
        .AsIntermediate()
        .AsExtra();
    AddAttr<float>("epsilon", "(float), attribute 0 for instance_norm op.")
        .SetDefault(1e-5);
    AddComment(R"DOC(
TODO: Documentation of instance_norm op.
)DOC");
  }
};


class InstanceNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetInstanceNormExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(instance_norm, InstanceNormInferShapeFunctor,
                            PD_INFER_META(phi::InstanceNormInferMeta));



class InverseOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of inverse op.");
    AddOutput("Output", "(Tensor), output 0 of inverse op.");
    AddComment(R"DOC(
TODO: Documentation of inverse op.
)DOC");
  }
};


class InverseOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(inverse, InverseInferShapeFunctor,
                            PD_INFER_META(phi::InverseInferMeta));



class IsEmptyOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of is_empty op.");
    AddOutput("Out", "(Tensor), output 0 of is_empty op.");
    AddComment(R"DOC(
TODO: Documentation of is_empty op.
)DOC");
  }
};


class IsEmptyOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(is_empty, IsEmptyInferShapeFunctor,
                            PD_INFER_META(phi::IsEmptyInferMeta));



class IscloseOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of isclose op.");
    AddInput("Other", "(Tensor), input 1 of isclose op.");
    AddOutput("Out", "(Tensor), output 0 of isclose op.");
    AddInput("Rtol", "attribute 0 for isclose op from 0D Tensor.")
        .AsDispensable();
    AddAttr<std::string>("rtol", "(std::string), attribute 0 for isclose op.")
        .SetDefault("1e-5");
    AddInput("Atol", "attribute 1 for isclose op from 0D Tensor.")
        .AsDispensable();
    AddAttr<std::string>("atol", "(std::string), attribute 1 for isclose op.")
        .SetDefault("1e-8");
    AddAttr<bool>("equal_nan", "(bool), attribute 2 for isclose op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of isclose op.
)DOC");
  }
};


class IscloseOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(isclose, IscloseInferShapeFunctor,
                            PD_INFER_META(phi::ValueCompareInferMeta));



class IsfiniteV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of isfinite_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of isfinite_v2 op.");
    AddComment(R"DOC(
TODO: Documentation of isfinite_v2 op.
)DOC");
  }
};


class IsfiniteV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(isfinite_v2, IsfiniteV2InferShapeFunctor,
                            PD_INFER_META(phi::IsfiniteInferMeta));



class IsinfV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of isinf_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of isinf_v2 op.");
    AddComment(R"DOC(
TODO: Documentation of isinf_v2 op.
)DOC");
  }
};


class IsinfV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(isinf_v2, IsinfV2InferShapeFunctor,
                            PD_INFER_META(phi::IsfiniteInferMeta));



class IsnanV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of isnan_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of isnan_v2 op.");
    AddComment(R"DOC(
TODO: Documentation of isnan_v2 op.
)DOC");
  }
};


class IsnanV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(isnan_v2, IsnanV2InferShapeFunctor,
                            PD_INFER_META(phi::IsfiniteInferMeta));



class KldivLossOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of kldiv_loss op.");
    AddInput("Target", "(Tensor), input 1 of kldiv_loss op.");
    AddOutput("Loss", "(Tensor), output 0 of kldiv_loss op.");
    AddAttr<std::string>("reduction", "(std::string), attribute 0 for kldiv_loss op.")
        .SetDefault("mean");
    AddComment(R"DOC(
TODO: Documentation of kldiv_loss op.
)DOC");
  }
};


class KldivLossOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(kldiv_loss, KldivLossInferShapeFunctor,
                            PD_INFER_META(phi::KLDivInferMeta));



class KronOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of kron op.");
    AddInput("Y", "(Tensor), input 1 of kron op.");
    AddOutput("Out", "(Tensor), output 0 of kron op.");
    AddComment(R"DOC(
TODO: Documentation of kron op.
)DOC");
  }
};


class KronOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(kron, KronInferShapeFunctor,
                            PD_INFER_META(phi::KronInferMeta));



class KthvalueOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of kthvalue op.");
    AddOutput("Out", "(Tensor), output 0 of kthvalue op.");
    AddOutput("Indices", "(Tensor), output 1 of kthvalue op.");
    AddAttr<int>("k", "(int), attribute 0 for kthvalue op.")
        .SetDefault(1);
    AddAttr<int>("axis", "(int), attribute 1 for kthvalue op.")
        .SetDefault(-1);
    AddAttr<bool>("keepdim", "(bool), attribute 2 for kthvalue op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of kthvalue op.
)DOC");
  }
};


class KthvalueOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(kthvalue, KthvalueInferShapeFunctor,
                            PD_INFER_META(phi::KthvalueInferMeta));



class LabelSmoothOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of label_smooth op.");
    AddInput("PriorDist", "(Tensor), input 1 of label_smooth op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of label_smooth op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for label_smooth op.")
        .SetDefault(0.0f);
    AddComment(R"DOC(
TODO: Documentation of label_smooth op.
)DOC");
  }
};


class LabelSmoothOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(label_smooth, LabelSmoothInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class LambOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Param", "(Tensor), input 0 of lamb op.");
    AddInput("Grad", "(Tensor), input 1 of lamb op.");
    AddInput("LearningRate", "(Tensor), input 2 of lamb op.");
    AddInput("Moment1", "(Tensor), input 3 of lamb op.");
    AddInput("Moment2", "(Tensor), input 4 of lamb op.");
    AddInput("Beta1Pow", "(Tensor), input 5 of lamb op.");
    AddInput("Beta2Pow", "(Tensor), input 6 of lamb op.");
    AddInput("MasterParam", "(Tensor), input 7 of lamb op.")
        .AsDispensable();
    AddInput("SkipUpdate", "(Tensor), input 8 of lamb op.")
        .AsDispensable();
    AddOutput("ParamOut", "(Tensor), output 0 of lamb op.");
    AddOutput("Moment1Out", "(Tensor), output 1 of lamb op.");
    AddOutput("Moment2Out", "(Tensor), output 2 of lamb op.");
    AddOutput("Beta1PowOut", "(Tensor), output 3 of lamb op.")
        .AsDispensable();
    AddOutput("Beta2PowOut", "(Tensor), output 4 of lamb op.")
        .AsDispensable();
    AddOutput("MasterParamOut", "(Tensor), output 5 of lamb op.")
        .AsDispensable();
    AddAttr<float>("weight_decay", "(float), attribute 0 for lamb op.")
    ;
    AddAttr<float>("beta1", "(float), attribute 1 for lamb op.")
        .SetDefault(0.9);
    AddAttr<float>("beta2", "(float), attribute 2 for lamb op.")
        .SetDefault(0.999);
    AddAttr<float>("epsilon", "(float), attribute 3 for lamb op.")
        .SetDefault(1.0e-6f);
    AddAttr<bool>("always_adapt", "(bool), attribute 4 for lamb op.")
        .SetDefault(false);
    AddAttr<bool>("multi_precision", "(bool), attribute 5 for lamb op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of lamb op.
)DOC");
  }
};


class LambOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Param");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(lamb, LambInferShapeFunctor,
                            PD_INFER_META(phi::LambInferMeta));



class LayerNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of layer_norm op.");
    AddInput("Scale", "(Tensor), input 1 of layer_norm op.")
        .AsDispensable();
    AddInput("Bias", "(Tensor), input 2 of layer_norm op.")
        .AsDispensable();
    AddOutput("Y", "(Tensor), output 0 of layer_norm op.");
    AddOutput("Mean", "(Tensor), output 1 of layer_norm op.")
        .AsIntermediate();
    AddOutput("Variance", "(Tensor), output 2 of layer_norm op.")
        .AsIntermediate();
    AddAttr<float>("epsilon", "(float), attribute 0 for layer_norm op.")
        .SetDefault(1e-5);
    AddAttr<int>("begin_norm_axis", "(int), attribute 1 for layer_norm op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of layer_norm op.
)DOC");
  }
};


class LayerNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetLayerNormExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(layer_norm, LayerNormInferShapeFunctor,
                            PD_INFER_META(phi::LayerNormInferMeta));




template <typename T>
class DistGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("dist_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class DistGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(dist_grad, DistGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class DotGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("dot_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class DotGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(dot_grad, DotGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class EigGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("eig_grad");

    grad_op->SetInput("Eigenvalues", this->Output("Eigenvalues"));
    grad_op->SetInput("Eigenvectors", this->Output("Eigenvectors"));
    grad_op->SetInput(GradVarName("Eigenvalues"), this->OutputGrad("Eigenvalues"));
    grad_op->SetInput(GradVarName("Eigenvectors"), this->OutputGrad("Eigenvectors"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class EigGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Eigenvectors");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(eig_grad, EigGradInferShapeFunctor,
                            PD_INFER_META(phi::EigGradInferMeta));



template <typename T>
class EighGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("eigh_grad");

    grad_op->SetInput("Eigenvalues", this->Output("Eigenvalues"));
    grad_op->SetInput("Eigenvectors", this->Output("Eigenvectors"));
    grad_op->SetInput(GradVarName("Eigenvalues"), this->OutputGrad("Eigenvalues"));
    grad_op->SetInput(GradVarName("Eigenvectors"), this->OutputGrad("Eigenvectors"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class EighGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Eigenvectors");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(eigh_grad, EighGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class EigvalshGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("eigvalsh_grad");

    grad_op->SetInput("Eigenvectors", this->Output("Eigenvectors"));
    grad_op->SetInput(GradVarName("Eigenvalues"), this->OutputGrad("Eigenvalues"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class EigvalshGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Eigenvectors");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(eigvalsh_grad, EigvalshGradInferShapeFunctor,
                            PD_INFER_META(phi::EigvalshGradInferMeta));



template <typename T>
class EluGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elu_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class EluGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(elu_grad, EluGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(EluGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class EluGradGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elu_grad_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("grad_out", this->Input(GradVarName("Out")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class EluGradGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(elu_grad_grad, EluGradGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));
DECLARE_INPLACE_OP_INFERER(EluGradGradInplaceInferer,
                           {GradVarName("grad_x"), GradVarName("grad_out")});



template <typename T>
class ErfGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("erf_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ErfGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(erf_grad, ErfGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


class ErfCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing erf_grad composite func";
    prim::erf_grad<prim::DescTensor>(x, out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class ErfinvGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("erfinv_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ErfinvGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(erfinv_grad, ErfinvGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class ExpGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("exp_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ExpGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(exp_grad, ExpGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ExpGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


class ExpCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto out = this->GetSingleForwardOutput("Out");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing exp_grad composite func";
    prim::exp_grad<prim::DescTensor>(out, out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class ExpandV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("expand_v2_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("Shape")) {
      grad_op->SetInput("Shape", this->Input("Shape"));
    }
    if (this->HasInput("expand_shapes_tensor")) {
      grad_op->SetInput("expand_shapes_tensor", this->Input("expand_shapes_tensor"));
    }
  }
};


class ExpandV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(expand_v2_grad, ExpandV2GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(ExpandV2GradNoNeedBufferVarInferer,
                                    "X");

class ExpandV2CompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");

    auto tensor_shape = this->GetOptionalSingleForwardInput("Shape");
    if (tensor_shape) {
      PADDLE_THROW(platform::errors::Unimplemented(
          "We don't support dynamic tensor attribute Shape for expand_v2_grad composite"
          "for now. "));
    }    auto tensors_shape = this->GetOptionalMultiForwardInput("expand_shapes_tensor");
    if (tensors_shape) {
      PADDLE_THROW(platform::errors::Unimplemented(
          "We don't support dynamic tensors attribute Shape for expand_v2_grad composite "
          "for now. "));
    }
    //get attr
    const std::vector<int> shape = this->Attr<std::vector<int>>("shape");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing expand_grad composite func";
    prim::expand_grad<prim::DescTensor>(x, out_grad, shape, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};
template <typename T>
class ExpandV2DoubleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("expand_v2");

    grad_op->SetInput("X", this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput("Out", this->InputGrad(GradVarName("Out")));

    if (this->HasInput("Shape")) {
      grad_op->SetInput("Shape", this->Input("Shape"));
    }
    if (this->HasInput("expand_shapes_tensor")) {
      grad_op->SetInput("expand_shapes_tensor", this->Input("expand_shapes_tensor"));
    }

    grad_op->SetAttr("shape", this->GetAttr("shape"));
  }
};


template <typename T>
class ExpandAsV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("expand_as_v2_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ExpandAsV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(expand_as_v2_grad, ExpandAsV2GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(ExpandAsV2GradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class Expm1GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("expm1_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class Expm1GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(expm1_grad, Expm1GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Expm1GradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class FftC2cGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fft_c2c_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FftC2cGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fft_c2c_grad, FftC2cGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class FftC2rGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fft_c2r_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FftC2rGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fft_c2r_grad, FftC2rGradInferShapeFunctor,
                            PD_INFER_META(phi::FFTC2RGradInferMeta));



template <typename T>
class FftR2cGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fft_r2c_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FftR2cGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fft_r2c_grad, FftR2cGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(FftR2cGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class FillAnyGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fill_any_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FillAnyGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fill_any_grad, FillAnyGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(FillAnyGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class FillDiagonalGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fill_diagonal_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FillDiagonalGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fill_diagonal_grad, FillDiagonalGradInferShapeFunctor,
                            PD_INFER_META(phi::FillDiagonalGradInferMeta));



template <typename T>
class FillDiagonalTensorGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fill_diagonal_tensor_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FillDiagonalTensorGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(fill_diagonal_tensor_grad, FillDiagonalTensorGradInferShapeFunctor,
                            PD_INFER_META(phi::FillDiagonalTensorGradInferMeta));
DECLARE_INPLACE_OP_INFERER(FillDiagonalTensorGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class FlashAttnGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("flash_attn_grad");

    grad_op->SetInput("q", this->Input("q"));
    grad_op->SetInput("k", this->Input("k"));
    grad_op->SetInput("v", this->Input("v"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput("softmax_lse", this->Output("softmax_lse"));
    grad_op->SetInput("seed_offset", this->Output("seed_offset"));
    grad_op->SetInput("attn_mask", this->Input("attn_mask"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("q"), this->InputGrad("q"));
    grad_op->SetOutput(GradVarName("k"), this->InputGrad("k"));
    grad_op->SetOutput(GradVarName("v"), this->InputGrad("v"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FlashAttnGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(flash_attn_grad, FlashAttnGradInferShapeFunctor,
                            PD_INFER_META(phi::FlashAttnGradInferMeta));



template <typename T>
class FlashAttnUnpaddedGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("flash_attn_unpadded_grad");

    grad_op->SetInput("q", this->Input("q"));
    grad_op->SetInput("k", this->Input("k"));
    grad_op->SetInput("v", this->Input("v"));
    grad_op->SetInput("cu_seqlens_q", this->Input("cu_seqlens_q"));
    grad_op->SetInput("cu_seqlens_k", this->Input("cu_seqlens_k"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput("softmax_lse", this->Output("softmax_lse"));
    grad_op->SetInput("seed_offset", this->Output("seed_offset"));
    grad_op->SetInput("attn_mask", this->Input("attn_mask"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("q"), this->InputGrad("q"));
    grad_op->SetOutput(GradVarName("k"), this->InputGrad("k"));
    grad_op->SetOutput(GradVarName("v"), this->InputGrad("v"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FlashAttnUnpaddedGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(flash_attn_unpadded_grad, FlashAttnUnpaddedGradInferShapeFunctor,
                            PD_INFER_META(phi::FlashAttnGradInferMeta));



template <typename T>
class FlattenContiguousRangeGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("flatten_contiguous_range_grad");

    grad_op->SetInput("XShape", this->Output("XShape"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FlattenContiguousRangeGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(flatten_contiguous_range_grad, FlattenContiguousRangeGradInferShapeFunctor,
                            PD_INFER_META(phi::KernelWithXShapeInferMeta));
DECLARE_INPLACE_OP_INFERER(FlattenContiguousRangeGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


template <typename T>
class FlipGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("flip");

    grad_op->SetInput("X", this->OutputGrad("Out"));

    grad_op->SetOutput("Out", this->InputGrad("X"));


    grad_op->SetAttr("axis", this->GetAttr("axis"));
  }
};


template <typename T>
class FloorGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("floor_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FloorGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(floor_grad, FloorGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(FloorGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


class FloorCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing floor_grad composite func";
    prim::floor_grad<prim::DescTensor>(out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class ElementwiseFmaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elementwise_fmax_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ElementwiseFmaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_fmax_grad, ElementwiseFmaxGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class ElementwiseFminGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elementwise_fmin_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ElementwiseFminGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_fmin_grad, ElementwiseFminGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class FoldGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fold_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FoldGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Y"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fold_grad, FoldGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(FoldGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class FrameGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("frame_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FrameGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(frame_grad, FrameGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class GatherGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("gather_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Index", this->Input("Index"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("Axis")) {
      grad_op->SetInput("Axis", this->Input("Axis"));
    }
  }
};


class GatherGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(gather_grad, GatherGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralUnaryGradInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(GatherGradNoNeedBufferVarInferer,
                                    "X");

class GatherCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto index = this->GetSingleForwardInput("Index");
    auto out_grad = this->GetSingleOutputGrad("Out");

    auto tensor_axis = this->GetOptionalSingleForwardInput("Axis");
    if (tensor_axis) {
      PADDLE_THROW(platform::errors::Unimplemented(
          "We don't support dynamic tensor attribute Axis for gather_grad composite"
          "for now. "));
    }
    //get attr
    const int axis = this->Attr<int>("axis");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing gather_grad composite func";
    prim::gather_grad<prim::DescTensor>(x, index, out_grad, axis, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class GatherNdGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("gather_nd_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Index", this->Input("Index"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class GatherNdGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(gather_nd_grad, GatherNdGradInferShapeFunctor,
                            PD_INFER_META(phi::GatherNdGradInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(GatherNdGradNoNeedBufferVarInferer,
                                    "X");

class GatherNdCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto index = this->GetSingleForwardInput("Index");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing gather_nd_grad composite func";
    prim::gather_nd_grad<prim::DescTensor>(x, index, out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class GeluGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("gelu_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class GeluGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(gelu_grad, GeluGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


class GeluCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr
    const bool approximate = this->Attr<bool>("approximate");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing gelu_grad composite func";
    prim::gelu_grad<prim::DescTensor>(x, out_grad, approximate, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class GridSamplerGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("grid_sampler_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Grid", this->Input("Grid"));
    grad_op->SetInput(GradVarName("Output"), this->OutputGrad("Output"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Grid"), this->InputGrad("Grid"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class GridSamplerGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(grid_sampler_grad, GridSamplerGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class GroupNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("group_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Scale", this->Input("Scale"));
    grad_op->SetInput("Bias", this->Input("Bias"));
    grad_op->SetInput("Y", this->Output("Y"));
    grad_op->SetInput("Mean", this->Output("Mean"));
    grad_op->SetInput("Variance", this->Output("Variance"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Scale"), this->InputGrad("Scale"));
    grad_op->SetOutput(GradVarName("Bias"), this->InputGrad("Bias"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class GroupNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Y"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(group_norm_grad, GroupNormGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralTernaryGradInferMeta));
DECLARE_INPLACE_OP_INFERER(GroupNormGradInplaceInferer,
                           {GradVarName("Y"), GradVarName("X")});


class GroupNormCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto scale = this->GetOptionalSingleForwardInput("Scale");
    auto bias = this->GetOptionalSingleForwardInput("Bias");
    auto y = this->GetSingleForwardOutput("Y");
    auto mean = this->GetSingleForwardOutput("Mean");
    auto variance = this->GetSingleForwardOutput("Variance");
    auto y_grad = this->GetSingleOutputGrad("Y");


    //get attr
    const float epsilon = this->Attr<float>("epsilon");
    const int groups = this->Attr<int>("groups");
    const std::string data_layout = this->Attr<std::string>("data_layout");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto scale_grad_t = this->GetSingleInputGrad("Scale");
    auto bias_grad_t = this->GetSingleInputGrad("Bias");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto scale_grad = this->GetOutputPtr(&scale_grad_t);
    auto bias_grad = this->GetOutputPtr(&bias_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto scale_grad_name = this->GetOutputName(scale_grad_t);
    auto bias_grad_name = this->GetOutputName(bias_grad_t);

    //call composite backward func
    VLOG(6) << "Runing group_norm_grad composite func";
    prim::group_norm_grad<prim::DescTensor>(x, scale, bias, y, mean, variance, y_grad, epsilon, groups, data_layout, x_grad, scale_grad, bias_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(scale_grad_t, scale_grad_name);
    this->RecoverOutputName(bias_grad_t, bias_grad_name);

  }
};

template <typename T>
class GumbelSoftmaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("gumbel_softmax_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class GumbelSoftmaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(gumbel_softmax_grad, GumbelSoftmaxGradInferShapeFunctor,
                            PD_INFER_META(phi::GumbelSoftmaxGradInferMeta));



template <typename T>
class HardShrinkGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("hard_shrink_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class HardShrinkGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(hard_shrink_grad, HardShrinkGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(HardShrinkGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class HardSigmoidGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("hard_sigmoid_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class HardSigmoidGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(hard_sigmoid_grad, HardSigmoidGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(HardSigmoidGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class BreluGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("brelu_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class BreluGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(brelu_grad, BreluGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(BreluGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class ElementwiseHeavisideGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elementwise_heaviside_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ElementwiseHeavisideGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_heaviside_grad, ElementwiseHeavisideGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class HuberLossGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("huber_loss_grad");

    grad_op->SetInput("Residual", this->Output("Residual"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class HuberLossGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(huber_loss_grad, HuberLossGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class I0GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("i0_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class I0GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i0_grad, I0GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class I0eGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("i0e_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class I0eGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i0e_grad, I0eGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class I1GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("i1_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class I1GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i1_grad, I1GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class I1eGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("i1e_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class I1eGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(i1e_grad, I1eGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class ImagGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("imag_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ImagGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    data_type = framework::ToComplexType(data_type);
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(imag_grad, ImagGradInferShapeFunctor,
                            PD_INFER_META(phi::RealAndImagGradInferMeta));



template <typename T>
class IndexAddGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("index_add_grad");

    grad_op->SetInput("Index", this->Input("Index"));
    grad_op->SetInput("AddValue", this->Input("AddValue"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("AddValue"), this->InputGrad("AddValue"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class IndexAddGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_add_grad, IndexAddGradInferShapeFunctor,
                            PD_INFER_META(phi::IndexAddGradInferMeta));
DECLARE_INPLACE_OP_INFERER(IndexAddGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class IndexPutGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("index_put_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput("indices", this->Input("indices"));
    grad_op->SetInput("value", this->Input("value"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));
    grad_op->SetOutput(GradVarName("value"), this->InputGrad("value"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class IndexPutGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_put_grad, IndexPutGradInferShapeFunctor,
                            PD_INFER_META(phi::IndexPutGradInferMeta));



template <typename T>
class IndexSampleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("index_sample_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Index", this->Input("Index"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class IndexSampleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_sample_grad, IndexSampleGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(IndexSampleGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class IndexSelectGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("index_select_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Index", this->Input("Index"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class IndexSelectGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_select_grad, IndexSelectGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(IndexSelectGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class IndexSelectStridedGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("index_select_strided_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class IndexSelectStridedGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(index_select_strided_grad, IndexSelectStridedGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(IndexSelectStridedGradNoNeedBufferVarInferer,
                                    "x");


template <typename T>
class InstanceNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("instance_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Scale", this->Input("Scale"));
    grad_op->SetInput("SavedMean", this->Output("SavedMean"));
    grad_op->SetInput("SavedVariance", this->Output("SavedVariance"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Scale"), this->InputGrad("Scale"));
    grad_op->SetOutput(GradVarName("Bias"), this->InputGrad("Bias"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class InstanceNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(instance_norm_grad, InstanceNormGradInferShapeFunctor,
                            PD_INFER_META(phi::InstanceNormGradInferMeta));


class InstanceNormCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto scale = this->GetOptionalSingleForwardInput("Scale");
    auto saved_mean = this->GetSingleForwardOutput("SavedMean");
    auto saved_variance = this->GetSingleForwardOutput("SavedVariance");
    auto y_grad = this->GetSingleOutputGrad("Y");


    //get attr
    const float epsilon = this->Attr<float>("epsilon");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto scale_grad_t = this->GetSingleInputGrad("Scale");
    auto bias_grad_t = this->GetSingleInputGrad("Bias");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto scale_grad = this->GetOutputPtr(&scale_grad_t);
    auto bias_grad = this->GetOutputPtr(&bias_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto scale_grad_name = this->GetOutputName(scale_grad_t);
    auto bias_grad_name = this->GetOutputName(bias_grad_t);

    //call composite backward func
    VLOG(6) << "Runing instance_norm_grad composite func";
    prim::instance_norm_grad<prim::DescTensor>(x, scale, saved_mean, saved_variance, y_grad, epsilon, x_grad, scale_grad, bias_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(scale_grad_t, scale_grad_name);
    this->RecoverOutputName(bias_grad_t, bias_grad_name);

  }
};

template <typename T>
class InstanceNormDoubleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("instance_norm_double_grad");

    grad_op->SetInput("x", this->Input("X"));
    grad_op->SetInput("fwd_scale", this->Input("Scale"));
    grad_op->SetInput("saved_mean", this->Input("SavedMean"));
    grad_op->SetInput("saved_variance", this->Input("SavedVariance"));
    grad_op->SetInput("grad_y", this->Input(GradVarName("Y")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));
    grad_op->SetInput(GradVarName("grad_scale"), this->OutputGrad(GradVarName("Scale")));
    grad_op->SetInput(GradVarName("grad_bias"), this->OutputGrad(GradVarName("Bias")));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("fwd_scale"), this->InputGrad("Scale"));
    grad_op->SetOutput(GradVarName("grad_y"), this->InputGrad(GradVarName("Y")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class InstanceNormDoubleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(instance_norm_double_grad, InstanceNormDoubleGradInferShapeFunctor,
                            PD_INFER_META(phi::InstanceNormDoubleGradInferMeta));



template <typename T>
class InverseGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("inverse_grad");

    grad_op->SetInput("Output", this->Output("Output"));
    grad_op->SetInput(GradVarName("Output"), this->OutputGrad("Output"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class InverseGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(inverse_grad, InverseGradInferShapeFunctor,
                            PD_INFER_META(phi::InverseGradInferMeta));



template <typename T>
class KldivLossGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("kldiv_loss_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Target", this->Input("Target"));
    grad_op->SetInput(GradVarName("Loss"), this->OutputGrad("Loss"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class KldivLossGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(kldiv_loss_grad, KldivLossGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(KldivLossGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class KronGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("kron_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class KronGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(kron_grad, KronGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class KthvalueGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("kthvalue_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Indices", this->Output("Indices"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class KthvalueGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(kthvalue_grad, KthvalueGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LabelSmoothGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("label_smooth_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LabelSmoothGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(label_smooth_grad, LabelSmoothGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LayerNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("layer_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Scale", this->Input("Scale"));
    grad_op->SetInput("Bias", this->Input("Bias"));
    grad_op->SetInput("Mean", this->Output("Mean"));
    grad_op->SetInput("Variance", this->Output("Variance"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Scale"), this->InputGrad("Scale"));
    grad_op->SetOutput(GradVarName("Bias"), this->InputGrad("Bias"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LayerNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(layer_norm_grad, LayerNormGradInferShapeFunctor,
                            PD_INFER_META(phi::LayerNormGradInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(LayerNormGradNoNeedBufferVarInferer,
                                    "Bias");

class LayerNormCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto scale = this->GetOptionalSingleForwardInput("Scale");
    auto bias = this->GetOptionalSingleForwardInput("Bias");
    auto mean = this->GetSingleForwardOutput("Mean");
    auto variance = this->GetSingleForwardOutput("Variance");
    auto out_grad = this->GetSingleOutputGrad("Y");


    //get attr
    const float epsilon = this->Attr<float>("epsilon");
    const int begin_norm_axis = this->Attr<int>("begin_norm_axis");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto scale_grad_t = this->GetSingleInputGrad("Scale");
    auto bias_grad_t = this->GetSingleInputGrad("Bias");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto scale_grad = this->GetOutputPtr(&scale_grad_t);
    auto bias_grad = this->GetOutputPtr(&bias_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto scale_grad_name = this->GetOutputName(scale_grad_t);
    auto bias_grad_name = this->GetOutputName(bias_grad_t);

    //call composite backward func
    VLOG(6) << "Runing layer_norm_grad composite func";
    prim::layer_norm_grad<prim::DescTensor>(x, scale, bias, mean, variance, out_grad, epsilon, begin_norm_axis, x_grad, scale_grad, bias_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(scale_grad_t, scale_grad_name);
    this->RecoverOutputName(bias_grad_t, bias_grad_name);

  }
};
}  // namespace operators
}  // namespace paddle

namespace ops = paddle::operators;
REGISTER_OPERATOR(dirichlet, ops::DirichletOp,
                  ops::DirichletOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::DirichletInferShapeFunctor);


REGISTER_OPERATOR(dist, ops::DistOp,
                  ops::DistOpMaker,
                  ops::DistGradOpMaker<paddle::framework::OpDesc>,
                  ops::DistGradOpMaker<paddle::imperative::OpBase>,
                  ops::DistInferShapeFunctor);


REGISTER_OPERATOR(dot, ops::DotOp,
                  ops::DotOpMaker,
                  ops::DotGradOpMaker<paddle::framework::OpDesc>,
                  ops::DotGradOpMaker<paddle::imperative::OpBase>,
                  ops::DotInferShapeFunctor);


REGISTER_OPERATOR(edit_distance, ops::EditDistanceOp,
                  ops::EditDistanceOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EditDistanceInferShapeFunctor);


REGISTER_OPERATOR(eig, ops::EigOp,
                  ops::EigOpMaker,
                  ops::EigGradOpMaker<paddle::framework::OpDesc>,
                  ops::EigGradOpMaker<paddle::imperative::OpBase>,
                  ops::EigInferShapeFunctor);


REGISTER_OPERATOR(eigh, ops::EighOp,
                  ops::EighOpMaker,
                  ops::EighGradOpMaker<paddle::framework::OpDesc>,
                  ops::EighGradOpMaker<paddle::imperative::OpBase>,
                  ops::EighInferShapeFunctor);


REGISTER_OPERATOR(eigvals, ops::EigvalsOp,
                  ops::EigvalsOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EigvalsInferShapeFunctor);


REGISTER_OPERATOR(eigvalsh, ops::EigvalshOp,
                  ops::EigvalshOpMaker,
                  ops::EigvalshGradOpMaker<paddle::framework::OpDesc>,
                  ops::EigvalshGradOpMaker<paddle::imperative::OpBase>,
                  ops::EigvalshInferShapeFunctor);


REGISTER_OPERATOR(elu, ops::EluOp,
                  ops::EluOpMaker,
                  ops::EluGradOpMaker<paddle::framework::OpDesc>,
                  ops::EluGradOpMaker<paddle::imperative::OpBase>,
                  ops::EluInplaceInferer,
                  ops::EluInferShapeFunctor);


REGISTER_OPERATOR(equal_all, ops::EqualAllOp,
                  ops::EqualAllOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EqualAllInferShapeFunctor);


REGISTER_OPERATOR(erf, ops::ErfOp,
                  ops::ErfOpMaker,
                  ops::ErfGradOpMaker<paddle::framework::OpDesc>,
                  ops::ErfGradOpMaker<paddle::imperative::OpBase>,
                  ops::ErfInplaceInferer,
                  ops::ErfCompositeGradOpMaker,
                  ops::ErfInferShapeFunctor);


REGISTER_OPERATOR(erfinv, ops::ErfinvOp,
                  ops::ErfinvOpMaker,
                  ops::ErfinvGradOpMaker<paddle::framework::OpDesc>,
                  ops::ErfinvGradOpMaker<paddle::imperative::OpBase>,
                  ops::ErfinvInplaceInferer,
                  ops::ErfinvInferShapeFunctor);


REGISTER_OPERATOR(exp, ops::ExpOp,
                  ops::ExpOpMaker,
                  ops::ExpGradOpMaker<paddle::framework::OpDesc>,
                  ops::ExpGradOpMaker<paddle::imperative::OpBase>,
                  ops::ExpInplaceInferer,
                  ops::ExpCompositeGradOpMaker,
                  ops::ExpInferShapeFunctor);


REGISTER_OPERATOR(expand_v2, ops::ExpandV2Op,
                  ops::ExpandV2OpMaker,
                  ops::ExpandV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::ExpandV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::ExpandV2CompositeGradOpMaker,
                  ops::ExpandV2InferShapeFunctor);


REGISTER_OPERATOR(expand_as_v2, ops::ExpandAsV2Op,
                  ops::ExpandAsV2OpMaker,
                  ops::ExpandAsV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::ExpandAsV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::ExpandAsV2InferShapeFunctor);


REGISTER_OPERATOR(expm1, ops::Expm1Op,
                  ops::Expm1OpMaker,
                  ops::Expm1GradOpMaker<paddle::framework::OpDesc>,
                  ops::Expm1GradOpMaker<paddle::imperative::OpBase>,
                  ops::Expm1InplaceInferer,
                  ops::Expm1InferShapeFunctor);


REGISTER_OPERATOR(fft_c2c, ops::FftC2cOp,
                  ops::FftC2cOpMaker,
                  ops::FftC2cGradOpMaker<paddle::framework::OpDesc>,
                  ops::FftC2cGradOpMaker<paddle::imperative::OpBase>,
                  ops::FftC2cInferShapeFunctor);


REGISTER_OPERATOR(fft_c2r, ops::FftC2rOp,
                  ops::FftC2rOpMaker,
                  ops::FftC2rGradOpMaker<paddle::framework::OpDesc>,
                  ops::FftC2rGradOpMaker<paddle::imperative::OpBase>,
                  ops::FftC2rInferShapeFunctor);


REGISTER_OPERATOR(fft_r2c, ops::FftR2cOp,
                  ops::FftR2cOpMaker,
                  ops::FftR2cGradOpMaker<paddle::framework::OpDesc>,
                  ops::FftR2cGradOpMaker<paddle::imperative::OpBase>,
                  ops::FftR2cInferShapeFunctor);


REGISTER_OPERATOR(fill_any, ops::FillAnyOp,
                  ops::FillAnyOpMaker,
                  ops::FillAnyGradOpMaker<paddle::framework::OpDesc>,
                  ops::FillAnyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FillAnyInplaceInferer,
                  ops::FillAnyInferShapeFunctor);


REGISTER_OPERATOR(fill_diagonal, ops::FillDiagonalOp,
                  ops::FillDiagonalOpMaker,
                  ops::FillDiagonalGradOpMaker<paddle::framework::OpDesc>,
                  ops::FillDiagonalGradOpMaker<paddle::imperative::OpBase>,
                  ops::FillDiagonalInplaceInferer,
                  ops::FillDiagonalInferShapeFunctor);


REGISTER_OPERATOR(fill_diagonal_tensor, ops::FillDiagonalTensorOp,
                  ops::FillDiagonalTensorOpMaker,
                  ops::FillDiagonalTensorGradOpMaker<paddle::framework::OpDesc>,
                  ops::FillDiagonalTensorGradOpMaker<paddle::imperative::OpBase>,
                  ops::FillDiagonalTensorInplaceInferer,
                  ops::FillDiagonalTensorInferShapeFunctor);


REGISTER_OPERATOR(flash_attn, ops::FlashAttnOp,
                  ops::FlashAttnOpMaker,
                  ops::FlashAttnGradOpMaker<paddle::framework::OpDesc>,
                  ops::FlashAttnGradOpMaker<paddle::imperative::OpBase>,
                  ops::FlashAttnInferShapeFunctor);


REGISTER_OPERATOR(flash_attn_unpadded, ops::FlashAttnUnpaddedOp,
                  ops::FlashAttnUnpaddedOpMaker,
                  ops::FlashAttnUnpaddedGradOpMaker<paddle::framework::OpDesc>,
                  ops::FlashAttnUnpaddedGradOpMaker<paddle::imperative::OpBase>,
                  ops::FlashAttnUnpaddedInferShapeFunctor);


REGISTER_OPERATOR(flatten_contiguous_range, ops::FlattenContiguousRangeOp,
                  ops::FlattenContiguousRangeOpMaker,
                  ops::FlattenContiguousRangeGradOpMaker<paddle::framework::OpDesc>,
                  ops::FlattenContiguousRangeGradOpMaker<paddle::imperative::OpBase>,
                  ops::FlattenContiguousRangeInplaceInferer,
                  ops::FlattenContiguousRangeInferShapeFunctor);


REGISTER_OPERATOR(flip, ops::FlipOp,
                  ops::FlipOpMaker,
                  ops::FlipGradOpMaker<paddle::framework::OpDesc>,
                  ops::FlipGradOpMaker<paddle::imperative::OpBase>,
                  ops::FlipInferShapeFunctor);

REGISTER_OP_VERSION(flip)
  .AddCheckpoint(
    R"ROC(Upgrade flip, add new attr [axis] and delete attr [dims])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("axis", "The added attr 'axis' doesn't set default value", paddle::none)
        .DeleteAttr("dims", "The attr 'dims' is deleted."))
;

REGISTER_OPERATOR(floor, ops::FloorOp,
                  ops::FloorOpMaker,
                  ops::FloorGradOpMaker<paddle::framework::OpDesc>,
                  ops::FloorGradOpMaker<paddle::imperative::OpBase>,
                  ops::FloorInplaceInferer,
                  ops::FloorCompositeGradOpMaker,
                  ops::FloorInferShapeFunctor);


REGISTER_OPERATOR(elementwise_fmax, ops::ElementwiseFmaxOp,
                  ops::ElementwiseFmaxOpMaker,
                  ops::ElementwiseFmaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::ElementwiseFmaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseFmaxInferShapeFunctor);


REGISTER_OPERATOR(elementwise_fmin, ops::ElementwiseFminOp,
                  ops::ElementwiseFminOpMaker,
                  ops::ElementwiseFminGradOpMaker<paddle::framework::OpDesc>,
                  ops::ElementwiseFminGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseFminInferShapeFunctor);


REGISTER_OPERATOR(fold, ops::FoldOp,
                  ops::FoldOpMaker,
                  ops::FoldGradOpMaker<paddle::framework::OpDesc>,
                  ops::FoldGradOpMaker<paddle::imperative::OpBase>,
                  ops::FoldInferShapeFunctor);


REGISTER_OPERATOR(frame, ops::FrameOp,
                  ops::FrameOpMaker,
                  ops::FrameGradOpMaker<paddle::framework::OpDesc>,
                  ops::FrameGradOpMaker<paddle::imperative::OpBase>,
                  ops::FrameInferShapeFunctor);


REGISTER_OPERATOR(full_int_array, ops::FullIntArrayOp,
                  ops::FullIntArrayOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FullIntArrayInferShapeFunctor);


REGISTER_OPERATOR(fused_bias_residual_layernorm, ops::FusedBiasResidualLayernormOp,
                  ops::FusedBiasResidualLayernormOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedBiasResidualLayernormInferShapeFunctor);


REGISTER_OPERATOR(gather, ops::GatherOp,
                  ops::GatherOpMaker,
                  ops::GatherGradOpMaker<paddle::framework::OpDesc>,
                  ops::GatherGradOpMaker<paddle::imperative::OpBase>,
                  ops::GatherCompositeGradOpMaker,
                  ops::GatherInferShapeFunctor);

REGISTER_OP_VERSION(gather)
  .AddCheckpoint(
    R"ROC(Upgrade gather, add a new input [Axis])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewInput("Axis", "Specify the axis of gather operation."))
;

REGISTER_OPERATOR(gather_nd, ops::GatherNdOp,
                  ops::GatherNdOpMaker,
                  ops::GatherNdGradOpMaker<paddle::framework::OpDesc>,
                  ops::GatherNdGradOpMaker<paddle::imperative::OpBase>,
                  ops::GatherNdCompositeGradOpMaker,
                  ops::GatherNdInferShapeFunctor);


REGISTER_OPERATOR(gather_tree, ops::GatherTreeOp,
                  ops::GatherTreeOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GatherTreeInferShapeFunctor);


REGISTER_OPERATOR(gelu, ops::GeluOp,
                  ops::GeluOpMaker,
                  ops::GeluGradOpMaker<paddle::framework::OpDesc>,
                  ops::GeluGradOpMaker<paddle::imperative::OpBase>,
                  ops::GeluCompositeGradOpMaker,
                  ops::GeluInferShapeFunctor);


REGISTER_OPERATOR(generate_proposals_v2, ops::GenerateProposalsV2Op,
                  ops::GenerateProposalsV2OpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GenerateProposalsV2InferShapeFunctor);

REGISTER_OP_VERSION(generate_proposals_v2)
  .AddCheckpoint(
    R"ROC(Registe generate_proposals_v2 for adding the attribute of pixel_offset)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("pixel_offset", "If true, im_shape pixel offset is 1.", true))
;

REGISTER_OPERATOR(grid_sampler, ops::GridSamplerOp,
                  ops::GridSamplerOpMaker,
                  ops::GridSamplerGradOpMaker<paddle::framework::OpDesc>,
                  ops::GridSamplerGradOpMaker<paddle::imperative::OpBase>,
                  ops::GridSamplerInferShapeFunctor);

REGISTER_OP_VERSION(grid_sampler)
  .AddCheckpoint(
    R"ROC(Upgrade grid_sampler add a new attribute [mode])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("mode", "In order to specify interpolation mode", std::string("bilinear")))
;

REGISTER_OPERATOR(group_norm, ops::GroupNormOp,
                  ops::GroupNormOpMaker,
                  ops::GroupNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::GroupNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::GroupNormCompositeGradOpMaker,
                  ops::GroupNormInferShapeFunctor);


REGISTER_OPERATOR(gumbel_softmax, ops::GumbelSoftmaxOp,
                  ops::GumbelSoftmaxOpMaker,
                  ops::GumbelSoftmaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::GumbelSoftmaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::GumbelSoftmaxInferShapeFunctor);


REGISTER_OPERATOR(hard_shrink, ops::HardShrinkOp,
                  ops::HardShrinkOpMaker,
                  ops::HardShrinkGradOpMaker<paddle::framework::OpDesc>,
                  ops::HardShrinkGradOpMaker<paddle::imperative::OpBase>,
                  ops::HardShrinkInferShapeFunctor);


REGISTER_OPERATOR(hard_sigmoid, ops::HardSigmoidOp,
                  ops::HardSigmoidOpMaker,
                  ops::HardSigmoidGradOpMaker<paddle::framework::OpDesc>,
                  ops::HardSigmoidGradOpMaker<paddle::imperative::OpBase>,
                  ops::HardSigmoidInferShapeFunctor);


REGISTER_OPERATOR(brelu, ops::BreluOp,
                  ops::BreluOpMaker,
                  ops::BreluGradOpMaker<paddle::framework::OpDesc>,
                  ops::BreluGradOpMaker<paddle::imperative::OpBase>,
                  ops::BreluInplaceInferer,
                  ops::BreluInferShapeFunctor);


REGISTER_OPERATOR(elementwise_heaviside, ops::ElementwiseHeavisideOp,
                  ops::ElementwiseHeavisideOpMaker,
                  ops::ElementwiseHeavisideGradOpMaker<paddle::framework::OpDesc>,
                  ops::ElementwiseHeavisideGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseHeavisideInferShapeFunctor);


REGISTER_OPERATOR(histogram, ops::HistogramOp,
                  ops::HistogramOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::HistogramInferShapeFunctor);


REGISTER_OPERATOR(huber_loss, ops::HuberLossOp,
                  ops::HuberLossOpMaker,
                  ops::HuberLossGradOpMaker<paddle::framework::OpDesc>,
                  ops::HuberLossGradOpMaker<paddle::imperative::OpBase>,
                  ops::HuberLossInferShapeFunctor);


REGISTER_OPERATOR(i0, ops::I0Op,
                  ops::I0OpMaker,
                  ops::I0GradOpMaker<paddle::framework::OpDesc>,
                  ops::I0GradOpMaker<paddle::imperative::OpBase>,
                  ops::I0InplaceInferer,
                  ops::I0InferShapeFunctor);


REGISTER_OPERATOR(i0e, ops::I0eOp,
                  ops::I0eOpMaker,
                  ops::I0eGradOpMaker<paddle::framework::OpDesc>,
                  ops::I0eGradOpMaker<paddle::imperative::OpBase>,
                  ops::I0eInferShapeFunctor);


REGISTER_OPERATOR(i1, ops::I1Op,
                  ops::I1OpMaker,
                  ops::I1GradOpMaker<paddle::framework::OpDesc>,
                  ops::I1GradOpMaker<paddle::imperative::OpBase>,
                  ops::I1InferShapeFunctor);


REGISTER_OPERATOR(i1e, ops::I1eOp,
                  ops::I1eOpMaker,
                  ops::I1eGradOpMaker<paddle::framework::OpDesc>,
                  ops::I1eGradOpMaker<paddle::imperative::OpBase>,
                  ops::I1eInferShapeFunctor);


REGISTER_OPERATOR(imag, ops::ImagOp,
                  ops::ImagOpMaker,
                  ops::ImagGradOpMaker<paddle::framework::OpDesc>,
                  ops::ImagGradOpMaker<paddle::imperative::OpBase>,
                  ops::ImagInferShapeFunctor);


REGISTER_OPERATOR(index_add, ops::IndexAddOp,
                  ops::IndexAddOpMaker,
                  ops::IndexAddGradOpMaker<paddle::framework::OpDesc>,
                  ops::IndexAddGradOpMaker<paddle::imperative::OpBase>,
                  ops::IndexAddInplaceInferer,
                  ops::IndexAddInferShapeFunctor);


REGISTER_OPERATOR(index_put, ops::IndexPutOp,
                  ops::IndexPutOpMaker,
                  ops::IndexPutGradOpMaker<paddle::framework::OpDesc>,
                  ops::IndexPutGradOpMaker<paddle::imperative::OpBase>,
                  ops::IndexPutInplaceInferer,
                  ops::IndexPutInferShapeFunctor);


REGISTER_OPERATOR(index_sample, ops::IndexSampleOp,
                  ops::IndexSampleOpMaker,
                  ops::IndexSampleGradOpMaker<paddle::framework::OpDesc>,
                  ops::IndexSampleGradOpMaker<paddle::imperative::OpBase>,
                  ops::IndexSampleInferShapeFunctor);


REGISTER_OPERATOR(index_select, ops::IndexSelectOp,
                  ops::IndexSelectOpMaker,
                  ops::IndexSelectGradOpMaker<paddle::framework::OpDesc>,
                  ops::IndexSelectGradOpMaker<paddle::imperative::OpBase>,
                  ops::IndexSelectInferShapeFunctor);


REGISTER_OPERATOR(index_select_strided, ops::IndexSelectStridedOp,
                  ops::IndexSelectStridedOpMaker,
                  ops::IndexSelectStridedGradOpMaker<paddle::framework::OpDesc>,
                  ops::IndexSelectStridedGradOpMaker<paddle::imperative::OpBase>,
                  ops::IndexSelectStridedInferShapeFunctor);


REGISTER_OPERATOR(instance_norm, ops::InstanceNormOp,
                  ops::InstanceNormOpMaker,
                  ops::InstanceNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::InstanceNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::InstanceNormCompositeGradOpMaker,
                  ops::InstanceNormInferShapeFunctor);

REGISTER_OP_VERSION(instance_norm)
  .AddCheckpoint(
    R"ROC(Change dispensable of attribute from False to True in instance_norm.)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("Bias", "The arg 'dispensable' of Input 'Bias' is changed: from 'False' to 'True'.", true)
        .ModifyAttr("Scale", "The arg 'dispensable' of Input 'Scale' is changed: from 'False' to 'True'.", true))
;

REGISTER_OPERATOR(inverse, ops::InverseOp,
                  ops::InverseOpMaker,
                  ops::InverseGradOpMaker<paddle::framework::OpDesc>,
                  ops::InverseGradOpMaker<paddle::imperative::OpBase>,
                  ops::InverseInferShapeFunctor);


REGISTER_OPERATOR(is_empty, ops::IsEmptyOp,
                  ops::IsEmptyOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::IsEmptyInferShapeFunctor);


REGISTER_OPERATOR(isclose, ops::IscloseOp,
                  ops::IscloseOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::IscloseInferShapeFunctor);


REGISTER_OPERATOR(isfinite_v2, ops::IsfiniteV2Op,
                  ops::IsfiniteV2OpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::IsfiniteV2InferShapeFunctor);


REGISTER_OPERATOR(isinf_v2, ops::IsinfV2Op,
                  ops::IsinfV2OpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::IsinfV2InferShapeFunctor);


REGISTER_OPERATOR(isnan_v2, ops::IsnanV2Op,
                  ops::IsnanV2OpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::IsnanV2InferShapeFunctor);


REGISTER_OPERATOR(kldiv_loss, ops::KldivLossOp,
                  ops::KldivLossOpMaker,
                  ops::KldivLossGradOpMaker<paddle::framework::OpDesc>,
                  ops::KldivLossGradOpMaker<paddle::imperative::OpBase>,
                  ops::KldivLossInferShapeFunctor);


REGISTER_OPERATOR(kron, ops::KronOp,
                  ops::KronOpMaker,
                  ops::KronGradOpMaker<paddle::framework::OpDesc>,
                  ops::KronGradOpMaker<paddle::imperative::OpBase>,
                  ops::KronInferShapeFunctor);


REGISTER_OPERATOR(kthvalue, ops::KthvalueOp,
                  ops::KthvalueOpMaker,
                  ops::KthvalueGradOpMaker<paddle::framework::OpDesc>,
                  ops::KthvalueGradOpMaker<paddle::imperative::OpBase>,
                  ops::KthvalueInferShapeFunctor);


REGISTER_OPERATOR(label_smooth, ops::LabelSmoothOp,
                  ops::LabelSmoothOpMaker,
                  ops::LabelSmoothGradOpMaker<paddle::framework::OpDesc>,
                  ops::LabelSmoothGradOpMaker<paddle::imperative::OpBase>,
                  ops::LabelSmoothInferShapeFunctor);


REGISTER_OPERATOR(lamb, ops::LambOp,
                  ops::LambOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LambInferShapeFunctor);

REGISTER_OP_VERSION(lamb)
  .AddCheckpoint(
    R"ROC(Upgrade lamb, add two new outputs [Beta1PowOut] and [Beta2PowOut].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewOutput("Beta1PowOut", "The Output beta1 power accumulator. 'Beta1PowOut' is dispensable.")
        .NewOutput("Beta2PowOut", "The Output beta2 power accumulator. 'Beta2PowOut' is dispensable."))
;

REGISTER_OPERATOR(layer_norm, ops::LayerNormOp,
                  ops::LayerNormOpMaker,
                  ops::LayerNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::LayerNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::LayerNormCompositeGradOpMaker,
                  ops::LayerNormInferShapeFunctor);


REGISTER_OPERATOR(dist_grad, ops::DistGradOp,
                  ops::DistGradInferShapeFunctor);


REGISTER_OPERATOR(dot_grad, ops::DotGradOp,
                  ops::DotGradInferShapeFunctor);


REGISTER_OPERATOR(eig_grad, ops::EigGradOp,
                  ops::EigGradInferShapeFunctor);


REGISTER_OPERATOR(eigh_grad, ops::EighGradOp,
                  ops::EighGradInferShapeFunctor);


REGISTER_OPERATOR(eigvalsh_grad, ops::EigvalshGradOp,
                  ops::EigvalshGradInferShapeFunctor);


REGISTER_OPERATOR(elu_grad, ops::EluGradOp,
                  ops::EluGradGradOpMaker<paddle::framework::OpDesc>,
                  ops::EluGradGradOpMaker<paddle::imperative::OpBase>,
                  ops::EluGradInplaceInferer,
                  ops::EluGradInferShapeFunctor);


REGISTER_OPERATOR(elu_grad_grad, ops::EluGradGradOp,
                  ops::EluGradGradInplaceInferer,
                  ops::EluGradGradInferShapeFunctor);


REGISTER_OPERATOR(erf_grad, ops::ErfGradOp,
                  ops::ErfGradInferShapeFunctor);


REGISTER_OPERATOR(erfinv_grad, ops::ErfinvGradOp,
                  ops::ErfinvGradInferShapeFunctor);


REGISTER_OPERATOR(exp_grad, ops::ExpGradOp,
                  ops::ExpGradInplaceInferer,
                  ops::ExpGradInferShapeFunctor);


REGISTER_OPERATOR(expand_v2_grad, ops::ExpandV2GradOp,
                  ops::ExpandV2DoubleGradOpMaker<paddle::framework::OpDesc>,
                  ops::ExpandV2DoubleGradOpMaker<paddle::imperative::OpBase>,
                  ops::ExpandV2GradNoNeedBufferVarInferer,
                  ops::ExpandV2GradInferShapeFunctor);


REGISTER_OPERATOR(expand_as_v2_grad, ops::ExpandAsV2GradOp,
                  ops::ExpandAsV2GradNoNeedBufferVarInferer,
                  ops::ExpandAsV2GradInferShapeFunctor);


REGISTER_OPERATOR(expm1_grad, ops::Expm1GradOp,
                  ops::Expm1GradInplaceInferer,
                  ops::Expm1GradInferShapeFunctor);


REGISTER_OPERATOR(fft_c2c_grad, ops::FftC2cGradOp,
                  ops::FftC2cGradInferShapeFunctor);


REGISTER_OPERATOR(fft_c2r_grad, ops::FftC2rGradOp,
                  ops::FftC2rGradInferShapeFunctor);


REGISTER_OPERATOR(fft_r2c_grad, ops::FftR2cGradOp,
                  ops::FftR2cGradNoNeedBufferVarInferer,
                  ops::FftR2cGradInferShapeFunctor);


REGISTER_OPERATOR(fill_any_grad, ops::FillAnyGradOp,
                  ops::FillAnyGradInplaceInferer,
                  ops::FillAnyGradInferShapeFunctor);


REGISTER_OPERATOR(fill_diagonal_grad, ops::FillDiagonalGradOp,
                  ops::FillDiagonalGradInferShapeFunctor);


REGISTER_OPERATOR(fill_diagonal_tensor_grad, ops::FillDiagonalTensorGradOp,
                  ops::FillDiagonalTensorGradInplaceInferer,
                  ops::FillDiagonalTensorGradInferShapeFunctor);


REGISTER_OPERATOR(flash_attn_grad, ops::FlashAttnGradOp,
                  ops::FlashAttnGradInferShapeFunctor);


REGISTER_OPERATOR(flash_attn_unpadded_grad, ops::FlashAttnUnpaddedGradOp,
                  ops::FlashAttnUnpaddedGradInferShapeFunctor);


REGISTER_OPERATOR(flatten_contiguous_range_grad, ops::FlattenContiguousRangeGradOp,
                  ops::FlattenContiguousRangeGradInplaceInferer,
                  ops::FlattenContiguousRangeGradInferShapeFunctor);


REGISTER_OPERATOR(floor_grad, ops::FloorGradOp,
                  ops::FloorGradInplaceInferer,
                  ops::FloorGradInferShapeFunctor);


REGISTER_OPERATOR(elementwise_fmax_grad, ops::ElementwiseFmaxGradOp,
                  ops::ElementwiseFmaxGradInferShapeFunctor);


REGISTER_OPERATOR(elementwise_fmin_grad, ops::ElementwiseFminGradOp,
                  ops::ElementwiseFminGradInferShapeFunctor);


REGISTER_OPERATOR(fold_grad, ops::FoldGradOp,
                  ops::FoldGradNoNeedBufferVarInferer,
                  ops::FoldGradInferShapeFunctor);


REGISTER_OPERATOR(frame_grad, ops::FrameGradOp,
                  ops::FrameGradInferShapeFunctor);


REGISTER_OPERATOR(gather_grad, ops::GatherGradOp,
                  ops::GatherGradNoNeedBufferVarInferer,
                  ops::GatherGradInferShapeFunctor);


REGISTER_OPERATOR(gather_nd_grad, ops::GatherNdGradOp,
                  ops::GatherNdGradNoNeedBufferVarInferer,
                  ops::GatherNdGradInferShapeFunctor);


REGISTER_OPERATOR(gelu_grad, ops::GeluGradOp,
                  ops::GeluGradInferShapeFunctor);


REGISTER_OPERATOR(grid_sampler_grad, ops::GridSamplerGradOp,
                  ops::GridSamplerGradInferShapeFunctor);


REGISTER_OPERATOR(group_norm_grad, ops::GroupNormGradOp,
                  ops::GroupNormGradInplaceInferer,
                  ops::GroupNormGradInferShapeFunctor);


REGISTER_OPERATOR(gumbel_softmax_grad, ops::GumbelSoftmaxGradOp,
                  ops::GumbelSoftmaxGradInferShapeFunctor);


REGISTER_OPERATOR(hard_shrink_grad, ops::HardShrinkGradOp,
                  ops::HardShrinkGradInplaceInferer,
                  ops::HardShrinkGradInferShapeFunctor);


REGISTER_OPERATOR(hard_sigmoid_grad, ops::HardSigmoidGradOp,
                  ops::HardSigmoidGradInplaceInferer,
                  ops::HardSigmoidGradInferShapeFunctor);


REGISTER_OPERATOR(brelu_grad, ops::BreluGradOp,
                  ops::BreluGradInplaceInferer,
                  ops::BreluGradInferShapeFunctor);


REGISTER_OPERATOR(elementwise_heaviside_grad, ops::ElementwiseHeavisideGradOp,
                  ops::ElementwiseHeavisideGradInferShapeFunctor);


REGISTER_OPERATOR(huber_loss_grad, ops::HuberLossGradOp,
                  ops::HuberLossGradInferShapeFunctor);


REGISTER_OPERATOR(i0_grad, ops::I0GradOp,
                  ops::I0GradInferShapeFunctor);


REGISTER_OPERATOR(i0e_grad, ops::I0eGradOp,
                  ops::I0eGradInferShapeFunctor);


REGISTER_OPERATOR(i1_grad, ops::I1GradOp,
                  ops::I1GradInferShapeFunctor);


REGISTER_OPERATOR(i1e_grad, ops::I1eGradOp,
                  ops::I1eGradInferShapeFunctor);


REGISTER_OPERATOR(imag_grad, ops::ImagGradOp,
                  ops::ImagGradInferShapeFunctor);


REGISTER_OPERATOR(index_add_grad, ops::IndexAddGradOp,
                  ops::IndexAddGradInplaceInferer,
                  ops::IndexAddGradInferShapeFunctor);


REGISTER_OPERATOR(index_put_grad, ops::IndexPutGradOp,
                  ops::IndexPutGradInferShapeFunctor);


REGISTER_OPERATOR(index_sample_grad, ops::IndexSampleGradOp,
                  ops::IndexSampleGradNoNeedBufferVarInferer,
                  ops::IndexSampleGradInferShapeFunctor);


REGISTER_OPERATOR(index_select_grad, ops::IndexSelectGradOp,
                  ops::IndexSelectGradNoNeedBufferVarInferer,
                  ops::IndexSelectGradInferShapeFunctor);


REGISTER_OPERATOR(index_select_strided_grad, ops::IndexSelectStridedGradOp,
                  ops::IndexSelectStridedGradNoNeedBufferVarInferer,
                  ops::IndexSelectStridedGradInferShapeFunctor);


REGISTER_OPERATOR(instance_norm_grad, ops::InstanceNormGradOp,
                  ops::InstanceNormDoubleGradOpMaker<paddle::framework::OpDesc>,
                  ops::InstanceNormDoubleGradOpMaker<paddle::imperative::OpBase>,
                  ops::InstanceNormGradInferShapeFunctor);


REGISTER_OPERATOR(instance_norm_double_grad, ops::InstanceNormDoubleGradOp,
                  ops::InstanceNormDoubleGradInferShapeFunctor);


REGISTER_OPERATOR(inverse_grad, ops::InverseGradOp,
                  ops::InverseGradInferShapeFunctor);


REGISTER_OPERATOR(kldiv_loss_grad, ops::KldivLossGradOp,
                  ops::KldivLossGradNoNeedBufferVarInferer,
                  ops::KldivLossGradInferShapeFunctor);


REGISTER_OPERATOR(kron_grad, ops::KronGradOp,
                  ops::KronGradInferShapeFunctor);


REGISTER_OPERATOR(kthvalue_grad, ops::KthvalueGradOp,
                  ops::KthvalueGradInferShapeFunctor);


REGISTER_OPERATOR(label_smooth_grad, ops::LabelSmoothGradOp,
                  ops::LabelSmoothGradInferShapeFunctor);


REGISTER_OPERATOR(layer_norm_grad, ops::LayerNormGradOp,
                  ops::LayerNormGradNoNeedBufferVarInferer,
                  ops::LayerNormGradInferShapeFunctor);


