// this file is generated by paddle/phi/api/yaml/generator/generate_op.py, do not edit.
#include <string>
#include "paddle/fluid/framework/convert_utils.h"
#include "paddle/fluid/framework/infershape_utils.h"
#include "paddle/fluid/framework/op_registry.h"
#include "paddle/fluid/framework/op_version_registry.h"
#include "paddle/fluid/prim/api/composite_backward/composite_backward_api.h"
#include "paddle/fluid/prim/utils/static/composite_grad_desc_maker.h"
#include "paddle/fluid/prim/utils/static/desc_tensor.h"
#include "paddle/fluid/operators/generator/get_expected_kernel_func.h"
#include "paddle/phi/core/infermeta_utils.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"

namespace paddle {
namespace operators {

using paddle::framework::GradVarName;


class ReduceAllOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_all op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_all op.");
    AddInput("AxisTensor", "attribute 0 for reduce_all op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("AxisTensorList", "attribute 0 for reduce_all op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("dim", "(std::vector<int64_t>), attribute 0 for reduce_all op.")
        .SetDefault({0});
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_all op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_all op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_all op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_all op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_all op.
)DOC");
  }
};


class ReduceAllOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_all, ReduceAllInferShapeFunctor,
                            PD_INFER_META(phi::ReduceInferMetaBase));



class AllGatherOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of all_gather op.");
    AddOutput("out", "(Tensor), output 0 of all_gather op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for all_gather op.")
        .SetDefault(0);
    AddAttr<int>("nranks", "(int), attribute 1 for all_gather op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of all_gather op.
)DOC");
  }
};


class AllGatherOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(all_gather, AllGatherInferShapeFunctor,
                            PD_INFER_META(phi::AllGatherInferMeta));



class AllReduceOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of all_reduce op.");
    AddOutput("out", "(Tensor), output 0 of all_reduce op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for all_reduce op.")
        .SetDefault(0);
    AddAttr<int>("reduce_type", "(int), attribute 1 for all_reduce op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of all_reduce op.
)DOC");
  }
};


class AllReduceOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(all_reduce, AllReduceInferShapeFunctor,
                            PD_INFER_META(phi::AllReduceInferMeta));



class AllToAllOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of all_to_all op.");
    AddOutput("out", "(Tensor), output 0 of all_to_all op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for all_to_all op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of all_to_all op.
)DOC");
  }
};


class AllToAllOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(all_to_all, AllToAllInferShapeFunctor,
                            PD_INFER_META(phi::AllToAllInferMeta));



class ReduceAmaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_amax op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_amax op.");
    AddInput("AxisTensor", "attribute 0 for reduce_amax op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("AxisTensorList", "attribute 0 for reduce_amax op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("dim", "(std::vector<int64_t>), attribute 0 for reduce_amax op.")
        .SetDefault({0});
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_amax op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_amax op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_amax op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_amax op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_amax op.
)DOC");
  }
};


class ReduceAmaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_amax, ReduceAmaxInferShapeFunctor,
                            PD_INFER_META(phi::ReduceInferMeta));



class ReduceAminOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_amin op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_amin op.");
    AddInput("AxisTensor", "attribute 0 for reduce_amin op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("AxisTensorList", "attribute 0 for reduce_amin op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("dim", "(std::vector<int64_t>), attribute 0 for reduce_amin op.")
        .SetDefault({0});
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_amin op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_amin op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_amin op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_amin op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_amin op.
)DOC");
  }
};


class ReduceAminOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_amin, ReduceAminInferShapeFunctor,
                            PD_INFER_META(phi::ReduceInferMeta));



class ReduceAnyOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_any op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_any op.");
    AddInput("AxisTensor", "attribute 0 for reduce_any op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("AxisTensorList", "attribute 0 for reduce_any op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("dim", "(std::vector<int64_t>), attribute 0 for reduce_any op.")
        .SetDefault({0});
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_any op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_any op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_any op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_any op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_any op.
)DOC");
  }
};


class ReduceAnyOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceOpUseInputPlaceExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_any, ReduceAnyInferShapeFunctor,
                            PD_INFER_META(phi::ReduceInferMetaBase));



class RangeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Start", "(Tensor), input 0 of range op.");
    AddInput("End", "(Tensor), input 1 of range op.");
    AddInput("Step", "(Tensor), input 2 of range op.");
    AddOutput("Out", "(Tensor), output 0 of range op.");
    AddComment(R"DOC(
TODO: Documentation of range op.
)DOC");
  }
};


class RangeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "Start" || var_name == "End" || var_name == "Step") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(range, RangeInferShapeFunctor,
                            PD_INFER_META(phi::ArangeInferMeta));



class AssignOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of assign op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of assign op.");
    AddComment(R"DOC(
TODO: Documentation of assign op.
)DOC");
  }
};


class AssignOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetAssignExpectedKernelType(ctx, this);
  }

};


class AssignInferVarType : public framework::VarTypeInference {
 public:
  void operator()(framework::InferVarTypeContext *ctx) const override {
    ctx->SyncTypeAndDataType("X", "Out");
  }
};


DECLARE_INFER_SHAPE_FUNCTOR(assign, AssignInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(AssignInplaceInferer,
                           {"X", "Out"});



class AssignValueOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of assign_value op.");
    AddAttr<std::vector<int>>("shape", "(std::vector<int>), attribute 0 for assign_value op.")
    ;
    AddAttr<int>("dtype", "(int), attribute 1 for assign_value op.")
    ;
    AddAttr<std::vector<int>>("bool_values", "(std::vector<int>), attribute 2 for assign_value op.")
        .SetDefault({});
    AddAttr<std::vector<float>>("fp32_values", "(std::vector<float>), attribute 3 for assign_value op.")
        .SetDefault({});
    AddAttr<std::vector<int>>("int32_values", "(std::vector<int>), attribute 4 for assign_value op.")
        .SetDefault({});
    AddAttr<std::vector<int64_t>>("int64_values", "(std::vector<int64_t>), attribute 5 for assign_value op.")
        .SetDefault({});
    AddComment(R"DOC(
TODO: Documentation of assign_value op.
)DOC");
  }
};


class AssignValueOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(assign_value, AssignValueInferShapeFunctor,
                            PD_INFER_META(phi::AssignValueInferMeta));



class BroadcastOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of broadcast op.");
    AddOutput("out", "(Tensor), output 0 of broadcast op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for broadcast op.")
        .SetDefault(0);
    AddAttr<int>("root", "(int), attribute 1 for broadcast op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of broadcast op.
)DOC");
  }
};


class BroadcastOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(broadcast, BroadcastInferShapeFunctor,
                            PD_INFER_META(phi::DistBroadcastInferMeta));



class Conv2dTransposeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of conv2d_transpose op.");
    AddInput("Filter", "(Tensor), input 1 of conv2d_transpose op.");
    AddInput("Bias", "(Tensor), input 2 of conv2d_transpose op.")
        .AsDispensable()
        .AsExtra();
    AddOutput("Output", "(Tensor), output 0 of conv2d_transpose op.");
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 0 for conv2d_transpose op.")
        .SetDefault({1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 1 for conv2d_transpose op.")
        .SetDefault({0, 0});
    AddAttr<std::vector<int>>("output_padding", "(std::vector<int>), attribute 2 for conv2d_transpose op.")
        .SetDefault({});
    AddAttr<std::vector<int>>("output_size", "(std::vector<int>), attribute 3 for conv2d_transpose op.")
        .SetDefault({})
        .SupportTensor();
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 4 for conv2d_transpose op.")
        .SetDefault("EXPLICIT");
    AddAttr<int>("groups", "(int), attribute 5 for conv2d_transpose op.")
        .SetDefault(1);
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 6 for conv2d_transpose op.")
        .SetDefault({1, 1});
    AddAttr<std::string>("data_format", "(std::string), attribute 7 for conv2d_transpose op.")
        .SetDefault("NCHW");
    AddComment(R"DOC(
TODO: Documentation of conv2d_transpose op.
)DOC");
  }
};


class Conv2dTransposeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(conv2d_transpose, Conv2dTransposeInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dTransposeInferMeta));



class DecodeJpegOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of decode_jpeg op.");
    AddOutput("Out", "(Tensor), output 0 of decode_jpeg op.");
    AddAttr<std::string>("mode", "(std::string), attribute 0 for decode_jpeg op.")
        .SetDefault("unchanged");
    AddComment(R"DOC(
TODO: Documentation of decode_jpeg op.
)DOC");
  }
};


class DecodeJpegOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(decode_jpeg, DecodeJpegInferShapeFunctor,
                            PD_INFER_META(phi::DecodeJpegInferMeta));



class DeformableConvOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of deformable_conv op.");
    AddInput("Offset", "(Tensor), input 1 of deformable_conv op.");
    AddInput("Filter", "(Tensor), input 2 of deformable_conv op.");
    AddInput("Mask", "(Tensor), input 3 of deformable_conv op.");
    AddOutput("Output", "(Tensor), output 0 of deformable_conv op.");
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 0 for deformable_conv op.")
        .SetDefault({1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 1 for deformable_conv op.")
        .SetDefault({0, 0});
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 2 for deformable_conv op.")
        .SetDefault({1, 1});
    AddAttr<int>("deformable_groups", "(int), attribute 3 for deformable_conv op.")
        .SetDefault(1);
    AddAttr<int>("groups", "(int), attribute 4 for deformable_conv op.")
        .SetDefault(1);
    AddAttr<int>("im2col_step", "(int), attribute 5 for deformable_conv op.")
        .SetDefault(64);
    AddComment(R"DOC(
TODO: Documentation of deformable_conv op.
)DOC");
  }
};


class DeformableConvOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(deformable_conv, DeformableConvInferShapeFunctor,
                            PD_INFER_META(phi::DeformableConvInferMeta));



class DepthwiseConv2dTransposeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of depthwise_conv2d_transpose op.");
    AddInput("Filter", "(Tensor), input 1 of depthwise_conv2d_transpose op.");
    AddInput("Bias", "(Tensor), input 2 of depthwise_conv2d_transpose op.")
        .AsDispensable()
        .AsExtra();
    AddOutput("Output", "(Tensor), output 0 of depthwise_conv2d_transpose op.");
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 0 for depthwise_conv2d_transpose op.")
        .SetDefault({1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 1 for depthwise_conv2d_transpose op.")
        .SetDefault({0, 0});
    AddAttr<std::vector<int>>("output_padding", "(std::vector<int>), attribute 2 for depthwise_conv2d_transpose op.")
        .SetDefault({});
    AddAttr<std::vector<int>>("output_size", "(std::vector<int>), attribute 3 for depthwise_conv2d_transpose op.")
        .SetDefault({})
        .SupportTensor();
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 4 for depthwise_conv2d_transpose op.")
        .SetDefault("EXPLICIT");
    AddAttr<int>("groups", "(int), attribute 5 for depthwise_conv2d_transpose op.")
        .SetDefault(1);
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 6 for depthwise_conv2d_transpose op.")
        .SetDefault({1, 1});
    AddAttr<std::string>("data_format", "(std::string), attribute 7 for depthwise_conv2d_transpose op.")
        .SetDefault("NCHW");
    AddComment(R"DOC(
TODO: Documentation of depthwise_conv2d_transpose op.
)DOC");
  }
};


class DepthwiseConv2dTransposeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(depthwise_conv2d_transpose, DepthwiseConv2dTransposeInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dTransposeInferMeta));



class DistConcatOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of dist_concat op.");
    AddOutput("out", "(Tensor), output 0 of dist_concat op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for dist_concat op.")
        .SetDefault(0);
    AddAttr<int>("nranks", "(int), attribute 1 for dist_concat op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of dist_concat op.
)DOC");
  }
};


class DistConcatOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(dist_concat, DistConcatInferShapeFunctor,
                            PD_INFER_META(phi::DistConcatInferMeta));



class EinsumOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Operands", "(Tensor[]), input 0 of einsum op.")
        .AsDuplicable();
    AddOutput("Out", "(Tensor), output 0 of einsum op.");
    AddOutput("InnerCache", "(Tensor[]), output 1 of einsum op.")
        .AsDuplicable()
        .AsIntermediate()
        .AsExtra();
    AddOutput("XShape", "(Tensor[]), output 2 of einsum op.")
        .AsDuplicable()
        .AsIntermediate()
        .AsExtra();
    AddAttr<std::string>("equation", "(std::string), attribute 0 for einsum op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of einsum op.
)DOC");
  }
};


class EinsumOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(einsum, EinsumInferShapeFunctor,
                            PD_INFER_META(phi::EinsumRawInferMeta));



class ElementwisePowOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_pow op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_pow op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_pow op.");
    AddAttr<int>("axis", "(int), attribute 0 for elementwise_pow op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of elementwise_pow op.
)DOC");
  }
};


class ElementwisePowOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_pow, ElementwisePowInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseRawInferMeta));



class LookupTableV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Ids", "(Tensor), input 0 of lookup_table_v2 op.");
    AddInput("W", "(Tensor), input 1 of lookup_table_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of lookup_table_v2 op.");
    AddAttr<int64_t>("padding_idx", "(int64_t), attribute 0 for lookup_table_v2 op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of lookup_table_v2 op.
)DOC");
  }
};


class LookupTableV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "W");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(lookup_table_v2, LookupTableV2InferShapeFunctor,
                            PD_INFER_META(phi::EmbeddingInferMeta));



class EmptyOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of empty op.");
    AddInput("ShapeTensor", "attribute 0 for empty op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("ShapeTensorList", "attribute 0 for empty op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("shape", "(std::vector<int64_t>), attribute 0 for empty op.")
        .SetDefault({});
    AddAttr<int>("dtype", "(int), attribute 1 for empty op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddComment(R"DOC(
TODO: Documentation of empty op.
)DOC");
  }
};


class EmptyOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(empty, EmptyInferShapeFunctor,
                            PD_INFER_META(phi::CreateInferMeta));



class EqualOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of equal op.");
    AddInput("Y", "(Tensor), input 1 of equal op.");
    AddOutput("Out", "(Tensor), output 0 of equal op.");
    AddAttr<int>("axis", "(int), attribute 0 for equal op.")
        .SetDefault(-1);
    AddAttr<bool>("force_cpu", "(bool), attribute 1 for equal op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of equal op.
)DOC");
  }
};


class EqualOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
      kt = OperatorWithKernel::GetExpectedKernelType(ctx);
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
      if (ctx.Attr<bool>("force_cpu")) {
        kt.set_backend(phi::Backend::CPU);
      }
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(equal, EqualInferShapeFunctor,
                            PD_INFER_META(phi::CompareRawInferMeta));



class ExponentialOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of exponential op.");
    AddOutput("Out", "(Tensor), output 0 of exponential op.");
    AddAttr<float>("lambda", "(float), attribute 0 for exponential op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of exponential op.
)DOC");
  }
};


class ExponentialOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(exponential, ExponentialInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ExponentialInplaceInferer,
                           {"X", "Out"});



class EyeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of eye op.");
    AddAttr<int64_t>("num_rows", "(int64_t), attribute 0 for eye op.")

        .SupportTensor();
    AddAttr<int64_t>("num_columns", "(int64_t), attribute 1 for eye op.")
        .SetDefault(-1)
        .SupportTensor();
    AddAttr<int>("dtype", "(int), attribute 2 for eye op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddComment(R"DOC(
TODO: Documentation of eye op.
)DOC");
  }
};


class EyeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(eye, EyeInferShapeFunctor,
                            PD_INFER_META(phi::EyeInferMeta));



class ElementwiseFloordivOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_floordiv op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_floordiv op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_floordiv op.");
    AddAttr<int>("axis", "(int), attribute 0 for elementwise_floordiv op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of elementwise_floordiv op.
)DOC");
  }
};


class ElementwiseFloordivOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_floordiv, ElementwiseFloordivInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseRawInferMeta));



class FrobeniusNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of frobenius_norm op.");
    AddOutput("Out", "(Tensor), output 0 of frobenius_norm op.");
    AddAttr<std::vector<int>>("dim", "(std::vector<int>), attribute 0 for frobenius_norm op.")
        .SetDefault({0})
        .SupportTensor();
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for frobenius_norm op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for frobenius_norm op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for frobenius_norm op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for frobenius_norm op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of frobenius_norm op.
)DOC");
  }
};


class FrobeniusNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(frobenius_norm, FrobeniusNormInferShapeFunctor,
                            PD_INFER_META(phi::ReduceInferMetaBase));



class FillAnyLikeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fill_any_like op.");
    AddOutput("Out", "(Tensor), output 0 of fill_any_like op.");
    AddAttr<float>("value", "(float), attribute 0 for fill_any_like op.")
        .SetDefault(0.0)
        .SupportTensor();
    AddAttr<int>("dtype", "(int), attribute 1 for fill_any_like op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of fill_any_like op.
)DOC");
  }
};


class FillAnyLikeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    if (data_type == static_cast<framework::proto::VarType::Type>(-1)) {
      data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    }
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fill_any_like, FillAnyLikeInferShapeFunctor,
                            PD_INFER_META(phi::FillAnyLikeInferMeta));



class GaussianRandomOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of gaussian_random op.");
    AddInput("ShapeTensor", "attribute 0 for gaussian_random op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("ShapeTensorList", "attribute 0 for gaussian_random op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("shape", "(std::vector<int64_t>), attribute 0 for gaussian_random op.")
        .SetDefault({});
    AddAttr<float>("mean", "(float), attribute 1 for gaussian_random op.")
        .SetDefault(.0f);
    AddAttr<float>("std", "(float), attribute 2 for gaussian_random op.")
        .SetDefault(1.0f);
    AddAttr<int>("seed", "(int), attribute 3 for gaussian_random op.")
        .SetDefault(0);
    AddAttr<int>("dtype", "(int), attribute 4 for gaussian_random op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddComment(R"DOC(
TODO: Documentation of gaussian_random op.
)DOC");
  }
};


class GaussianRandomOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(gaussian_random, GaussianRandomInferShapeFunctor,
                            PD_INFER_META(phi::GaussianInferMeta));



class GreaterEqualOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of greater_equal op.");
    AddInput("Y", "(Tensor), input 1 of greater_equal op.");
    AddOutput("Out", "(Tensor), output 0 of greater_equal op.");
    AddAttr<int>("axis", "(int), attribute 0 for greater_equal op.")
        .SetDefault(-1);
    AddAttr<bool>("force_cpu", "(bool), attribute 1 for greater_equal op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of greater_equal op.
)DOC");
  }
};


class GreaterEqualOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
      kt = OperatorWithKernel::GetExpectedKernelType(ctx);
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
      if (ctx.Attr<bool>("force_cpu")) {
        kt.set_backend(phi::Backend::CPU);
      }
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(greater_equal, GreaterEqualInferShapeFunctor,
                            PD_INFER_META(phi::CompareRawInferMeta));



class GreaterThanOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of greater_than op.");
    AddInput("Y", "(Tensor), input 1 of greater_than op.");
    AddOutput("Out", "(Tensor), output 0 of greater_than op.");
    AddAttr<int>("axis", "(int), attribute 0 for greater_than op.")
        .SetDefault(-1);
    AddAttr<bool>("force_cpu", "(bool), attribute 1 for greater_than op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of greater_than op.
)DOC");
  }
};


class GreaterThanOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
      kt = OperatorWithKernel::GetExpectedKernelType(ctx);
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
      if (ctx.Attr<bool>("force_cpu")) {
        kt.set_backend(phi::Backend::CPU);
      }
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(greater_than, GreaterThanInferShapeFunctor,
                            PD_INFER_META(phi::CompareRawInferMeta));



class HardSwishOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of hard_swish op.");
    AddOutput("Out", "(Tensor), output 0 of hard_swish op.");
    AddAttr<float>("threshold", "(float), attribute 0 for hard_swish op.")
        .SetDefault(6.0f);
    AddAttr<float>("scale", "(float), attribute 1 for hard_swish op.")
        .SetDefault(6.0f);
    AddAttr<float>("offset", "(float), attribute 2 for hard_swish op.")
        .SetDefault(3.0f);
    AddComment(R"DOC(
TODO: Documentation of hard_swish op.
)DOC");
  }
};


class HardSwishOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(hard_swish, HardSwishInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class LessEqualOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of less_equal op.");
    AddInput("Y", "(Tensor), input 1 of less_equal op.");
    AddOutput("Out", "(Tensor), output 0 of less_equal op.");
    AddAttr<int>("axis", "(int), attribute 0 for less_equal op.")
        .SetDefault(-1);
    AddAttr<bool>("force_cpu", "(bool), attribute 1 for less_equal op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of less_equal op.
)DOC");
  }
};


class LessEqualOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
      kt = OperatorWithKernel::GetExpectedKernelType(ctx);
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
      if (ctx.Attr<bool>("force_cpu")) {
        kt.set_backend(phi::Backend::CPU);
      }
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(less_equal, LessEqualInferShapeFunctor,
                            PD_INFER_META(phi::CompareRawInferMeta));



class LessThanOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of less_than op.");
    AddInput("Y", "(Tensor), input 1 of less_than op.");
    AddOutput("Out", "(Tensor), output 0 of less_than op.");
    AddAttr<int>("axis", "(int), attribute 0 for less_than op.")
        .SetDefault(-1);
    AddAttr<bool>("force_cpu", "(bool), attribute 1 for less_than op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of less_than op.
)DOC");
  }
};


class LessThanOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
      kt = OperatorWithKernel::GetExpectedKernelType(ctx);
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
      if (ctx.Attr<bool>("force_cpu")) {
        kt.set_backend(phi::Backend::CPU);
      }
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(less_than, LessThanInferShapeFunctor,
                            PD_INFER_META(phi::CompareRawInferMeta));



class LinspaceOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Start", "(Tensor), input 0 of linspace op.");
    AddInput("Stop", "(Tensor), input 1 of linspace op.");
    AddInput("Num", "(Tensor), input 2 of linspace op.");
    AddOutput("Out", "(Tensor), output 0 of linspace op.");
    AddAttr<int>("dtype", "(int), attribute 0 for linspace op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of linspace op.
)DOC");
  }
};


class LinspaceOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(linspace, LinspaceInferShapeFunctor,
                            PD_INFER_META(phi::LinspaceInferMeta));



class MatmulV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of matmul_v2 op.");
    AddInput("Y", "(Tensor), input 1 of matmul_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of matmul_v2 op.");
    AddAttr<bool>("trans_x", "(bool), attribute 0 for matmul_v2 op.")
        .SetDefault(false);
    AddAttr<bool>("trans_y", "(bool), attribute 1 for matmul_v2 op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of matmul_v2 op.
)DOC");
  }
};


class MatmulV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(matmul_v2, MatmulV2InferShapeFunctor,
                            PD_INFER_META(phi::MatmulInferMeta));



class MatrixRankOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of matrix_rank op.");
    AddInput("TolTensor", "(Tensor), input 1 of matrix_rank op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of matrix_rank op.");
    AddAttr<float>("tol", "(float), attribute 0 for matrix_rank op.")
        .SetDefault(0.0f);
    AddAttr<bool>("hermitian", "(bool), attribute 1 for matrix_rank op.")
        .SetDefault(false);
    AddAttr<bool>("use_default_tol", "(bool), attribute 2 for matrix_rank op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of matrix_rank op.
)DOC");
  }
};


class MatrixRankOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(matrix_rank, MatrixRankInferShapeFunctor,
                            PD_INFER_META(phi::MatrixRankStaticInferMeta));



class ReduceMaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_max op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_max op.");
    AddAttr<std::vector<int>>("dim", "(std::vector<int>), attribute 0 for reduce_max op.")
        .SetDefault({0})
        .SupportTensor();
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_max op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_max op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_max op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_max op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_max op.
)DOC");
  }
};


class ReduceMaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_max, ReduceMaxInferShapeFunctor,
                            PD_INFER_META(phi::ReduceIntArrayAxisInferMetaBase));



class ElementwiseMaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_max op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_max op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_max op.");
    AddAttr<int>("axis", "(int), attribute 0 for elementwise_max op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of elementwise_max op.
)DOC");
  }
};


class ElementwiseMaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_max, ElementwiseMaxInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseRawInferMeta));



class ReduceMinOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_min op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_min op.");
    AddAttr<std::vector<int>>("dim", "(std::vector<int>), attribute 0 for reduce_min op.")
        .SetDefault({0})
        .SupportTensor();
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_min op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_min op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_min op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_min op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_min op.
)DOC");
  }
};


class ReduceMinOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_min, ReduceMinInferShapeFunctor,
                            PD_INFER_META(phi::ReduceIntArrayAxisInferMetaBase));



class ElementwiseMinOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_min op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_min op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_min op.");
    AddAttr<int>("axis", "(int), attribute 0 for elementwise_min op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of elementwise_min op.
)DOC");
  }
};


class ElementwiseMinOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_min, ElementwiseMinInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseRawInferMeta));



class NormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of norm op.");
    AddOutput("Out", "(Tensor), output 0 of norm op.");
    AddOutput("Norm", "(Tensor), output 1 of norm op.")
        .AsIntermediate()
        .AsExtra();
    AddAttr<int>("axis", "(int), attribute 0 for norm op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 1 for norm op.")
        .SetDefault(1.0e-10f);
    AddAttr<bool>("is_test", "(bool), attribute 2 for norm op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of norm op.
)DOC");
  }
};


class NormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(norm, NormInferShapeFunctor,
                            PD_INFER_META(phi::NormInferMeta));



class NotEqualOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of not_equal op.");
    AddInput("Y", "(Tensor), input 1 of not_equal op.");
    AddOutput("Out", "(Tensor), output 0 of not_equal op.");
    AddAttr<int>("axis", "(int), attribute 0 for not_equal op.")
        .SetDefault(-1);
    AddAttr<bool>("force_cpu", "(bool), attribute 1 for not_equal op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of not_equal op.
)DOC");
  }
};


class NotEqualOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
      kt = OperatorWithKernel::GetExpectedKernelType(ctx);
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
      if (ctx.Attr<bool>("force_cpu")) {
        kt.set_backend(phi::Backend::CPU);
      }
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(not_equal, NotEqualInferShapeFunctor,
                            PD_INFER_META(phi::CompareRawInferMeta));



class OneHotV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of one_hot_v2 op.");
    AddOutput("Out", "(Tensor), output 0 of one_hot_v2 op.");
    AddInput("depth_tensor", "attribute 0 for one_hot_v2 op from 0D Tensor.")
        .AsDispensable();
    AddAttr<int>("depth", "(int), attribute 0 for one_hot_v2 op.")
        .SetDefault(-1);
    AddAttr<int>("dtype", "(int), attribute 1 for one_hot_v2 op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddAttr<bool>("allow_out_of_range", "(bool), attribute 2 for one_hot_v2 op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of one_hot_v2 op.
)DOC");
  }
};


class OneHotV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(one_hot_v2, OneHotV2InferShapeFunctor,
                            PD_INFER_META(phi::OneHotRawInferMeta));



class PRecvOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("out", "(Tensor), output 0 of p_recv op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for p_recv op.")
        .SetDefault(0);
    AddAttr<int>("peer", "(int), attribute 1 for p_recv op.")
        .SetDefault(0);
    AddAttr<int>("dtype", "(int), attribute 2 for p_recv op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddAttr<bool>("dynamic_shape", "(bool), attribute 3 for p_recv op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of p_recv op.
)DOC");
  }
};


class PRecvOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(p_recv, PRecvInferShapeFunctor,
                            PD_INFER_META(phi::PRecvInferMeta));



class PRecvArrayOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("out", "(Tensor), output 0 of p_recv_array op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for p_recv_array op.")
        .SetDefault(0);
    AddAttr<int>("peer", "(int), attribute 1 for p_recv_array op.")
        .SetDefault(0);
    AddAttr<int>("dtype", "(int), attribute 2 for p_recv_array op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddAttr<std::vector<int>>("out_shape", "(std::vector<int>), attribute 3 for p_recv_array op.")
        .SetDefault({});
    AddComment(R"DOC(
TODO: Documentation of p_recv_array op.
)DOC");
  }
};


class PRecvArrayOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(p_recv_array, PRecvArrayInferShapeFunctor,
                            PD_INFER_META(phi::PRecvArrayInferMeta));



class Pool2dOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of pool2d op.");
    AddOutput("Out", "(Tensor), output 0 of pool2d op.");
    AddAttr<std::vector<int>>("ksize", "(std::vector<int>), attribute 0 for pool2d op.")

        .SupportTensor();
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 1 for pool2d op.")
        .SetDefault({1,1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 2 for pool2d op.")
        .SetDefault({0,0});
    AddAttr<bool>("ceil_mode", "(bool), attribute 3 for pool2d op.")
        .SetDefault(false);
    AddAttr<bool>("exclusive", "(bool), attribute 4 for pool2d op.")
        .SetDefault(true);
    AddAttr<std::string>("data_format", "(std::string), attribute 5 for pool2d op.")
        .SetDefault("NCHW");
    AddAttr<std::string>("pooling_type", "(std::string), attribute 6 for pool2d op.")
        .SetDefault("");
    AddAttr<bool>("global_pooling", "(bool), attribute 7 for pool2d op.")
        .SetDefault(false);
    AddAttr<bool>("adaptive", "(bool), attribute 8 for pool2d op.")
        .SetDefault(false);
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 9 for pool2d op.")
        .SetDefault("EXPLICIT");
    AddAttr<bool>("use_cudnn", "(bool), attribute 10 for pool2d op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of pool2d op.
)DOC");
  }
};


class Pool2dOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetPoolExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pool2d, Pool2dInferShapeFunctor,
                            PD_INFER_META(phi::Pool2DInferMeta));



class Pool3dOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of pool3d op.");
    AddOutput("Out", "(Tensor), output 0 of pool3d op.");
    AddAttr<std::vector<int>>("ksize", "(std::vector<int>), attribute 0 for pool3d op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 1 for pool3d op.")
        .SetDefault({1,1,1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 2 for pool3d op.")
        .SetDefault({0,0,0});
    AddAttr<bool>("ceil_mode", "(bool), attribute 3 for pool3d op.")
        .SetDefault(false);
    AddAttr<bool>("exclusive", "(bool), attribute 4 for pool3d op.")
        .SetDefault(true);
    AddAttr<std::string>("data_format", "(std::string), attribute 5 for pool3d op.")
        .SetDefault("NCDHW");
    AddAttr<std::string>("pooling_type", "(std::string), attribute 6 for pool3d op.")
        .SetDefault("");
    AddAttr<bool>("global_pooling", "(bool), attribute 7 for pool3d op.")
        .SetDefault(false);
    AddAttr<bool>("adaptive", "(bool), attribute 8 for pool3d op.")
        .SetDefault(false);
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 9 for pool3d op.")
        .SetDefault("EXPLICIT");
    AddAttr<bool>("use_cudnn", "(bool), attribute 10 for pool3d op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of pool3d op.
)DOC");
  }
};


class Pool3dOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetPoolExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pool3d, Pool3dInferShapeFunctor,
                            PD_INFER_META(phi::PoolInferMeta));



class ReduceProdOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_prod op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_prod op.");
    AddAttr<std::vector<int>>("dim", "(std::vector<int>), attribute 0 for reduce_prod op.")
        .SetDefault({0})
        .SupportTensor();
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_prod op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_prod op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_prod op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_prod op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_prod op.
)DOC");
  }
};


class ReduceProdOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_prod, ReduceProdInferShapeFunctor,
                            PD_INFER_META(phi::ReduceIntArrayAxisInferMetaBase));



class RandintOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of randint op.");
    AddAttr<int>("low", "(int), attribute 0 for randint op.")
    ;
    AddAttr<int>("high", "(int), attribute 1 for randint op.")
    ;
    AddInput("ShapeTensor", "attribute 2 for randint op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("ShapeTensorList", "attribute 2 for randint op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("shape", "(std::vector<int64_t>), attribute 2 for randint op.")
        .SetDefault({});
    AddAttr<int>("dtype", "(int), attribute 3 for randint op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::INT64)));
    AddAttr<int>("seed", "(int), attribute 4 for randint op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of randint op.
)DOC");
  }
};


class RandintOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(randint, RandintInferShapeFunctor,
                            PD_INFER_META(phi::RandintInferMeta));



class RandpermOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of randperm op.");
    AddAttr<int>("n", "(int), attribute 0 for randperm op.")
    ;
    AddAttr<int>("dtype", "(int), attribute 1 for randperm op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::INT64)));
    AddComment(R"DOC(
TODO: Documentation of randperm op.
)DOC");
  }
};


class RandpermOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(randperm, RandpermInferShapeFunctor,
                            PD_INFER_META(phi::RandpermInferMeta));



class ReduceOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of reduce op.");
    AddOutput("out", "(Tensor), output 0 of reduce op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for reduce op.")
        .SetDefault(0);
    AddAttr<int>("root_id", "(int), attribute 1 for reduce op.")
        .SetDefault(0);
    AddAttr<int>("reduce_type", "(int), attribute 2 for reduce op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of reduce op.
)DOC");
  }
};


class ReduceOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reduce, ReduceInferShapeFunctor,
                            PD_INFER_META(phi::DistReduceInferMeta));



class ReduceScatterOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of reduce_scatter op.");
    AddOutput("out", "(Tensor), output 0 of reduce_scatter op.");
    AddAttr<int>("ring_id", "(int), attribute 0 for reduce_scatter op.")
        .SetDefault(0);
    AddAttr<int>("nranks", "(int), attribute 1 for reduce_scatter op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of reduce_scatter op.
)DOC");
  }
};


class ReduceScatterOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_scatter, ReduceScatterInferShapeFunctor,
                            PD_INFER_META(phi::ReduceScatterInferMeta));



class ElementwiseModOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of elementwise_mod op.");
    AddInput("Y", "(Tensor), input 1 of elementwise_mod op.");
    AddOutput("Out", "(Tensor), output 0 of elementwise_mod op.");
    AddAttr<int>("axis", "(int), attribute 0 for elementwise_mod op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of elementwise_mod op.
)DOC");
  }
};


class ElementwiseModOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
     auto data_type =
          OperatorWithKernel::IndicateOrPromoteVarDataTypes(ctx, "X", "Y");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_mod, ElementwiseModInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseRawInferMeta));
DECLARE_INPLACE_OP_INFERER(ElementwiseModInplaceInferer,
                           {"X", "Out"});



class RnnOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of rnn op.");
    AddInput("PreState", "(Tensor[]), input 1 of rnn op.")
        .AsDuplicable();
    AddInput("WeightList", "(Tensor[]), input 2 of rnn op.")
        .AsDuplicable();
    AddInput("SequenceLength", "(Tensor), input 3 of rnn op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of rnn op.");
    AddOutput("DropoutState", "(Tensor), output 1 of rnn op.")
        .AsDispensable();
    AddOutput("State", "(Tensor[]), output 2 of rnn op.")
        .AsDuplicable();
    AddOutput("Reserve", "(Tensor), output 3 of rnn op.")
        .AsIntermediate();
    AddAttr<float>("dropout_prob", "(float), attribute 0 for rnn op.")
        .SetDefault(0.0);
    AddAttr<bool>("is_bidirec", "(bool), attribute 1 for rnn op.")
        .SetDefault(false);
    AddAttr<int>("input_size", "(int), attribute 2 for rnn op.")
        .SetDefault(10);
    AddAttr<int>("hidden_size", "(int), attribute 3 for rnn op.")
        .SetDefault(100);
    AddAttr<int>("num_layers", "(int), attribute 4 for rnn op.")
        .SetDefault(1);
    AddAttr<std::string>("mode", "(std::string), attribute 5 for rnn op.")
        .SetDefault("RNN_TANH");
    AddAttr<int>("seed", "(int), attribute 6 for rnn op.")
        .SetDefault(0);
    AddAttr<bool>("is_test", "(bool), attribute 7 for rnn op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of rnn op.
)DOC");
  }
};


class RnnOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(rnn, RnnInferShapeFunctor,
                            PD_INFER_META(phi::RnnInferMeta));



class ShareBufferOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor[]), input 0 of share_buffer op.")
        .AsDuplicable();
    AddOutput("Out", "(Tensor[]), output 0 of share_buffer op.")
        .AsDuplicable();
    AddOutput("XOut", "(Tensor[]), output 1 of share_buffer op.")
        .AsDuplicable();
    AddAttr<std::vector<bool>>("share_dims_and_dtype", "(std::vector<bool>), attribute 0 for share_buffer op.")
        .SetDefault({});
    AddComment(R"DOC(
TODO: Documentation of share_buffer op.
)DOC");
  }
};


class ShareBufferOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(share_buffer, ShareBufferInferShapeFunctor,
                            PD_INFER_META(phi::ShareBufferInferMeta));



class SoftmaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of softmax op.");
    AddOutput("Out", "(Tensor), output 0 of softmax op.");
    AddAttr<int>("axis", "(int), attribute 0 for softmax op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of softmax op.
)DOC");
  }
};


class SoftmaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetSoftmaxExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(softmax, SoftmaxInferShapeFunctor,
                            PD_INFER_META(phi::SoftmaxInferMeta));
DECLARE_INPLACE_OP_INFERER(SoftmaxInplaceInferer,
                           {"X", "Out"});



class StridedSliceOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of strided_slice op.");
    AddOutput("Out", "(Tensor), output 0 of strided_slice op.");
    AddAttr<std::vector<int>>("axes", "(std::vector<int>), attribute 0 for strided_slice op.")
    ;
    AddInput("StartsTensor", "attribute 1 for strided_slice op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("StartsTensorList", "attribute 1 for strided_slice op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int>>("starts", "(std::vector<int>), attribute 1 for strided_slice op.")
        .SetDefault({});
    AddInput("EndsTensor", "attribute 2 for strided_slice op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("EndsTensorList", "attribute 2 for strided_slice op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int>>("ends", "(std::vector<int>), attribute 2 for strided_slice op.")
        .SetDefault({});
    AddInput("StridesTensor", "attribute 3 for strided_slice op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("StridesTensorList", "attribute 3 for strided_slice op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 3 for strided_slice op.")
        .SetDefault({});
    AddAttr<std::vector<int>>("infer_flags", "(std::vector<int>), attribute 4 for strided_slice op.")
        .SetDefault({});
    AddAttr<std::vector<int>>("decrease_axis", "(std::vector<int>), attribute 5 for strided_slice op.")
        .SetDefault({});
    AddComment(R"DOC(
TODO: Documentation of strided_slice op.
)DOC");
  }
};


class StridedSliceOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetStridedSliceExpectedKernelType(ctx, this);
  }

};


class StridedSliceInferVarType : public framework::VarTypeInference {
 public:
  void operator()(framework::InferVarTypeContext *ctx) const override {
    ctx->SetOutputType("Out", ctx->GetInputType("Input"));
    ctx->SetOutputDataType("Out", ctx->GetInputDataType("Input"));
  }
};


DECLARE_INFER_SHAPE_FUNCTOR(strided_slice, StridedSliceInferShapeFunctor,
                            PD_INFER_META(phi::StridedSliceRawInferMeta));



class ReduceSumOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reduce_sum op.");
    AddOutput("Out", "(Tensor), output 0 of reduce_sum op.");
    AddAttr<std::vector<int>>("dim", "(std::vector<int>), attribute 0 for reduce_sum op.")
        .SetDefault({0})
        .SupportTensor();
    AddAttr<bool>("keep_dim", "(bool), attribute 1 for reduce_sum op.")
        .SetDefault(false);
    AddAttr<bool>("reduce_all", "(bool), attribute 2 for reduce_sum op.")
        .SetDefault(false);
    AddAttr<int>("in_dtype", "(int), attribute 3 for reduce_sum op.")
        .SetDefault(-1);
    AddAttr<int>("out_dtype", "(int), attribute 4 for reduce_sum op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of reduce_sum op.
)DOC");
  }
};


class ReduceSumOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_sum, ReduceSumInferShapeFunctor,
                            PD_INFER_META(phi::SumRawInferMeta));



class SwishOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of swish op.");
    AddOutput("Out", "(Tensor), output 0 of swish op.");
    AddComment(R"DOC(
TODO: Documentation of swish op.
)DOC");
  }
};


class SwishOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(swish, SwishInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class TrilIndicesOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("out", "(Tensor), output 0 of tril_indices op.");
    AddAttr<int>("rows", "(int), attribute 0 for tril_indices op.")
        .SetDefault(0);
    AddAttr<int>("cols", "(int), attribute 1 for tril_indices op.")
        .SetDefault(0);
    AddAttr<int>("offset", "(int), attribute 2 for tril_indices op.")
        .SetDefault(0);
    AddAttr<int>("dtype", "(int), attribute 3 for tril_indices op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::INT64)));
    AddComment(R"DOC(
TODO: Documentation of tril_indices op.
)DOC");
  }
};


class TrilIndicesOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(tril_indices, TrilIndicesInferShapeFunctor,
                            PD_INFER_META(phi::TrilIndicesInferMeta));



class TrilTriuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of tril_triu op.");
    AddOutput("Out", "(Tensor), output 0 of tril_triu op.");
    AddAttr<int>("diagonal", "(int), attribute 0 for tril_triu op.")
        .SetDefault(0);
    AddAttr<bool>("lower", "(bool), attribute 1 for tril_triu op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of tril_triu op.
)DOC");
  }
};


class TrilTriuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(tril_triu, TrilTriuInferShapeFunctor,
                            PD_INFER_META(phi::TrilTriuInferMeta));



class TriuIndicesOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("out", "(Tensor), output 0 of triu_indices op.");
    AddAttr<int>("row", "(int), attribute 0 for triu_indices op.")
        .SetDefault(0);
    AddAttr<int>("col", "(int), attribute 1 for triu_indices op.")
        .SetDefault(0);
    AddAttr<int>("offset", "(int), attribute 2 for triu_indices op.")
        .SetDefault(0);
    AddAttr<int>("dtype", "(int), attribute 3 for triu_indices op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::INT64)));
    AddComment(R"DOC(
TODO: Documentation of triu_indices op.
)DOC");
  }
};


class TriuIndicesOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(triu_indices, TriuIndicesInferShapeFunctor,
                            PD_INFER_META(phi::TriuIndicesInferMeta));



class TruncatedGaussianRandomOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of truncated_gaussian_random op.");
    AddAttr<std::vector<int>>("shape", "(std::vector<int>), attribute 0 for truncated_gaussian_random op.")
    ;
    AddAttr<float>("mean", "(float), attribute 1 for truncated_gaussian_random op.")
        .SetDefault(.0f);
    AddAttr<float>("std", "(float), attribute 2 for truncated_gaussian_random op.")
        .SetDefault(1.0f);
    AddAttr<int>("seed", "(int), attribute 3 for truncated_gaussian_random op.")
        .SetDefault(0);
    AddAttr<int>("dtype", "(int), attribute 4 for truncated_gaussian_random op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddComment(R"DOC(
TODO: Documentation of truncated_gaussian_random op.
)DOC");
  }
};


class TruncatedGaussianRandomOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(truncated_gaussian_random, TruncatedGaussianRandomInferShapeFunctor,
                            PD_INFER_META(phi::TruncatedGaussianRandomInferMeta));



class UniformRandomOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddOutput("Out", "(Tensor), output 0 of uniform_random op.");
    AddInput("ShapeTensor", "attribute 0 for uniform_random op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("ShapeTensorList", "attribute 0 for uniform_random op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("shape", "(std::vector<int64_t>), attribute 0 for uniform_random op.")
        .SetDefault({});
    AddAttr<int>("dtype", "(int), attribute 1 for uniform_random op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::FLOAT32)));
    AddAttr<float>("min", "(float), attribute 2 for uniform_random op.")
        .SetDefault(-1.0f)
        .SupportTensor();
    AddAttr<float>("max", "(float), attribute 3 for uniform_random op.")
        .SetDefault(1.0f)
        .SupportTensor();
    AddAttr<int>("seed", "(int), attribute 4 for uniform_random op.")
        .SetDefault(0);
    AddAttr<int>("diag_num", "(int), attribute 5 for uniform_random op.")
        .SetDefault(0);
    AddAttr<int>("diag_step", "(int), attribute 6 for uniform_random op.")
        .SetDefault(0);
    AddAttr<float>("diag_val", "(float), attribute 7 for uniform_random op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of uniform_random op.
)DOC");
  }
};


class UniformRandomOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(uniform_random, UniformRandomInferShapeFunctor,
                            PD_INFER_META(phi::UniformRandomInferMeta));



class UniqueOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of unique op.");
    AddOutput("Out", "(Tensor), output 0 of unique op.");
    AddOutput("Indices", "(Tensor), output 1 of unique op.")
        .AsDispensable();
    AddOutput("Index", "(Tensor), output 2 of unique op.");
    AddOutput("Counts", "(Tensor), output 3 of unique op.")
        .AsDispensable();
    AddAttr<bool>("return_index", "(bool), attribute 0 for unique op.")
        .SetDefault(false);
    AddAttr<bool>("return_inverse", "(bool), attribute 1 for unique op.")
        .SetDefault(false);
    AddAttr<bool>("return_counts", "(bool), attribute 2 for unique op.")
        .SetDefault(false);
    AddAttr<std::vector<int>>("axis", "(std::vector<int>), attribute 3 for unique op.")
        .SetDefault({});
    AddAttr<int>("dtype", "(int), attribute 4 for unique op.")
        .SetDefault(static_cast<int>(framework::TransToProtoVarType(phi::DataType::INT64)));
    AddAttr<bool>("is_sorted", "(bool), attribute 5 for unique op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of unique op.
)DOC");
  }
};


class UniqueOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetUniqueExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(unique, UniqueInferShapeFunctor,
                            PD_INFER_META(phi::UniqueRawInferMeta));



class UnpoolOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of unpool op.");
    AddInput("Indices", "(Tensor), input 1 of unpool op.");
    AddOutput("Out", "(Tensor), output 0 of unpool op.");
    AddAttr<std::vector<int>>("ksize", "(std::vector<int>), attribute 0 for unpool op.")
    ;
    AddAttr<std::string>("unpooling_type", "(std::string), attribute 1 for unpool op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 2 for unpool op.")
        .SetDefault({1,1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 3 for unpool op.")
        .SetDefault({0,0});
    AddAttr<std::vector<int>>("output_size", "(std::vector<int>), attribute 4 for unpool op.")
        .SetDefault({0,0})
        .SupportTensor();
    AddAttr<std::string>("data_format", "(std::string), attribute 5 for unpool op.")
        .SetDefault("NCHW");
    AddComment(R"DOC(
TODO: Documentation of unpool op.
)DOC");
  }
};


class UnpoolOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(unpool, UnpoolInferShapeFunctor,
                            PD_INFER_META(phi::UnpoolInferMeta));




template <typename T>
class ReduceAmaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_amax_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }
  }
};


class ReduceAmaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_amax_grad, ReduceAmaxGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class ReduceAminGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_amin_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }
  }
};


class ReduceAminGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_amin_grad, ReduceAminGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


template <typename T>
class AssignGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("assign");

    grad_op->SetInput("X", this->OutputGrad("Out"));

    grad_op->SetOutput("Out", this->InputGrad("X"));


  }
};

class AssignCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing assign_grad composite func";
    prim::assign_grad<prim::DescTensor>(out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class Conv2dTransposeGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("conv2d_transpose_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput("Filter", this->Input("Filter"));
    grad_op->SetInput("Bias", this->Input("Bias"));
    grad_op->SetInput(GradVarName("Output"), this->OutputGrad("Output"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));
    grad_op->SetOutput(GradVarName("Filter"), this->InputGrad("Filter"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("OutputSizeTensor")) {
      grad_op->SetInput("OutputSizeTensor", this->Input("OutputSizeTensor"));
    }
    if (this->HasInput("OutputSizeTensorList")) {
      grad_op->SetInput("OutputSizeTensorList", this->Input("OutputSizeTensorList"));
    }
  }
};


class Conv2dTransposeGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(conv2d_transpose_grad, Conv2dTransposeGradInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dTransposeGradInferMeta));



template <typename T>
class Conv2dTransposeGradGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("conv2d_transpose_grad_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput("Filter", this->Input("Filter"));
    grad_op->SetInput("grad_out", this->Input(GradVarName("Output")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("Input")));
    grad_op->SetInput(GradVarName("grad_filter"), this->OutputGrad(GradVarName("Filter")));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));
    grad_op->SetOutput(GradVarName("Filter"), this->InputGrad("Filter"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Output")));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("OutputSizeTensor")) {
      grad_op->SetInput("OutputSizeTensor", this->Input("OutputSizeTensor"));
    }
    if (this->HasInput("OutputSizeTensorList")) {
      grad_op->SetInput("OutputSizeTensorList", this->Input("OutputSizeTensorList"));
    }
  }
};


class Conv2dTransposeGradGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(conv2d_transpose_grad_grad, Conv2dTransposeGradGradInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dTransposeDoubleGradInferMeta));



template <typename T>
class DeformableConvGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("deformable_conv_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput("Offset", this->Input("Offset"));
    grad_op->SetInput("Filter", this->Input("Filter"));
    grad_op->SetInput("Mask", this->Input("Mask"));
    grad_op->SetInput(GradVarName("Output"), this->OutputGrad("Output"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));
    grad_op->SetOutput(GradVarName("Offset"), this->InputGrad("Offset"));
    grad_op->SetOutput(GradVarName("Filter"), this->InputGrad("Filter"));
    grad_op->SetOutput(GradVarName("Mask"), this->InputGrad("Mask"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class DeformableConvGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(deformable_conv_grad, DeformableConvGradInferShapeFunctor,
                            PD_INFER_META(phi::DeformableConvGradInferMeta));



template <typename T>
class DepthwiseConv2dTransposeGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("depthwise_conv2d_transpose_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput("Filter", this->Input("Filter"));
    grad_op->SetInput("Bias", this->Input("Bias"));
    grad_op->SetInput(GradVarName("Output"), this->OutputGrad("Output"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));
    grad_op->SetOutput(GradVarName("Filter"), this->InputGrad("Filter"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("OutputSizeTensor")) {
      grad_op->SetInput("OutputSizeTensor", this->Input("OutputSizeTensor"));
    }
    if (this->HasInput("OutputSizeTensorList")) {
      grad_op->SetInput("OutputSizeTensorList", this->Input("OutputSizeTensorList"));
    }
  }
};


class DepthwiseConv2dTransposeGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(depthwise_conv2d_transpose_grad, DepthwiseConv2dTransposeGradInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dTransposeGradInferMeta));



template <typename T>
class EinsumGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("einsum_grad");

    grad_op->SetInput("x_shape", this->Output("XShape"));
    grad_op->SetInput("InnerCache", this->Output("InnerCache"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("Operands"), this->InputGrad("Operands", false));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class EinsumGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(einsum_grad, EinsumGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedMultiInferMeta));



template <typename T>
class ElementwisePowGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elementwise_pow_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ElementwisePowGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_pow_grad, ElementwisePowGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));


class ElementwisePowCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto y = this->GetSingleForwardInput("Y");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto y_grad_t = this->GetSingleInputGrad("Y");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto y_grad = this->GetOutputPtr(&y_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto y_grad_name = this->GetOutputName(y_grad_t);

    //call composite backward func
    VLOG(6) << "Runing elementwise_pow_grad composite func";
    prim::elementwise_pow_grad<prim::DescTensor>(x, y, out_grad, x_grad, y_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(y_grad_t, y_grad_name);

  }
};

template <typename T>
class LookupTableV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("lookup_table_v2_grad");

    grad_op->SetInput("Ids", this->Input("Ids"));
    grad_op->SetInput("W", this->Input("W"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("W"), this->InputGrad("W"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LookupTableV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


class LookupTableV2GradInferVarType : public framework::VarTypeInference {
 public:
  void operator()(framework::InferVarTypeContext* ctx) const override {
    auto out_var_name = framework::GradVarName("W");
    auto attr = ctx->GetAttr("is_sparse");
    bool is_sparse = PADDLE_GET(bool, attr);
    if (is_sparse) {
      VLOG(3) << "lookup_table_v2_grad op " << framework::GradVarName("W")
              << " is set to SelectedRows";
      ctx->SetOutputType(out_var_name,
                         framework::proto::VarType::SELECTED_ROWS);
    } else {
      VLOG(3) << "lookup_table_v2_grad op " << framework::GradVarName("W")
              << " is set to phi::DenseTensor";
      ctx->SetOutputType(out_var_name, framework::proto::VarType::LOD_TENSOR);
    }
    ctx->SetOutputDataType(out_var_name, ctx->GetInputDataType("W"));
  }
};


DECLARE_INFER_SHAPE_FUNCTOR(lookup_table_v2_grad, LookupTableV2GradInferShapeFunctor,
                            PD_INFER_META(phi::EmbeddingGradInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(LookupTableV2GradNoNeedBufferVarInferer,
                                    "W");

template <typename T>
class ExponentialGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fill_any_like");

    grad_op->SetInput("X", this->OutputGrad("Out"));

    grad_op->SetOutput("Out", this->InputGrad("X"));


    grad_op->SetAttr("value", 0.0f);
  }
};


template <typename T>
class FrobeniusNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("frobenius_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }
  }
};


class FrobeniusNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(frobenius_norm_grad, FrobeniusNormGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class HardSwishGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("hard_swish_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class HardSwishGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(hard_swish_grad, HardSwishGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(HardSwishGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class MatmulV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("matmul_v2_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MatmulV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(matmul_v2_grad, MatmulV2GradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class MatmulV2GradGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("matmul_v2_grad_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput("grad_out", this->Input(GradVarName("Out")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));
    grad_op->SetInput(GradVarName("grad_y"), this->OutputGrad(GradVarName("Y")));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MatmulV2GradGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(matmul_v2_grad_grad, MatmulV2GradGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralTernaryGradInferMeta));


class MatmulV2GradCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto y = this->GetSingleForwardInput("Y");
    auto grad_out = this->GetSingleForwardInput(GradVarName("Out"));
    auto grad_x_grad = this->GetOptionalSingleOutputGrad(GradVarName("X"));
    auto grad_y_grad = this->GetOptionalSingleOutputGrad(GradVarName("Y"));


    //get attr
    const bool transpose_x = this->Attr<bool>("trans_x");
    const bool transpose_y = this->Attr<bool>("trans_y");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto y_grad_t = this->GetSingleInputGrad("Y");
    auto grad_out_grad_t = this->GetSingleInputGrad(GradVarName("Out"));

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto y_grad = this->GetOutputPtr(&y_grad_t);
    auto grad_out_grad = this->GetOutputPtr(&grad_out_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto y_grad_name = this->GetOutputName(y_grad_t);
    auto grad_out_grad_name = this->GetOutputName(grad_out_grad_t);

    //call composite backward func
    VLOG(6) << "Runing matmul_double_grad composite func";
    prim::matmul_double_grad<prim::DescTensor>(x, y, grad_out, grad_x_grad, grad_y_grad, transpose_x, transpose_y, x_grad, y_grad, grad_out_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(y_grad_t, y_grad_name);
    this->RecoverOutputName(grad_out_grad_t, grad_out_grad_name);

  }
};

template <typename T>
class MatmulV2TripleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("matmul_v2_triple_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput("grad_out", this->Input("grad_out"));
    grad_op->SetInput("grad_grad_x", this->Input(GradVarName("grad_x")));
    grad_op->SetInput("grad_grad_y", this->Input(GradVarName("grad_y")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));
    grad_op->SetInput(GradVarName("grad_y"), this->OutputGrad(GradVarName("Y")));
    grad_op->SetInput(GradVarName("grad_grad_out"), this->OutputGrad(GradVarName("grad_out")));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad("grad_out"));
    grad_op->SetOutput(GradVarName("grad_grad_x"), this->InputGrad(GradVarName("grad_x")));
    grad_op->SetOutput(GradVarName("grad_grad_y"), this->InputGrad(GradVarName("grad_y")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MatmulV2TripleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(matmul_v2_triple_grad, MatmulV2TripleGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralQuinaryGradInferMeta));



template <typename T>
class ReduceMaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_max_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }
  }
};


class ReduceMaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_max_grad, ReduceMaxGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


class ReduceMaxCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out = this->GetSingleForwardOutput("Out");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr
    const std::vector<int> axis = this->Attr<std::vector<int>>("dim");
    const bool keepdim = this->Attr<bool>("keep_dim");
    const bool reduce_all = this->Attr<bool>("reduce_all");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing max_grad composite func";
    prim::max_grad<prim::DescTensor>(x, out, out_grad, axis, keepdim, reduce_all, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class ElementwiseMaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elementwise_max_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ElementwiseMaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_max_grad, ElementwiseMaxGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));


class ElementwiseMaxCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto y = this->GetSingleForwardInput("Y");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto y_grad_t = this->GetSingleInputGrad("Y");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto y_grad = this->GetOutputPtr(&y_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto y_grad_name = this->GetOutputName(y_grad_t);

    //call composite backward func
    VLOG(6) << "Runing maximum_grad composite func";
    prim::maximum_grad<prim::DescTensor>(x, y, out_grad, x_grad, y_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(y_grad_t, y_grad_name);

  }
};

template <typename T>
class ReduceMinGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_min_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }
  }
};


class ReduceMinGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_min_grad, ReduceMinGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class ElementwiseMinGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("elementwise_min_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ElementwiseMinGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (framework::IsComplexType(expected_kernel_type.dtype())) {
        // only promote inputs’s types when contains complex input
          return phi::KernelKey(tensor.place(), tensor.layout(), tensor.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(elementwise_min_grad, ElementwiseMinGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));


class ElementwiseMinCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto y = this->GetSingleForwardInput("Y");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto y_grad_t = this->GetSingleInputGrad("Y");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto y_grad = this->GetOutputPtr(&y_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto y_grad_name = this->GetOutputName(y_grad_t);

    //call composite backward func
    VLOG(6) << "Runing minimum_grad composite func";
    prim::minimum_grad<prim::DescTensor>(x, y, out_grad, x_grad, y_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(y_grad_t, y_grad_name);

  }
};

template <typename T>
class NormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Norm", this->Output("Norm"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class NormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(norm_grad, NormGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class Pool2dGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pool2d_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("KernelSizeTensor")) {
      grad_op->SetInput("KernelSizeTensor", this->Input("KernelSizeTensor"));
    }
    if (this->HasInput("KernelSizeTensorList")) {
      grad_op->SetInput("KernelSizeTensorList", this->Input("KernelSizeTensorList"));
    }
  }
};


class Pool2dGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetPoolExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pool2d_grad, Pool2dGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class Pool2dDoubleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pool2d_double_grad");

    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("KernelSizeTensor")) {
      grad_op->SetInput("KernelSizeTensor", this->Input("KernelSizeTensor"));
    }
    if (this->HasInput("KernelSizeTensorList")) {
      grad_op->SetInput("KernelSizeTensorList", this->Input("KernelSizeTensorList"));
    }
  }
};


class Pool2dDoubleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetPoolDoubleGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pool2d_double_grad, Pool2dDoubleGradInferShapeFunctor,
                            PD_INFER_META(phi::Pool2DInferMeta));



template <typename T>
class Pool3dGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pool3d_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class Pool3dGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetPoolExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pool3d_grad, Pool3dGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class ReduceProdGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_prod_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("DimsTensor")) {
      grad_op->SetInput("DimsTensor", this->Input("DimsTensor"));
    }
    if (this->HasInput("DimsTensorList")) {
      grad_op->SetInput("DimsTensorList", this->Input("DimsTensorList"));
    }
  }
};


class ReduceProdGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_prod_grad, ReduceProdGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


class ReduceProdCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out = this->GetSingleForwardOutput("Out");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr
    const std::vector<int> dims = this->Attr<std::vector<int>>("dim");
    const bool keep_dim = this->Attr<bool>("keep_dim");
    const bool reduce_all = this->Attr<bool>("reduce_all");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing prod_grad composite func";
    prim::prod_grad<prim::DescTensor>(x, out, out_grad, dims, keep_dim, reduce_all, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class RnnGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("rnn_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput("PreState", this->Input("PreState"));
    grad_op->SetInput("WeightList", this->Input("WeightList"));
    grad_op->SetInput("SequenceLength", this->Input("SequenceLength"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput("DropoutState", this->Output("DropoutState"));
    grad_op->SetInput("Reserve", this->Output("Reserve"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));
    grad_op->SetInput(GradVarName("State"), this->OutputGrad("State"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));
    grad_op->SetOutput(GradVarName("PreState"), this->InputGrad("PreState", false));
    grad_op->SetOutput(GradVarName("WeightList"), this->InputGrad("WeightList", false));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class RnnGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(rnn_grad, RnnGradInferShapeFunctor,
                            PD_INFER_META(phi::RnnGradInferMeta));



template <typename T>
class SoftmaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("softmax_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class SoftmaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetSoftmaxGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(softmax_grad, SoftmaxGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


class SoftmaxCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto out = this->GetSingleForwardOutput("Out");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr
    const int axis = this->Attr<int>("axis");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing softmax_grad composite func";
    prim::softmax_grad<prim::DescTensor>(out, out_grad, axis, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class StridedSliceGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("strided_slice_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("StartsTensor")) {
      grad_op->SetInput("StartsTensor", this->Input("StartsTensor"));
    }
    if (this->HasInput("StartsTensorList")) {
      grad_op->SetInput("StartsTensorList", this->Input("StartsTensorList"));
    }
    if (this->HasInput("EndsTensor")) {
      grad_op->SetInput("EndsTensor", this->Input("EndsTensor"));
    }
    if (this->HasInput("EndsTensorList")) {
      grad_op->SetInput("EndsTensorList", this->Input("EndsTensorList"));
    }
    if (this->HasInput("StridesTensor")) {
      grad_op->SetInput("StridesTensor", this->Input("StridesTensor"));
    }
    if (this->HasInput("StridesTensorList")) {
      grad_op->SetInput("StridesTensorList", this->Input("StridesTensorList"));
    }
  }
};


class StridedSliceGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetStridedSliceGradExpectedKernelType(ctx, this);
  }

};


class StridedSliceGradInferVarType : public framework::VarTypeInference {
 public:
  void operator()(framework::InferVarTypeContext *ctx) const override {
    ctx->SetOutputType(framework::GradVarName("Input"),
                       ctx->GetInputType(framework::GradVarName("Out")));
    ctx->SetOutputDataType(
        framework::GradVarName("Input"),
        ctx->GetInputDataType(framework::GradVarName("Out")));
  }
};


DECLARE_INFER_SHAPE_FUNCTOR(strided_slice_grad, StridedSliceGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralUnaryGradInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(StridedSliceGradNoNeedBufferVarInferer,
                                    "Input");


template <typename T>
class ReduceSumGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_sum_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }
  }
};


class ReduceSumGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetReduceGradExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reduce_sum_grad, ReduceSumGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(ReduceSumGradNoNeedBufferVarInferer,
                                    "X");

class ReduceSumCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr
    const std::vector<int> axis = this->Attr<std::vector<int>>("dim");
    const bool keepdim = this->Attr<bool>("keep_dim");
    const bool reduce_all = this->Attr<bool>("reduce_all");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing sum_grad composite func";
    prim::sum_grad<prim::DescTensor>(x, out_grad, axis, keepdim, reduce_all, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};
template <typename T>
class SumDoubleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reduce_sum");

    grad_op->SetInput("X", this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput("Out", this->InputGrad(GradVarName("Out")));

    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }

    grad_op->SetAttr("dim", this->GetAttr("dim"));
    grad_op->SetAttr("keep_dim", this->GetAttr("keep_dim"));
    grad_op->SetAttr("reduce_all", this->GetAttr("reduce_all"));
  }
};


template <typename T>
class SwishGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("swish_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class SwishGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(swish_grad, SwishGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralUnaryGradInferMeta));
DECLARE_INPLACE_OP_INFERER(SwishGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class TrilTriuGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("tril_triu_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class TrilTriuGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(tril_triu_grad, TrilTriuGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class UnpoolGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("unpool_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Indices", this->Input("Indices"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("OutputSizeTensor")) {
      grad_op->SetInput("OutputSizeTensor", this->Input("OutputSizeTensor"));
    }
    if (this->HasInput("OutputSizeTensorList")) {
      grad_op->SetInput("OutputSizeTensorList", this->Input("OutputSizeTensorList"));
    }
  }
};


class UnpoolGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(unpool_grad, UnpoolGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


}  // namespace operators
}  // namespace paddle

namespace ops = paddle::operators;
REGISTER_OPERATOR(reduce_all, ops::ReduceAllOp,
                  ops::ReduceAllOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceAllInferShapeFunctor);


REGISTER_OPERATOR(all_gather, ops::AllGatherOp,
                  ops::AllGatherOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AllGatherInferShapeFunctor);


REGISTER_OPERATOR(all_reduce, ops::AllReduceOp,
                  ops::AllReduceOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AllReduceInferShapeFunctor);


REGISTER_OPERATOR(all_to_all, ops::AllToAllOp,
                  ops::AllToAllOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AllToAllInferShapeFunctor);


REGISTER_OPERATOR(reduce_amax, ops::ReduceAmaxOp,
                  ops::ReduceAmaxOpMaker,
                  ops::ReduceAmaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReduceAmaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceAmaxInferShapeFunctor);


REGISTER_OPERATOR(reduce_amin, ops::ReduceAminOp,
                  ops::ReduceAminOpMaker,
                  ops::ReduceAminGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReduceAminGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceAminInferShapeFunctor);


REGISTER_OPERATOR(reduce_any, ops::ReduceAnyOp,
                  ops::ReduceAnyOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceAnyInferShapeFunctor);


REGISTER_OPERATOR(range, ops::RangeOp,
                  ops::RangeOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::RangeInferShapeFunctor);


REGISTER_OPERATOR(assign, ops::AssignOp,
                  ops::AssignOpMaker,
                  ops::AssignGradOpMaker<paddle::framework::OpDesc>,
                  ops::AssignGradOpMaker<paddle::imperative::OpBase>,
                  ops::AssignInplaceInferer,
                  ops::AssignInferVarType,
                  ops::AssignCompositeGradOpMaker,
                  ops::AssignInferShapeFunctor);


REGISTER_OPERATOR(assign_value, ops::AssignValueOp,
                  ops::AssignValueOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AssignValueInferShapeFunctor);


REGISTER_OPERATOR(broadcast, ops::BroadcastOp,
                  ops::BroadcastOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::BroadcastInferShapeFunctor);


REGISTER_OPERATOR(conv2d_transpose, ops::Conv2dTransposeOp,
                  ops::Conv2dTransposeOpMaker,
                  ops::Conv2dTransposeGradOpMaker<paddle::framework::OpDesc>,
                  ops::Conv2dTransposeGradOpMaker<paddle::imperative::OpBase>,
                  ops::Conv2dTransposeInferShapeFunctor);

REGISTER_OP_VERSION(conv2d_transpose)
  .AddCheckpoint(
    R"ROC(Upgrade convtranspose add a new attribute [output_padding].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("output_padding", "In order to add additional size to one side of each dimension in the output.", std::vector<int>{}))
  .AddCheckpoint(
    R"ROC(Upgrade conv2d transpose to add a new attributes [force_fp32_output, mkldnn_data_type].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("force_fp32_output", "Force BF16 kernel output FP32, only used in MKL-DNN BF16.", false)
        .NewAttr("mkldnn_data_type", "Data type of mkldnn kernel.", "float32"))
;

REGISTER_OPERATOR(decode_jpeg, ops::DecodeJpegOp,
                  ops::DecodeJpegOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::DecodeJpegInferShapeFunctor);


REGISTER_OPERATOR(deformable_conv, ops::DeformableConvOp,
                  ops::DeformableConvOpMaker,
                  ops::DeformableConvGradOpMaker<paddle::framework::OpDesc>,
                  ops::DeformableConvGradOpMaker<paddle::imperative::OpBase>,
                  ops::DeformableConvInferShapeFunctor);


REGISTER_OPERATOR(depthwise_conv2d_transpose, ops::DepthwiseConv2dTransposeOp,
                  ops::DepthwiseConv2dTransposeOpMaker,
                  ops::DepthwiseConv2dTransposeGradOpMaker<paddle::framework::OpDesc>,
                  ops::DepthwiseConv2dTransposeGradOpMaker<paddle::imperative::OpBase>,
                  ops::DepthwiseConv2dTransposeInferShapeFunctor);

REGISTER_OP_VERSION(depthwise_conv2d_transpose)
  .AddCheckpoint(
    R"ROC(Upgrade convtranspose add a new attribute [output_padding].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("output_padding", "In order to add additional size to one side of each dimension in the output.", std::vector<int>{}))
;

REGISTER_OPERATOR(dist_concat, ops::DistConcatOp,
                  ops::DistConcatOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::DistConcatInferShapeFunctor);


REGISTER_OPERATOR(einsum, ops::EinsumOp,
                  ops::EinsumOpMaker,
                  ops::EinsumGradOpMaker<paddle::framework::OpDesc>,
                  ops::EinsumGradOpMaker<paddle::imperative::OpBase>,
                  ops::EinsumInferShapeFunctor);


REGISTER_OPERATOR(elementwise_pow, ops::ElementwisePowOp,
                  ops::ElementwisePowOpMaker,
                  ops::ElementwisePowGradOpMaker<paddle::framework::OpDesc>,
                  ops::ElementwisePowGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwisePowCompositeGradOpMaker,
                  ops::ElementwisePowInferShapeFunctor);

REGISTER_OP_VERSION(elementwise_pow)
  .AddCheckpoint(
    R"ROC(Register elementwise_pow for adding the attribute of Scale_y)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("Scale_y", "In order to support the function of scaling the input Y when using the operator of elementwise_pow.", 1.0))
;

REGISTER_OPERATOR(lookup_table_v2, ops::LookupTableV2Op,
                  ops::LookupTableV2OpMaker,
                  ops::LookupTableV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::LookupTableV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::LookupTableV2InferShapeFunctor);

REGISTER_OP_VERSION(lookup_table_v2)
  .AddCheckpoint(
    R"ROC(Upgrade flip, add new attr [axis] and delete attr [dims])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .BugfixWithBehaviorChanged("lookup_table_v2 support input type `int64`; after support input type `int32/int64`"))
;

REGISTER_OPERATOR(empty, ops::EmptyOp,
                  ops::EmptyOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EmptyInferShapeFunctor);


REGISTER_OPERATOR(equal, ops::EqualOp,
                  ops::EqualOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EqualInferShapeFunctor);

REGISTER_OP_VERSION(equal)
  .AddCheckpoint(
    R"ROC(Upgrade compare ops, add a new attribute [force_cpu])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("force_cpu", "In order to force fill output variable to gpu memory.", false))
;

REGISTER_OPERATOR(exponential, ops::ExponentialOp,
                  ops::ExponentialOpMaker,
                  ops::ExponentialGradOpMaker<paddle::framework::OpDesc>,
                  ops::ExponentialGradOpMaker<paddle::imperative::OpBase>,
                  ops::ExponentialInplaceInferer,
                  ops::ExponentialInferShapeFunctor);


REGISTER_OPERATOR(eye, ops::EyeOp,
                  ops::EyeOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EyeInferShapeFunctor);


REGISTER_OPERATOR(elementwise_floordiv, ops::ElementwiseFloordivOp,
                  ops::ElementwiseFloordivOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseFloordivInferShapeFunctor);


REGISTER_OPERATOR(frobenius_norm, ops::FrobeniusNormOp,
                  ops::FrobeniusNormOpMaker,
                  ops::FrobeniusNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::FrobeniusNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::FrobeniusNormInferShapeFunctor);


REGISTER_OPERATOR(fill_any_like, ops::FillAnyLikeOp,
                  ops::FillAnyLikeOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FillAnyLikeInferShapeFunctor);


REGISTER_OPERATOR(gaussian_random, ops::GaussianRandomOp,
                  ops::GaussianRandomOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GaussianRandomInferShapeFunctor);


REGISTER_OPERATOR(greater_equal, ops::GreaterEqualOp,
                  ops::GreaterEqualOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GreaterEqualInferShapeFunctor);

REGISTER_OP_VERSION(greater_equal)
  .AddCheckpoint(
    R"ROC(Upgrade compare ops, add a new attribute [force_cpu])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("force_cpu", "In order to force fill output variable to gpu memory.", false))
;

REGISTER_OPERATOR(greater_than, ops::GreaterThanOp,
                  ops::GreaterThanOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GreaterThanInferShapeFunctor);

REGISTER_OP_VERSION(greater_than)
  .AddCheckpoint(
    R"ROC(Upgrade compare ops, add a new attribute [force_cpu])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("force_cpu", "In order to force fill output variable to gpu memory.", false))
;

REGISTER_OPERATOR(hard_swish, ops::HardSwishOp,
                  ops::HardSwishOpMaker,
                  ops::HardSwishGradOpMaker<paddle::framework::OpDesc>,
                  ops::HardSwishGradOpMaker<paddle::imperative::OpBase>,
                  ops::HardSwishInferShapeFunctor);


REGISTER_OPERATOR(less_equal, ops::LessEqualOp,
                  ops::LessEqualOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LessEqualInferShapeFunctor);

REGISTER_OP_VERSION(less_equal)
  .AddCheckpoint(
    R"ROC(Upgrade compare ops, add a new attribute [force_cpu])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("force_cpu", "In order to force fill output variable to gpu memory.", false))
;

REGISTER_OPERATOR(less_than, ops::LessThanOp,
                  ops::LessThanOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LessThanInferShapeFunctor);

REGISTER_OP_VERSION(less_than)
  .AddCheckpoint(
    R"ROC(Upgrade compare ops, add a new attribute [force_cpu])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("force_cpu", "In order to force fill output variable to gpu memory.", false))
;

REGISTER_OPERATOR(linspace, ops::LinspaceOp,
                  ops::LinspaceOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LinspaceInferShapeFunctor);

REGISTER_OP_VERSION(linspace)
  .AddCheckpoint(
    R"ROC(Upgrade linspace to add a new attribute [dtype])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("dtype", "In order to change output data type", 5))
;

REGISTER_OPERATOR(matmul_v2, ops::MatmulV2Op,
                  ops::MatmulV2OpMaker,
                  ops::MatmulV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::MatmulV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::MatmulV2InferShapeFunctor);


REGISTER_OPERATOR(matrix_rank, ops::MatrixRankOp,
                  ops::MatrixRankOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MatrixRankInferShapeFunctor);


REGISTER_OPERATOR(reduce_max, ops::ReduceMaxOp,
                  ops::ReduceMaxOpMaker,
                  ops::ReduceMaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReduceMaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceMaxCompositeGradOpMaker,
                  ops::ReduceMaxInferShapeFunctor);


REGISTER_OPERATOR(elementwise_max, ops::ElementwiseMaxOp,
                  ops::ElementwiseMaxOpMaker,
                  ops::ElementwiseMaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::ElementwiseMaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseMaxCompositeGradOpMaker,
                  ops::ElementwiseMaxInferShapeFunctor);


REGISTER_OPERATOR(reduce_min, ops::ReduceMinOp,
                  ops::ReduceMinOpMaker,
                  ops::ReduceMinGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReduceMinGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceMinInferShapeFunctor);


REGISTER_OPERATOR(elementwise_min, ops::ElementwiseMinOp,
                  ops::ElementwiseMinOpMaker,
                  ops::ElementwiseMinGradOpMaker<paddle::framework::OpDesc>,
                  ops::ElementwiseMinGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseMinCompositeGradOpMaker,
                  ops::ElementwiseMinInferShapeFunctor);


REGISTER_OPERATOR(norm, ops::NormOp,
                  ops::NormOpMaker,
                  ops::NormGradOpMaker<paddle::framework::OpDesc>,
                  ops::NormGradOpMaker<paddle::imperative::OpBase>,
                  ops::NormInferShapeFunctor);


REGISTER_OPERATOR(not_equal, ops::NotEqualOp,
                  ops::NotEqualOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::NotEqualInferShapeFunctor);

REGISTER_OP_VERSION(not_equal)
  .AddCheckpoint(
    R"ROC(Upgrade compare ops, add a new attribute [force_cpu])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .ModifyAttr("force_cpu", "In order to force fill output variable to gpu memory.", false))
;

REGISTER_OPERATOR(one_hot_v2, ops::OneHotV2Op,
                  ops::OneHotV2OpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::OneHotV2InferShapeFunctor);


REGISTER_OPERATOR(p_recv, ops::PRecvOp,
                  ops::PRecvOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::PRecvInferShapeFunctor);


REGISTER_OPERATOR(p_recv_array, ops::PRecvArrayOp,
                  ops::PRecvArrayOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::PRecvArrayInferShapeFunctor);


REGISTER_OPERATOR(pool2d, ops::Pool2dOp,
                  ops::Pool2dOpMaker,
                  ops::Pool2dGradOpMaker<paddle::framework::OpDesc>,
                  ops::Pool2dGradOpMaker<paddle::imperative::OpBase>,
                  ops::Pool2dInferShapeFunctor);


REGISTER_OPERATOR(pool3d, ops::Pool3dOp,
                  ops::Pool3dOpMaker,
                  ops::Pool3dGradOpMaker<paddle::framework::OpDesc>,
                  ops::Pool3dGradOpMaker<paddle::imperative::OpBase>,
                  ops::Pool3dInferShapeFunctor);


REGISTER_OPERATOR(reduce_prod, ops::ReduceProdOp,
                  ops::ReduceProdOpMaker,
                  ops::ReduceProdGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReduceProdGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceProdCompositeGradOpMaker,
                  ops::ReduceProdInferShapeFunctor);


REGISTER_OPERATOR(randint, ops::RandintOp,
                  ops::RandintOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::RandintInferShapeFunctor);


REGISTER_OPERATOR(randperm, ops::RandpermOp,
                  ops::RandpermOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::RandpermInferShapeFunctor);


REGISTER_OPERATOR(reduce, ops::ReduceOp,
                  ops::ReduceOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceInferShapeFunctor);


REGISTER_OPERATOR(reduce_scatter, ops::ReduceScatterOp,
                  ops::ReduceScatterOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceScatterInferShapeFunctor);


REGISTER_OPERATOR(elementwise_mod, ops::ElementwiseModOp,
                  ops::ElementwiseModOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ElementwiseModInplaceInferer,
                  ops::ElementwiseModInferShapeFunctor);


REGISTER_OPERATOR(rnn, ops::RnnOp,
                  ops::RnnOpMaker,
                  ops::RnnGradOpMaker<paddle::framework::OpDesc>,
                  ops::RnnGradOpMaker<paddle::imperative::OpBase>,
                  ops::RnnInferShapeFunctor);


REGISTER_OPERATOR(share_buffer, ops::ShareBufferOp,
                  ops::ShareBufferOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::ShareBufferInferShapeFunctor);


REGISTER_OPERATOR(softmax, ops::SoftmaxOp,
                  ops::SoftmaxOpMaker,
                  ops::SoftmaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::SoftmaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::SoftmaxInplaceInferer,
                  ops::SoftmaxCompositeGradOpMaker,
                  ops::SoftmaxInferShapeFunctor);


REGISTER_OPERATOR(strided_slice, ops::StridedSliceOp,
                  ops::StridedSliceOpMaker,
                  ops::StridedSliceGradOpMaker<paddle::framework::OpDesc>,
                  ops::StridedSliceGradOpMaker<paddle::imperative::OpBase>,
                  ops::StridedSliceInferVarType,
                  ops::StridedSliceInferShapeFunctor);


REGISTER_OPERATOR(reduce_sum, ops::ReduceSumOp,
                  ops::ReduceSumOpMaker,
                  ops::ReduceSumGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReduceSumGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceSumCompositeGradOpMaker,
                  ops::ReduceSumInferShapeFunctor);


REGISTER_OPERATOR(swish, ops::SwishOp,
                  ops::SwishOpMaker,
                  ops::SwishGradOpMaker<paddle::framework::OpDesc>,
                  ops::SwishGradOpMaker<paddle::imperative::OpBase>,
                  ops::SwishInferShapeFunctor);


REGISTER_OPERATOR(tril_indices, ops::TrilIndicesOp,
                  ops::TrilIndicesOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::TrilIndicesInferShapeFunctor);


REGISTER_OPERATOR(tril_triu, ops::TrilTriuOp,
                  ops::TrilTriuOpMaker,
                  ops::TrilTriuGradOpMaker<paddle::framework::OpDesc>,
                  ops::TrilTriuGradOpMaker<paddle::imperative::OpBase>,
                  ops::TrilTriuInferShapeFunctor);


REGISTER_OPERATOR(triu_indices, ops::TriuIndicesOp,
                  ops::TriuIndicesOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::TriuIndicesInferShapeFunctor);


REGISTER_OPERATOR(truncated_gaussian_random, ops::TruncatedGaussianRandomOp,
                  ops::TruncatedGaussianRandomOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::TruncatedGaussianRandomInferShapeFunctor);


REGISTER_OPERATOR(uniform_random, ops::UniformRandomOp,
                  ops::UniformRandomOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::UniformRandomInferShapeFunctor);


REGISTER_OPERATOR(unique, ops::UniqueOp,
                  ops::UniqueOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::UniqueInferShapeFunctor);


REGISTER_OPERATOR(unpool, ops::UnpoolOp,
                  ops::UnpoolOpMaker,
                  ops::UnpoolGradOpMaker<paddle::framework::OpDesc>,
                  ops::UnpoolGradOpMaker<paddle::imperative::OpBase>,
                  ops::UnpoolInferShapeFunctor);


REGISTER_OPERATOR(reduce_amax_grad, ops::ReduceAmaxGradOp,
                  ops::ReduceAmaxGradInferShapeFunctor);


REGISTER_OPERATOR(reduce_amin_grad, ops::ReduceAminGradOp,
                  ops::ReduceAminGradInferShapeFunctor);


REGISTER_OPERATOR(conv2d_transpose_grad, ops::Conv2dTransposeGradOp,
                  ops::Conv2dTransposeGradGradOpMaker<paddle::framework::OpDesc>,
                  ops::Conv2dTransposeGradGradOpMaker<paddle::imperative::OpBase>,
                  ops::Conv2dTransposeGradInferShapeFunctor);


REGISTER_OPERATOR(conv2d_transpose_grad_grad, ops::Conv2dTransposeGradGradOp,
                  ops::Conv2dTransposeGradGradInferShapeFunctor);


REGISTER_OPERATOR(deformable_conv_grad, ops::DeformableConvGradOp,
                  ops::DeformableConvGradInferShapeFunctor);


REGISTER_OPERATOR(depthwise_conv2d_transpose_grad, ops::DepthwiseConv2dTransposeGradOp,
                  ops::DepthwiseConv2dTransposeGradInferShapeFunctor);


REGISTER_OPERATOR(einsum_grad, ops::EinsumGradOp,
                  ops::EinsumGradInferShapeFunctor);


REGISTER_OPERATOR(elementwise_pow_grad, ops::ElementwisePowGradOp,
                  ops::ElementwisePowGradInferShapeFunctor);


REGISTER_OPERATOR(lookup_table_v2_grad, ops::LookupTableV2GradOp,
                  ops::LookupTableV2GradInferVarType,
                  ops::LookupTableV2GradNoNeedBufferVarInferer,
                  ops::LookupTableV2GradInferShapeFunctor);


REGISTER_OPERATOR(frobenius_norm_grad, ops::FrobeniusNormGradOp,
                  ops::FrobeniusNormGradInferShapeFunctor);


REGISTER_OPERATOR(hard_swish_grad, ops::HardSwishGradOp,
                  ops::HardSwishGradInplaceInferer,
                  ops::HardSwishGradInferShapeFunctor);


REGISTER_OPERATOR(matmul_v2_grad, ops::MatmulV2GradOp,
                  ops::MatmulV2GradGradOpMaker<paddle::framework::OpDesc>,
                  ops::MatmulV2GradGradOpMaker<paddle::imperative::OpBase>,
                  ops::MatmulV2GradCompositeGradOpMaker,
                  ops::MatmulV2GradInferShapeFunctor);


REGISTER_OPERATOR(matmul_v2_grad_grad, ops::MatmulV2GradGradOp,
                  ops::MatmulV2TripleGradOpMaker<paddle::framework::OpDesc>,
                  ops::MatmulV2TripleGradOpMaker<paddle::imperative::OpBase>,
                  ops::MatmulV2GradGradInferShapeFunctor);


REGISTER_OPERATOR(matmul_v2_triple_grad, ops::MatmulV2TripleGradOp,
                  ops::MatmulV2TripleGradInferShapeFunctor);


REGISTER_OPERATOR(reduce_max_grad, ops::ReduceMaxGradOp,
                  ops::ReduceMaxGradInferShapeFunctor);


REGISTER_OPERATOR(elementwise_max_grad, ops::ElementwiseMaxGradOp,
                  ops::ElementwiseMaxGradInferShapeFunctor);


REGISTER_OPERATOR(reduce_min_grad, ops::ReduceMinGradOp,
                  ops::ReduceMinGradInferShapeFunctor);


REGISTER_OPERATOR(elementwise_min_grad, ops::ElementwiseMinGradOp,
                  ops::ElementwiseMinGradInferShapeFunctor);


REGISTER_OPERATOR(norm_grad, ops::NormGradOp,
                  ops::NormGradInferShapeFunctor);


REGISTER_OPERATOR(pool2d_grad, ops::Pool2dGradOp,
                  ops::Pool2dDoubleGradOpMaker<paddle::framework::OpDesc>,
                  ops::Pool2dDoubleGradOpMaker<paddle::imperative::OpBase>,
                  ops::Pool2dGradInferShapeFunctor);


REGISTER_OPERATOR(pool2d_double_grad, ops::Pool2dDoubleGradOp,
                  ops::Pool2dDoubleGradInferShapeFunctor);


REGISTER_OPERATOR(pool3d_grad, ops::Pool3dGradOp,
                  ops::Pool3dGradInferShapeFunctor);


REGISTER_OPERATOR(reduce_prod_grad, ops::ReduceProdGradOp,
                  ops::ReduceProdGradInferShapeFunctor);


REGISTER_OPERATOR(rnn_grad, ops::RnnGradOp,
                  ops::RnnGradInferShapeFunctor);


REGISTER_OPERATOR(softmax_grad, ops::SoftmaxGradOp,
                  ops::SoftmaxGradInferShapeFunctor);


REGISTER_OPERATOR(strided_slice_grad, ops::StridedSliceGradOp,
                  ops::StridedSliceGradInferVarType,
                  ops::StridedSliceGradNoNeedBufferVarInferer,
                  ops::StridedSliceGradInferShapeFunctor);


REGISTER_OPERATOR(reduce_sum_grad, ops::ReduceSumGradOp,
                  ops::SumDoubleGradOpMaker<paddle::framework::OpDesc>,
                  ops::SumDoubleGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReduceSumGradNoNeedBufferVarInferer,
                  ops::ReduceSumGradInferShapeFunctor);


REGISTER_OPERATOR(swish_grad, ops::SwishGradOp,
                  ops::SwishGradInplaceInferer,
                  ops::SwishGradInferShapeFunctor);


REGISTER_OPERATOR(tril_triu_grad, ops::TrilTriuGradOp,
                  ops::TrilTriuGradInferShapeFunctor);


REGISTER_OPERATOR(unpool_grad, ops::UnpoolGradOp,
                  ops::UnpoolGradInferShapeFunctor);


