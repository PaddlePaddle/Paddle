/* Copyright (c) 2022 PaddlePaddle Authors. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

  http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License. */

#pragma once

#include "paddle/fluid/framework/fleet/heter_ps/heter_comm_kernel.h"
#ifdef PADDLE_WITH_XPU
#include "xpu/kernel/cluster_header.h"
#include "xpu/kernel/simd.h"
#include "xpu/kernel/math.h"
#include <xpu/runtime.h>
#endif

#ifdef PADDLE_WITH_HETERPS
namespace paddle {
namespace framework {

#ifdef PADDLE_WITH_XPU

template <typename T>
__global__ void fill_idx(T* idx, size_t len) {
  int cid = core_id();
  int ncores = core_num();
  int nthreads = ncores * cluster_num();
  const int thread_id = ncores * cluster_id() + cid;
  for (int i = thread_id; i < static_cast<int>(len); i += nthreads) {
    idx[i] = i;
  }
}


template <typename T>
__global__ void calc_shard_offset(T* idx, T* left, T* right, size_t len) {
  int cid = core_id();
  int ncores = core_num();
  if (cid >= ncores) {
    return;
  }
  int thread_id = ncores * cluster_id() + cid;
  int nthreads = ncores * cluster_num();

  const size_t i = blockIdx.x * blockDim.x + threadIdx.x;
  if (i < len - 1) {
    if (idx[i] != idx[i + 1]) {
      right[idx[i]] = i;
      left[idx[i + 1]] = i + 1;
    }
  }
  if (i == 0) {
    left[idx[i]] = i;
  }
  if (i == (len - 1)) {
    right[idx[i]] = i;
  }

}


template <typename KeyType, typename T>
__global__ void calc_shard_index(KeyType* d_keys, size_t len, T* shard_index,
                                 int total_xpu) {
  int cid = core_id();
  int ncores = core_num();
  int nthreads = ncores * cluster_num();
  const int thread_id = ncores * cluster_id() + cid;
  for (int i = thread_id; i < static_cast<int>(len); i += nthreads) {
    shard_index[i] = d_keys[i] % total_xpu;
  }
}

template <typename KeyType, typename T>
__global__ void fill_shard_key(KeyType* d_shard_keys, KeyType* d_keys, T* idx,
                               size_t len) {
  int cid = core_id();
  int ncores = core_num();
  int nthreads = ncores * cluster_num();
  const int thread_id = ncores * cluster_id() + cid;
  for (int i = thread_id; i < static_cast<int>(len); i += nthreads) {
    d_shard_keys[i] = d_keys[idx[i]];
  }
}

template <typename KeyType, typename GradType, typename T>
__global__ void fill_shard_grads(KeyType* d_shard_keys, KeyType* d_keys,
                                 GradType* d_shard_grads, GradType* d_grads,
                                 T* idx, size_t len) {
  int cid = core_id();
  int ncores = core_num();
  int nthreads = ncores * cluster_num();
  const int thread_id = ncores * cluster_id() + cid;
  for (int i = thread_id; i < static_cast<int>(len); i += nthreads) {
    d_shard_keys[i] = d_keys[idx[i]];
    d_shard_grads[i] = d_grads[idx[i]];
  }
}

template <typename ValType, typename T>
__global__ void fill_dvals(ValType* d_shard_vals, ValType* d_vals, T* idx,
                           size_t len) {
  int cid = core_id();
  int ncores = core_num();
  int nthreads = ncores * cluster_num();
  const int thread_id = ncores * cluster_id() + cid;
  for (int i = thread_id; i < static_cast<int>(len); i += nthreads) {
    d_vals[idx[i]] = d_shard_vals[i];
  }
}

// xpu implementation of heter_comm_kernel.h

template <typename T, typename StreamType>
void HeterCommKernel::fill_idx(T* idx, size_t len, StreamType& stream) {
  fill_idx<<<4, 64, stream>>>(idx, len);
}

template <typename T, typename StreamType>
void HeterCommKernel::calc_shard_offset(T* idx, T* left, T* right, size_t len, StreamType& stream) {
  calc_shard_offset<<<<4, 64, stream>>>(idx,
                                        left, right, len);
}

template <typename KeyType, typename T, typename StreamType>
void HeterCommKernel::calc_shard_index(KeyType* d_keys, size_t len, T* shard_index,
                                 int total_xpu, StreamType& stream) {
  calc_shard_index<<<4, 64, stream>>>(d_keys, len, shard_index,
                                      total_xpu);
}

template <typename KeyType, typename T, typename StreamType>
void HeterCommKernel::fill_shard_key(KeyType* d_shard_keys, KeyType* d_keys, T* idx,
                               size_t len, StreamType& stream) {
  fill_shard_key<<<4, 64, stream>>>(d_shard_keys, d_keys, idx, len);
}

template <typename KeyType, typename GradType, typename T, typename StreamType>
void HeterCommKernel::fill_shard_grads(KeyType* d_shard_keys, KeyType* d_keys,
                                 GradType* d_shard_grads, GradType* d_grads,
                                 T* idx, size_t len, StreamType& stream) {
  fill_shard_grads<<<4, 64, stream>>>(d_shard_keys, d_keys,
                                      d_shard_grads, d_grads, idx,
                                      len);
}

template <typename ValType, typename T, typename StreamType>
void HeterCommKernel::fill_dvals(ValType* d_shard_vals, ValType* d_vals, T* idx,
                           size_t len, StreamType& stream) {
  fill_dvals<<<4, 64, stream>>>(d_shard_vals, d_vals, idx, len);
}

#endif

}  // end namespace framework
}  // end namespace paddle
#endif
