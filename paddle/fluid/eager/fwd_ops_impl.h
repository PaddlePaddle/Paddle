// Copyright (c) 2021 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

/**
 * This File should be automatically generated by coding auto generator.
 * All ops C++ autograd logic is defined here, in Python-C extension API
 * system we try to avoid any autograd related code, and move them all to
 * here.
 *
 * Currently, we just manually do some fwd autograd here. And we will replace
 * them with auto code generator later.
 * **/

#pragma once

#include "paddle/fluid/eager/autograd_meta.h"
#include "paddle/fluid/eager/grad_node_info.h"
#include "paddle/top/api/all.h"

namespace egr {
namespace autograd {
std::vector<pt::Tensor> scale(const pt::Tensor& x, float scale, float bias,
                              bool bias_after_scale, bool trace_backward) {
  Tensor out;
  auto* p_p_autograd_in = &EagerUtils::autograd_meta(x);
  auto* p_p_autograd_out = &EagerUtils::autograd_meta(out);
  if (!EagerUtils::ComputeRequireGrad(p_p_autograd_in, 1, p_p_autograd_out, 1,
                                      trace_backward)) {
    pt::Scale<float>(dev_ctx, x, scale, bias, bias_after_scale, out);
    return {out};
  }
}

}  // namespace autograd
}  // namespace egr
