# The operators included in this file are:
# 1) Operators defined only in PIR, dynamic graphs do not exist;
# 2) The definitions of static graphs and dynamic graphs are inconsistent, but the final definition plan has not yet been clarified.
# After the definition is clearly defined, migrate to paddle/phi/ops/yaml/inconsistent/update_ops.yaml or paddle/phi/ops/yaml/ops.yaml

- backward_op : add_double_grad
  forward : add_grad (Tensor x, Tensor y, Tensor grad_out, int axis = -1) -> Tensor(grad_x), Tensor(grad_y)
  args : (Tensor y, Tensor grad_out, Tensor grad_x_grad, Tensor grad_y_grad, int axis = -1)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [grad_out]
  kernel :
    func : add_double_grad
  optional : grad_x_grad, grad_y_grad, grad_out_grad
  backward : add_triple_grad
  inplace : (grad_x_grad -> grad_out_grad)
  composite : add_double_grad(y, grad_out, grad_x_grad, grad_y_grad, axis, grad_out_grad)

- backward_op : add_grad
  forward : add (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, int axis = -1)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
    spmd_rule : ElementwiseBinaryGradInferSpmd
  kernel :
    func : add_grad
    data_type: out_grad
  no_need_buffer : x, y
  composite : add_grad(x, y, out_grad, axis, x_grad, y_grad)
  backward : add_double_grad
  inplace : (out_grad -> x_grad)

# this add_n_grad is only used for generate AddNOp::Vjp func
- backward_op : add_n_grad
  forward : add_n (Tensor[] inputs) -> Tensor(out)
  args : (Tensor[] inputs, Tensor out_grad)
  output : Tensor[](inputs_grad)

- backward_op : add_triple_grad
  forward : add_double_grad (Tensor y, Tensor grad_out, Tensor grad_grad_x, Tensor grad_grad_y, int axis = -1) -> Tensor(grad_grad_out)
  args : (Tensor grad_grad_x, Tensor grad_grad_y, Tensor grad_grad_out_grad, int axis = -1)
  output : Tensor(grad_grad_x_grad), Tensor(grad_grad_y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [grad_grad_x, grad_grad_y]
  kernel :
    func : add_triple_grad
  inplace : (grad_grad_out_grad -> grad_grad_x_grad)
  composite : add_triple_grad (grad_grad_x, grad_grad_y, grad_grad_out_grad, axis, grad_grad_x_grad, grad_grad_y_grad )

- backward_op : assign_grad
  forward : assign (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  composite: assign_grad(out_grad, x_grad)
  invoke : assign(out_grad)

- backward_op : assign_out__grad
  forward : assign_out_ (Tensor x, Tensor output) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
  kernel :
    func : assign
  inplace : (out_grad -> x_grad)

- backward_op : batch_norm_double_grad
  forward : batch_norm_grad (Tensor x, Tensor scale, Tensor bias, Tensor out_mean, Tensor out_variance, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor grad_out, float momentum, float epsilon, str data_format, bool is_test, bool use_global_stats, bool trainable_statistics) -> Tensor(grad_x), Tensor(grad_scale), Tensor(grad_bias)
  args : (Tensor x, Tensor scale, Tensor out_mean, Tensor out_variance, Tensor saved_mean, Tensor saved_variance, Tensor grad_out,  Tensor grad_x_grad, Tensor grad_scale_grad, Tensor grad_bias_grad, float momentum, float epsilon, str data_format, bool is_test, bool use_global_stats, bool trainable_statistics)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, scale, x]
  kernel :
    func : batch_norm_double_grad
    data_type : x
  optional : scale, out_mean, out_variance, grad_x_grad, grad_scale_grad, grad_bias_grad
  inplace : (grad_out -> grad_out_grad)

- backward_op : batch_norm_grad
  forward : batch_norm (Tensor x, Tensor mean, Tensor variance, Tensor scale, Tensor bias, bool is_test, float momentum, float epsilon, str data_format, bool use_global_stats, bool trainable_statistics) -> Tensor(out), Tensor(mean_out), Tensor(variance_out), Tensor(saved_mean), Tensor(saved_variance), Tensor(reserve_space)
  args : (Tensor x, Tensor scale, Tensor bias, Tensor mean_out, Tensor variance_out, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor out_grad, float momentum, float epsilon, str data_format, bool is_test, bool use_global_stats, bool trainable_statistics)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, scale, bias]
  kernel :
    func : batch_norm_grad
    data_type : out_grad
  optional : scale, bias, mean_out, variance_out, reserve_space
  composite: batch_norm_grad(x, scale, bias, mean_out, variance_out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, data_format, is_test, use_global_stats, trainable_statistics, x_grad, scale_grad, bias_grad)
  backward : batch_norm_double_grad

- backward_op : c_concat_grad
  forward : c_concat(Tensor x, int rank = 0, int nranks = 1, int ring_id = 0, bool use_calc_stream = true, bool use_model_parallel = true) -> Tensor(out)
  args : (Tensor out_grad, int rank = 0, int nranks = 1, int ring_id = 0, bool use_calc_stream = true, bool use_model_parallel = true)
  output : Tensor(x_grad)
  invoke : c_split(out_grad, rank, nranks, ring_id, use_calc_stream, use_model_parallel)

- backward_op : c_embedding_grad
  forward : c_embedding (Tensor weight, Tensor x, int64_t start_index=0) -> Tensor(out)
  args : (Tensor weight, Tensor x, Tensor out_grad, int64_t start_index=0)
  output : Tensor(weight_grad)
  infer_meta :
    func : EmbeddingGradInferMeta
    param : [x, weight]
  kernel :
    func : c_embedding_grad
  no_need_buffer : weight

- backward_op : c_softmax_with_cross_entropy_grad
  forward: c_softmax_with_cross_entropy (Tensor logits, Tensor label,  int64_t ignore_index=-100, int ring_id=0, int rank=0, int nranks=0) -> Tensor(softmax), Tensor(loss)
  args: (Tensor softmax, Tensor label, Tensor loss_grad,int64_t ignore_index=-100, int ring_id=0, int rank=0, int nranks=0)
  output: Tensor(logits_grad)
  infer_meta :
    func: CSoftmaxWithCrossEntropyGradInferMeta
  kernel:
    func: c_softmax_with_cross_entropy_grad
    data_type: loss_grad

- backward_op : divide_double_grad
  forward : divide_grad (Tensor x, Tensor y, Tensor out, Tensor grad_out, int axis = -1) -> Tensor(grad_x), Tensor(grad_y)
  args : (Tensor y, Tensor out, Tensor grad_out, Tensor grad_x, Tensor grad_x_grad, Tensor grad_y_grad, int axis = -1)
  output : Tensor(y_grad), Tensor(out_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [y, out, out]
  kernel :
    func : divide_double_grad
    data_type : out
  optional : grad_x, grad_x_grad, grad_y_grad, grad_out_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : divide_grad
  forward : divide (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out, Tensor out_grad, int axis = -1)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
    spmd_rule : ElementwiseBinaryGradInferSpmd
  kernel :
    func : divide_grad
  composite : divide_grad(x, y, out, out_grad, axis, x_grad, y_grad)
  backward : divide_double_grad

- backward_op : einsum_grad
  forward : einsum (Tensor[] x, str equation) -> Tensor(out), Tensor[](inner_cache), Tensor[](x_shape)
  args : (Tensor[] x_shape, Tensor[] inner_cache, Tensor out_grad, str equation)
  output : Tensor[](x_grad){x_shape.size()}
  infer_meta :
    func : UnchangedMultiInferMeta
    param : [x_shape]
  kernel :
    func : einsum_grad

- backward_op : elementwise_pow_grad
  forward : elementwise_pow(Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param: [x, y]
    spmd_rule : ElementwiseBinaryGradInferSpmd
  composite : elementwise_pow_grad(x, y, out_grad, x_grad, y_grad)
  kernel :
    func : elementwise_pow_grad

- backward_op : embedding_grad
  forward : embedding (Tensor x, Tensor weight, int64_t padding_idx=-1) -> Tensor(out)
  args : (Tensor x, Tensor weight, Tensor out_grad, int64_t padding_idx=-1)
  output : Tensor(weight_grad)
  infer_meta :
    func : EmbeddingGradSparseInferMeta
    param : [x,weight]
    spmd_rule : EmbeddingGradInferSpmd
  kernel :
    func : embedding_grad {dense, dense, dense -> dense}
           embedding_sparse_grad {dense, dense, dense -> selected_rows}
           sparse_weight_embedding_grad {selected_rows, dense, dense -> dense}
           sparse_weight_embedding_sparse_grad {selected_rows, dense, dense -> selected_rows}
    data_type : out_grad
  no_need_buffer : weight

- backward_op : exponential__grad
  forward : exponential_ (Tensor x, float lam) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
  invoke : zeros_like(out_grad)

- backward_op : fused_attention_grad
  forward: fused_attention (Tensor x, Tensor ln_scale, Tensor ln_bias, Tensor qkv_weight, Tensor qkv_bias, Tensor cache_kv, Tensor src_mask, Tensor out_linear_weight, Tensor out_linear_bias, Tensor ln_scale_2, Tensor ln_bias_2, int num_heads, bool transpose_qkv_wb, bool pre_layer_norm, float epsilon, float attn_dropout_rate, bool is_test, bool attn_dropout_fix_seed, int attn_dropout_seed, str attn_dropout_implementation, float dropout_rate, bool dropout_fix_seed, int dropout_seed, str dropout_implementation, float ln_epsilon, bool add_residual, int ring_id) -> Tensor(ln_mean), Tensor(ln_var), Tensor(ln_out), Tensor(qkv_out), Tensor(qkv_bias_out), Tensor(transpose_out_2), Tensor(qk_out), Tensor(qktv_out), Tensor(softmax_out), Tensor(attn_dropout_mask_out), Tensor(attn_dropout_out), Tensor(src_mask_out), Tensor(fmha_out), Tensor(out_linear_out), Tensor(dropout_mask_out), Tensor(ln_mean_2), Tensor(ln_var_2), Tensor(bias_dropout_residual_out), Tensor(cache_kv_out), Tensor(out)
  args : (Tensor out_grad, Tensor x, Tensor qkv_weight, Tensor qkv_bias, Tensor qkv_bias_out, Tensor src_mask, Tensor src_mask_out, Tensor out_linear_weight, Tensor out_linear_bias, Tensor ln_scale, Tensor ln_bias, Tensor ln_scale_2, Tensor ln_bias_2, Tensor ln_out, Tensor ln_mean, Tensor ln_var, Tensor ln_mean_2, Tensor ln_var_2, Tensor bias_dropout_residual_out, Tensor qkv_out, Tensor transpose_out_2, Tensor qk_out, Tensor qktv_out, Tensor softmax_out, Tensor attn_dropout_mask_out, Tensor attn_dropout_out, Tensor fmha_out, Tensor out_linear_out, Tensor dropout_mask_out, int num_heads, bool transpose_qkv_wb, bool pre_layer_norm, float epsilon, float attn_dropout_rate, bool is_test, bool attn_dropout_fix_seed, int attn_dropout_seed, str attn_dropout_implementation, float dropout_rate, bool dropout_fix_seed, int dropout_seed, str dropout_implementation, float ln_epsilon, bool add_residual, int ring_id)
  output : Tensor(qkv_bias_grad), Tensor(qkv_bias_out_grad), Tensor(src_mask_out_grad), Tensor(out_linear_bias_grad), Tensor(ln_scale_grad), Tensor(ln_bias_grad), Tensor(ln_scale_2_grad), Tensor(ln_bias_2_grad), Tensor(x_grad), Tensor(qkv_weight_grad), Tensor(out_linear_weight_grad), Tensor(ln_out_grad), Tensor(bias_dropout_residual_out_grad), Tensor(qkv_out_grad), Tensor(qktv_out_grad), Tensor(transpose_out_2_grad), Tensor(qk_out_grad), Tensor(softmax_out_grad), Tensor(attn_dropout_out_grad), Tensor(fmha_out_grad), Tensor(out_linear_out_grad)
  infer_meta:
    func: FusedAttentionGradInferMeta
  kernel:
    func: fused_attention_grad
    data_type : x
  optional: ln_scale, ln_bias, qkv_bias, src_mask, out_linear_bias, ln_scale_2, ln_bias_2, qkv_bias_grad, qkv_bias_out_grad, src_mask_out_grad, out_linear_bias_grad, ln_scale_grad, ln_bias_grad, ln_scale_2_grad, ln_bias_2_grad, ln_out_grad, bias_dropout_residual_out_grad, ln_out, ln_mean, ln_var,  ln_mean_2, ln_var_2, bias_dropout_residual_out, qkv_bias, qkv_bias_out, src_mask, src_mask_out, out_linear_bias
  no_need_buffer: qkv_bias_out, qkv_out, qk_out, qktv_out, out_linear_out, src_mask

- backward_op : fused_feedforward_grad
  forward: fused_feedforward (Tensor x, Tensor dropout1_seed, Tensor dropout2_seed, Tensor linear1_weight, Tensor linear1_bias, Tensor linear2_weight, Tensor linear2_bias, Tensor ln1_scale, Tensor ln1_bias, Tensor ln2_scale, Tensor ln2_bias, bool pre_layer_norm, float ln1_epsilon, float ln2_epsilon, str act_method, float dropout1_prob, float dropout2_prob, str dropout1_implementation, str dropout2_implementation, bool is_test, bool dropout1_fix_seed, bool dropout2_fix_seed, int dropout1_seed_val, int dropout2_seed_val, bool add_residual, int ring_id) -> Tensor(out), Tensor(dropout1_mask), Tensor(dropout2_mask), Tensor(ln1_mean), Tensor(ln1_variance), Tensor(ln2_mean), Tensor(ln2_variance), Tensor(linear1_out), Tensor(ln1_out), Tensor(dropout1_out), Tensor(dropout2_out)
  args : (Tensor out_grad, Tensor x, Tensor linear1_weight, Tensor linear1_bias, Tensor linear2_weight, Tensor dropout1_mask, Tensor dropout2_mask, Tensor linear1_out, Tensor dropout1_out, Tensor dropout2_out, Tensor ln1_scale, Tensor ln1_bias, Tensor ln1_out, Tensor ln1_mean, Tensor ln1_variance, Tensor ln2_scale, Tensor ln2_bias, Tensor ln2_mean, Tensor ln2_variance, Tensor linear2_bias, bool pre_layer_norm, float ln1_epsilon, float ln2_epsilon, str act_method, float dropout1_prob, float dropout2_prob, str dropout1_implementation, str dropout2_implementation, bool is_test, bool dropout1_fix_seed, bool dropout2_fix_seed, int dropout1_seed_val, int dropout2_seed_val, bool add_residual, int ring_id)
  output : Tensor(x_grad), Tensor(linear1_weight_grad), Tensor(linear1_bias_grad), Tensor(linear2_weight_grad),  Tensor(linear2_bias_grad), Tensor(ln1_scale_grad), Tensor(ln1_bias_grad), Tensor(ln2_scale_grad), Tensor(ln2_bias_grad)
  infer_meta:
    func: FusedFeedForwardGradInferMeta
  kernel:
    func: fused_feedforward_grad
  optional: linear1_bias, linear2_bias, ln1_scale, ln1_bias, ln1_out, ln1_mean, ln1_variance, ln2_scale, ln2_bias, ln2_mean, ln2_variance, dropout2_out, ln1_scale_grad, ln1_bias_grad, ln2_scale_grad, ln2_bias_grad,  linear2_bias_grad

- backward_op : fused_gate_attention_grad
  forward: fused_gate_attention(Tensor query, Tensor key, Tensor query_weight, Tensor key_weight, Tensor
    value_weight, Tensor qkv_weight, Tensor nonbatched_bias, Tensor src_mask, Tensor
    gate_weight, Tensor gate_bias, Tensor out_linear_weight, Tensor out_linear_bias,
    bool has_gating = true, bool merge_qkv = true, bool use_flash_attn = false) ->
    Tensor (query_transpose_out), Tensor (key_transpose_out), Tensor (value_transpose_out),
    Tensor (qkv_transpose_out), Tensor (softmax_out), Tensor (softmax_lse), Tensor
    (fmha_out), Tensor (gate_out), Tensor (out)
  args: (Tensor query, Tensor key, Tensor query_weight, Tensor key_weight, Tensor
    value_weight, Tensor qkv_weight, Tensor nonbatched_bias, Tensor src_mask, Tensor
    gate_weight, Tensor gate_bias, Tensor out_linear_weight, Tensor out_linear_bias,
    Tensor query_transpose_out, Tensor key_transpose_out, Tensor value_transpose_out,
    Tensor qkv_transpose_out,
    Tensor softmax_out, Tensor softmax_lse, Tensor fmha_out, Tensor gate_out, Tensor out_grad,
    bool has_gating = true, bool merge_qkv = true, bool use_flash_attn = false)
  output: Tensor (query_grad), Tensor (key_grad), Tensor (query_weight_grad),
    Tensor (key_weight_grad), Tensor (value_weight_grad), Tensor (qkv_weight_grad),
    Tensor (nonbatched_bias_grad), Tensor (gate_weight_grad), Tensor (gate_bias_grad),
    Tensor (out_linear_weight_grad), Tensor (out_linear_bias_grad)
  infer_meta:
    func: FusedGateAttentionGradInferMeta
  kernel:
    func: fused_gate_attention_grad
    data_type: query
  optional: key, query_weight, key_weight, value_weight, qkv_weight, nonbatched_bias, src_mask,
    gate_weight, gate_bias, query_transpose_out, key_transpose_out, value_transpose_out,
    qkv_transpose_out, softmax_out, softmax_lse, gate_out
  data_transform :
    skip_transform : softmax_lse

- backward_op : hardswish_grad
  forward : hardswish (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : hardswish_grad
  inplace : (out_grad -> x_grad)
  composite : hardswish_grad(x, out_grad, x_grad)

- backward_op : legacy_matmul_grad
  forward : legacy_matmul (Tensor x, Tensor y, bool transpose_x=false, bool transpose_y=false, float alpha=1.0f) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, bool transpose_x=false, bool transpose_y=false, float alpha=1.0f)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : legacy_matmul_grad
    param: [x, y, out_grad, transpose_x, transpose_y, alpha]
    data_type: out_grad

- backward_op : legacy_reshape_grad
  forward : legacy_reshape (Tensor x, IntArray shape) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : GradSameWithXInferMeta
    param : [x, out_grad]
    spmd_rule: ReshapeGradInferSpmd
    local_shape : x_grad
  kernel :
    func : reshape_grad
    param : [x, out_grad]
    data_type: out_grad
    backend: out_grad
    layout: out_grad
  no_need_buffer : x

- backward_op : lod_reset_grad
  forward: lod_reset(Tensor x, Tensor y, int[] target_lod={}, bool append=false) -> Tensor(out)
  args: (Tensor x, Tensor out_grad, int[] target_lod={}, bool append=false)
  output: Tensor(x_grad)
  infer_meta:
    func: LodResetGradInferMeta
  kernel:
    func: lod_reset_grad
  no_need_buffer: x
  inplace: (out_grad -> x_grad)

- backward_op : lookup_table_grad
  forward : lookup_table (Tensor w, Tensor ids, bool is_sparse = false, bool is_distributed = false,
    int64_t padding_idx = -1, bool remote_prefetch = false, str entry_config
    = "", bool is_test = false, str entry = "none", str table_class = "none", str[]
    table_names = {}, int trainer_id = 0, int slot = 0, bool grad_inplace = false, str[]
    epmap = {}, int64_t[] height_sections = {}) -> Tensor (out)
  args : (Tensor w, Tensor ids, Tensor out_grad, bool is_sparse = false, bool is_distributed = false,
    int64_t padding_idx = -1, bool remote_prefetch = false, str entry_config
    = "", bool is_test = false, str entry = "none", str table_class = "none", str[]
    table_names = {}, int trainer_id = 0, int slot = 0, bool grad_inplace = false, str[]
    epmap = {}, int64_t[] height_sections = {})
  output : Tensor (w_grad)
  infer_meta:
    func: UnchangedInferMeta
    param: [w]
  kernel :
    func : lookup_table_grad {dense, dense, dense -> dense}
           lookup_table_sparse_grad {dense, dense, dense -> selected_rows}
           lookup_table_grad_sr {selected_rows, dense, dense -> dense}
           lookup_table_sparse_grad_sr {selected_rows, dense, dense -> selected_rows}
    param: [w, ids, out_grad, is_sparse, is_distributed, padding_idx, remote_prefetch, entry_config, is_test, entry, table_class, table_names, trainer_id, grad_inplace, epmap, height_sections]
    data_type: out_grad
  no_need_buffer: w

- backward_op : lrn_grad
  forward: lrn (Tensor x, int n = 5, float k = 2.0, float alpha = 0.0001, float beta = 0.75, str data_format = "AnyLayout") -> Tensor (out), Tensor (mid_out)
  args: (Tensor x, Tensor out, Tensor mid_out, Tensor out_grad, int n = 5, float k = 2.0, float alpha = 0.0001, float beta = 0.75, str data_format
    = "AnyLayout")
  output: Tensor (x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param: [x]
  kernel:
    func: lrn_grad
    data_type: out_grad

- backward_op : matmul_double_grad
  forward : matmul_grad (Tensor x, Tensor y, Tensor grad_out, bool transpose_x=false, bool transpose_y=false) -> Tensor(grad_x), Tensor(grad_y)
  args : (Tensor x, Tensor y, Tensor grad_out, Tensor grad_x_grad, Tensor grad_y_grad, bool transpose_x=false, bool transpose_y=false)
  output : Tensor(x_grad), Tensor(y_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, y, grad_out]
  kernel :
    func : matmul_double_grad
  composite : matmul_double_grad(x, y, grad_out, grad_x_grad, grad_y_grad, transpose_x=false, transpose_y=false)
  optional : grad_x_grad, grad_y_grad
  backward : matmul_triple_grad

- backward_op : matmul_grad
  forward : matmul (Tensor x, Tensor y, bool transpose_x=false, bool transpose_y=false) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, bool transpose_x=false, bool transpose_y=false)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
    spmd_rule : MatmulGradInferSpmd
  kernel :
    func : matmul_grad
  composite: matmul_grad(x, y, out_grad, transpose_x, transpose_y, x_grad, y_grad)
  backward : matmul_double_grad

- backward_op : matmul_triple_grad
  forward : matmul_double_grad (Tensor x, Tensor y, Tensor grad_out, Tensor grad_grad_x, Tensor grad_grad_y, bool transpose_x=false, bool transpose_y=false) -> Tensor(grad_x), Tensor(grad_y), Tensor(grad_grad_out)
  args : (Tensor x, Tensor y, Tensor grad_out, Tensor grad_grad_x, Tensor grad_grad_y, Tensor grad_x_grad, Tensor grad_y_grad, Tensor grad_grad_out_grad, bool transpose_x=false, bool transpose_y=false)
  output : Tensor(x_grad), Tensor(y_grad), Tensor(grad_out_grad), Tensor(grad_grad_x_grad), Tensor(grad_grad_y_grad)
  infer_meta :
    func : GeneralQuinaryGradInferMeta
    param : [x, y, grad_out, grad_grad_x, grad_grad_y]
  kernel :
    func : matmul_triple_grad

- backward_op : matmul_with_flatten_grad
  forward : matmul_with_flatten (Tensor x, Tensor y, int x_num_col_dims=1, int y_num_col_dims=1) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, int x_num_col_dims=1, int y_num_col_dims=1)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : matmul_with_flatten_grad

- backward_op : maximum_grad
  forward : maximum(Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param: [x, y]
    spmd_rule: ElementwiseBinaryGradInferSpmd
  kernel :
    func : maximum_grad
  composite : maximum_grad(x, y, out_grad, x_grad, y_grad)

- backward_op : min_grad
  forward: min (Tensor x,  IntArray axis={},  bool keepdim=false) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray axis={}, bool keepdim=false, bool reduce_all=false)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : min_grad

- backward_op : minimum_grad
  forward : minimum(Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param: [x, y]
  kernel :
    func : minimum_grad
  composite : minimum_grad(x, y, out_grad, x_grad, y_grad)

- backward_op : multiply_double_grad
  forward : multiply_grad (Tensor x, Tensor y, Tensor grad_out, int axis = -1) -> Tensor(grad_x), Tensor(grad_y)
  args : (Tensor x, Tensor y, Tensor grad_out, Tensor grad_x_grad, Tensor grad_y_grad, int axis = -1)
  output : Tensor(x_grad), Tensor(y_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, y, grad_out]
  kernel :
    func : multiply_double_grad
  optional : grad_x_grad, grad_y_grad, grad_out_grad
  inplace : (grad_x_grad -> grad_out_grad)
  backward : multiply_triple_grad
  composite : multiply_double_grad(x, y, grad_out, grad_x_grad, grad_y_grad, axis, x_grad, y_grad, grad_out_grad)

- backward_op : multiply_grad
  forward : multiply (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, int axis = -1)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
    spmd_rule : ElementwiseBinaryGradInferSpmd
  kernel :
    func : multiply_grad
  composite: multiply_grad(x, y, out_grad, axis, x_grad, y_grad)
  backward : multiply_double_grad

- backward_op : multiply_triple_grad
  forward : multiply_double_grad (Tensor x, Tensor y, Tensor fwd_grad_out, Tensor fwd_grad_grad_x, Tensor fwd_grad_grad_y, int axis = -1) -> Tensor(grad_x), Tensor(grad_y), Tensor(grad_grad_out)
  args : (Tensor x, Tensor y, Tensor fwd_grad_out, Tensor fwd_grad_grad_x, Tensor fwd_grad_grad_y, Tensor grad_x_grad, Tensor grad_y_grad, Tensor grad_grad_out_grad, int axis = -1)
  output : Tensor(x_grad), Tensor(y_grad), Tensor(fwd_grad_out_grad), Tensor(fwd_grad_grad_x_grad), Tensor(fwd_grad_grad_y_grad)
  infer_meta :
    func : GeneralQuinaryGradInferMeta
    param : [x, y, fwd_grad_out, fwd_grad_grad_x, fwd_grad_grad_y]
  kernel :
    func : multiply_triple_grad
  optional : fwd_grad_grad_x, fwd_grad_grad_y, grad_x_grad, grad_y_grad, grad_grad_out_grad

- backward_op : nce_grad
  forward: nce (Tensor input, Tensor label, Tensor weight, Tensor bias, Tensor sample_weight, Tensor custom_dist_probs, Tensor custom_dist_alias, Tensor custom_dist_alias_probs, int num_total_classes, int[] custom_neg_classes={}, int num_neg_samples=10, int sampler=0, int seed=0, bool is_sparse=false, bool remote_prefetch=false, bool is_test=false) ->  Tensor(cost), Tensor(sample_logits), Tensor(sample_labels)
  args: (Tensor input, Tensor label, Tensor bias, Tensor weight, Tensor sample_logits, Tensor sample_labels, Tensor sample_weight, Tensor custom_dist_probs, Tensor custom_dist_alias, Tensor custom_dist_alias_probs, Tensor cost_grad, int num_total_classes, int[] custom_neg_classes={}, int num_neg_samples=10, int sampler=0, int seed=0, bool is_sparse=false, bool remote_prefetch=false, bool is_test=false)
  output: Tensor(input_grad), Tensor(bias_grad), Tensor(weight_grad)
  infer_meta:
    func: NceGradInferMeta
    param: [input, bias, weight]
  kernel:
    func : nce_grad {dense, dense, dense, dense, dense, dense, dense, dense, dense, dense, dense -> dense, dense, dense}
           nce_sr_grad {dense, dense, dense, dense, dense, dense, dense, dense, dense, dense, dense -> dense, dense, selected_rows}
    data_type: input
  optional: bias, sample_weight, custom_dist_probs, custom_dist_alias, custom_dist_alias_probs

- backward_op : push_box_sparse
  forward : pull_box_sparse (Tensor w, Tensor[] ids, bool is_sparse = false, bool is_distributed = false, int size = 1) -> Tensor[](out){ids.size()}
  args : (Tensor[] ids, Tensor[] out_grad_in, bool is_sparse = false, bool is_distributed = false, int size = 1)
  output : Tensor[](out_grad_out){out_grad_in.size()}
  infer_meta :
    func : UnchangedMultiInferMeta
    param : [out_grad_in]
  kernel :
    func : push_box_sparse
    data_type : out_grad_in
  inplace : (out_grad_in -> out_grad_out)

- backward_op : row_conv_grad
  forward: row_conv (Tensor x, Tensor filter) -> Tensor(out)
  args: (Tensor x, Tensor filter, Tensor out_grad)
  output: Tensor(x_grad),Tensor(filter_grad)
  infer_meta :
    func : RowConvGradInferMeta
    param : [out_grad, filter]
  kernel:
    func : row_conv_grad

- backward_op : sequence_expand_grad
  forward: sequence_expand (Tensor x, Tensor y, int ref_level = -1) -> Tensor (out)
  args: (Tensor x, Tensor y, Tensor out_grad, int ref_level = -1)
  output: Tensor (x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param: [x]
  kernel:
    func: sequence_expand_grad
    data_type: out_grad
  no_need_buffer: x, y

- backward_op : sequence_softmax_grad
  forward : sequence_softmax (Tensor x) -> Tensor(out)
  args: (Tensor x, Tensor out, Tensor out_grad)
  output: Tensor (x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: sequence_softmax_grad
  no_need_buffer: x

- backward_op : set_value_grad
  forward : set_value (Tensor x, IntArray starts, IntArray ends, IntArray steps, int64_t[] axes, int64_t[] decrease_axes, int64_t[] none_axes, int64_t[] shape, Scalar[] values) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param: [out_grad]
  kernel:
    func: assign
    param: [out_grad]

- backward_op : soft_relu_grad
  forward : soft_relu (Tensor x, float threshold = 40.0f) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, float threshold = 40.0f)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : soft_relu_grad

- backward_op : softmax_grad
  forward : softmax (Tensor x, int axis) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
    spmd_rule: SoftmaxGradInferSpmd
  kernel :
    func : softmax_grad
  composite : softmax_grad(out, out_grad, axis, x_grad)

- backward_op : subtract_double_grad
  forward : subtract_grad (Tensor x, Tensor y, Tensor grad_out, int axis = -1) -> Tensor(grad_x), Tensor(grad_y)
  args : (Tensor y, Tensor grad_out, Tensor grad_x_grad, Tensor grad_y_grad, int axis = -1)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [grad_out]
  kernel :
    func : subtract_double_grad
  optional : grad_x_grad, grad_y_grad, grad_out_grad
  no_need_buffer : y, grad_out
  inplace : (grad_x_grad -> grad_out_grad)
  composite : subtract_double_grad(y, grad_out, grad_x_grad, grad_y_grad, axis, grad_out_grad)

- backward_op : subtract_grad
  forward : subtract (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, int axis = -1)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
    spmd_rule : ElementwiseBinaryGradInferSpmd
  kernel :
    func : subtract_grad
  no_need_buffer : x, y
  composite : subtract_grad(x, y, out_grad, axis, x_grad, y_grad)
  backward : subtract_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : tile_double_grad
  forward : tile_grad (Tensor x, Tensor grad_out, IntArray repeat_times) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray repeat_times)
  output : Tensor(grad_out_grad)
  invoke : tile(grad_x_grad, repeat_times)

- backward_op : tile_grad
  forward : tile (Tensor x, IntArray repeat_times) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray repeat_times)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : tile_grad
  no_need_buffer : x
  composite : tile_grad(x, out_grad, repeat_times, x_grad)
  backward : tile_double_grad
