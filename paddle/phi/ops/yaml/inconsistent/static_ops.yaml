# The operators included in this file are:
# 1) Operators defined only in PIR, dynamic graphs do not exist;
# 2) The definitions of static graphs and dynamic graphs are inconsistent, but the final definition plan has not yet been clarified.
# After the definition is clearly defined, migrate to paddle/phi/ops/yaml/inconsistent/update_ops.yaml or paddle/phi/ops/yaml/ops.yaml

- op : add
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
    spmd_rule : ElementwiseBinaryInferSpmd
  kernel :
    func : add
    data_type: x
  inplace : (x -> out)
  backward : add_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface, paddle::dialect::LayoutTransformationInterface

# this add_n is only for ops_api_gen.py and onednn
- op : add_n
  args : (Tensor[] inputs)
  output : Tensor(out)
  infer_meta:
    func: AddNInferMeta
    spmd_rule : AddNInferSpmd
    param: [inputs]
  kernel:
    func: add_n
    param: [inputs]
  backward : add_n_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : all_reduce
  args : (Tensor x, int ring_id = 0, int reduce_type = 0)
  output : Tensor(out)
  infer_meta :
    func : AllReduceInferMeta
    param: [x]
  kernel :
    func : all_reduce
    param: [x, reduce_type]
  inplace : (x -> out)

- op : assign
  args : (Tensor x)
  output : Tensor
  infer_meta :
    func : UnchangedInferMeta
  kernel :
    func : assign_raw
  backward : assign_grad
  inplace : (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface, paddle::dialect::LayoutTransformationInterface

- op : assign_value
  args : (int[] shape, DataType dtype, Scalar[] values, Place place = {})
  output : Tensor(out)
  infer_meta :
    func : AssignValueInferMeta
    param: [shape, dtype]
  kernel :
    func : assign_value
    param : [shape, dtype, values]
    backend: place>
    data_type : dtype
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : barrier
  args : (Tensor x, int ring_id=0)
  output : Tensor(out)
  kernel :
    func : barrier

- op : batch_norm
  args : (Tensor x, Tensor mean, Tensor variance, Tensor scale, Tensor bias, bool is_test, float momentum, float epsilon, str data_format, bool use_global_stats, bool trainable_statistics)
  output : Tensor(out), Tensor(mean_out), Tensor(variance_out), Tensor(saved_mean), Tensor(saved_variance), Tensor(reserve_space)
  infer_meta:
    func : BatchNormInferMeta
  kernel :
    func : batch_norm
    data_type : x
  view : (mean -> mean_out), (variance -> variance_out)
  backward : batch_norm_grad
  optional : scale, bias, reserve_space

- op : beam_search_decode
  args: (Tensor ids, Tensor scores, int beam_size, int end_id)
  output: Tensor (sentence_ids), Tensor (sentence_scores)
  infer_meta:
    func: BeamSearchDecodeInferMeta
  kernel:
    func: beam_search_decode

- op : c_allreduce_avg
  args : (Tensor x, int ring_id, bool use_calc_stream, bool use_model_parallel)
  output : Tensor(out)
  infer_meta :
    func : AllReduceInferMeta
    param : [x]
  kernel :
    func : c_allreduce_avg
  inplace : (x -> out)

- op : c_embedding
  args : (Tensor weight, Tensor x, int64_t start_index=0, int64_t vocab_size=-1)
  output : Tensor(out)
  infer_meta :
    func : CEmbeddingInferMeta
    param : [weight, x, start_index]
  kernel :
    func : c_embedding
    param : [weight, x, start_index, vocab_size]
    data_type : weight
  backward : c_embedding_grad

- op : c_reduce_avg
  args : (Tensor x, int ring_id, int root_id, bool use_calc_stream)
  output : Tensor(out)
  infer_meta :
    func : DistReduceInferMeta
    param : [x]
  kernel :
    func : c_reduce_avg
  inplace : (x -> out)

- op : c_reduce_max
  args : (Tensor x, int ring_id, int root_id, bool use_calc_stream)
  output : Tensor(out)
  infer_meta :
    func : DistReduceInferMeta
    param : [x]
  kernel :
    func : c_reduce_max
  inplace : (x -> out)

- op : c_reduce_min
  args : (Tensor x, int ring_id, int root_id, bool use_calc_stream)
  output : Tensor(out)
  infer_meta :
    func : DistReduceInferMeta
    param : [x]
  kernel :
    func : c_reduce_min
  inplace : (x -> out)

- op : c_reduce_prod
  args : (Tensor x, int ring_id, int root_id, bool use_calc_stream)
  output : Tensor(out)
  infer_meta :
    func : DistReduceInferMeta
    param : [x]
  kernel :
    func : c_reduce_prod
  inplace : (x -> out)

- op : c_reducescatter
  args : (Tensor x, int ring_id = 0, int nranks = 1, bool use_calc_stream = false)
  output : Tensor(out)
  infer_meta :
    func : ReduceScatterInferMeta
    param: [x, nranks]
  kernel :
    func : c_reducescatter

- op : c_scatter
  args : (Tensor x, int ring_id = 0, int root = 0, int nranks = 0, bool use_calc_stream = false)
  output : Tensor(out)
  infer_meta :
    func : CScatterInferMeta
    param : [x, nranks]
  kernel :
    func : c_scatter

- op : c_split
  args : (Tensor x, int rank = 0, int nranks = 1, int ring_id = 0, bool use_calc_stream = false, bool use_model_parallel = true)
  output : Tensor(out)
  infer_meta :
    func : CSplitInferMeta
    param : [x, nranks]
  kernel :
    func : c_split

- op : coalesce_tensor_
  args : (Tensor[] input, DataType dtype, bool copy_data = false, bool set_constant = false, bool persist_output = false, float constant = 0.0, bool use_align = true, int align_size = -1, int size_of_dtype = -1, int64_t[] concated_shapes = {}, int64_t[] concated_ranks = {})
  output : Tensor[](output){input.size()}, Tensor(fused_output)
  infer_meta :
    func : CoalesceTensorInferMeta
  kernel :
    func : coalesce_tensor
    data_type : dtype
  inplace: (input -> output)

- op : dequantize_linear
  args : (Tensor x, Tensor scale, Tensor zero_point, Tensor in_accum, Tensor in_state, int quant_axis = 0, int bit_length = 8, int qmin = -128, int qmax = 127, int round_type = 0, bool is_test = true, bool only_observer = false)
  output : Tensor(y), Tensor(out_state), Tensor(out_accum), Tensor(out_scale)
  infer_meta :
    func : QuantizeLinearInferMeta
    param : [x, scale, zero_point, in_accum, in_state, quant_axis, bit_length, round_type, is_test, only_observer]
  kernel :
    func : quantize_linear
    param : [x, scale, zero_point, in_accum, in_state, quant_axis, bit_length, qmin, qmax, round_type, is_test, only_observer]
    data_type : x
  optional : scale, in_accum, in_state, out_state, out_accum, out_scale
  inplace : (scale -> out_scale, in_accum -> out_accum, in_state -> out_state)

- op : distribute_fpn_proposals
  args : (Tensor fpn_rois, Tensor rois_num, int min_level, int max_level, int refer_level, int refer_scale, bool pixel_offset)
  output : Tensor[](multi_fpn_rois){max_level - min_level + 1}, Tensor[](multi_level_rois_num){max_level - min_level + 1}, Tensor(restore_index)
  infer_meta :
    func : DistributeFpnProposalsInferMeta
  kernel :
    func : distribute_fpn_proposals
    data_type : fpn_rois
  optional : rois_num, multi_level_rois_num
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : distributed_fused_lamb
  args : (Tensor[] param, Tensor[] grad, Tensor fp32_fused_param, Tensor fp32_fused_grad, Tensor fp16_fused_param, Tensor fp16_fused_grad, Tensor moment1, Tensor moment2, Tensor beta1pow, Tensor beta2pow, Tensor fused_param_offsets, Tensor fp32_shard_fused_param_offsets, Tensor fp16_shard_fused_param_offsets, Tensor param_info, Tensor param_order, Tensor learning_rate, Tensor global_scale, float beta1, float beta2, float epsilon, float max_global_grad_norm, float weight_decay, bool clip_after_allreduce, int[] ring_ids= {}, int acc_steps = 1, bool use_master_param_norm = true, bool use_master_acc_grad = true, bool is_grad_scaled_by_nranks = true, int64_t nranks = 1, bool use_hierarchical_allreduce = false)
  output : Tensor(fp32_fused_param_out), Tensor(fp16_fused_param_out), Tensor(fp32_acc_fused_grad), Tensor(fp16_acc_fused_grad), Tensor(moment1_out), Tensor(moment2_out), Tensor(beta1pow_out), Tensor(beta2pow_out), Tensor[](param_out){param.size()}, Tensor(found_inf), Tensor(acc_step), Tensor(stop_update), Tensor(step)
  kernel :
    func : distributed_fused_lamb
    data_type : DataType::FLOAT32
    param : [param, grad, fp32_fused_param, fp32_fused_grad, fp16_fused_param, fp16_fused_grad, moment1, moment2, beta1pow, beta2pow, fused_param_offsets, fp32_shard_fused_param_offsets, fp16_shard_fused_param_offsets, param_info, param_order, learning_rate, global_scale, acc_steps, beta1, beta2, epsilon, max_global_grad_norm, weight_decay, clip_after_allreduce, use_master_param_norm, use_master_acc_grad, is_grad_scaled_by_nranks, use_hierarchical_allreduce, nranks, ring_ids]
  optional : fp32_fused_param, fp32_fused_grad, fp16_fused_param, fp16_fused_grad, fp32_fused_param_out, fp16_fused_param_out, fp32_acc_fused_grad, fp16_acc_fused_grad, acc_step, stop_update
  inplace : (fp32_fused_param -> fp32_fused_param_out), (fp16_fused_param -> fp16_fused_param_out), (moment1 -> moment1_out), (moment2 -> moment2_out), (beta1pow -> beta1pow_out), (beta2pow -> beta2pow_out), (param -> param_out)

- op : distributed_lookup_table
  args : (Tensor[] ids, Tensor w, int table_id = 0, bool is_distributed = false, str lookup_table_version = "lookup_table", int64_t padding_idx = -1, DataType dtype = DataType::FLOAT32, bool is_test = false)
  output : Tensor[](outputs){ids.size()}
  infer_meta :
    func : DistributeLookupTableInferMeta
  kernel :
    func : distributed_lookup_table
    data_type : dtype

- op : distributed_push_sparse
  args : (Tensor[] ids, Tensor[] shows, Tensor[] clicks, int table_id = 0, int size = 8, bool is_distributed = false, str push_sparse_version = "push_sparse", int64_t padding_idx = -1, DataType dtype=DataType::FLOAT32, bool is_test = false, bool use_cvm_op = false)
  output : Tensor[](output){ids.size()}
  infer_meta :
    func : DistributedPushSparseInferMeta
  kernel :
    func: distributed_push_sparse
    data_type : dtype

- op : divide
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
    spmd_rule : ElementwiseBinaryInferSpmd
  kernel :
    func : divide
  inplace: (x -> out)
  data_transform :
    support_trans_dtype : x, y
  backward : divide_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : einsum
  args : (Tensor[] x, str equation)
  output : Tensor(out), Tensor[](inner_cache){x.size()}, Tensor[](xshape){x.size()}
  infer_meta :
    func : EinsumRawInferMeta
    param : [x, equation]
  kernel :
    func : einsum
  optional : inner_cache, xshape
  backward : einsum_grad

- op : elementwise_pow
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
    spmd_rule: ElementwiseBinaryInferSpmd
  kernel :
    func : elementwise_pow
  data_transform :
    support_trans_dtype : x, y
  backward : elementwise_pow_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : embedding
  args : (Tensor x, Tensor weight, int64_t padding_idx=-1, bool sparse=false)
  output : Tensor
  infer_meta :
    func : EmbeddingInferMeta
    param : [x, weight, padding_idx]
    spmd_rule: EmbeddingInferSpmdUnsupportVocabParallel
  kernel :
    func : embedding {dense, dense -> dense}
           sparse_weight_embedding {dense, selected_rows -> dense}
    param : [x, weight, padding_idx]
    data_type : weight
  backward : embedding_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : equal
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : CompareInferMeta
    spmd_rule: ElementwiseBinaryInferSpmd
  kernel :
    func : equal
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : feed
  args : (str name, int col)
  output : Tensor(out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits: pir::ImmutableLayoutTrait

- op : fetch
  args : (Tensor x, str name, int col)
  output : Tensor(out)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : fetch
    param : [x]
  traits : pir::SideEffectTrait, pir::ImmutableLayoutTrait, paddle::dialect::ForwardOnlyTrait
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : floor_divide
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
  kernel :
    func : floor_divide
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  traits : paddle::dialect::ForwardOnlyTrait

- op : fused_adam_
  args : (Tensor[] params, Tensor[] grads, Tensor learning_rate, Tensor[] moments1, Tensor[] moments2, Tensor[] beta1_pows, Tensor[] beta2_pows, Tensor[] master_params, Tensor skip_update, Scalar beta1, Scalar beta2, Scalar epsilon, int chunk_size, float weight_decay, bool use_adamw, bool multi_precision, bool use_global_beta_pow)
  output : Tensor[](params_out){params.size()}, Tensor[](moments1_out){params.size()}, Tensor[](moments2_out){params.size()}, Tensor[](beta1_pows_out){params.size()}, Tensor[](beta2_pows_out){params.size()}, Tensor[](master_params_out){params.size()}
  infer_meta :
    func : FusedAdamInferMeta
  kernel :
    func : fused_adam
    data_type : params
  optional : skip_update, master_params, master_params_out
  inplace : (params -> params_out), (moments1 -> moments1_out), (moments2 -> moments2_out), (beta1_pows -> beta1_pows_out), (beta2_pows -> beta2_pows_out), (master_params -> master_params_out)

- op : get_tensor_from_selected_rows
  args : (Tensor x)
  output : Tensor(out)
  infer_meta :
    func : UnchangedInferMeta
  kernel:
    func: get_tensor_from_selected_rows {selected_rows -> dense}

- op : global_gather
  args : (Tensor x, Tensor local_count, Tensor global_count, int ring_id = 0, bool use_calc_stream=false)
  output : Tensor(out)
  infer_meta:
    func : GlobalGatherInferMeta
  kernel :
    func : global_gather
    data_type: x
  backward : global_gather_grad

- op : global_scatter
  args : (Tensor x, Tensor local_count, Tensor global_count, int ring_id=0, bool use_calc_stream=false)
  output : Tensor(out)
  infer_meta :
    func : GlobalScatterInferMeta
  kernel :
    func : global_scatter
    data_type : x
  backward : global_scatter_grad

- op : greater_equal
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : CompareInferMeta
  kernel :
    func : greater_equal
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : greater_than
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : CompareInferMeta
  kernel :
    func : greater_than
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : hardswish
  args : (Tensor x)
  output : Tensor(out)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : hardswish
  backward : hardswish_grad

- op : hash
  args: (Tensor x, int num_hash = 1, int64_t mod_by = 100000, bool runtime_shape = true)
  output: Tensor (out)
  infer_meta:
    func: HashInferMeta
    param: [x, num_hash, mod_by]
  kernel:
    func: hash
    param: [x, num_hash, mod_by]
    data_type: x

- op : lars_momentum_
  args: (Tensor[] param, Tensor[] grad, Tensor[] velocity, Tensor[] learning_rate, Tensor[] master_param, float mu, float lars_coeff=0.001f, float[] lars_weight_decay={0.0005f}, float epsilon=0.0f, bool multi_precision=false, float rescale_grad=1.0f)
  output: Tensor[](param_out){param.size()}, Tensor[](velocity_out){param.size()}, Tensor[](master_param_out){param.size()}
  infer_meta:
    func: LarsMomentumInferMeta
    param: [param, velocity, learning_rate, grad, master_param, lars_weight_decay, mu, lars_coeff, epsilon, multi_precision, rescale_grad]
  kernel:
    func: lars_momentum
    param: [param, velocity, learning_rate, grad, master_param, lars_weight_decay, mu, lars_coeff, epsilon, multi_precision, rescale_grad]
    data_type: param
  optional: master_param, master_param_out
  inplace : (param -> param_out), (velocity -> velocity_out), (master_param -> master_param_out)
  traits : pir::SideEffectTrait

- op : less_equal
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : CompareInferMeta
  kernel :
    func : less_equal
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : less_than
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : CompareInferMeta
  kernel :
    func : less_than
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : load_combine
  args : (str file_path, bool load_as_fp16, bool model_from_memory)
  output : Tensor[](Out)
  kernel:
    func: load_combine
    param: [file_path, load_as_fp16, model_from_memory]
  optional : Out

- op : lod_array_length
  args : (Tensor[] x)
  output : Tensor(out)

- op : lrn
  args: (Tensor x, int n = 5, float k = 2.0, float alpha = 0.0001, float beta = 0.75, str data_format = "AnyLayout")
  output: Tensor (out), Tensor (mid_out)
  infer_meta:
    func: LrnInferMeta
    param : [x, n]
  kernel:
    func: lrn
    data_type: x
  backward: lrn_grad

- op : matmul
  args : (Tensor x, Tensor y, bool transpose_x = false, bool transpose_y = false)
  output : Tensor
  infer_meta :
    func : MatmulInferMeta
    spmd_rule : MatmulInferSpmd
  kernel :
    func : matmul
  data_transform :
    support_trans_dtype : x, y
  backward : matmul_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : matmul_with_flatten
  args : (Tensor x, Tensor y, int x_num_col_dims = 1, int y_num_col_dims = 1)
  output : Tensor
  infer_meta :
    func : MatmulWithFlattenInferMeta
  kernel :
    func : matmul_with_flatten
    data_type : x
  backward : matmul_with_flatten_grad

- op : maximum
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
    spmd_rule : ElementwiseBinaryInferSpmd
  kernel :
    func : maximum
  data_transform :
    support_trans_dtype : x, y
  backward : maximum_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : memcpy
  args : (Tensor x, int dst_place_type)
  output : Tensor(out)
  infer_meta:
    func: UnchangedInferMeta
    param: [x]
  kernel:
    func : memcpy
    param: [x, dst_place_type]

- op : min
  args : (Tensor x, IntArray axis={}, bool keepdim=false)
  output : Tensor(out)
  infer_meta :
    func : ReduceIntArrayAxisInferMeta
  kernel :
    func : min
  backward : min_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : minimum
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
  kernel :
    func : minimum
  data_transform :
    support_trans_dtype : x, y
  backward : minimum_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : multiply
  args : (Tensor x, Tensor y)
  output : Tensor
  infer_meta :
    func : ElementwiseInferMeta
    spmd_rule : ElementwiseBinaryInferSpmd
  kernel :
    func : multiply {dense, dense -> dense},
           multiply_sr {selected_rows, dense -> selected_rows}
  inplace : (x -> out)
  data_transform :
    support_trans_dtype : x, y
  backward : multiply_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface, paddle::dialect::LayoutTransformationInterface

- op : nop
  args : (Tensor x)
  output : Tensor(out)
  infer_meta :
    func : UnchangedInferMeta
  kernel :
    func : nop
  inplace: (x -> out)
  interfaces : paddle::dialect::ParseKernelKeyInterface

- op : not_equal
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : CompareInferMeta
    spmd_rule : ElementwiseBinaryInferSpmd
  kernel :
    func : not_equal
  data_transform :
    support_trans_dtype : x, y
  inplace: (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : partial_allgather
  args : (Tensor x, int nranks, int rank, int ring_id = 0, bool use_calc_stream = false)
  output : Tensor(out)
  infer_meta :
    func: PartialAllgatherInferMeta
  kernel :
    func : partial_allgather
  inplace : (x -> out)

- op : partial_recv
  args : (int ring_id = 0, int peer = 0, DataType dtype=DataType::FLOAT32, int[] out_shape= {}, bool use_calc_stream = false, int num = 1, int id = 0)
  output : Tensor(out)
  infer_meta :
    func: PartialRecvInferMeta
  kernel :
    func : partial_recv
    data_type : dtype

- op : print
  args : (Tensor in, int first_n, str message, int summarize, bool print_tensor_name = true, bool print_tensor_type = true, bool print_tensor_shape = true, bool print_tensor_layout = true, bool print_tensor_lod = true, str print_phase = "BOTH", bool is_forward = true)
  output : Tensor(out)
  infer_meta:
    func: UnchangedInferMeta
    param: [in]
  kernel :
    func : print_kernel
    param: [in, first_n, message, summarize, print_tensor_name, print_tensor_type, print_tensor_shape, print_tensor_layout, print_tensor_lod, print_phase, is_forward]
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : pir::SideEffectTrait, paddle::dialect::ForwardOnlyTrait

- op : pull_box_sparse
  args : (Tensor w, Tensor[] ids, bool is_sparse = false, bool is_distributed = false, int size = 1)
  output : Tensor[](out){ids.size()}
  infer_meta :
    func : PullBoxSparseInferMeta
  kernel :
    func : pull_box_sparse
    data_type : ids

- op : pull_gpups_sparse
  args : (Tensor w, Tensor[] ids, int[] size={}, bool is_sparse=false, bool is_distributed=false)
  output : Tensor[](out){ids.size()}
  infer_meta :
    func : PullGpupsSparseInferMeta
  kernel :
    func : pull_gpups_sparse
    data_type : PullGpupsSparseKernelKey
  optional : w
  backward: push_gpups_sparse

- op : pull_sparse_v2
  args : (Tensor[] ids, Tensor[] w, int embedding_dim = 11, int table_id = 0, str accessor_class = "", str ctr_label_name = "", int padding_id = 0, bool scale_sparse_grad = true, str[] input_names = {}, bool is_distributed = true)
  output : Tensor[](out){w.size()}
  infer_meta :
    func : PullSparseV2InferMeta
  kernel :
    func : pull_sparse_v2
    data_type : DataType::FLOAT32
  backward : pull_sparse_v2_grad

- op : push_dense
  args : (Tensor[] ids, int table_id = -1, float scale_data_norm = -1.0f, str[] input_names = {})
  output :
  infer_meta :
    func : PushDenseInferMeta
    param : [ids, table_id, scale_data_norm, input_names]
  kernel :
    func : push_dense
    data_type : DataType::FLOAT32

- op : push_sparse_v2
  args : (Tensor[] ids, Tensor[] w, Tensor[] out_grad_in, int embedding_dim = 11, int table_id = 0, str accessor_class = "", str ctr_label_name = "", int padding_id = 0, bool scale_sparse_grad = true, str[] input_names = {}, bool is_distributed = true)
  output : Tensor[](out_grad_out){out_grad_in.size()}
  infer_meta :
    func : UnchangedMultiInferMeta
    param : [out_grad_in]
  kernel :
    func : push_sparse_v2
    data_type : out_grad_in
  inplace: (out_grad_in -> out_grad_out)

# Note: dequantize_linear and quantize_linear are supported using one op maker in fluid, the out_scale can't be used in dequantize_linear
# so ,the out_scale is optional. Currently, we can't modify the op definition of dequantize_linear/quantize_linear and it can cause incompatibility problem
# We need modify dequantize_linear/quantize_linear yaml and make it more reasonable when we abandon Fluid op.
- op : quantize_linear
  args : (Tensor x, Tensor scale, Tensor zero_point, Tensor in_accum, Tensor in_state, int quant_axis = 0, int bit_length = 8, int qmin = -128, int qmax = 127, int round_type = 0, bool is_test = true, bool only_observer = false)
  output : Tensor(y), Tensor(out_state), Tensor(out_accum), Tensor(out_scale)
  infer_meta :
    func : QuantizeLinearInferMeta
    param : [x, scale, zero_point, in_accum, in_state, quant_axis, bit_length, round_type, is_test, only_observer]
  kernel :
    func : quantize_linear
    param : [x, scale, zero_point, in_accum, in_state, quant_axis, bit_length, qmin, qmax, round_type, is_test, only_observer]
    data_type : x
  optional : scale, in_accum, in_state, out_state, out_accum, out_scale
  inplace : (scale -> out_scale, in_accum -> out_accum, in_state -> out_state)

- op : recv_v2
  args : (int[] out_shape = {}, DataType dtype = DataType::FLOAT32, int peer = 0, int ring_id = 0, bool use_calc_stream = false, bool dynamic_shape = false)
  output : Tensor(out)
  infer_meta:
    func: RecvV2InferMeta
    param: [ring_id, dynamic_shape, peer, out_shape, dtype]
  kernel :
    func : recv_v2
    param : [ring_id, dynamic_shape, peer, out_shape, dtype, use_calc_stream]
    data_type : dtype

- op : remainder
  args : (Tensor x, Tensor y)
  output : Tensor (out)
  infer_meta :
    func : ElementwiseInferMeta
  kernel :
    func : remainder
  data_transform :
    support_trans_dtype : x, y
  inplace : (x -> out)
  interfaces : paddle::dialect::InferSymbolicShapeInterface
  traits : paddle::dialect::ForwardOnlyTrait

- op : reshape
  args : (Tensor x, IntArray shape)
  output : Tensor(out), Tensor(xshape)
  infer_meta :
    func : ReshapeWithXShapeInferMeta
    spmd_rule : ReshapeInferSpmdDynamic
  kernel :
    func : reshape
  inplace : (x -> out)
  view: (x -> out)
  intermediate : xshape
  backward: reshape_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : row_conv
  args : (Tensor x, Tensor filter)
  output : Tensor(out)
  infer_meta :
     func: RowConvInferMeta
  kernel :
     func : row_conv

- op : save_combine
  args : (Tensor[] x, str file_path, bool overwrite, bool save_as_fp16, bool save_to_memory)
  output : Tensor(out)
  kernel:
    func: save_combine_tensor
    param: [x, file_path, overwrite, save_as_fp16, save_to_memory]
  optional : out
  interfaces : paddle::dialect::ParseKernelKeyInterface

- op : seed
  args : (int seed, bool deterministic, str rng_name, bool force_cpu)
  output : Tensor(out)
  infer_meta:
    func: SeedInferMeta
    param: [seed]
  kernel:
    func: seed
  traits : pir::SideEffectTrait

- op : send_and_recv
  args : (Tensor[] x, str message_name, str[] send_var_name, str[] recv_var_name, int trainer_id = 0, str mode="forward", str[] endpoints={"127.0.0.1:6164"}, str[] next_endpoints={"127.0.0.1:6164"}, str[] previous_endpoints={"127.0.0.1:6164"})
  output : Tensor[](out){x.size()}
  infer_meta :
    func : UnchangedVectorInferMeta
    param : [x]
  kernel :
    func : send_and_recv
    param : [x, message_name, mode, send_var_name, recv_var_name, trainer_id, endpoints, next_endpoints, previous_endpoints]
  inplace: (x -> out)

- op : send_v2
  args : (Tensor x, int ring_id = 0, int peer = 0, bool use_calc_stream = false, bool dynamic_shape = false)
  output :
  infer_meta:
    func: SendV2InferMeta
    param: [peer, ring_id]
  kernel :
    func : send_v2
    param : [x, ring_id, dynamic_shape, peer, use_calc_stream]
  traits : pir::SideEffectTrait

- op : sequence_expand
  args: (Tensor x, Tensor y, int ref_level = -1)
  output: Tensor (out)
  infer_meta:
    func: SequenceExpandInferMeta
  kernel:
    func: sequence_expand
    data_type: x
  backward: sequence_expand_grad
  no_need_buffer: y

- op : sequence_softmax
  args: (Tensor x)
  output: Tensor (out)
  infer_meta:
    func: SequenceSoftmaxInferMeta
  kernel:
    func: sequence_softmax
    param: [x]
  backward: sequence_softmax_grad

- op : set_value
  args : (Tensor x, IntArray starts, IntArray ends, IntArray steps, int64_t[] axes, int64_t[] decrease_axes, int64_t[] none_axes, int64_t[] shape, Scalar[] values)
  output : Tensor(out)
  inplace: (x -> out)
  infer_meta :
    func : SetValueInferMeta
    param : [x]
  kernel :
    func : set_value
  backward: set_value_grad

- op : shadow_feed
  args : (Tensor x)
  output : Tensor(out)
  infer_meta:
    func: UnchangedInferMeta
    param: [x]
  kernel:
    func: shadow_feed
    param: [x]

- op : shadow_feed_tensors
  args : (Tensor[] x)
  output : Tensor[](out){x.size()}
  infer_meta:
    func: UnchangedVectorInferMeta
    param: [x]
  kernel:
    func: shadow_feed_tensors
    param: [x]

- op : share_data_
  args : (Tensor x)
  output : Tensor(out)
  infer_meta:
    func: UnchangedInferMeta
    param: [x]
  kernel:
    func: share_data
    param: [x]
  inplace : (x -> out)
  traits : paddle::dialect::ForwardOnlyTrait

- op : soft_relu
  args : (Tensor x, float threshold = 20.0f)
  output : Tensor(out)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : soft_relu
  backward : soft_relu_grad

- op : softmax
  args : (Tensor x, int axis)
  output : Tensor(out)
  infer_meta :
    func : SoftmaxInferMeta
    spmd_rule : SoftmaxInferSpmd
  kernel :
    func : softmax
  inplace : (x -> out)
  backward : softmax_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : sparse_momentum
  args: (Tensor param, Tensor grad, Tensor velocity, Tensor index, Tensor learning_rate, Tensor master_param,float mu, Scalar axis=0, bool use_nesterov=false,str regularization_method="", float regularization_coeff=0.0f, bool multi_precision=false, float rescale_grad=1.0f)
  output: Tensor(param_out), Tensor(velocity_out), Tensor(master_param_out)
  infer_meta:
    func: SparseMomentumInferMeta
    param: [param, grad, velocity, index, learning_rate]
  kernel:
    func: sparse_momentum
    data_type: param
  optional: master_param, master_param_out

- op : subtract
  args : (Tensor x, Tensor y)
  output : Tensor(out)
  infer_meta :
    func : ElementwiseInferMeta
    spmd_rule : ElementwiseBinaryInferSpmd
  kernel :
    func : subtract
  inplace : (x -> out)
  data_transform :
    support_trans_dtype : x, y
  backward : subtract_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : tile
  args : (Tensor x, IntArray repeat_times = {})
  output : Tensor(out)
  infer_meta :
    func : TileInferMeta
    spmd_rule : TileInferSpmdDynamic
  kernel :
    func : tile
  backward : tile_grad
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : unique
  args : (Tensor x, bool return_index=false, bool return_inverse=false, bool return_counts=false, int[] axis={}, DataType dtype=DataType::INT64, bool is_sorted=false)
  output : Tensor(out), Tensor(indices), Tensor(inverse), Tensor(counts)
  optional : indices, counts
  infer_meta :
    func : UniqueRawInferMeta
  kernel :
    func : unique
    data_type : x
  interfaces : paddle::dialect::ParseKernelKeyInterface
  interfaces : paddle::dialect::InferSymbolicShapeInterface

- op : write_to_array
  args : (Tensor i, Tensor x)
  output : Tensor[](out)

- op: c_softmax_with_cross_entropy
  args: (Tensor logits, Tensor label,  int64_t ignore_index=-100, int ring_id=0, int rank=0, int nranks=0)
  output: Tensor(softmax), Tensor(loss)
  infer_meta:
    func : CSoftmaxWithCrossEntropyInferMeta
  kernel:
    func: c_softmax_with_cross_entropy
    data_type : logits
  backward: c_softmax_with_cross_entropy_grad

- op: fused_attention
  args: (Tensor x, Tensor ln_scale, Tensor ln_bias, Tensor qkv_weight, Tensor qkv_bias, Tensor cache_kv, Tensor src_mask, Tensor out_linear_weight, Tensor out_linear_bias, Tensor ln_scale_2, Tensor ln_bias_2, int num_heads, bool transpose_qkv_wb, bool pre_layer_norm, float epsilon, float attn_dropout_rate, bool is_test, bool attn_dropout_fix_seed, int attn_dropout_seed, str attn_dropout_implementation, float dropout_rate, bool dropout_fix_seed, int dropout_seed, str dropout_implementation, float ln_epsilon, bool add_residual, int ring_id)
  output: Tensor(ln_mean), Tensor(ln_var), Tensor(ln_out), Tensor(qkv_out), Tensor(qkv_bias_out), Tensor(transpose_out_2), Tensor(qk_out), Tensor(qktv_out), Tensor(softmax_out), Tensor(attn_dropout_mask_out), Tensor(attn_dropout_out), Tensor(src_mask_out), Tensor(fmha_out), Tensor(out_linear_out), Tensor(dropout_mask_out), Tensor(ln_mean_2), Tensor(ln_var_2), Tensor(bias_dropout_residual_out), Tensor(cache_kv_out), Tensor(out)
  kernel:
    func: fused_attention
    data_type : x
  infer_meta:
    func: FusedAttentionInferMeta
  optional: cache_kv, ln_scale, ln_bias, qkv_bias, src_mask, out_linear_bias, ln_scale_2, ln_bias_2, ln_mean_2, ln_var_2, bias_dropout_residual_out, cache_kv_out
  backward: fused_attention_grad

- op: fused_feedforward
  args: (Tensor x, Tensor dropout1_seed, Tensor dropout2_seed, Tensor linear1_weight, Tensor linear1_bias, Tensor linear2_weight, Tensor linear2_bias, Tensor ln1_scale, Tensor ln1_bias, Tensor ln2_scale, Tensor ln2_bias, bool pre_layer_norm, float ln1_epsilon, float ln2_epsilon, str act_method, float dropout1_prob, float dropout2_prob, str dropout1_implementation, str dropout2_implementation, bool is_test, bool dropout1_fix_seed, bool dropout2_fix_seed, int dropout1_seed_val, int dropout2_seed_val, bool add_residual, int ring_id)
  output: Tensor(out), Tensor(dropout1_mask), Tensor(dropout2_mask), Tensor(ln1_mean), Tensor(ln1_variance), Tensor(ln2_mean), Tensor(ln2_variance), Tensor(linear1_out), Tensor(ln1_out), Tensor(dropout1_out), Tensor(dropout2_out)
  kernel:
    func: fused_feedforward
    data_type : x
  infer_meta:
    func: FusedFeedForwardInferMeta
  optional: dropout1_seed, dropout2_seed, linear1_bias, linear2_bias, ln1_scale, ln1_bias, ln2_scale, ln2_bias, ln2_mean, ln2_variance, ln1_mean, ln1_variance, ln1_out
  backward: fused_feedforward_grad

- op: moving_average_abs_max_scale
  args: (Tensor x, Tensor in_accum, Tensor in_state, float moving_rate=0.9f, bool is_test=false)
  output: Tensor(out), Tensor(out_scale), Tensor(out_state), Tensor(out_accum)
  infer_meta:
    func: MovingAverageAbsMaxScaleInferMeta
    param: [x, in_accum, in_state]
  kernel:
    func: moving_average_abs_max_scale
    param: [x, in_accum, in_state, moving_rate, is_test]
  optional : in_accum, in_state, out, out_state, out_accum
  inplace : (in_accum -> out_accum), (in_state -> out_state)

- op: nce
  args: (Tensor input, Tensor label, Tensor weight, Tensor bias, Tensor sample_weight, Tensor custom_dist_probs, Tensor custom_dist_alias, Tensor custom_dist_alias_probs, int num_total_classes, int[] custom_neg_classes={}, int num_neg_samples=10, int sampler=0, int seed=0, bool is_sparse=false, bool remote_prefetch=false, bool is_test=false)
  output: Tensor(cost), Tensor(sample_logits), Tensor(sample_labels)
  infer_meta:
    func: NceInferMeta
  kernel:
    func: nce
    data_type: input
  optional: bias, sample_weight, custom_dist_probs, custom_dist_alias, custom_dist_alias_probs
  backward: nce_grad

- op: onednn_to_paddle_layout
  args: (Tensor x, int dst_layout)
  output: Tensor(out)
  infer_meta:
    func : UnchangedInferMeta
    param : [x]
  kernel:
    func: onednn_to_paddle_layout

- op: partial_send
  args: (Tensor x, int ring_id = 0, int peer = 0, bool use_calc_stream = false, int num = 1, int id = 0)
  output :
  infer_meta:
    func: PartialSendInferMeta
    param: [x, ring_id, peer, use_calc_stream, num, id]
  kernel:
    func: partial_send
    param: [x, ring_id, peer, use_calc_stream, num, id]
