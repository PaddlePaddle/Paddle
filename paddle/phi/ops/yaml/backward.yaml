# This file is designed for backward C++ operators associated with
# the operator in ops.yaml.

- backward_op : abs_double_grad
  forward : abs_grad (Tensor x, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_x_grad)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  data_transform :
    support_trans_dtype : x, grad_x_grad
  kernel :
    func : abs_double_grad
    data_type : grad_x_grad
  backward : abs_triple_grad

- backward_op : abs_grad
  forward : abs (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : abs_grad
    data_type : x
  composite : abs_grad(x, out_grad, x_grad)
  backward : abs_double_grad

- backward_op : abs_triple_grad
  forward : abs_double_grad (Tensor x, Tensor grad_x_grad) -> Tensor(grad_out_grad)
  args : (Tensor x, Tensor grad_out_grad_grad)
  output : Tensor(grad_x_grad_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  data_transform :
    support_trans_dtype : x
  composite : abs_triple_grad(x, grad_out_grad_grad, grad_x_grad_grad)

- backward_op : acos_grad
  forward : acos (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : acos_grad
  inplace : (out_grad -> x_grad)

- backward_op : acosh_grad
  forward : acosh (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : acosh_grad
  inplace : (out_grad -> x_grad)

- backward_op : addmm_grad
  forward : addmm (Tensor input, Tensor x, Tensor y, float beta=1.0, float alpha=1.0) -> Tensor(out)
  args : (Tensor input, Tensor x, Tensor y, Tensor out_grad, float alpha, float beta)
  output : Tensor(input_grad), Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [input, x, y]
  kernel :
    func : addmm_grad

- backward_op : affine_grid_grad
  forward : affine_grid (Tensor input, IntArray output_shape={}, bool align_corners=true) -> Tensor(output)
  args : (Tensor input, Tensor output_grad, IntArray output_shape, bool align_corners=true)
  output : Tensor(input_grad)
  infer_meta :
    func : AffineGridGradInferMeta
    param : [output_grad, output_shape, align_corners]
  kernel :
    func : affine_grid_grad
    param : [output_grad, output_shape, align_corners]

- backward_op : amax_grad
  forward: amax (Tensor x,  int64_t[] axis={},  bool keepdim=false) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int64_t[] axis={},  bool keepdim=false, bool reduce_all=false)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : amax_grad

- backward_op : amin_grad
  forward: amin (Tensor x,  int64_t[] axis={},  bool keepdim=false) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int64_t[] axis={},  bool keepdim=false, bool reduce_all=false)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : amin_grad

- backward_op : angle_grad
  forward : angle (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : angle_grad

- backward_op : argsort_grad
  forward : argsort (Tensor x, int axis, bool descending, bool stable) -> Tensor(out), Tensor(indices)
  args : (Tensor indices, Tensor x, Tensor out_grad, int axis, bool descending, bool stable)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : argsort_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : as_complex_grad
  forward : as_complex (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  invoke : as_real(out_grad)

- backward_op : as_real_grad
  forward : as_real (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  invoke : as_complex(out_grad)

- backward_op : as_strided_grad
  forward : as_strided (Tensor input, int64_t[] dims = {}, int64_t[] stride = {}, int64_t offset = 0) -> Tensor(out)
  args : (Tensor input, Tensor out_grad, int64_t[] dims = {}, int64_t[] stride = {}, int64_t offset = 0)
  output : Tensor(input_grad)
  infer_meta :
    func : StridedUnChangedInferMeta
    param : [input]
  kernel :
    func : as_strided_grad

- backward_op : asin_grad
  forward : asin (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : asin_grad
  inplace : (out_grad -> x_grad)

- backward_op : asinh_grad
  forward : asinh (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : asinh_grad
  inplace : (out_grad -> x_grad)

- backward_op : atan2_grad
  forward : atan2 (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : atan2_grad

- backward_op : atan_grad
  forward : atan (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : atan_grad
  inplace : (out_grad -> x_grad)

- backward_op : atanh_grad
  forward : atanh (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : atanh_grad
  inplace : (out_grad -> x_grad)

- backward_op : bce_loss_grad
  forward : bce_loss (Tensor input, Tensor label) -> Tensor(out)
  args : (Tensor input, Tensor label, Tensor out_grad)
  output : Tensor(input_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [input]
  kernel :
    func : bce_loss_grad
  inplace : (out_grad -> input_grad)

- backward_op : bicubic_interp_grad
  forward : bicubic_interp (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, str data_format="NCHW", int out_d=0, int out_h=0, int out_w=0, float[] scale={}, str interp_method="bilinear", bool align_corners=true, int align_mode=1) -> Tensor(output)
  args : (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, Tensor output_grad, str data_format, int out_d, int out_h, int out_w, float[] scale, str interp_method, bool align_corners, int align_mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  optional: out_size, size_tensor, scale_tensor
  no_need_buffer : x
  kernel :
    func : bicubic_interp_grad
    data_type : output_grad
  data_transform :
    skip_transform : out_size, size_tensor, scale_tensor

- backward_op : bilinear_grad
  forward : bilinear (Tensor x, Tensor y, Tensor weight, Tensor bias) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor weight, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad), Tensor(weight_grad), Tensor(bias_grad)
  infer_meta :
    func : BilinearGradInferMeta
  kernel :
    func : bilinear_grad

- backward_op : bilinear_interp_grad
  forward : bilinear_interp (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, str data_format="NCHW", int out_d=0, int out_h=0, int out_w=0, float[] scale={}, str interp_method="bilinear", bool align_corners=true, int align_mode=1) -> Tensor(output)
  args : (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, Tensor output_grad, str data_format, int out_d, int out_h, int out_w, float[] scale, str interp_method, bool align_corners, int align_mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  no_need_buffer : x
  optional: out_size, size_tensor, scale_tensor
  kernel :
    func : bilinear_interp_grad
    data_type : output_grad
  data_transform :
    skip_transform : out_size, size_tensor, scale_tensor

- backward_op : bmm_grad
  forward : bmm (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : BmmGradInferMeta
  kernel :
    func : bmm_grad
    data_type : out_grad

- backward_op : broadcast_tensors_grad
  forward : broadcast_tensors (Tensor[] input) -> Tensor[](out)
  args : (Tensor[] input, Tensor[] out_grad)
  output : Tensor[](input_grad){input.size()}
  infer_meta :
    func : UnchangedMultiInferMeta
    param : [input]
  kernel :
    func : broadcast_tensors_grad
    param : [input, out_grad]
    data_type : out_grad
  no_need_buffer : input

- backward_op : cast_grad
  forward : cast (Tensor x, DataType dtype) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  invoke : cast (out_grad, x.dtype())
  composite: cast_grad(x, out_grad, x_grad)
  no_need_buffer : x

- backward_op : ceil_grad
  forward : ceil(Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [out_grad]
  kernel :
    func : ceil_grad
  inplace : (out_grad -> x_grad)

- backward_op : celu_double_grad
  forward : celu_grad(Tensor x, Tensor grad_out, float alpha) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad, float alpha)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : celu_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : celu_grad
  forward : celu(Tensor x, float alpha) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float alpha)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : celu_grad
  backward : celu_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : channel_shuffle_grad
  forward : channel_shuffle (Tensor x, int groups, str data_format="NCHW") -> Tensor(out)
  args : (Tensor out_grad, int groups, str data_format="NCHW")
  output : Tensor(x_grad)
  infer_meta :
    func : ChannelShuffleGradInferMeta
  kernel :
    func : channel_shuffle_grad

- backward_op : cholesky_grad
  forward : cholesky (Tensor x, bool upper) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, bool upper)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : cholesky_grad

- backward_op : cholesky_solve_grad
  forward : cholesky_solve (Tensor x, Tensor y, bool upper) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out, Tensor out_grad, bool upper)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : cholesky_solve_grad

- backward_op : clip_double_grad
  forward : clip_grad (Tensor x, Tensor grad_out, Scalar min = 0., Scalar max = 0.) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_x_grad, Scalar min = 0., Scalar max = 0.)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : clip_grad
    data_type : x

- backward_op : clip_grad
  forward : clip (Tensor x, Scalar min, Scalar max) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, Scalar min = 0., Scalar max = 0.)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : clip_grad
  backward : clip_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : complex_grad
  forward : complex (Tensor real, Tensor imag) -> Tensor(out)
  args : (Tensor real, Tensor imag, Tensor out_grad)
  output : Tensor(real_grad), Tensor(imag_grad)
  infer_meta :
    func : ComplexGradInferMeta
  kernel :
    func : complex_grad
    data_type : real

- backward_op : concat_double_grad
  forward : concat_grad (Tensor[] x, Tensor grad_out, Scalar axis=0) -> Tensor[](grad_x)
  args : (Tensor[] grad_x_grad, Scalar axis = 0)
  output : Tensor(grad_out_grad)
  invoke : concat(grad_x_grad, axis)

- backward_op : concat_grad
  forward : concat (Tensor[] x, Scalar axis=0) -> Tensor(out)
  args : (Tensor[] x, Tensor out_grad, Scalar axis = 0)
  output : Tensor[](x_grad){x.size()}
  infer_meta :
    func : UnchangedMultiInferMeta
    param : [x]
    spmd_rule: ConcatGradInferSpmdDynamic
  kernel :
    func : concat_grad
    data_type : out_grad
  composite : concat_grad(x, out_grad, axis, x_grad)
  no_need_buffer : x
  backward : concat_double_grad

- backward_op : conj_grad
  forward : conj (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  invoke : conj(out_grad)

- backward_op : conv2d_grad
  forward : conv2d (Tensor input, Tensor filter, int[] strides={1, 1}, int[] paddings={0, 0}, str padding_algorithm="EXPLICIT", int[] dilations={1, 1}, int groups=1, str data_format="NCHW") -> Tensor(out)
  args : (Tensor input, Tensor filter, Tensor out_grad,  int[] strides, int[] paddings, str padding_algorithm, int[] dilations, int groups, str data_format)
  output : Tensor(input_grad), Tensor(filter_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [input, filter]
  kernel :
    func : conv2d_grad
    data_type : input
  backward : conv2d_grad_grad

- backward_op : conv2d_grad_grad
  forward : conv2d_grad (Tensor input, Tensor filter, Tensor grad_out,  int[] strides, int[] paddings, str padding_algorithm, int[] dilations, int groups, str data_format) -> Tensor(grad_input), Tensor(grad_filter)
  args : (Tensor input, Tensor filter, Tensor grad_out, Tensor grad_input_grad, Tensor grad_filter_grad, int[] strides, int[] paddings, str padding_algorithm, int[] dilations, int groups, str data_format)
  output : Tensor(input_grad), Tensor(filter_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param: [input, filter, grad_out]
  kernel :
    func : conv2d_double_grad
    data_type : input
  optional : grad_input_grad, grad_filter_grad

- backward_op : conv2d_transpose_double_grad
  forward : conv2d_transpose_grad(Tensor x, Tensor filter, Tensor grad_out, int[] strides, int[] paddings, int[] output_padding, IntArray output_size, str padding_algorithm, int groups, int[] dilations, str data_format) -> Tensor(grad_x), Tensor(grad_filter)
  args : (Tensor x, Tensor filter, Tensor grad_out, Tensor grad_x_grad, Tensor grad_filter_grad, int[] strides, int[] paddings, int[] output_padding, IntArray output_size, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(x_grad), Tensor(filter_grad), Tensor(grad_out_grad)
  infer_meta :
    func : Conv2dTransposeDoubleGradInferMeta
  kernel :
    func : conv2d_transpose_double_grad
    data_type : x

- backward_op : conv2d_transpose_grad
  forward : conv2d_transpose(Tensor x, Tensor filter, int[] strides={1, 1}, int[] paddings={0, 0}, int[] output_padding={}, IntArray output_size={}, str padding_algorithm="EXPLICIT", int groups=1, int[] dilations={1, 1}, str data_format="NCHW") -> Tensor(out)
  args : (Tensor x, Tensor filter, Tensor out_grad, int[] strides, int[] paddings, int[] output_padding, IntArray output_size, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(x_grad), Tensor(filter_grad)
  infer_meta :
    func : Conv2dTransposeGradInferMeta
  kernel :
    func : conv2d_transpose_grad
    data_type : x
  backward : conv2d_transpose_double_grad

- backward_op : conv3d_double_grad
  forward : conv3d_grad (Tensor input, Tensor filter, Tensor grad_out,  int[] strides, int[] paddings, str padding_algorithm, int groups, int[] dilations, str data_format) -> Tensor(grad_input), Tensor(grad_filter)
  args : (Tensor input, Tensor filter, Tensor grad_out, Tensor grad_input_grad, Tensor grad_filter_grad, int[] strides, int[] paddings, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(input_grad), Tensor(filter_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param: [input, filter, grad_out]
  kernel :
    func : conv3d_double_grad
    data_type : input
  optional : grad_input_grad, grad_filter_grad

- backward_op : conv3d_grad
  forward : conv3d (Tensor input, Tensor filter, int[] strides={1, 1, 1}, int[] paddings={0, 0, 0}, str padding_algorithm="EXPLICIT", int groups=1, int[] dilations={1, 1, 1}, str data_format="NCDHW") -> Tensor(out)
  args : (Tensor input, Tensor filter, Tensor out_grad,  int[] strides, int[] paddings, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(input_grad), Tensor(filter_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [input, filter]
  kernel :
    func : conv3d_grad
    data_type : input
  backward : conv3d_double_grad

- backward_op : conv3d_transpose_grad
  forward : conv3d_transpose(Tensor x, Tensor filter, int[] strides={1, 1, 1}, int[] paddings={0, 0, 0}, int[] output_padding={}, int[] output_size={}, str padding_algorithm="EXPLICIT", int groups=1, int[] dilations={1, 1, 1}, str data_format="NCHW") -> Tensor(out)
  args : (Tensor x, Tensor filter, Tensor out_grad, int[] strides, int[] paddings, int[] output_padding, int[] output_size, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(x_grad), Tensor(filter_grad)
  infer_meta :
    func : ConvTransposeGradInferMeta
  kernel :
    func : conv3d_transpose_grad
    data_type : x

- backward_op : copysign_grad
  forward : copysign (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : copysign_grad
  inplace : (out_grad -> x_grad)

- backward_op : correlation_grad
  forward : correlation (Tensor input1, Tensor input2, int pad_size, int kernel_size, int max_displacement, int stride1, int stride2, int corr_type_multiply=1) -> Tensor(out)
  args : (Tensor input1, Tensor input2, Tensor out_grad, int pad_size, int kernel_size, int max_displacement, int stride1, int stride2, int corr_type_multiply=1)
  output : Tensor(input1_grad), Tensor(input2_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [input1, input2]
  kernel :
    func : correlation_grad

- backward_op : cos_double_grad
  forward : cos_grad (Tensor x, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : cos_double_grad
  backward : cos_triple_grad
  inplace : (grad_x_grad -> grad_out_grad)
  composite : cos_double_grad(x, grad_out, grad_x_grad, x_grad, grad_out_grad)

- backward_op : cos_grad
  forward : cos (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : cos_grad
  backward : cos_double_grad
  composite : cos_grad(x, out_grad, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : cos_triple_grad
  forward : cos_double_grad (Tensor x, Tensor grad_out_forward, Tensor grad_x_grad_forward) -> Tensor(grad_x), Tensor(grad_out_grad)
  args : (Tensor x, Tensor grad_out_forward, Tensor grad_x_grad_forward, Tensor grad_x_grad, Tensor grad_out_grad_grad)
  output : Tensor(x_grad), Tensor(grad_out_forward_grad), Tensor(grad_x_grad_forward_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, x, grad_x_grad_forward]
  kernel :
    func : cos_triple_grad
  optional: grad_out_forward, grad_x_grad_forward, grad_out_grad_grad
  inplace : (grad_x_grad_forward -> grad_out_forward_grad)

- backward_op : cosh_grad
  forward : cosh (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : cosh_grad
  inplace : (out_grad -> x_grad)

- backward_op : crop_grad
  forward : crop (Tensor x, IntArray shape, IntArray offsets) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray offsets)
  output : Tensor(x_grad)
  infer_meta :
    func : CropGradInferMeta
  kernel :
    func : crop_grad
    data_type : x

- backward_op : cross_entropy_with_softmax_grad
  forward : cross_entropy_with_softmax (Tensor input, Tensor label, bool soft_label=false, bool use_softmax=true, bool numeric_stable_mode=true, int ignore_index=-100, int axis=-1) -> Tensor(softmax), Tensor(loss)
  args : (Tensor label, Tensor softmax, Tensor loss_grad, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis)
  output : Tensor(input_grad)
  infer_meta :
    func : CrossEntropyWithSoftmaxGradInferMeta
    spmd_rule : CrossEntropyWithSoftmaxGradInferSpmd
  kernel :
    func : cross_entropy_with_softmax_grad
    data_type : loss_grad
  inplace : (softmax -> input_grad)

- backward_op : cross_grad
  forward : cross (Tensor x, Tensor y, int axis = 9) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad, int axis)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : cross_grad
    data_type : out_grad

- backward_op : cudnn_lstm_grad
  forward: cudnn_lstm (Tensor x, Tensor init_h, Tensor init_c, Tensor w, Tensor[] weight_list, Tensor sequence_length, float dropout_prob = 0.0, bool is_bidirec = false, int hidden_size = 100, int num_layers = 1, bool is_test = false, int seed = 0) -> Tensor (out), Tensor (last_h), Tensor (last_c), Tensor (reserve), Tensor (state_out)
  args: (Tensor x, Tensor init_h, Tensor init_c, Tensor[] weight_list, Tensor sequence_length, Tensor out, Tensor reserve, Tensor state_out, Tensor out_grad, Tensor last_h_grad, Tensor last_c_grad, float dropout_prob = 0.0, bool is_bidirec = false, int hidden_size = 100, int num_layers = 1, bool is_test = false, int seed = 0)
  output: Tensor (x_grad), Tensor (init_h_grad), Tensor (init_c_grad), Tensor[](weight_list_grad){weight_list.size()}
  infer_meta:
    func: CudnnLSTMGradInferMeta
    param : [x, init_h, init_c, weight_list]
  kernel:
    func: cudnn_lstm_grad
    data_type : out_grad
  optional: weight_list, sequence_length, weight_list_grad

- backward_op : cummax_grad
  forward : cummax(Tensor x, int axis=-1, DataType dtype = DataType::INT64) -> Tensor(out), Tensor(indices)
  args : (Tensor x, Tensor indices, Tensor out_grad, int axis, DataType dtype)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : cummax_grad
    data_type : out_grad

- backward_op : cummin_grad
  forward : cummin(Tensor x, int axis=-1, DataType dtype = DataType::INT64) -> Tensor(out), Tensor(indices)
  args : (Tensor x, Tensor indices, Tensor out_grad, int axis, DataType dtype)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : cummin_grad
    data_type : out_grad

- backward_op : cumprod_grad
  forward : cumprod (Tensor x, int dim, bool exclusive=false, bool reverse=false) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int dim, bool exclusive, bool reverse)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : cumprod_grad
  composite: cumprod_grad(x, out, out_grad, dim, exclusive, reverse, x_grad)

- backward_op : cumsum_grad
  forward : cumsum(Tensor x, Scalar axis=-1, bool flatten=false, bool exclusive=false, bool reverse=false) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, Scalar axis, bool flatten, bool exclusive, bool reverse)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : cumsum_grad
    data_type: x
  composite: cumsum_grad(x, out_grad, axis, flatten, exclusive, reverse, x_grad)

- backward_op : deformable_conv_grad
  forward : deformable_conv(Tensor x, Tensor offset, Tensor filter, Tensor mask, int[] strides, int[] paddings, int[] dilations, int deformable_groups, int groups, int im2col_step) -> Tensor(out)
  args : (Tensor x, Tensor offset, Tensor filter, Tensor mask, Tensor out_grad, int[] strides, int[] paddings, int[] dilations, int deformable_groups, int groups, int im2col_step)
  output : Tensor(x_grad), Tensor(offset_grad), Tensor(filter_grad), Tensor(mask_grad)
  infer_meta :
    func : DeformableConvGradInferMeta
  kernel :
    func : deformable_conv_grad
    data_type : x
  optional : mask

- backward_op : depthwise_conv2d_double_grad
  forward : depthwise_conv2d_grad (Tensor input, Tensor filter, Tensor grad_out, int[] strides, int[] paddings, str padding_algorithm, int groups, int[] dilations, str data_format) -> Tensor(grad_input), Tensor(grad_filter)
  args : (Tensor input, Tensor filter, Tensor grad_out, Tensor grad_input_grad, Tensor grad_filter_grad, int[] strides, int[] paddings, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(input_grad), Tensor(filter_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param: [input, filter, grad_out]
  kernel :
    func : depthwise_conv2d_double_grad
    data_type : input
  optional : grad_input_grad, grad_filter_grad

- backward_op : depthwise_conv2d_grad
  forward : depthwise_conv2d (Tensor input, Tensor filter, int[] strides={1, 1}, int[] paddings={0, 0}, str padding_algorithm="EXPLICIT", int groups=1, int[] dilations={1, 1}, str data_format="NCHW") -> Tensor(out)
  args : (Tensor input, Tensor filter, Tensor out_grad, int[] strides, int[] paddings, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(input_grad), Tensor(filter_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [input, filter]
  kernel :
    func : depthwise_conv2d_grad
    data_type : input
  backward : depthwise_conv2d_double_grad

- backward_op : depthwise_conv2d_transpose_grad
  forward : depthwise_conv2d_transpose(Tensor x, Tensor filter, int[] strides={1, 1}, int[] paddings={0, 0}, int[] output_padding={}, IntArray output_size={}, str padding_algorithm="EXPLICIT", int groups=1, int[] dilations={1, 1}, str data_format="NCHW") -> Tensor(out)
  args : (Tensor x, Tensor filter, Tensor out_grad, int[] strides, int[] paddings, int[] output_padding, IntArray output_size, str padding_algorithm, int groups, int[] dilations, str data_format)
  output : Tensor(x_grad), Tensor(filter_grad)
  infer_meta :
    func : Conv2dTransposeGradInferMeta
  kernel :
    func : depthwise_conv2d_transpose_grad
    data_type : x

- backward_op : det_grad
  forward : det (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : determinant_grad
    data_type : out_grad

- backward_op : diag_grad
  forward : diag (Tensor x, int offset, float padding_value) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int offset)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : diag_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : diagonal_grad
  forward : diagonal (Tensor x, int offset, int axis1, int axis2) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int offset = 0, int axis1 = 0, int axis2 = 1)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : diagonal_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : digamma_grad
  forward : digamma (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : digamma_grad

- backward_op : dist_grad
  forward : dist (Tensor x, Tensor y, float p) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out, Tensor out_grad, float p)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : dist_grad

- backward_op : dot_grad
  forward : dot (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : dot_grad
    data_type : out_grad

- backward_op : dropout_grad
  forward : dropout (Tensor x, Tensor seed_tensor, Scalar p, bool is_test, str mode, int seed, bool fix_seed) -> Tensor(out), Tensor(mask)
  args : (Tensor mask, Tensor out_grad, Scalar p, bool is_test, str mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : dropout_grad
  composite : dropout_grad(mask, out_grad, p, is_test, mode, x_grad)

- backward_op : eig_grad
  forward : eig (Tensor x) -> Tensor(out_w), Tensor(out_v)
  args : (Tensor out_w, Tensor out_v, Tensor out_w_grad, Tensor out_v_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : EigGradInferMeta
  kernel :
    func : eig_grad
    data_type : out_v

- backward_op : eigh_grad
  forward : eigh (Tensor x, str UPLO) -> Tensor(out_w), Tensor(out_v)
  args : (Tensor out_w, Tensor out_v, Tensor out_w_grad, Tensor out_v_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_v]
  kernel :
    func : eigh_grad
    data_type : out_v

- backward_op : eigvalsh_grad
  forward : eigvalsh (Tensor x, str uplo = "L", bool is_test = false) -> Tensor(eigenvalues), Tensor(eigenvectors)
  args : (Tensor eigenvectors, Tensor eigenvalues_grad, str uplo, bool is_test)
  output : Tensor(x_grad)
  infer_meta :
    func : EigvalshGradInferMeta
  kernel :
    func : eigvalsh_grad
    data_type : eigenvectors

- backward_op : elu_double_grad
  forward : elu_grad (Tensor x, Tensor out, Tensor grad_out, float alpha)-> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad, float alpha)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : elu_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : elu_grad
  forward : elu (Tensor x, float alpha) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, float alpha)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : elu_grad
  backward : elu_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : erf_grad
  forward : erf (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : erf_grad
    data_type : out_grad
  composite : erf_grad(x, out_grad, x_grad)

- backward_op : erfinv_grad
  forward : erfinv (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : erfinv_grad

- backward_op : exp_double_grad
  forward : exp_grad (Tensor out, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor out, Tensor grad_out, Tensor grad_x_grad)
  output : Tensor(out_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [out, out]
  composite : exp_double_grad(out, grad_out, grad_x_grad, out_grad, grad_out_grad)
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : exp_grad
  forward : exp (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : exp_grad
  inplace : (out_grad -> x_grad)
  backward : exp_double_grad
  composite : exp_grad(out, out_grad, x_grad)

- backward_op : expand_as_grad
  forward : expand_as (Tensor x, Tensor y, int[] target_shape = {}) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int[] target_shape)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : expand_as_grad
  no_need_buffer : x

- backward_op : expand_double_grad
  forward : expand_grad (Tensor x, Tensor grad_out, IntArray shape) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray shape)
  output : Tensor(grad_out_grad)
  invoke : expand(grad_x_grad, shape)

- backward_op : expand_grad
  forward : expand (Tensor x, IntArray shape) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray shape)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : expand_grad
    data_type : out_grad
  no_need_buffer : x
  backward : expand_double_grad
  composite: expand_grad(x, out_grad, shape, x_grad)

- backward_op : expm1_grad
  forward : expm1 (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : expm1_grad
  inplace : (out_grad -> x_grad)

- backward_op : fake_channel_wise_quantize_dequantize_abs_max_grad
  forward: fake_channel_wise_quantize_dequantize_abs_max(Tensor x, int bit_length = 8, int round_type = 1, int quant_axis = 0) -> Tensor(out), Tensor(out_scale)
  args : (Tensor out_grad, int bit_length = 8, int round_type = 1, int quant_axis = 0)
  output: Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : fake_channel_wise_quantize_dequantize_abs_max_grad

- backward_op : fake_quantize_dequantize_abs_max_grad
  forward: fake_quantize_dequantize_abs_max(Tensor x, int bit_length = 8, int round_type = 1) -> Tensor(out), Tensor(out_scale)
  args : (Tensor out_grad, int bit_length = 8, int round_type = 1)
  output: Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : fake_quantize_dequantize_abs_max_grad

- backward_op : fake_quantize_dequantize_moving_average_abs_max_grad
  forward: fake_quantize_dequantize_moving_average_abs_max(Tensor x, Tensor in_scale, Tensor in_accum, Tensor in_state, float moving_rate = 0.9, int bit_length = 8, bool is_test = false, int round_type = 1) -> Tensor(out), Tensor(out_scale), Tensor(out_state), Tensor(out_accum)
  args : (Tensor out_grad, float moving_rate = 0.9, int bit_length = 8, bool is_test = false, int round_type = 1)
  output: Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : fake_quantize_dequantize_moving_average_abs_max_grad

- backward_op : fft_c2c_grad
  forward: fft_c2c(Tensor x, int64_t[] axes, str normalization, bool forward) -> Tensor(out)
  args : (Tensor out_grad, int64_t[] axes, str normalization, bool forward)
  output: Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : fft_c2c_grad

- backward_op : fft_c2r_grad
  forward: fft_c2r(Tensor x, int64_t[] axes, str normalization, bool forward, int64_t last_dim_size) -> Tensor(out)
  args : (Tensor out_grad, int64_t[] axes, str normalization, bool forward, int64_t last_dim_size)
  output: Tensor(x_grad)
  infer_meta :
    func : FFTC2RGradInferMeta
  kernel :
    func : fft_c2r_grad
    data_type: out_grad

- backward_op : fft_r2c_grad
  forward: fft_r2c(Tensor x, int64_t[] axes, str normalization, bool forward, bool onesided) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int64_t[] axes, str normalization, bool forward, bool onesided)
  output: Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : fft_r2c_grad
    data_type: out_grad
  no_need_buffer: x

- backward_op : fill_diagonal_grad
  forward : fill_diagonal (Tensor x, float value=0, int offset=0, bool wrap=false) -> Tensor(out)
  args : (Tensor out_grad, float value, int offset, bool wrap)
  output : Tensor(x_grad)
  infer_meta :
    func : FillDiagonalGradInferMeta
  kernel :
    func : fill_diagonal_grad

- backward_op : fill_diagonal_tensor_grad
  forward : fill_diagonal_tensor (Tensor x, Tensor y, int64_t offset, int dim1, int dim2) -> Tensor(out)
  args : (Tensor out_grad, int64_t offset, int dim1, int dim2)
  output : Tensor(x_grad)
  infer_meta :
    func : FillDiagonalTensorGradInferMeta
  kernel :
    func : fill_diagonal_tensor_grad
  inplace : (out_grad -> x_grad)

- backward_op : fill_grad
  forward : fill (Tensor x, Scalar value=0) -> Tensor(out)
  args : (Tensor out_grad, Scalar value)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : fill_grad
  inplace : (out_grad -> x_grad)

- backward_op : flash_attn_grad
  forward : flash_attn (Tensor q, Tensor k, Tensor v,  Tensor fixed_seed_offset, Tensor attn_mask, float dropout = 0.0, bool causal = false, bool return_softmax = false, bool is_test = false, str rng_name = "") -> Tensor(out), Tensor(softmax), Tensor(softmax_lse), Tensor(seed_offset)
  args : (Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_lse, Tensor seed_offset, Tensor attn_mask, Tensor out_grad, float dropout = 0.0, bool causal = false)
  optional : attn_mask
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  infer_meta :
    func : FlashAttnGradInferMeta
    param : [q, k, v]
    spmd_rule : FlashAttGradInferSpmd
  kernel :
    func : flash_attn_grad
    data_type: q

- backward_op : flash_attn_qkvpacked_grad
  forward : flash_attn_qkvpacked (Tensor qkv,  Tensor fixed_seed_offset, Tensor attn_mask, float dropout = 0.0, bool causal = false, bool return_softmax = false, bool is_test = false, str rng_name = "") -> Tensor(out), Tensor(softmax), Tensor(softmax_lse), Tensor(seed_offset)
  args : (Tensor qkv, Tensor out, Tensor softmax_lse, Tensor seed_offset, Tensor attn_mask, Tensor out_grad, float dropout = 0.0, bool causal = false)
  optional : attn_mask
  output : Tensor(qkv_grad)
  infer_meta :
    func : FlashAttnQKVPackedGradInferMeta
    param : [qkv]
  kernel :
    func : flash_attn_qkvpacked_grad
    data_type: qkv

- backward_op : flash_attn_unpadded_grad
  forward : flash_attn_unpadded (Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, Tensor fixed_seed_offset, Tensor attn_mask, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout = 0.0, bool causal = false, bool return_softmax = false, bool is_test = false, str rng_name = "") -> Tensor(out), Tensor(softmax), Tensor(softmax_lse), Tensor(seed_offset)
  args : (Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, Tensor out, Tensor softmax_lse, Tensor seed_offset, Tensor attn_mask, Tensor out_grad, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout = 0.0, bool causal = false)
  optional : attn_mask
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  infer_meta :
    func : FlashAttnGradInferMeta
    param : [q, k, v]
  kernel :
    func : flash_attn_unpadded_grad
    data_type: q

- backward_op : flash_attn_varlen_qkvpacked_grad
  forward : flash_attn_varlen_qkvpacked (Tensor qkv, Tensor cu_seqlens_q, Tensor cu_seqlens_k, Tensor fixed_seed_offset, Tensor attn_mask, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout = 0.0, bool causal = false, bool return_softmax = false, bool is_test = false, str rng_name = "", bool varlen_padded = true) -> Tensor(out), Tensor(softmax), Tensor(softmax_lse), Tensor(seed_offset)
  args : (Tensor qkv, Tensor cu_seqlens_q, Tensor cu_seqlens_k, Tensor out, Tensor softmax_lse, Tensor seed_offset, Tensor attn_mask, Tensor out_grad, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout = 0.0, bool causal = false, bool varlen_padded = true)
  optional : attn_mask
  output : Tensor(qkv_grad)
  infer_meta :
    func : FlashAttnQKVPackedGradInferMeta
    param : [qkv]
  kernel :
    func : flash_attn_varlen_qkvpacked_grad
    data_type: qkv

- backward_op : flash_attn_with_sparse_mask_grad
  forward : flash_attn_with_sparse_mask (Tensor q, Tensor k, Tensor v, Tensor attn_mask_start_row_indices, Tensor fixed_seed_offset, float dropout = 0.0, bool causal = false, int attn_mask_start_row = 0, bool return_softmax = false, bool is_test = false, str rng_name = "") -> Tensor(out), Tensor(softmax), Tensor(softmax_lse), Tensor(seed_offset)
  args : (Tensor q, Tensor k, Tensor v, Tensor attn_mask_start_row_indices, Tensor out, Tensor softmax_lse, Tensor seed_offset, Tensor out_grad, float dropout = 0.0, bool causal = false, int attn_mask_start_row = 0)
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  infer_meta :
    func : FlashAttnGradInferMeta
    param : [q, k, v]
  kernel :
    func : flash_attn_with_sparse_mask_grad
    data_type: q

- backward_op : flatten_grad
  forward : flatten(Tensor x, int start_axis = 1, int stop_axis = 1) -> Tensor(out), Tensor(xshape)
  args : (Tensor xshape, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func :  KernelWithXShapeInferMeta
    param : [xshape, out_grad]
    spmd_rule : FlattenGradInferSpmd
  kernel :
    func : flatten_grad
    data_type : out_grad
  inplace : (out_grad -> x_grad)

- backward_op : flip_grad
  forward : flip (Tensor x, int[] axis) -> Tensor(out)
  args : (Tensor out_grad, int[] axis)
  output : Tensor(x_grad)
  invoke : flip(out_grad, axis)

- backward_op : floor_grad
  forward : floor(Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [out_grad]
  kernel :
    func : floor_grad
  composite : floor_grad(out_grad, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : fmax_grad
  forward : fmax(Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param: [x, y]
  kernel :
    func : fmax_grad
    data_type : out_grad

- backward_op : fmin_grad
  forward : fmin(Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param: [x, y]
  kernel :
    func : fmin_grad
    data_type : out_grad

- backward_op : fold_grad
  forward: fold (Tensor x, int[] output_sizes, int[] kernel_sizes, int[] strides, int[] paddings, int[] dilations) -> Tensor(out)
  args: (Tensor x, Tensor out_grad, int[] output_sizes, int[] kernel_sizes, int[] strides, int[] paddings, int[] dilations)
  output: Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: fold_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : fractional_max_pool2d_grad
  forward : fractional_max_pool2d(Tensor x, int[] output_size, int[] kernel_size = {0, 0}, float random_u = 0.0, bool return_mask = true) -> Tensor(out), Tensor(mask)
  args : (Tensor x, Tensor mask, Tensor out_grad, int[] output_size, int[] kernel_size, float random_u, bool return_mask)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : fractional_max_pool2d_grad

- backward_op : fractional_max_pool3d_grad
  forward : fractional_max_pool3d(Tensor x, int[] output_size, int[] kernel_size = {0, 0, 0}, float random_u = 0.0, bool return_mask = true) -> Tensor(out), Tensor(mask)
  args : (Tensor x, Tensor mask, Tensor out_grad, int[] output_size, int[] kernel_size, float random_u, bool return_mask)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : fractional_max_pool3d_grad

- backward_op : frame_grad
  forward : frame(Tensor x, int frame_length, int hop_length, int axis=-1) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int frame_length, int hop_length, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : frame_grad

- backward_op : frobenius_norm_grad
  forward : frobenius_norm(Tensor x, IntArray axis,  bool keep_dim,  bool reduce_all) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray axis,  bool keep_dim,  bool reduce_all)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : frobenius_norm_grad

- backward_op : fused_batch_norm_act_grad
  forward : fused_batch_norm_act (Tensor x, Tensor scale, Tensor bias, Tensor mean, Tensor variance, float momentum, float epsilon, str act_type) -> Tensor(out), Tensor(mean_out), Tensor(variance_out), Tensor(saved_mean), Tensor(saved_variance), Tensor(reserve_space)
  args : (Tensor x, Tensor scale, Tensor bias, Tensor out, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor out_grad, float momentum, float epsilon, str act_type)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, scale, bias]
  kernel :
    func : fused_batch_norm_act_grad
    data_type : out_grad
  optional : reserve_space

- backward_op : fused_bn_add_activation_grad
  forward : fused_bn_add_activation (Tensor x, Tensor z, Tensor scale, Tensor bias, Tensor mean, Tensor variance, float momentum, float epsilon, str act_type) -> Tensor(out), Tensor(mean_out), Tensor(variance_out), Tensor(saved_mean), Tensor(saved_variance), Tensor(reserve_space)
  args : (Tensor x, Tensor scale, Tensor bias, Tensor out, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor out_grad, float momentum, float epsilon, str act_type)
  output : Tensor(x_grad), Tensor(z_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralQuaternaryGradInferMeta
    param : [x, x, scale, bias]
  kernel :
    func : fused_bn_add_activation_grad
    data_type : out_grad
  optional : reserve_space

- backward_op : fused_softmax_mask_grad
  forward : fused_softmax_mask (Tensor x, Tensor mask) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param: [out]
  kernel :
    func : fused_softmax_mask_grad
    data_type : out

- backward_op : fused_softmax_mask_upper_triangle_grad
  forward : fused_softmax_mask_upper_triangle(Tensor X) -> Tensor(Out)
  args: (Tensor Out, Tensor Out_grad)
  output : Tensor(X_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [Out_grad]
  kernel:
    func : fused_softmax_mask_upper_triangle_grad

- backward_op : gammaincc_grad
  forward : gammaincc(Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(y_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [y]
  kernel :
    func : gammaincc_grad

- backward_op : gammaln_grad
  forward : gammaln(Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : gammaln_grad

- backward_op : gather_grad
  forward : gather(Tensor x, Tensor index, Scalar axis=0) -> Tensor(out)
  args : (Tensor x, Tensor index, Tensor out_grad, Scalar axis=0)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param: [x]
  kernel :
    data_type: out_grad
    func : gather_grad
  composite : gather_grad(x, index, out_grad, axis, x_grad)
  no_need_buffer : x

- backward_op : gather_nd_grad
  forward : gather_nd (Tensor x, Tensor index) -> Tensor(out)
  args : (Tensor x, Tensor index, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : GatherNdGradInferMeta
  kernel :
    func : gather_nd_grad
  composite : gather_nd_grad(x, index, out_grad, x_grad)
  no_need_buffer : x

- backward_op : gaussian_inplace_grad
  forward : gaussian_inplace(Tensor x, float mean=0, float std=1.0, int seed=0) -> Tensor(out)
  args : (Tensor out_grad, float mean=0, float std=1.0, int seed=0)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : gaussian_inplace_grad
  inplace : (out_grad -> x_grad)

- backward_op : gelu_grad
  forward : gelu(Tensor x,  bool approximate) -> Tensor(out)
  args : (Tensor x, Tensor out_grad,  bool approximate)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : gelu_grad
  composite: gelu_grad(x, out_grad, approximate, x_grad)

- backward_op : global_gather_grad
  forward : global_gather(Tensor x, Tensor local_count, Tensor global_count, int ring_id = 0, bool use_calc_stream=false) -> Tensor(out)
  args : (Tensor out_grad, Tensor local_count, Tensor global_count, int ring_id = 0, bool use_calc_stream=false)
  output : Tensor(x_grad)
  invoke : global_scatter(out_grad, local_count, global_count, ring_id, use_calc_stream)

- backward_op : global_scatter_grad
  forward : global_scatter(Tensor x, Tensor local_count, Tensor global_count, int ring_id = 0, bool use_calc_stream=false) -> Tensor(out)
  args : (Tensor out_grad, Tensor local_count, Tensor global_count, int ring_id = 0, bool use_calc_stream=false)
  output : Tensor(x_grad)
  invoke : global_gather(out_grad, local_count, global_count, ring_id, use_calc_stream)

- backward_op : grid_sample_grad
  forward : grid_sample (Tensor x, Tensor grid, str mode, str padding_mode, bool align_corners) -> Tensor(out)
  args : (Tensor x, Tensor grid, Tensor out_grad, str mode, str padding_mode, bool align_corners)
  output : Tensor(x_grad), Tensor(grid_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, grid]
  kernel :
    func : grid_sample_grad
    data_type : x

- backward_op : group_norm_grad
  forward : group_norm (Tensor x, Tensor scale, Tensor bias, float epsilon = 1e-5, int groups = -1, str data_format = "NCHW") -> Tensor(y), Tensor(mean), Tensor(variance)
  args : (Tensor x, Tensor scale, Tensor bias, Tensor y, Tensor mean, Tensor variance, Tensor y_grad, float epsilon, int groups, str data_format)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [y, scale, bias]
  kernel :
    func : group_norm_grad
    data_type : y_grad
  composite : group_norm_grad(x, scale, bias, y, mean, variance, y_grad, epsilon, groups, data_format, x_grad, scale_grad, bias_grad)
  optional: scale, bias
  inplace : (y_grad -> x_grad)

- backward_op : gru_grad
  forward: gru (Tensor input, Tensor h0, Tensor weight, Tensor bias, str activation = "tanh",
    str gate_activation = "sigmoid", bool is_reverse = false, bool origin_mode = false, bool is_test=false) ->
    Tensor (batch_gate), Tensor (batch_reset_hidden_prev), Tensor (batch_hidden),
    Tensor (hidden)
  args: (Tensor input, Tensor h0, Tensor weight, Tensor bias, Tensor batch_gate,
    Tensor batch_reset_hidden_prev, Tensor batch_hidden, Tensor hidden,
    Tensor hidden_grad, str activation = "tanh",
    str gate_activation = "sigmoid", bool is_reverse = false, bool origin_mode = false, bool is_test=false)
  output: Tensor(input_grad), Tensor(h0_grad), Tensor(weight_grad), Tensor(bias_grad)
  infer_meta:
    func: GruGradInferMeta
    param: [input, h0, weight, bias]
  kernel:
    func: gru_grad
    data_type: hidden_grad
  optional: h0, bias
  no_need_buffer: input, bias

- backward_op : gru_unit_grad
  forward: gru_unit (Tensor input, Tensor hidden_prev, Tensor weight, Tensor bias, int activation
    = 2, int gate_activation = 1, bool origin_mode = false) -> Tensor (gate), Tensor (reset_hidden_prev), Tensor (hidden)
  args: (Tensor input, Tensor hidden_prev, Tensor weight, Tensor bias, Tensor gate, Tensor reset_hidden_prev, Tensor hidden_grad,
    int activation, int gate_activation, bool origin_mode)
  output: Tensor (input_grad), Tensor (hidden_prev_grad), Tensor (weight_grad), Tensor (bias_grad)
  infer_meta:
    func: GruUnitGradInferMeta
    param : [input, hidden_prev, weight, bias]
  kernel:
    func: gru_unit_grad
    data_type: hidden_grad
  optional: bias
  no_need_buffer: bias

- backward_op : gumbel_softmax_grad
  forward : gumbel_softmax (Tensor x, float temperature, bool hard, int axis) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : GumbelSoftmaxGradInferMeta
  kernel :
    func : gumbel_softmax_grad

- backward_op : hardshrink_grad
  forward : hardshrink (Tensor x, float threshold) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float threshold)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : hard_shrink_grad
  inplace : (out_grad -> x_grad)

- backward_op : hardsigmoid_grad
  forward : hardsigmoid (Tensor x, float slope, float offset) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, float slope, float offset)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : hardsigmoid_grad
  inplace : (out_grad -> x_grad)

- backward_op : hardtanh_grad
  forward : hardtanh (Tensor x, float t_min=0, float t_max=24) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float t_min, float t_max)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : hardtanh_grad
  inplace : (out_grad -> x_grad)

- backward_op : heaviside_grad
  forward : heaviside (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : heaviside_grad
    data_type : out_grad

- backward_op : hsigmoid_loss_grad
  forward : hsigmoid_loss (Tensor x, Tensor label, Tensor w, Tensor bias, Tensor path, Tensor code, int num_classes, bool is_sparse) -> Tensor(out), Tensor(pre_out), Tensor(w_out)
  args : (Tensor x, Tensor w, Tensor label, Tensor path, Tensor code, Tensor bias, Tensor pre_out, Tensor out_grad, int num_classes, bool is_sparse)
  output : Tensor(x_grad), Tensor(w_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x ,w, bias]
  optional: path, code, bias
  kernel :
    func : hsigmoid_loss_grad

- backward_op : huber_loss_grad
  forward : huber_loss (Tensor input, Tensor label, float delta) -> Tensor(out), Tensor(residual)
  args : (Tensor residual, Tensor out_grad, float delta)
  output : Tensor(input_grad), Tensor(label_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [residual, residual]
  kernel :
    func : huber_loss_grad

- backward_op : i0_grad
  forward : i0 (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : i0_grad

- backward_op : i0e_grad
  forward : i0e (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : i0e_grad

- backward_op : i1_grad
  forward : i1 (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : i1_grad

- backward_op : i1e_grad
  forward : i1e (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : i1e_grad

- backward_op : identity_loss_grad
  forward : identity_loss (Tensor x, int reduction) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int reduction)
  output : Tensor(x_grad)
  infer_meta :
    func : IdentityLossGradInferMeta
  kernel :
    func : identity_loss_grad
    data_type : out_grad
  inplace : (out_grad -> x_grad)

- backward_op : imag_grad
  forward : imag (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : RealAndImagGradInferMeta
  kernel :
    func : imag_grad
    data_type : complex(out_grad)

- backward_op : index_add_grad
  forward : index_add(Tensor x, Tensor index,  Tensor add_value, int axis=0) -> Tensor(out)
  args : (Tensor index, Tensor add_value, Tensor out_grad, int axis)
  output : Tensor(x_grad), Tensor(add_value_grad)
  infer_meta :
    func : IndexAddGradInferMeta
  kernel :
    func : index_add_grad
    data_type : out_grad
  inplace : (out_grad -> x_grad)

- backward_op : index_put_grad
  forward : index_put (Tensor x, Tensor[] indices, Tensor value, bool accumulate=false) -> Tensor(out)
  args : (Tensor x, Tensor[] indices, Tensor value, Tensor out_grad, bool accumulate=false)
  output : Tensor(x_grad), Tensor(value_grad)
  infer_meta :
    func : IndexPutGradInferMeta
  kernel :
    func : index_put_grad
    data_type : out_grad
  data_transform :
    skip_transform : indices

- backward_op : index_sample_grad
  forward : index_sample (Tensor x, Tensor index) -> Tensor(out)
  args : (Tensor x, Tensor index, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : index_sample_grad
    data_type : out_grad
  no_need_buffer : x
  data_transform :
    skip_transform : index

- backward_op : index_select_grad
  forward : index_select(Tensor x, Tensor index, int axis) -> Tensor(out)
  args : (Tensor x, Tensor index, Tensor out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : index_select_grad
    data_type : out_grad
  no_need_buffer : x
  data_transform :
    skip_transform : index

- backward_op : index_select_strided_grad
  forward : index_select_strided(Tensor x, int64_t index, int axis) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int64_t index, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : index_select_strided_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : instance_norm_double_grad
  forward : instance_norm_grad(Tensor x, Tensor fwd_scale, Tensor saved_mean, Tensor saved_variance, Tensor grad_y, float epsilon) -> Tensor(grad_x), Tensor(grad_scale), Tensor(grad_bias)
  args : (Tensor x, Tensor fwd_scale, Tensor saved_mean, Tensor saved_variance, Tensor grad_y, Tensor grad_x_grad, Tensor grad_scale_grad, Tensor grad_bias_grad, float epsilon)
  output : Tensor(x_grad), Tensor(fwd_scale_grad), Tensor(grad_y_grad)
  infer_meta :
    func : InstanceNormDoubleGradInferMeta
  kernel :
    func : instance_norm_double_grad
    data_type : x
  optional : fwd_scale, grad_x_grad, grad_scale_grad, grad_bias_grad

- backward_op : instance_norm_grad
  forward : instance_norm(Tensor x, Tensor scale, Tensor bias, float epsilon) -> Tensor(y), Tensor(saved_mean), Tensor(saved_variance)
  args : (Tensor x, Tensor scale, Tensor saved_mean, Tensor saved_variance, Tensor y_grad, float epsilon=1e-5)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : InstanceNormGradInferMeta
  kernel :
    func : instance_norm_grad
    data_type : x
  optional : scale
  backward : instance_norm_double_grad
  composite: instance_norm_grad(x, scale, saved_mean, saved_variance, y_grad, epsilon, x_grad, scale_grad, bias_grad)

- backward_op : inverse_grad
  forward : inverse(Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta:
    func : InverseGradInferMeta
  kernel :
    func : inverse_grad

- backward_op : kldiv_loss_grad
  forward : kldiv_loss(Tensor x, Tensor label, str reduction="mean", bool log_target = false) -> Tensor(out)
  args : (Tensor x, Tensor label, Tensor out_grad, str reduction, bool log_target)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : kldiv_loss_grad
  no_need_buffer : x

- backward_op : kron_grad
  forward : kron (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : kron_grad
    data_type : out_grad

- backward_op : kthvalue_grad
  forward : kthvalue(Tensor x, int k, int axis, bool keepdim) -> Tensor(out), Tensor(indices)
  args : (Tensor x, Tensor indices, Tensor out_grad, int k, int axis, bool keepdim)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : kthvalue_grad
    data_type : out_grad

- backward_op : l1_norm_grad
  forward : l1_norm (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : l1_norm_grad
    data_type : x

- backward_op : label_smooth_grad
  forward : label_smooth (Tensor label, Tensor prior_dist, float epsilon) -> Tensor(out)
  args : (Tensor out_grad, float epsilon)
  output : Tensor(label_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : label_smooth_grad

- backward_op : layer_norm_grad
  forward : layer_norm (Tensor x, Tensor scale, Tensor bias, float epsilon = 1e-5, int begin_norm_axis = 1) -> Tensor(out), Tensor(mean), Tensor(variance)
  args : (Tensor x,  Tensor scale, Tensor bias, Tensor mean, Tensor variance, Tensor out_grad, float epsilon = 1e-5, int begin_norm_axis = 1)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : LayerNormGradInferMeta
    spmd_rule : LayerNormGradInferSpmd
    param : [x, scale, bias]
  kernel :
    func : layer_norm_grad
    data_type : x
  composite : layer_norm_grad(x, scale, bias, mean, variance, out_grad, epsilon, begin_norm_axis, x_grad, scale_grad, bias_grad)
  no_need_buffer : bias
  optional : scale, bias

- backward_op : leaky_relu_double_grad
  forward : leaky_relu_grad (Tensor x, Tensor grad_out, float negative_slope) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_x_grad, float negative_slope)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [grad_x_grad]
  kernel :
    func : leaky_relu_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : leaky_relu_grad
  forward : leaky_relu (Tensor x, float negative_slope) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float negative_slope)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : leaky_relu_grad
  backward : leaky_relu_double_grad
  composite: leaky_relu_grad(x, out_grad, negative_slope, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : lerp_grad
  forward : lerp (Tensor x, Tensor y, Tensor weight) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor weight, Tensor out, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : lerp_grad

- backward_op : lgamma_grad
  forward : lgamma(Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : lgamma_grad

- backward_op : linear_interp_grad
  forward : linear_interp (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, str data_format="NCHW", int out_d=0, int out_h=0, int out_w=0, float[] scale={}, str interp_method="bilinear", bool align_corners=true, int align_mode=1) -> Tensor(output)
  args : (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, Tensor output_grad, str data_format, int out_d, int out_h, int out_w, float[] scale, str interp_method, bool align_corners, int align_mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  optional: out_size, size_tensor, scale_tensor
  no_need_buffer : x
  kernel :
    func : linear_interp_grad
    data_type : output_grad
  data_transform :
    skip_transform : out_size, size_tensor, scale_tensor

- backward_op : log10_grad
  forward : log10 (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : log10_grad
  inplace : (out_grad -> x_grad)

- backward_op : log1p_grad
  forward : log1p (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : log1p_grad
  inplace : (out_grad -> x_grad)

- backward_op : log2_grad
  forward : log2 (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : log2_grad
  inplace : (out_grad -> x_grad)

- backward_op : log_double_grad
  forward : log_grad (Tensor x, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : log_double_grad
  composite : log_double_grad(x, grad_out, grad_x_grad, x_grad, grad_out_grad)
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : log_grad
  forward : log (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : log_grad
  backward : log_double_grad
  composite : log_grad(x, out_grad, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : log_loss_grad
  forward : log_loss (Tensor input, Tensor label, float epsilon) -> Tensor(out)
  args : (Tensor input, Tensor label, Tensor out_grad, float epsilon)
  output : Tensor(input_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [input]
  kernel :
    func : log_loss_grad

- backward_op : log_softmax_grad
  forward : log_softmax(Tensor x,  int axis = -1) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [out]
  kernel :
    func : log_softmax_grad
    data_type : out_grad

- backward_op : logcumsumexp_grad
  forward : logcumsumexp(Tensor x, int axis=-1, bool flatten=false, bool exclusive=false, bool reverse=false) -> Tensor(out)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  args : (Tensor x, Tensor out, Tensor out_grad, int axis, bool flatten, bool exclusive, bool reverse)
  output : Tensor(x_grad)
  kernel :
    func : logcumsumexp_grad

- backward_op : logit_grad
  forward : logit (Tensor x, float eps = 1e-6f) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float eps)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : logit_grad

- backward_op : logsigmoid_grad
  forward : logsigmoid (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : logsigmoid_grad
  inplace : (out_grad -> x_grad)

- backward_op : logsumexp_grad
  forward : logsumexp(Tensor x, int64_t[] axis,  bool keepdim,  bool reduce_all) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int64_t[] axis,  bool keepdim,  bool reduce_all)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : logsumexp_grad

- backward_op : lp_pool2d_grad
  forward : lp_pool2d(Tensor x, IntArray kernel_size, int[] strides = {1,1}, int[] paddings = {0,0}, bool ceil_mode = false, bool exclusive = true, str data_format = "NCHW", str pooling_type = "", bool global_pooling = false, bool adaptive = false, str padding_algorithm = "EXPLICIT", float norm_type = 0.0f) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm, float norm_type)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : lp_pool2d_grad
    param : [x, out, out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, norm_type]

- backward_op : lstm_grad
  forward: lstm (Tensor input, Tensor h0, Tensor c0, Tensor weight, Tensor bias, bool use_peepholes
    = true, bool is_reverse = false, bool is_test = false, str gate_activation = "sigmoid",
    str cell_activation = "tanh", str candidate_activation = "tanh") -> Tensor (hidden), Tensor (cell), Tensor (batch_gate), Tensor (batch_cell_pre_act)
  args: (Tensor input, Tensor h0, Tensor c0, Tensor weight, Tensor bias, Tensor hidden, Tensor cell,
    Tensor batch_gate, Tensor batch_cell_pre_act, Tensor hidden_grad, bool use_peepholes, bool is_reverse, bool is_test, str gate_activation,
    str cell_activation, str candidate_activation)
  output: Tensor(input_grad), Tensor(h0_grad), Tensor(c0_grad), Tensor(weight_grad), Tensor(bias_grad)
  infer_meta:
    func: LSTMGradInferMeta
    param: [input, h0, c0, weight, bias]
  kernel:
    func: lstm_grad
    data_type: input
  optional: h0, c0

- backward_op : lu_grad
  forward : lu (Tensor x, bool pivot = true) -> Tensor(out), Tensor(pivots), Tensor(infos)
  args : (Tensor x, Tensor out, Tensor pivots, Tensor out_grad, bool pivot)
  output : Tensor(x_grad)
  infer_meta :
    func : LUGradInferMeta
  kernel :
    func : lu_grad
  inplace : (out_grad -> x_grad)

- backward_op : lu_unpack_grad
  forward : lu_unpack (Tensor x, Tensor y, bool unpack_ludata = true, bool unpack_pivots = true) -> Tensor(pmat), Tensor(l), Tensor(u)
  args : (Tensor x, Tensor y, Tensor l, Tensor u, Tensor pmat, Tensor l_grad, Tensor u_grad, bool unpack_ludata, bool unpack_pivots)
  output : Tensor(x_grad)
  infer_meta :
    func : LUUnpackGradInferMeta
  kernel :
    func : lu_unpack_grad

- backward_op : margin_cross_entropy_grad
  forward : margin_cross_entropy (Tensor logits, Tensor label, bool return_softmax=false, int ring_id=0, int rank=0, int nranks=1, float margin1=1.0f, float margin2=0.5f, float margin3=0.0f, float scale=64.0f) -> Tensor(softmax), Tensor(loss)
  args : (Tensor logits, Tensor label, Tensor softmax, Tensor loss_grad, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale)
  output : Tensor(logits_grad)
  infer_meta :
    func : MarginCrossEntropyGradInferMeta
  kernel :
    func : margin_cross_entropy_grad
    data_type : softmax
  inplace : (softmax -> logits_grad)

- backward_op : masked_select_grad
  forward : masked_select (Tensor x, Tensor mask) -> Tensor(out)
  args : (Tensor x, Tensor mask, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : masked_select_grad
    data_type : x
  no_need_buffer : x

- backward_op : matrix_power_grad
  forward : matrix_power (Tensor x, int n) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int n)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : matrix_power_grad

- backward_op : max_grad
  forward: max (Tensor x,  IntArray axis={},  bool keepdim=false) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray axis={}, bool keepdim=false, bool reduce_all=false)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
    spmd_rule : ReductionGradInferSpmd
  kernel :
    func : max_grad
  composite : max_grad(x, out, out_grad, axis, keepdim, reduce_all, x_grad)

- backward_op : max_pool2d_with_index_grad
  forward : max_pool2d_with_index(Tensor x, int[] kernel_size, int[] strides = {1, 1}, int[] paddings = {0, 0}, bool global_pooling = false, bool adaptive = false) -> Tensor(out), Tensor(mask)
  args : (Tensor x, Tensor mask, Tensor out_grad, int[] kernel_size, int[] strides, int[] paddings, bool global_pooling, bool adaptive)
  output : Tensor(x_grad)
  infer_meta :
    func : MaxPoolWithIndexGradInferMeta
  kernel :
    func : max_pool2d_with_index_grad

- backward_op : max_pool3d_with_index_grad
  forward : max_pool3d_with_index(Tensor x, int[] kernel_size, int[] strides = {1, 1, 1}, int[] paddings = {0, 0, 0}, bool global_pooling = false, bool adaptive = false) -> Tensor(out), Tensor(mask)
  args : (Tensor x, Tensor mask, Tensor out_grad, int[] kernel_size, int[] strides, int[] paddings, bool global_pooling, bool adaptive)
  output : Tensor(x_grad)
  infer_meta :
    func : MaxPoolWithIndexGradInferMeta
  kernel :
    func : max_pool3d_with_index_grad

- backward_op : maxout_grad
  forward : maxout(Tensor x, int groups, int axis) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int groups, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param: [x]
  kernel :
    func : maxout_grad

- backward_op : mean_all_grad
  forward : mean_all(Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedExceptLayoutInferMeta
    param: [x]
  kernel :
    func : mean_all_grad
    data_type: out_grad
  no_need_buffer : x

- backward_op : mean_double_grad
  forward: mean_grad (Tensor x, Tensor grad_out, IntArray axis={},  bool keepdim=false, bool reduce_all = false) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray axis={},  bool keepdim=false)
  output : Tensor(grad_out_grad)
  invoke : mean(grad_x_grad, axis, keepdim)

- backward_op : mean_grad
  forward: mean (Tensor x,  IntArray axis={},  bool keepdim=false) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray axis={},  bool keepdim=false, bool reduce_all=false)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
    spmd_rule : ReductionGradInferSpmd
  kernel :
    func : mean_grad
  backward : mean_double_grad
  no_need_buffer : x

- backward_op : memory_efficient_attention_grad
  forward : memory_efficient_attention (Tensor query, Tensor key, Tensor value, Tensor bias, Tensor cu_seqlens_q, Tensor cu_seqlens_k, Tensor causal_diagonal, Tensor seqlen_k, Scalar max_seqlen_q, Scalar max_seqlen_k, bool causal, double dropout_p, float scale, bool is_test) -> Tensor(output), Tensor(logsumexp), Tensor(seed_and_offset)
  args : (Tensor query, Tensor key, Tensor value, Tensor bias, Tensor cu_seqlens_q, Tensor cu_seqlens_k, Tensor output, Tensor logsumexp, Tensor seed_and_offset, Tensor output_grad, Scalar max_seqlen_q, Scalar max_seqlen_k, bool causal, double dropout_p, float scale)
  output : Tensor(query_grad), Tensor(key_grad), Tensor(value_grad), Tensor(bias_grad)
  infer_meta :
    func : MemoryEfficientAttentionGradInferMeta
  kernel :
    func : memory_efficient_attention_grad
    data_type : output_grad
  optional : bias, cu_seqlens_q, cu_seqlens_k

- backward_op : meshgrid_grad
  forward : meshgrid (Tensor[] inputs) -> Tensor[](outputs)
  args : (Tensor[] inputs, Tensor[] outputs_grad)
  output : Tensor[](inputs_grad){inputs.size()}
  infer_meta :
    func : MeshgridGradInferMeta
  kernel :
    func : meshgrid_grad
    data_type : outputs_grad

- backward_op : mish_grad
  forward : mish (Tensor x, float lambda) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float lambda)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : mish_grad
  inplace : (out_grad -> x_grad)

- backward_op : mode_grad
  forward : mode(Tensor x,  int axis = -1,  bool keepdim = false) -> Tensor(out), Tensor(indices)
  args : (Tensor x, Tensor indices, Tensor out_grad,  int axis,  bool keepdim)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : mode_grad

- backward_op : multi_dot_grad
  forward : multi_dot (Tensor[] x) -> Tensor(out)
  args : (Tensor[] x, Tensor out_grad)
  output : Tensor[](x_grad) {x.size()}
  infer_meta :
    func : MultiDotGradInferMeta
  kernel :
    func : multi_dot_grad

- backward_op : multiplex_grad
  forward : multiplex (Tensor[] inputs, Tensor index) -> Tensor(out)
  args : (Tensor[] inputs, Tensor index, Tensor out_grad)
  output : Tensor[](inputs_grad){inputs.size()}
  infer_meta :
    func : MultiplexGradInferMeta
    param : [index, out_grad]
  kernel :
    func : multiplex_grad
    param : [index, out_grad]
    data_type : out_grad
  data_transform :
    skip_transform : index

- backward_op : mv_grad
  forward : mv (Tensor x, Tensor vec) -> Tensor(out)
  args : (Tensor x, Tensor vec, Tensor out_grad)
  output : Tensor(x_grad), Tensor(vec_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, vec]
  kernel :
    func : mv_grad

- backward_op : nanmedian_grad
  forward : nanmedian (Tensor x, IntArray axis, bool keepdim, str mode) -> Tensor(out), Tensor(medians)
  args : (Tensor x, Tensor medians, Tensor out_grad, IntArray axis, bool keepdim, str mode)
  output : Tensor(x_grad)
  infer_meta :
    func : NanmedianGradInferMeta
  kernel :
    func : nanmedian_grad

- backward_op : nearest_interp_grad
  forward : nearest_interp (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, str data_format="NCHW", int out_d=0, int out_h=0, int out_w=0, float[] scale={}, str interp_method="bilinear", bool align_corners=true, int align_mode=1) -> Tensor(output)
  args : (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, Tensor output_grad, str data_format, int out_d, int out_h, int out_w, float[] scale, str interp_method, bool align_corners, int align_mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  optional: out_size, size_tensor, scale_tensor
  no_need_buffer : x
  kernel :
    func : nearest_interp_grad
    data_type : output_grad
  data_transform :
    skip_transform : out_size, size_tensor, scale_tensor

- backward_op : nll_loss_grad
  forward : nll_loss (Tensor input, Tensor label, Tensor weight, int64_t ignore_index = -100, str reduction = "mean") -> Tensor(out), Tensor(total_weight)
  args : (Tensor input, Tensor label, Tensor weight, Tensor total_weight, Tensor out_grad, int64_t ignore_index, str reduction)
  output : Tensor(input_grad)
  infer_meta :
    func : NllLossGradInferMeta
  kernel :
    func : nll_loss_grad
    data_type : input
  optional : weight

- backward_op : norm_grad
  forward : norm (Tensor x, int axis, float epsilon, bool is_test) -> Tensor(out), Tensor(norm)
  args : (Tensor x, Tensor norm, Tensor out_grad, int axis, float epsilon, bool is_test)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : norm_grad

- backward_op : overlap_add_grad
  forward : overlap_add(Tensor x, int hop_length, int axis) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int hop_length, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : OverlapAddGradInferMeta
  kernel :
    func : overlap_add_grad
    data_type : x

- backward_op : p_norm_grad
  forward : p_norm(Tensor x,  float porder=2,  int axis=-1,  float epsilon=1.0e-12f,  bool keepdim=false,  bool asvector=false) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad,  float porder,  int axis,  float epsilon,  bool keepdim,  bool asvector)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param: [x]
  kernel :
    func : p_norm_grad

- backward_op : pad3d_double_grad
  forward : pad3d_grad(Tensor x, Tensor grad_out, IntArray paddings, str mode="constant", float pad_value=0.0, str data_format="NCDHW") -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray paddings, str mode, float pad_value, str data_format)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : Pad3dInferMeta
  kernel :
    func : pad3d

- backward_op : pad3d_grad
  forward : pad3d(Tensor x, IntArray paddings, str mode="constant", float pad_value=0.0, str data_format="NCDHW") -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray paddings, str mode, float pad_value, str data_format)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : pad3d_grad
  no_need_buffer : x
  backward : pad3d_double_grad

- backward_op : pad_double_grad
  forward : pad_grad(Tensor x, Tensor grad_out, int[] paddings, Scalar pad_value) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, int[] paddings, Scalar pad_value)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : PadInferMeta
  kernel :
    func : pad

- backward_op : pad_grad
  forward : pad(Tensor x, int[] paddings, Scalar pad_value) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int[] paddings, Scalar pad_value)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : pad_grad
    param: [out_grad, paddings, pad_value]
  no_need_buffer : x
  composite : pad_grad(x, out_grad, paddings, pad_value, x_grad)
  backward : pad_double_grad

- backward_op : partial_concat_grad
  forward : partial_concat (Tensor[] x, int start_index = 0, int length = -1) -> Tensor(out)
  args : (Tensor[] x, Tensor out_grad, int start_index, int length)
  output : Tensor[](x_grad){x.size()}
  infer_meta :
    func : PartialConcatGradInferMeta
    param : [x]
  kernel :
    func : partial_concat_grad

- backward_op : partial_sum_grad
  forward : partial_sum (Tensor[] x, int start_index = 0, int length = -1) -> Tensor(out)
  args : (Tensor[] x, Tensor out_grad, int start_index, int length)
  output : Tensor[](x_grad){x.size()}
  infer_meta :
    func : PartialSumGradInferMeta
    param : [x]
  kernel :
    func : partial_sum_grad

- backward_op : pixel_shuffle_grad
  forward : pixel_shuffle (Tensor x, int upscale_factor=1, str data_format="NCHW") -> Tensor(out)
  args : (Tensor out_grad, int upscale_factor, str data_format)
  output : Tensor(x_grad)
  infer_meta :
    func : PixelShuffleGradInferMeta
  kernel :
    func : pixel_shuffle_grad

- backward_op : pixel_unshuffle_grad
  forward : pixel_unshuffle (Tensor x, int downscale_factor=1, str data_format="NCHW") -> Tensor(out)
  args : (Tensor out_grad, int downscale_factor, str data_format)
  output : Tensor(x_grad)
  infer_meta :
    func : PixelUnshuffleGradInferMeta
  kernel :
    func : pixel_unshuffle_grad

- backward_op : poisson_grad
  forward : poisson (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : poisson_grad

- backward_op : polygamma_grad
  forward : polygamma (Tensor x, int n) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int n)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : polygamma_grad

- backward_op : pool2d_double_grad
  forward : pool2d_grad(Tensor x, Tensor out, Tensor grad_out, IntArray kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_x_grad, IntArray kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : Pool2DInferMeta
    param : [grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm]
  kernel :
    func : pool2d_double_grad
    param : [grad_x_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm]
  no_need_buffer : x

- backward_op : pool2d_grad
  forward : pool2d(Tensor x, IntArray kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : pool2d_grad
    param : [x, out, out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm]
  backward : pool2d_double_grad

- backward_op : pool3d_grad
  forward : pool3d(Tensor x, int[] kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, int[] kernel_size, int[] strides, int[] paddings, bool ceil_mode, bool exclusive, str data_format, str pooling_type, bool global_pooling, bool adaptive, str padding_algorithm)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : pool3d_grad
    param : [x, out, out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm]

- backward_op : pow_double_grad
  forward : pow_grad(Tensor x, Tensor grad_out, Scalar y) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad, Scalar y)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param: [x, grad_out]
  kernel :
    func : pow_double_grad
    data_type : x
  backward : pow_triple_grad
  inplace : (grad_x_grad -> x_grad)
  composite: pow_double_grad(x, grad_out, grad_x_grad, y, x_grad, grad_out_grad)

- backward_op : pow_grad
  forward : pow(Tensor x, Scalar y=1.0f) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, Scalar y=-1)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
    spmd_rule: PowGradInferSpmd
  kernel :
    func : pow_grad
    data_type : out_grad
  backward: pow_double_grad
  inplace : (out_grad -> x_grad)
  composite: pow_grad(x, out_grad, y, x_grad)

- backward_op : pow_triple_grad
  forward : pow_double_grad(Tensor x, Tensor grad_out, Tensor grad_grad_x, Scalar y) -> Tensor(grad_x), Tensor(grad_grad_out)
  args : (Tensor x, Tensor grad_out, Tensor grad_grad_x, Tensor grad_x_grad, Tensor grad_grad_out_grad, Scalar y)
  output : Tensor(x_grad), Tensor(grad_out_grad), Tensor(grad_grad_x_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param: [x, grad_out, grad_grad_x]
  kernel :
    func : pow_triple_grad
    data_type : x
  optional : grad_grad_out_grad

- backward_op : prelu_grad
  forward : prelu(Tensor x, Tensor alpha, str data_format="NCHW", str mode="all") -> Tensor(out)
  args : (Tensor x, Tensor alpha, Tensor out_grad, str data_format, str mode)
  output : Tensor(x_grad), Tensor(alpha_grad)
  infer_meta :
    func : PreluGradInferMeta
    param: [x, alpha]
  kernel :
    func : prelu_grad
    data_type : x

- backward_op : prod_grad
  forward : prod (Tensor x, IntArray dims, bool keep_dim, bool reduce_all) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray dims,  bool keep_dim, bool reduce_all)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : prod_grad
  composite: prod_grad(x, out, out_grad, dims, keep_dim, reduce_all, x_grad)

- backward_op : psroi_pool_grad
  forward : psroi_pool (Tensor x, Tensor boxes, Tensor boxes_num, int pooled_height=1, int pooled_width=1, int output_channels=1, float spatial_scale=1.0) -> Tensor(out)
  args : (Tensor x, Tensor boxes, Tensor boxes_num, Tensor out_grad, int pooled_height, int pooled_width, int output_channels, float spatial_scale)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param : [x]
  kernel :
    func : psroi_pool_grad
    data_type : x
  optional : boxes_num

- backward_op : pull_sparse_v2_grad
  forward : pull_sparse_v2 (Tensor[] ids, Tensor[] w, int embedding_dim = 11, int table_id = 0, str accessor_class = "", str ctrlabel_name = "", int padding_id = 0, bool scale_sparse_grad = true, str[] input_names = {}, bool is_distributed = true) -> Tensor[](out)
  args : (Tensor[] ids, Tensor[] w, Tensor[] out_grad, int embedding_dim = 11, int table_id = 0, str accessor_class = "", str ctrlabel_name = "", int padding_id = 0, bool scale_sparse_grad = true, str[] input_names = {}, bool is_distributed = true)
  output : Tensor[](out_grad_out)
  invoke : push_sparse_v2(ids, w, out_grad, embedding_dim, table_id, accessor_class, ctrlabel_name, padding_id, scale_sparse_grad, input_names, is_distributed)

- backward_op : push_gpups_sparse
  forward : pull_gpups_sparse (Tensor w, Tensor[] ids, int[] size={}, bool is_sparse=false, bool is_distributed=false) -> Tensor[](out)
  args : (Tensor[] ids, Tensor[] out_grad, int[] size, bool is_sparse, bool is_distributed)
  output : Tensor[](out_grad_grad){out_grad.size()}
  infer_meta :
    func : PushGpupsSparseInferMeta
  kernel :
    func : push_gpups_sparse
  inplace : (out_grad -> out_grad_grad)

- backward_op : put_along_axis_grad
  forward : put_along_axis (Tensor arr, Tensor indices, Tensor values, int axis, str reduce = "assign", bool include_self = true) -> Tensor(out)
  args : (Tensor arr, Tensor indices, Tensor values, Tensor out, Tensor out_grad, int axis, str reduce, bool include_self)
  output : Tensor(arr_grad), Tensor(values_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [arr, indices]
  kernel :
    func : put_along_axis_grad

- backward_op : qr_grad
  forward : qr (Tensor x, str mode = "reduced") -> Tensor(q), Tensor(r)
  args : (Tensor x, Tensor q, Tensor r, Tensor q_grad, Tensor r_grad, str mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : qr_grad

- backward_op : rank_attention_grad
  forward : rank_attention (Tensor x, Tensor rank_offset, Tensor rank_param, int max_rank = 3, int max_size = 0) -> Tensor(input_help), Tensor(out), Tensor(ins_rank)
  args : (Tensor x, Tensor rank_offset, Tensor rank_param, Tensor input_help, Tensor ins_rank, Tensor out_grad, int max_rank = 3, int max_size = 0)
  output : Tensor(rank_param_grad)
  infer_meta :
    func : RankAttentionGradInferMeta
  kernel :
    func : rank_attention_grad
    data_type : out_grad
  no_need_buffer : x, rank_offset, rank_param

- backward_op : real_grad
  forward : real (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : RealAndImagGradInferMeta
  kernel :
    func : real_grad
    data_type : complex(out_grad)

- backward_op : reciprocal_grad
  forward : reciprocal (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : reciprocal_grad
  inplace : (out_grad -> x_grad)

- backward_op : reduce_as_grad
  forward : reduce_as(Tensor x, Tensor target) -> Tensor(out)
  args : (Tensor x, Tensor target, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : reduce_as_grad

- backward_op : relu6_grad
  forward : relu6 (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : relu6_grad
  inplace : (out_grad -> x_grad)

- backward_op : relu_double_grad
  forward : relu_grad (Tensor out, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor out, Tensor grad_x_grad)
  output : Tensor(grad_out_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : relu_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : relu_grad
  forward : relu (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : relu_grad
  backward: relu_double_grad
  composite: relu_grad(out, out_grad, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : renorm_grad
  forward : renorm (Tensor x, float p, int axis, float max_norm) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float p, int axis, float max_norm)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : renorm_grad

- backward_op : repeat_interleave_grad
  forward : repeat_interleave(Tensor x, int repeats, int axis) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int repeats, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : repeat_interleave_grad

- backward_op : repeat_interleave_with_tensor_index_grad
  forward : repeat_interleave_with_tensor_index(Tensor x, Tensor repeats, int axis) -> Tensor(out)
  args : (Tensor x, Tensor repeats, Tensor out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : repeat_interleave_with_tensor_index_grad
    data_type : x

- backward_op : reverse_grad
  forward : reverse (Tensor x, IntArray axis) -> Tensor(out)
  args : (Tensor out_grad, IntArray axis)
  output : Tensor(x_grad)
  invoke : reverse(out_grad, axis)

- backward_op : rms_norm_grad
  forward : rms_norm (Tensor x, Tensor bias, Tensor residual, Tensor norm_weight, Tensor norm_bias, float epsilon, int begin_norm_axis, float quant_scale, int quant_round_type, float quant_max_bound, float quant_min_bound) -> Tensor(out), Tensor(residual_out), Tensor(inv_var)
  args : (Tensor x, Tensor bias, Tensor residual, Tensor norm_weight, Tensor norm_bias, Tensor inv_var, Tensor out_grad,float epsilon, int begin_norm_axis, float quant_scale)
  output : Tensor(x_grad), Tensor(norm_weight_grad)
  infer_meta :
    func: RmsNormGradInferMeta
    param: [x, norm_weight]
  kernel :
    func : rms_norm_grad
    data_type : x
  optional : bias, residual, norm_bias

- backward_op : rnn_grad
  forward : rnn (Tensor x, Tensor[] pre_state, Tensor[] weight_list, Tensor sequence_length, Tensor dropout_state_in, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, str mode, int seed, bool is_test) -> Tensor(out), Tensor(dropout_state_out), Tensor[](state), Tensor(reserve)
  args : (Tensor x, Tensor[] pre_state, Tensor[] weight_list, Tensor sequence_length, Tensor out, Tensor dropout_state_out, Tensor reserve, Tensor out_grad, Tensor[] state_grad, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, str mode, int seed, bool is_test)
  output : Tensor(x_grad), Tensor[](pre_state_grad){pre_state.size()}, Tensor[](weight_list_grad){weight_list.size()}
  infer_meta :
    func : RnnGradInferMeta
    param : [x, pre_state, weight_list]
  kernel :
    func : rnn_grad
    data_type: out_grad
  optional : sequence_length

- backward_op : roi_align_grad
  forward : roi_align (Tensor x, Tensor boxes, Tensor boxes_num, int pooled_height=1, int pooled_width=1, float spatial_scale=1.0, int sampling_ratio=-1, bool aligned=false) -> Tensor(out)
  args : (Tensor x, Tensor boxes, Tensor boxes_num, Tensor out_grad, int pooled_height, int pooled_width, float spatial_scale, int sampling_ratio, bool aligned)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : roi_align_grad
    data_type : boxes
  no_need_buffer : x
  optional : boxes_num

- backward_op : roi_pool_grad
  forward : roi_pool (Tensor x, Tensor boxes, Tensor boxes_num, int pooled_height=1, int pooled_width=1, float spatial_scale=1.0) -> Tensor(out), Tensor(arg_max)
  args : (Tensor x, Tensor boxes, Tensor boxes_num, Tensor arg_max, Tensor out_grad, int pooled_height, int pooled_width, float spatial_scale)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : roi_pool_grad
    data_type : x
  optional : boxes_num

- backward_op : roll_grad
  forward : roll(Tensor x, IntArray shifts, int64_t[] axis) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray shifts, int64_t[] axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : roll_grad
    data_type : x
  composite : roll_grad(x, out_grad, shifts, axis, x_grad)
  no_need_buffer : x

- backward_op : round_grad
  forward : round(Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [out_grad]
  kernel :
    func : round_grad
  inplace : (out_grad -> x_grad)

- backward_op : rrelu_grad
  forward : rrelu (Tensor x, float lower, float upper, bool is_test) -> Tensor(out), Tensor(noise)
  args : (Tensor x, Tensor noise, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : RReluGradInferMeta
    param : [out_grad, noise]
  kernel :
    func : rrelu_grad
    data_type : x

- backward_op : rsqrt_double_grad
  forward : rsqrt_grad (Tensor out, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor out, Tensor grad_x, Tensor grad_x_grad)
  output : Tensor(out_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [out, out]
  kernel :
    func : rsqrt_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : rsqrt_grad
  forward : rsqrt (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : rsqrt_grad
  composite : rsqrt_grad(out, out_grad, x_grad)
  backward : rsqrt_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : scale_grad
  forward : scale (Tensor x, Scalar scale, Scalar bias, bool bias_after_scale) -> Tensor(out)
  args : (Tensor out_grad, Scalar scale=1.0)
  output : Tensor(x_grad)
  invoke : scale(out_grad, scale, 0.0f, true)

- backward_op : scatter_grad
  forward : scatter (Tensor x, Tensor index, Tensor updates, bool overwrite=true) -> Tensor(out)
  args : (Tensor index, Tensor updates, Tensor out_grad, bool overwrite)
  output : Tensor(x_grad), Tensor(updates_grad)
  infer_meta :
    func : ScatterGradInferMeta
    param : [index, updates, out_grad, overwrite]
  kernel :
    func : scatter_grad
  no_need_buffer : updates
  composite: scatter_grad(index, updates, out_grad, overwrite, x_grad, updates_grad)

- backward_op : scatter_nd_add_grad
  forward : scatter_nd_add (Tensor x, Tensor index, Tensor updates) -> Tensor(out)
  args : (Tensor index, Tensor updates, Tensor out_grad)
  output : Tensor(x_grad), Tensor(updates_grad)
  infer_meta :
    func : ScatterNdAddGradInferMeta
    param : [index, updates, out_grad]
  kernel :
    func : scatter_nd_add_grad
  no_need_buffer : updates
  composite: scatter_nd_add_grad(index, updates, out_grad, x_grad, updates_grad)

- backward_op : segment_pool_grad
  forward : segment_pool (Tensor x, Tensor segment_ids, str pooltype="SUM") -> Tensor(out), Tensor(summed_ids)
  args : (Tensor x, Tensor segment_ids, Tensor out, Tensor summed_ids, Tensor out_grad, str pooltype)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : segment_pool_grad
    data_type : out_grad
  optional : summed_ids

- backward_op : selu_grad
  forward : selu (Tensor x, float scale=1.0507009873554804934193349852946, float alpha=1.6732632423543772848170429916717) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, float scale, float alpha)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : selu_grad
    data_type : out

- backward_op : send_u_recv_grad
  forward : send_u_recv (Tensor x, Tensor src_index, Tensor dst_index, str reduce_op = "SUM", IntArray out_size = {0}) -> Tensor(out), Tensor(dst_count)
  args : (Tensor x, Tensor src_index, Tensor dst_index, Tensor out, Tensor dst_count, Tensor out_grad, str reduce_op = "SUM")
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param : [x]
  kernel :
    func : send_u_recv_grad
    data_type : out_grad
  optional: out, dst_count

- backward_op : send_ue_recv_grad
  forward : send_ue_recv (Tensor x, Tensor y, Tensor src_index, Tensor dst_index, str message_op="ADD", str reduce_op="SUM", IntArray out_size={0}) -> Tensor(out), Tensor(dst_count)
  args : (Tensor x, Tensor y, Tensor src_index, Tensor dst_index, Tensor out, Tensor dst_count, Tensor out_grad, str message_op, str reduce_op)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : send_ue_recv_grad
    data_type : out_grad
  optional: out, dst_count

- backward_op : send_uv_grad
  forward : send_uv (Tensor x, Tensor y, Tensor src_index, Tensor dst_index, str message_op = "ADD") -> Tensor(out)
  args: (Tensor x, Tensor y, Tensor src_index, Tensor dst_index, Tensor out_grad, str message_op = "ADD")
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : send_uv_grad
    data_type : x

- backward_op : sequence_conv_grad
  forward: sequence_conv (Tensor x, Tensor padding_data, Tensor filter, int context_length, bool padding_trainable = false,
    int context_start = 0, int context_stride = 1) -> Tensor (out)
  args: (Tensor x, Tensor padding_data, Tensor filter, Tensor out_grad, int context_length, bool padding_trainable = false,
    int context_start = 0, int context_stride = 1)
  output: Tensor (x_grad), Tensor (padding_data_grad), Tensor (filter_grad)
  infer_meta:
    func: SequenceConvGradInferMeta
  kernel:
    func: sequence_conv_grad
    data_type: out_grad
  optional: padding_data

- backward_op : set_value_with_tensor_grad
  forward: set_value_with_tensor (Tensor x, Tensor values, IntArray starts, IntArray ends, IntArray steps, int64_t[] axes, int64_t[] decrease_axes, int64_t[] none_axes) -> Tensor(out)
  args : (Tensor values,Tensor out_grad, IntArray starts, IntArray ends, IntArray steps, int64_t[] axes, int64_t[] decrease_axes, int64_t[] none_axes)
  output : Tensor(x_grad), Tensor(values_grad)
  infer_meta:
    func: SetValueGradInferMeta
    param: [out_grad, values]
  kernel:
    func: set_value_grad
    param: [out_grad, starts, ends, steps, axes, decrease_axes, none_axes]

- backward_op : shuffle_channel_grad
  forward : shuffle_channel (Tensor x, int group = 1) -> Tensor(out)
  args : (Tensor out_grad, int group = 1)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : shuffle_channel_grad

- backward_op : sigmoid_cross_entropy_with_logits_grad
  forward : sigmoid_cross_entropy_with_logits (Tensor x, Tensor label, Tensor pos_weight, bool normalize=false, int ignore_index=-100) -> Tensor(out)
  args : (Tensor x, Tensor label, Tensor pos_weight, Tensor out_grad, bool normalize, int ignore_index)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : sigmoid_cross_entropy_with_logits_grad
  inplace : (out_grad -> x_grad)
  optional : pos_weight

- backward_op : sigmoid_double_grad
  forward : sigmoid_grad (Tensor out, Tensor fwd_grad_out) -> Tensor(grad_x)
  args : (Tensor out, Tensor fwd_grad_out, Tensor grad_x_grad)
  output : Tensor(out_grad), Tensor(fwd_grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [out, fwd_grad_out]
  kernel :
    func : sigmoid_double_grad
  backward : sigmoid_triple_grad
  inplace : (grad_x_grad -> fwd_grad_out_grad)

- backward_op : sigmoid_grad
  forward : sigmoid (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : sigmoid_grad
  backward : sigmoid_double_grad
  inplace : (out_grad -> x_grad)
  composite : sigmoid_grad(out, out_grad, x_grad)

- backward_op : sigmoid_triple_grad
  forward : sigmoid_double_grad (Tensor out, Tensor fwd_grad_out, Tensor grad_grad_x) -> Tensor(grad_out), Tensor(grad_grad_out)
  args : (Tensor out, Tensor fwd_grad_out, Tensor grad_grad_x, Tensor grad_out_grad, Tensor grad_grad_out_grad)
  output : Tensor(out_grad), Tensor(fwd_grad_out_grad), Tensor(grad_grad_x_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [out, fwd_grad_out, grad_grad_x]
  kernel :
    func : sigmoid_triple_grad
  optional : grad_grad_out_grad
  inplace : (grad_grad_x -> fwd_grad_out_grad)

- backward_op : sign_grad
  forward : sign (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  invoke : scale(out_grad, 0.0f, 0.0f, true)

- backward_op : silu_grad
  forward : silu (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : silu_grad
  backward : silu_double_grad
  composite : silu_grad(x, out, out_grad, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : sin_double_grad
  forward : sin_grad (Tensor x, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : sin_double_grad
  backward : sin_triple_grad
  inplace : (grad_x_grad -> grad_out_grad)
  composite : sin_double_grad(x, grad_out, grad_x_grad, x_grad, grad_out_grad)

- backward_op : sin_grad
  forward : sin (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : sin_grad
  backward : sin_double_grad
  composite : sin_grad(x, out_grad, x_grad)
  inplace : (out_grad -> x_grad)

- backward_op : sin_triple_grad
  forward : sin_double_grad (Tensor x, Tensor grad_out_forward, Tensor grad_x_grad_forward) -> Tensor(grad_x), Tensor(grad_out_grad)
  args : (Tensor x, Tensor grad_out_forward, Tensor grad_x_grad_forward, Tensor grad_x_grad, Tensor grad_out_grad_grad)
  output : Tensor(x_grad), Tensor(grad_out_forward_grad), Tensor(grad_x_grad_forward_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, x, grad_x_grad_forward]
  kernel :
    func : sin_triple_grad
  optional: grad_out_forward, grad_x_grad_forward, grad_out_grad_grad
  inplace : (grad_x_grad_forward -> grad_out_forward_grad)

- backward_op : sinh_grad
  forward : sinh (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : sinh_grad
  inplace : (out_grad -> x_grad)

- backward_op : slice_double_grad
  forward : slice_grad (Tensor input, Tensor grad_out, int64_t[] axes, IntArray starts, IntArray ends, int64_t[] infer_flags, int64_t[] decrease_axis) -> Tensor(grad_input)
  args : (Tensor grad_input_grad, int64_t[] axes, IntArray starts, IntArray ends, int64_t[] infer_flags, int64_t[] decrease_axis)
  output : Tensor(grad_out_grad)
  invoke : slice(grad_input_grad, axes, starts, ends, infer_flags, decrease_axis)

- backward_op : slice_grad
  forward : slice (Tensor input, int64_t[] axes, IntArray starts, IntArray ends, int64_t[] infer_flags, int64_t[] decrease_axis) -> Tensor(out)
  args : (Tensor input, Tensor out_grad, int64_t[] axes, IntArray starts, IntArray ends, int64_t[] infer_flags, int64_t[] decrease_axis)
  output : Tensor(input_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [input]
    spmd_rule: SliceGradInferSpmdDynamic
  kernel :
    func : slice_grad
  composite: slice_grad(input, out_grad, axes, starts, ends, infer_flags, decrease_axis, input_grad)
  backward : slice_double_grad
  no_need_buffer : input

- backward_op : slogdet_grad
  forward : slogdet (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param : [x]
  kernel :
    func : slogdet_grad
    data_type : out_grad

- backward_op : softplus_double_grad
  forward : softplus_grad (Tensor x, Tensor grad_out, float beta, float threshold) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad, float beta, float threshold)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : softplus_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : softplus_grad
  forward : softplus (Tensor x, float beta, float threshold) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float beta, float threshold)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : softplus_grad
  backward : softplus_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : softshrink_grad
  forward : softshrink (Tensor x, float threshold) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float threshold)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : softshrink_grad
  inplace : (out_grad -> x_grad)

- backward_op : softsign_grad
  forward : softsign (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : softsign_grad
  inplace : (out_grad -> x_grad)

- backward_op : solve_grad
  forward : solve (Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : solve_grad

- backward_op : spectral_norm_grad
  forward : spectral_norm (Tensor weight, Tensor u, Tensor v, int dim = 0, int power_iters = 1, float eps=1e-12f) -> Tensor(out)
  args : (Tensor weight, Tensor u, Tensor v, Tensor out_grad, int dim, int power_iters, float eps)
  output : Tensor(weight_grad)
  infer_meta :
    func : SpectralNormGradInferMeta
  kernel :
    func : spectral_norm_grad
    data_type : weight

- backward_op : split_grad
  forward : split (Tensor x, IntArray num_or_sections, Scalar axis) -> Tensor[](out)
  args : (Tensor[] out_grad, Scalar axis = -1)
  output : Tensor(x_grad)
  invoke : concat( out_grad, axis)
  composite : split_grad(out_grad, axis, x_grad)

- backward_op : split_with_num_grad
  forward : split_with_num (Tensor x, int num, Scalar axis) -> Tensor[](out)
  args : (Tensor[] out_grad, Scalar axis = -1)
  output : Tensor(x_grad)
  invoke : concat( out_grad, axis)
  composite : split_grad(out_grad, axis, x_grad)

- backward_op : sqrt_double_grad
  forward : sqrt_grad (Tensor out, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor out, Tensor grad_x, Tensor grad_x_grad)
  output : Tensor(out_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [out, out]
  kernel :
    func : sqrt_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : sqrt_grad
  forward : sqrt (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : sqrt_grad
  composite : sqrt_grad(out, out_grad, x_grad)
  backward : sqrt_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : square_double_grad
  forward : square_grad (Tensor x, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor x, Tensor grad_out, Tensor grad_x_grad)
  output : Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  kernel :
    func : square_double_grad
  inplace : (grad_x_grad -> grad_out_grad)

- backward_op : square_grad
  forward : square (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
    spmd_rule : ElementwiseUnaryGradInferSpmd
  kernel :
    func : square_grad
  backward : square_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : squared_l2_norm_grad
  forward : squared_l2_norm(Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : squared_l2_norm_grad

- backward_op : squeeze_double_grad
  forward : squeeze_grad(Tensor xshape, Tensor grad_out, IntArray axis) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray axis)
  output : Tensor(grad_out_grad), Tensor(xshape)
  invoke: squeeze(grad_x_grad, axis)
  intermediate : xshape

- backward_op : squeeze_grad
  forward : squeeze(Tensor x, IntArray axis) -> Tensor(out), Tensor(xshape)
  args : (Tensor xshape, Tensor out_grad, IntArray axis)
  output : Tensor(x_grad)
  infer_meta :
    func : KernelWithXShapeInferMeta
    param: [xshape, out_grad]
    spmd_rule : SqueezeGradInferSpmd
  kernel :
    func : squeeze_grad
    data_type : out_grad
  inplace : (out_grad -> x_grad)
  backward: squeeze_double_grad

- backward_op : stack_double_grad
  forward : stack_grad (Tensor[] x, Tensor grad_out, int axis=0) -> Tensor[](grad_x)
  args : (Tensor[] grad_x_grad, int axis = 0)
  output : Tensor(grad_out_grad)
  invoke : stack(grad_x_grad, axis)

- backward_op : stack_grad
  forward : stack (Tensor[] x, int axis) -> Tensor(out)
  args : (Tensor[] x, Tensor out_grad, int axis)
  output : Tensor[](x_grad){x.size()}
  infer_meta :
    func : StackGradInferMeta
    param: [out_grad, axis]
    spmd_rule : StackGradInferSpmd
  kernel :
    func : stack_grad
    param : [out_grad, axis]
    data_type : out_grad
  no_need_buffer : x
  composite : stack_grad(x, out_grad, axis, x_grad)
  backward: stack_double_grad

- backward_op : stanh_grad
  forward : stanh(Tensor x, float scale_a, float scale_b) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float scale_a, float scale_b)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : stanh_grad

- backward_op : strided_slice_grad
  forward : strided_slice (Tensor x, int[] axes, IntArray starts, IntArray ends, IntArray strides) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int[] axes, IntArray starts, IntArray ends, IntArray strides)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param : [x]
    spmd_rule : StridedSliceGradInferSpmdDynamic
  kernel :
    func : strided_slice_grad
  no_need_buffer : x

- backward_op : sum_double_grad
  forward : sum_grad (Tensor x, Tensor grad_out, IntArray axis, bool keepdim, bool reduce_all=false) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray axis={}, bool keepdim=false)
  output : Tensor(grad_out_grad)
  invoke : sum(grad_x_grad, axis, grad_x_grad.dtype(), keepdim)

- backward_op : sum_grad
  forward : sum (Tensor x, IntArray axis={}, DataType dtype=DataType::UNDEFINED, bool keepdim=false) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, IntArray axis, bool keepdim, bool reduce_all=false)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
    spmd_rule : ReductionGradInferSpmd
  kernel :
    func : sum_grad
  composite : sum_grad(x, out_grad, axis, keepdim, reduce_all, x_grad)
  no_need_buffer : x
  backward : sum_double_grad

- backward_op : svd_grad
  forward : svd (Tensor x, bool full_matrices = false) -> Tensor(u), Tensor(s), Tensor(vh)
  args : (Tensor x, Tensor u, Tensor vh, Tensor s, Tensor u_grad, Tensor vh_grad, Tensor s_grad, bool full_matrices)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : svd_grad
  optional: u_grad, vh_grad, s_grad

- backward_op : swiglu_grad
  forward : swiglu (Tensor x, Tensor y) -> Tensor(out)
  args: (Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta:
    func: SwiGLUGradInferMeta
    param: [x, y]
    spmd_rule: SwiGLUGradInferSpmd
  kernel:
    func: swiglu_grad
  optional: y

- backward_op : swish_grad
  forward : swish (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : GeneralUnaryGradInferMeta
    param : [x]
  kernel :
    func : swish_grad
  inplace : (out_grad -> x_grad)

- backward_op : sync_batch_norm_grad
  forward : sync_batch_norm_ (Tensor x, Tensor mean, Tensor variance, Tensor scale, Tensor bias, bool is_test, float momentum, float epsilon, str data_format, bool use_global_stats, bool trainable_statistics) -> Tensor(out), Tensor(mean_out), Tensor(variance_out), Tensor(saved_mean), Tensor(saved_variance), Tensor(reserve_space)
  args : (Tensor x, Tensor scale, Tensor bias, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor out_grad, float momentum, float epsilon, str data_format, bool is_test, bool use_global_stats, bool trainable_statistics)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, scale, bias]
  kernel :
    func : sync_batch_norm_grad
    data_type : out_grad
  optional : reserve_space

- backward_op : take_along_axis_grad
  forward : take_along_axis (Tensor arr, Tensor indices, int axis) -> Tensor(out)
  args : (Tensor arr, Tensor indices, Tensor out_grad, int axis)
  output : Tensor(arr_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [arr]
  kernel :
    func : take_along_axis_grad

- backward_op : tan_grad
  forward : tan (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : tan_grad
  inplace : (out_grad -> x_grad)

- backward_op : tanh_double_grad
  forward : tanh_grad (Tensor out, Tensor grad_out) -> Tensor(grad_x)
  args : (Tensor out, Tensor grad_out, Tensor grad_x_grad)
  output : Tensor(out_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [out, out]
  kernel :
    func : tanh_double_grad
  composite : tanh_double_grad(out, grad_out, grad_x_grad, out_grad, grad_out_grad)
  inplace : (grad_x_grad -> grad_out_grad)
  backward : tanh_triple_grad

- backward_op : tanh_grad
  forward : tanh (Tensor x) -> Tensor(out)
  args : (Tensor out, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : tanh_grad
  composite : tanh_grad(out, out_grad, x_grad)
  backward : tanh_double_grad
  inplace : (out_grad -> x_grad)

- backward_op : tanh_shrink_grad
  forward : tanh_shrink (Tensor x) -> Tensor(out)
  args : (Tensor x, Tensor out_grad)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : tanh_shrink_grad
  inplace : (out_grad -> x_grad)

- backward_op : tanh_triple_grad
  forward : tanh_double_grad (Tensor out, Tensor grad_out_forward, Tensor grad_x_grad_forward) -> Tensor(grad_out_new), Tensor(grad_out_grad)
  args : (Tensor out, Tensor grad_out_forward, Tensor grad_x_grad_forward, Tensor grad_out_new_grad, Tensor grad_out_grad_grad)
  output : Tensor(out_grad), Tensor(grad_out_forward_grad), Tensor(grad_x_grad_forward_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [out, out, grad_x_grad_forward]
  kernel :
    func : tanh_triple_grad
  composite : tanh_triple_grad(out, grad_out_forward, grad_x_grad_forward, grad_out_new_grad, grad_out_grad_grad, out_grad, grad_out_forward_grad, grad_x_grad_forward_grad)
  inplace : (grad_x_grad_forward -> grad_out_forward_grad)
  optional : grad_out_new_grad, grad_out_grad_grad

- backward_op : temporal_shift_grad
  forward : temporal_shift(Tensor x, int seg_num, float shift_ratio = 0.25f, str data_format = "NCHW") -> Tensor(out)
  args : (Tensor out_grad, int seg_num, float shift_ratio, str data_format)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : temporal_shift_grad
    data_type : out_grad

- backward_op : tensor_unfold_grad
  forward : tensor_unfold (Tensor input, int64_t axis, int64_t size, int64_t step) -> Tensor(out)
  args : (Tensor input, Tensor out_grad, int64_t axis, int64_t size, int64_t step)
  output : Tensor(input_grad)
  infer_meta :
    func : StridedUnChangedInferMeta
    param : [input]
  kernel :
    func : tensor_unfold_grad

- backward_op : thresholded_relu_grad
  forward : thresholded_relu (Tensor x, float threshold, float value) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, float threshold, float value)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : thresholded_relu_grad
  inplace : (out_grad -> x_grad)

- backward_op : topk_grad
  forward : topk (Tensor x, Scalar k, int axis = -1, bool largest = true, bool sorted = true) -> Tensor(out), Tensor(indices)
  args : (Tensor x, Tensor indices, Tensor out_grad, Scalar k, int axis, bool largest, bool sorted)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : topk_grad
    data_type : out_grad
  composite : topk_grad(x, indices, out_grad, k, axis, largest, sorted, x_grad)

- backward_op : trace_grad
  forward : trace (Tensor x, int offset, int axis1, int axis2) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int offset, int axis1, int axis2)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : trace_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : trans_layout_grad
  forward : trans_layout (Tensor x, int[] perm) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int[] perm)
  output : Tensor(x_grad)
  infer_meta :
    func : TransLayoutGradInferMeta
  kernel :
    func : trans_layout_grad

- backward_op : transpose_double_grad
  forward : transpose_grad (Tensor grad_out, int[] perm) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, int[] perm)
  output : Tensor(grad_out_grad)
  invoke : transpose(grad_x_grad, perm)

- backward_op : transpose_grad
  forward : transpose (Tensor x, int[] perm) -> Tensor(out)
  args : (Tensor out_grad, int[] perm)
  output : Tensor(x_grad)
  infer_meta :
    func : TransposeGradInferMeta
    param : [out_grad, perm]
    spmd_rule: TransposeGradInferSpmd
  kernel :
    func : transpose_grad
  backward : transpose_double_grad
  composite: transpose_grad(out_grad, perm, x_grad)

- backward_op : triangular_solve_grad
  forward : triangular_solve (Tensor x, Tensor y, bool upper=true, bool transpose=false, bool unitriangular=false) -> Tensor(out)
  args : (Tensor x, Tensor y, Tensor out, Tensor out_grad, bool upper, bool transpose, bool unitriangular)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
  kernel :
    func : triangular_solve_grad

- backward_op : tril_grad
  forward : tril(Tensor x,  int diagonal) -> Tensor(out)
  args : (Tensor out_grad,  int diagonal)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : tril_grad

- backward_op : trilinear_interp_grad
  forward : trilinear_interp (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, str data_format="NCHW", int out_d=0, int out_h=0, int out_w=0, float[] scale={}, str interp_method="bilinear", bool align_corners=true, int align_mode=1) -> Tensor(output)
  args : (Tensor x, Tensor out_size, Tensor[] size_tensor, Tensor scale_tensor, Tensor output_grad, str data_format, int out_d, int out_h, int out_w, float[] scale, str interp_method, bool align_corners, int align_mode)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  optional: out_size, size_tensor, scale_tensor
  no_need_buffer : x
  kernel :
    func : trilinear_interp_grad
    data_type : output_grad
  data_transform :
    skip_transform : out_size, size_tensor, scale_tensor

- backward_op : triu_grad
  forward : triu(Tensor x,  int diagonal) -> Tensor(out)
  args : (Tensor out_grad,  int diagonal)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
    spmd_rule : TriuGradInferSpmd
  kernel :
    func : triu_grad

- backward_op : trunc_grad
  forward : trunc (Tensor input) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(input_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out_grad]
  kernel :
    func : trunc_grad

- backward_op : unbind_grad
  forward : unbind (Tensor input, int axis) -> Tensor[](out)
  args : (Tensor[] out_grad, int axis)
  output : Tensor(input_grad)
  invoke : stack(out_grad, axis)

- backward_op : unfold_grad
  forward : unfold (Tensor x, int[] kernel_sizes, int[] strides, int[] paddings, int[] dilations) -> Tensor(out)
  args : (Tensor x, Tensor out_grad, int[] kernel_sizes, int[] strides, int[] paddings, int[] dilations)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : unfold_grad
    data_type : out_grad
  no_need_buffer : x

- backward_op : uniform_inplace_grad
  forward : uniform_inplace(Tensor x, float min = -1.0, float max = 1.0, int seed = 0, int diag_num = 0, int diag_step = 0, float diag_val = 1.0) -> Tensor(out)
  args : (Tensor out_grad, float min = -1.0, float max = 1.0, int seed = 0, int diag_num = 0, int diag_step = 0, float diag_val = 1.0)
  output : Tensor(x_grad)
  infer_meta :
    func : UniformRandomInplaceGradInferMeta
  kernel :
    func : uniform_inplace_grad
  inplace : (out_grad -> x_grad)

- backward_op : unsqueeze_double_grad
  forward : unsqueeze_grad(Tensor xshape, Tensor grad_out, IntArray axis) -> Tensor(grad_x)
  args : (Tensor grad_x_grad, IntArray axis)
  output : Tensor(grad_out_grad), Tensor(xshape)
  invoke : unsqueeze(grad_x_grad, axis)
  intermediate : xshape

- backward_op : unsqueeze_grad
  forward : unsqueeze(Tensor x, IntArray axis) -> Tensor(out), Tensor(xshape)
  args : (Tensor xshape, Tensor out_grad, IntArray axis)
  output : Tensor(x_grad)
  infer_meta :
    func : KernelWithXShapeInferMeta
    param: [xshape, out_grad]
    spmd_rule : UnsqueezeGradInferSpmd
  kernel :
    func : unsqueeze_grad
    param : [xshape, out_grad]
    data_type : out_grad
  inplace : (out_grad -> x_grad)
  backward : unsqueeze_double_grad

- backward_op : unstack_grad
  forward : unstack (Tensor x, int axis=0, int num=0) -> Tensor[](out)
  args : (Tensor[] out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnStackGradInferMeta
  kernel :
    func : unstack_grad

- backward_op : view_dtype_grad
  forward : view_dtype (Tensor input, DataType dtype) -> Tensor(out)
  args : (Tensor input, Tensor out_grad, DataType dtype)
  output : Tensor(input_grad)
  infer_meta :
    func : StridedUnChangedInferMeta
    param : [input]
  kernel :
    func : view_dtype_grad
    data_type : out_grad

- backward_op : view_shape_grad
  forward : view_shape (Tensor input, int64_t[] dims = {}) -> Tensor(out)
  args : (Tensor input, Tensor out_grad, int64_t[] dims = {})
  output : Tensor(input_grad)
  infer_meta :
    func : StridedUnChangedInferMeta
    param : [input]
  kernel :
    func : view_shape_grad

- backward_op : warpctc_grad
  forward : warpctc (Tensor logits, Tensor label, Tensor logits_length, Tensor labels_length, int blank = 0, bool norm_by_times = false) -> Tensor(loss), Tensor(warpctcgrad)
  args : (Tensor logits, Tensor logits_length, Tensor warpctcgrad, Tensor loss_grad, int blank, bool norm_by_times)
  output : Tensor(logits_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [logits]
  kernel :
    func : warpctc_grad
    data_type : loss_grad
  optional : logits_length
  no_need_buffer : logits

- backward_op : warprnnt_grad
  forward : warprnnt (Tensor input, Tensor label, Tensor input_lengths, Tensor label_lengths, int blank = 0, float fastemit_lambda = 0.0) -> Tensor(loss), Tensor(warprnntgrad)
  args : (Tensor input, Tensor input_lengths, Tensor warprnntgrad, Tensor loss_grad, int blank = 0, float fastemit_lambda = 0.0)
  output : Tensor(input_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [input]
  kernel :
    func : warprnnt_grad
  no_need_buffer : input

- backward_op : weight_only_linear_grad
  forward : weight_only_linear(Tensor x, Tensor weight, Tensor bias, Tensor weight_scale, str weight_dtype, int arch, int group_size) -> Tensor(out)
  args : (Tensor x, Tensor weight, Tensor bias, Tensor weight_scale, Tensor out_grad, str weight_dtype, int arch, int group_size)
  output : Tensor(x_grad)
  infer_meta :
    func : WeightOnlyLinearGradInferMeta
  kernel :
    func : weight_only_linear_grad
    data_type : out_grad
  optional: bias
  no_need_buffer: x

- backward_op : where_grad
  forward : where (Tensor condition, Tensor x, Tensor y) -> Tensor(out)
  args : (Tensor condition, Tensor x, Tensor y, Tensor out_grad)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, y]
    spmd_rule: WhereGradInferSpmd
  kernel :
    func : where_grad
  no_need_buffer : x, y

- backward_op : yolo_loss_grad
  forward : yolo_loss (Tensor x, Tensor gt_box, Tensor gt_label, Tensor gt_score, int[] anchors={}, int[] anchor_mask={}, int class_num =1 , float ignore_thresh=0.7, int downsample_ratio=32, bool use_label_smooth=true, float scale_x_y=1.0) -> Tensor(loss), Tensor(objectness_mask), Tensor(gt_match_mask)
  args : (Tensor x, Tensor gt_box, Tensor gt_label, Tensor gt_score, Tensor objectness_mask, Tensor gt_match_mask, Tensor loss_grad, int[] anchors, int[] anchor_mask, int class_num, float ignore_thresh, int downsample_ratio, bool use_label_smooth, float scale_x_y)
  output : Tensor(x_grad), Tensor(gt_box_grad), Tensor(gt_label_grad), Tensor(gt_score_grad)
  infer_meta :
    func : YoloLossGradInferMeta
  kernel :
    func : yolo_loss_grad
  optional : gt_score

- backward_op: disable_check_model_nan_inf_grad
  forward: disable_check_model_nan_inf (Tensor x, int flag=0) -> Tensor(out)
  args: (Tensor out_grad, int unsetflag = 1)
  output : Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [out_grad]
  kernel:
    func: check_model_nan_inf
    data_type: out_grad

- backward_op: enable_check_model_nan_inf_grad
  forward: enable_check_model_nan_inf (Tensor x, int flag=1) -> Tensor(out)
  args: (Tensor out_grad, int unsetflag = 0)
  output : Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [out_grad]
  kernel:
    func: check_model_nan_inf
    data_type: out_grad

- backward_op: pyramid_hash_grad
  forward: pyramid_hash (Tensor x, Tensor w, Tensor white_list, Tensor black_list, int num_emb = 0,
    int space_len = 0, int pyramid_layer = 2, int rand_len = 0, float drop_out_percent
    = 0, int is_training = 0, bool use_filter = true, int white_list_len = 0, int
    black_list_len = 0, int seed = 0, float lr = 0.0, str distribute_update_vars =
    "") -> Tensor (out), Tensor (drop_pos), Tensor (x_temp_out)
  args: (Tensor x, Tensor w, Tensor drop_pos, Tensor x_temp_out, Tensor out_grad, int num_emb = 0,
    int space_len = 0, int pyramid_layer = 2, int rand_len = 0, float drop_out_percent
    = 0, int is_training = 0, bool use_filter = true, int white_list_len = 0, int
    black_list_len = 0, int seed = 0, float lr = 0.0, str distribute_update_vars =
    "")
  output: Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: pyramid_hash_grad
    data_type: w

- backward_op: shuffle_batch_grad
  forward: shuffle_batch (Tensor x, Tensor seed, int startup_seed=0) -> Tensor(out), Tensor(shuffle_idx), Tensor(seed_out)
  args: (Tensor shuffle_idx, Tensor out_grad,int startup_seed=0)
  output : Tensor(x_grad)
  infer_meta:
    func: ShuffleBatchGradInferMeta
  kernel:
    func: shuffle_batch_grad
    data_type : out_grad

- backward_op: silu_double_grad
  forward: silu_grad (Tensor x, Tensor out, Tensor grad_out) -> Tensor(grad_x)
  args: (Tensor x, Tensor out, Tensor grad_out, Tensor grad_x_grad)
  output: Tensor(x_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralBinaryGradInferMeta
    param : [x, x]
  composite: silu_double_grad(x, out, grad_out, grad_x_grad, x_grad, grad_out_grad)

- backward_op: sparse_attention_grad
  forward: sparse_attention(Tensor q, Tensor k, Tensor v, Tensor offset, Tensor columns, Tensor key_padding_mask,
    Tensor attn_mask) -> Tensor (out), Tensor (sparse_dot_sdd), Tensor (softmax)
  args: (Tensor q, Tensor k, Tensor v, Tensor offset, Tensor columns, Tensor sparse_dot_sdd,
    Tensor softmax, Tensor out_grad)
  output: Tensor (q_grad), Tensor (k_grad), Tensor (v_grad)
  infer_meta:
    func: GeneralTernaryGradInferMeta
    param: [q, k, v]
  kernel:
    func: sparse_attention_grad
    data_type: out_grad

- backward_op: stft_grad
  forward: stft (Tensor x, Tensor window, int n_fft, int hop_length, bool normalized, bool onesided) -> Tensor (out)
  args: (Tensor x, Tensor window, Tensor out_grad, int n_fft, int hop_length, bool normalized, bool onesided)
  output: Tensor (x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: stft_grad
    data_type: x

- backward_op: unpool3d_grad
  forward: unpool3d (Tensor x, Tensor indices, int[] ksize, int[] strides={1,1,1}, int[] paddings={0,0,0}, int[] output_size={0,0,0}, str data_format="NCDHW") -> Tensor(out)
  args: (Tensor x, Tensor indices, Tensor out, Tensor out_grad, int[] ksize, int[] strides, int[] paddings, int[] output_size, str data_format)
  output: Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: unpool3d_grad
    data_type: x

- backward_op: unpool_grad
  forward: unpool (Tensor x, Tensor indices, int[] ksize, int[] strides, int[] padding,  IntArray output_size, str data_format) -> Tensor(out)
  args: (Tensor x, Tensor indices, Tensor out, Tensor out_grad, int[] ksize, int[] strides, int[] padding, IntArray output_size, str data_format)
  output: Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: unpool_grad
    data_type: x
