# This file is designed for fusion C++ backward operators, which manages the
# generated code for static mode and dynamic mode (when `support_dygraph_mode` is true).
# "support_dygraph_mode" is an extra configuration item in this file,
# if one operator have "support_dygraph_mode : true", it supports dygraph mode,
# otherwise the operator only could be used in static mode.

- backward_op : fused_bias_dropout_residual_layer_norm_grad
  forward: fused_bias_dropout_residual_layer_norm (Tensor x, Tensor residual, Tensor bias, Tensor ln_scale, Tensor ln_bias, float dropout_rate, bool is_test, bool dropout_fix_seed, int dropout_seed, str dropout_implementation, float ln_epsilon) -> Tensor(y), Tensor(bias_dropout_residual_out), Tensor(dropout_mask_out), Tensor(ln_mean), Tensor(ln_variance)
  args : (Tensor x, Tensor residual, Tensor bias, Tensor ln_scale, Tensor ln_bias, Tensor ln_mean, Tensor ln_variance, Tensor bias_dropout_residual_out, Tensor dropout_mask_out, Tensor y_grad, float dropout_rate = 0.5f, bool is_test = false, bool dropout_fix_seed = true, int dropout_seed = true, str dropout_implementation = "downgrade_in_infer", float ln_epsilon = 1e-5)
  output : Tensor(x_grad), Tensor(residual_grad), Tensor(bias_grad), Tensor(ln_scale_grad), Tensor(ln_bias_grad)
  optional :  bias, ln_scale, ln_bias, bias_grad, ln_scale_grad, ln_bias_grad
  infer_meta :
    func : FusedBiasDropoutResidualLnGradInferMeta
  kernel :
    func : fused_bias_dropout_residual_layer_norm_grad
    data_type : y_grad
  support_dygraph_mode : true

- backward_op : fused_dot_product_attention_grad
  forward : fused_dot_product_attention (Tensor q, Tensor k, Tensor v, Tensor mask, float scaling_factor, float dropout_probability, bool is_training, bool is_causal_masking) -> Tensor(out), Tensor(softmax_out), Tensor(rng_state)
  args : (Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_out, Tensor rng_state, Tensor mask, Tensor out_grad, float scaling_factor, float dropout_probability, bool is_causal_masking = false)
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  infer_meta :
    func : FusedDotProductAttentionGradInferMeta
    param: [q, k, v]
  kernel :
    func : fused_dot_product_attention_grad
    data_type : q
  support_dygraph_mode : true

- backward_op : fused_dropout_add_grad
  forward : fused_dropout_add (Tensor x, Tensor y, Tensor seed_tensor, Scalar p, bool is_test, str mode, int seed, bool fix_seed) -> Tensor(out), Tensor(seed_offset)
  args : (Tensor seed_offset, Tensor out_grad, Scalar p, bool is_test, str mode, bool fix_seed)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : FusedDropoutAddGradInferMeta
    param : [seed_offset, out_grad]
  kernel :
    func : fused_dropout_add_grad
    data_type : out_grad
  support_dygraph_mode : true

- backward_op : fused_elemwise_activation_grad
  forward: fused_elemwise_activation (Tensor x, Tensor y, str[] functor_list, int axis = -1, float scale = 0.0, bool save_intermediate_out
    = false) -> Tensor (out), Tensor (intermediate_out)
  args: (Tensor x, Tensor y, Tensor out, Tensor intermediate_out, Tensor out_grad, str[] functor_list, int axis = -1, float scale = 0.0, bool save_intermediate_out = false)
  output: Tensor (x_grad), Tensor (y_grad)
  infer_meta:
    func: FusedElemwiseActivationGradInferMeta
  kernel:
    func: fused_elemwise_activation_grad
    data_type: out_grad

- backward_op : fused_elemwise_add_activation_grad
  forward: fused_elemwise_add_activation (Tensor x, Tensor y, str[] functor_list, int axis = -1, float scale = 0.0, bool save_intermediate_out = false) -> Tensor (out), Tensor (intermediate_out)
  args: (Tensor x, Tensor y, Tensor out, Tensor intermediate_out, Tensor out_grad, str[] functor_list, int axis = -1, float scale = 0.0, bool save_intermediate_out = false)
  output: Tensor (x_grad), Tensor (y_grad)
  infer_meta:
    func: FusedElemwiseActivationGradInferMeta
  kernel:
    func: fused_elemwise_add_activation_grad
    data_type: out_grad
  optional: x, intermediate_out
  no_need_buffer: x, y

- backward_op : fused_rotary_position_embedding_grad
  forward: fused_rotary_position_embedding (Tensor q, Tensor k, Tensor v, Tensor sin, Tensor cos, Tensor position_ids, bool use_neox_rotary_style, bool time_major, float rotary_emb_base) -> Tensor(out_q), Tensor(out_k), Tensor(out_v)
  args : (Tensor sin, Tensor cos, Tensor position_ids, Tensor out_q_grad, Tensor out_k_grad,Tensor out_v_grad, bool use_neox_rotary_style, bool time_major, float rotary_emb_base)
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  optional :  sin, cos, position_ids, out_k_grad, out_v_grad, k_grad, v_grad
  infer_meta :
    func : FusedRopeGradInferMeta
    spmd_rule : FusedRopeGradInferSpmd
  kernel :
    func : fused_rotary_position_embedding_grad
    data_type : out_q_grad
  support_dygraph_mode : true

- backward_op : fused_seqpool_cvm_grad
  forward: fused_seqpool_cvm(Tensor[] x, Tensor cvm, str pooltype = "SUM", float pad_value = 0.0, bool
    use_cvm = true, int cvm_offset = 2) -> Tensor[] (out)
  args: (Tensor[] x, Tensor cvm, Tensor[] out_grad, str pooltype = "SUM", float pad_value = 0.0, bool
    use_cvm = true, int cvm_offset = 2)
  output: Tensor[] (x_grad){x.size()}, Tensor (cvm_grad)
  infer_meta:
    func: FusedSeqpoolCvmGradInferMeta
  kernel:
    func: fused_seqpool_cvm_grad
    data_type: x

- backward_op : max_pool2d_v2_grad
  forward : max_pool2d_v2(Tensor x, int[] kernel_size, int[] strides= {1, 1}, int[] paddings = {0, 0}, str data_format = "NCHW", bool global_pooling = false, bool adaptive = false) -> Tensor(out), Tensor(saved_idx)
  args : (Tensor x, Tensor out, Tensor saved_idx, Tensor out_grad, int[] kernel_size, int[] strides, int[] paddings, str data_format, bool global_pooling, bool adaptive)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param: [x]
  kernel :
    func : max_pool2d_v2_grad
    param: [x, out, saved_idx, out_grad, kernel_size, strides, paddings, data_format, global_pooling, adaptive]
