// Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// this file is generated by paddle/phi/op/yaml/generator/generate_op.py, do not
// edit.
#include "paddle/phi/core/compat/op_utils.h"
#include "paddle/utils/small_vector.h"

namespace phi {

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AllGatherOpArgumentMapping:

return KernelSignature("all_gather", {"x"}, {"ring_id", "nranks"}, {"out"});
******************************************************************
*/

KernelSignature AllGatherOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("nranks");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "all_gather", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AllReduceOpArgumentMapping:

return KernelSignature("all_reduce", {"x"}, {"ring_id", "reduce_type"},
{"out"});
******************************************************************
*/

KernelSignature AllReduceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("reduce_type");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "all_reduce", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AllToAllOpArgumentMapping:

return KernelSignature("all_to_all", {"x"}, {"ring_id"}, {"out"});
******************************************************************
*/

KernelSignature AllToAllOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "all_to_all", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArangeOpArgumentMapping:

return KernelSignature("arange_tensor", {"Start", "End", "Step"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RangeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Start", "End", "Step"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "arange_tensor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BroadcastOpArgumentMapping:

return KernelSignature("broadcast", {"x"}, {"ring_id", "root"}, {"out"});
******************************************************************
*/

KernelSignature BroadcastOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("root");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "broadcast", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dTransposeOpArgumentMapping:

return KernelSignature("conv2d_transpose", {"Input", "Filter"}, {"strides",
"paddings", "output_padding", "output_size", "padding_algorithm", "groups",
"dilations", "data_format"}, {"Output"}); return
KernelSignature("conv2d_transpose", {"Input", "Filter"}, {"strides", "paddings",
"output_padding", "OutputSizeTensor", "padding_algorithm", "groups",
"dilations", "data_format"}, {"Output"}); return
KernelSignature("conv2d_transpose", {"Input", "Filter"}, {"strides", "paddings",
"output_padding", "OutputSizeTensorList", "padding_algorithm", "groups",
"dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv2dTransposeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input",
      "Filter",
  };
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature("conv2d_transpose",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DecodeJpegOpArgumentMapping:

return KernelSignature("decode_jpeg", {"X"}, {"mode"}, {"Out"});
******************************************************************
*/

KernelSignature DecodeJpegOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "decode_jpeg", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DeformableConvOpArgumentMapping:

return KernelSignature("deformable_conv", {"Input", "Offset", "Filter", "Mask"},
{"strides", "paddings", "dilations", "deformable_groups", "groups",
"im2col_step"}, {"Output"});
******************************************************************
*/

KernelSignature DeformableConvOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Offset", "Filter", "Mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("deformable_groups");
  attrs.emplace_back("groups");
  attrs.emplace_back("im2col_step");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature("deformable_conv",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
DepthwiseConv2dTransposeOpArgumentMapping:

return KernelSignature("depthwise_conv2d_transpose", {"Input", "Filter"},
{"strides", "paddings", "output_padding", "output_size", "padding_algorithm",
"groups", "dilations", "data_format"}, {"Output"}); return
KernelSignature("depthwise_conv2d_transpose", {"Input", "Filter"}, {"strides",
"paddings", "output_padding", "OutputSizeTensor", "padding_algorithm", "groups",
"dilations", "data_format"}, {"Output"}); return
KernelSignature("depthwise_conv2d_transpose", {"Input", "Filter"}, {"strides",
"paddings", "output_padding", "OutputSizeTensorList", "padding_algorithm",
"groups", "dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature DepthwiseConv2dTransposeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input",
      "Filter",
  };
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature("depthwise_conv2d_transpose",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DistConcatOpArgumentMapping:

return KernelSignature("dist_concat", {"x"}, {"ring_id", "nranks"}, {"out"});
******************************************************************
*/

KernelSignature DistConcatOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("nranks");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "dist_concat", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EinsumOpArgumentMapping:

return KernelSignature("einsum", {"Operands"}, {"equation"}, {"Out",
"InnerCache", "XShape"});
******************************************************************
*/

KernelSignature EinsumOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Operands"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("equation");
  paddle::small_vector<const char*> outputs{"Out", "InnerCache", "XShape"};
  return KernelSignature(
      "einsum", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EmbeddingOpArgumentMapping:

return KernelSignature("embedding", {"Ids", "W"}, {"padding_idx"}, {"Out"});
return KernelSignature("sparse_weight_embedding", {"Ids", "W"}, {"padding_idx"},
{"Out"});
******************************************************************
*/

KernelSignature LookupTableV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Ids", "W"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("padding_idx");
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("Ids") && ctx.IsDenseTensorInput("W")) {
    return KernelSignature(
        "embedding", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsDenseTensorInput("Ids") && ctx.IsSelectedRowsInput("W")) {
    return KernelSignature("sparse_weight_embedding",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EmptyOpArgumentMapping:

return KernelSignature("empty", {}, {"shape", "dtype"}, {"Out"});
return KernelSignature("empty", {}, {"ShapeTensor", "dtype"}, {"Out"});
return KernelSignature("empty", {}, {"ShapeTensorList", "dtype"}, {"Out"});
******************************************************************
*/

KernelSignature EmptyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("ShapeTensor")            ? "ShapeTensor"
                     : ctx.InputSize("ShapeTensorList") > 0 ? "ShapeTensorList"
                                                            : "shape");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "empty", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EqualOpArgumentMapping:

return KernelSignature("equal_raw", {"X", "Y"}, {"axis", "force_cpu"}, {"Out"});
******************************************************************
*/

KernelSignature EqualOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "equal_raw", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExponentialOpArgumentMapping:

return KernelSignature("exponential", {"X"}, {"lambda"}, {"Out"});
******************************************************************
*/

KernelSignature ExponentialOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("lambda");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "exponential", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EyeOpArgumentMapping:

return KernelSignature("eye", {}, {"num_rows", "num_columns", "dtype"},
{"Out"});
******************************************************************
*/

KernelSignature EyeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("num_rows");
  attrs.emplace_back("num_columns");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "eye", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FrobeniusNormOpArgumentMapping:

return KernelSignature("frobenius_norm", {"X"}, {"dim", "keep_dim",
"reduce_all", "in_dtype", "out_dtype"}, {"Out"}); return
KernelSignature("frobenius_norm", {"X"}, {"AxisTensor", "keep_dim",
"reduce_all", "in_dtype", "out_dtype"}, {"Out"}); return
KernelSignature("frobenius_norm", {"X"}, {"AxisTensorList", "keep_dim",
"reduce_all", "in_dtype", "out_dtype"}, {"Out"});
******************************************************************
*/

KernelSignature FrobeniusNormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("frobenius_norm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FullLikeOpArgumentMapping:

return KernelSignature("full_like", {"X"}, {"value", "dtype"}, {"Out"});
return KernelSignature("full_like", {"X"}, {"ValueTensor", "dtype"}, {"Out"});
******************************************************************
*/

KernelSignature FillAnyLikeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "full_like", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GreaterEqualOpArgumentMapping:

return KernelSignature("greater_equal_raw", {"X", "Y"}, {"axis", "force_cpu"},
{"Out"});
******************************************************************
*/

KernelSignature GreaterEqualOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("greater_equal_raw",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GreaterThanOpArgumentMapping:

return KernelSignature("greater_than_raw", {"X", "Y"}, {"axis", "force_cpu"},
{"Out"});
******************************************************************
*/

KernelSignature GreaterThanOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("greater_than_raw",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LessEqualOpArgumentMapping:

return KernelSignature("less_equal_raw", {"X", "Y"}, {"axis", "force_cpu"},
{"Out"});
******************************************************************
*/

KernelSignature LessEqualOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("less_equal_raw",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LessThanOpArgumentMapping:

return KernelSignature("less_than_raw", {"X", "Y"}, {"axis", "force_cpu"},
{"Out"});
******************************************************************
*/

KernelSignature LessThanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "less_than_raw", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LinspaceOpArgumentMapping:

return KernelSignature("linspace", {"Start", "Stop", "Num"}, {"dtype"},
{"Out"});
******************************************************************
*/

KernelSignature LinspaceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Start", "Stop", "Num"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "linspace", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatmulOpArgumentMapping:

return KernelSignature("matmul", {"X", "Y"}, {"trans_x", "trans_y"}, {"Out"});
******************************************************************
*/

KernelSignature MatmulV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("trans_x");
  attrs.emplace_back("trans_y");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "matmul", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NormOpArgumentMapping:

return KernelSignature("norm", {"X"}, {"axis", "epsilon", "is_test"}, {"Out",
"Norm"});
******************************************************************
*/

KernelSignature NormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{"Out", "Norm"};
  return KernelSignature(
      "norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NotEqualOpArgumentMapping:

return KernelSignature("not_equal_raw", {"X", "Y"}, {"axis", "force_cpu"},
{"Out"});
******************************************************************
*/

KernelSignature NotEqualOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "not_equal_raw", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by OneHotOpArgumentMapping:

return KernelSignature("one_hot_raw", {"X"}, {"depth", "dtype",
"allow_out_of_range"}, {"Out"});
******************************************************************
*/

KernelSignature OneHotV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("depth_tensor") ? "depth_tensor" : "depth");
  attrs.emplace_back("dtype");
  attrs.emplace_back("allow_out_of_range");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "one_hot_raw", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PRecvOpArgumentMapping:

return KernelSignature("p_recv", {}, {"ring_id", "peer", "dtype",
"dynamic_shape"}, {"out"});
******************************************************************
*/

KernelSignature PRecvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("peer");
  attrs.emplace_back("dtype");
  attrs.emplace_back("dynamic_shape");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "p_recv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PRecvArrayOpArgumentMapping:

return KernelSignature("p_recv_array", {}, {"ring_id", "peer", "dtype",
"out_shape"}, {"out"});
******************************************************************
*/

KernelSignature PRecvArrayOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("peer");
  attrs.emplace_back("dtype");
  attrs.emplace_back("out_shape");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "p_recv_array", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pool2dOpArgumentMapping:

return KernelSignature("pool2d", {"X"}, {"ksize", "strides", "paddings",
"ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling",
"adaptive", "padding_algorithm", "use_cudnn"}, {"Out"}); return
KernelSignature("pool2d", {"X"}, {"KernelSizeTensor", "strides", "paddings",
"ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling",
"adaptive", "padding_algorithm", "use_cudnn"}, {"Out"}); return
KernelSignature("pool2d", {"X"}, {"KernelSizeTensorList", "strides", "paddings",
"ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling",
"adaptive", "padding_algorithm", "use_cudnn"}, {"Out"});
******************************************************************
*/

KernelSignature Pool2dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("ceil_mode");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("data_format");
  attrs.emplace_back("pooling_type");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  attrs.emplace_back("padding_algorithm");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "pool2d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pool3dOpArgumentMapping:

return KernelSignature("pool3d", {"X"}, {"ksize", "strides", "paddings",
"ceil_mode", "exclusive", "data_format", "pooling_type", "global_pooling",
"adaptive", "padding_algorithm", "use_cudnn"}, {"Out"});
******************************************************************
*/

KernelSignature Pool3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("ceil_mode");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("data_format");
  attrs.emplace_back("pooling_type");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  attrs.emplace_back("padding_algorithm");

  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "pool3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QuantLinearOpArgumentMapping:

return KernelSignature("quant_linear", {"x", "w", "bias"}, {"in_num_col_dims",
"activation_type", "padding_weights", "scale_in", "scale_weights",
"quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out"});
******************************************************************
*/

KernelSignature QuantLinearOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "w", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("in_num_col_dims");
  attrs.emplace_back("activation_type");
  attrs.emplace_back("padding_weights");
  attrs.emplace_back("scale_in");
  attrs.emplace_back("scale_weights");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "quant_linear", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RandpermOpArgumentMapping:

return KernelSignature("randperm", {}, {"n", "dtype"}, {"Out"});
******************************************************************
*/

KernelSignature RandpermOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "randperm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReduceOpArgumentMapping:

return KernelSignature("reduce", {"x"}, {"ring_id", "root_id", "reduce_type"},
{"out"});
******************************************************************
*/

KernelSignature ReduceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("root_id");
  attrs.emplace_back("reduce_type");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "reduce", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReduceScatterOpArgumentMapping:

return KernelSignature("reduce_scatter", {"x"}, {"ring_id", "nranks"}, {"out"});
******************************************************************
*/

KernelSignature ReduceScatterOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  attrs.emplace_back("nranks");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("reduce_scatter",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RnnOpArgumentMapping:

return KernelSignature("rnn", {"Input", "PreState", "WeightList",
"SequenceLength"}, {"dropout_prob", "is_bidirec", "input_size", "hidden_size",
"num_layers", "mode", "seed", "is_test"}, {"Out", "DropoutState", "State",
"Reserve"});
******************************************************************
*/

KernelSignature RnnOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "PreState", "WeightList", "SequenceLength"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout_prob");
  attrs.emplace_back("is_bidirec");
  attrs.emplace_back("input_size");
  attrs.emplace_back("hidden_size");
  attrs.emplace_back("num_layers");
  attrs.emplace_back("mode");
  attrs.emplace_back("seed");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{
      "Out", "DropoutState", "State", "Reserve"};
  return KernelSignature(
      "rnn", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ShadowOutputOpArgumentMapping:

return KernelSignature("shadow_output", {"x"}, {"name"}, {"out"});
******************************************************************
*/

KernelSignature ShadowOutputOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;

  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "shadow_output", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ShareBufferOpArgumentMapping:

return KernelSignature("share_buffer", {"X"}, {"share_dims_and_dtype"}, {"Out",
"XOut"});
******************************************************************
*/

KernelSignature ShareBufferOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("share_dims_and_dtype");
  paddle::small_vector<const char*> outputs{"Out", "XOut"};
  return KernelSignature(
      "share_buffer", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftmaxOpArgumentMapping:

return KernelSignature("softmax", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature SoftmaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "softmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SwishOpArgumentMapping:

return KernelSignature("swish", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SwishOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "swish", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilIndicesOpArgumentMapping:

return KernelSignature("tril_indices", {}, {"rows", "cols", "offset", "dtype"},
{"out"});
******************************************************************
*/

KernelSignature TrilIndicesOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("rows");
  attrs.emplace_back("cols");
  attrs.emplace_back("offset");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "tril_indices", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilTriuOpArgumentMapping:

return KernelSignature("tril_triu", {"X"}, {"diagonal", "lower"}, {"Out"});
******************************************************************
*/

KernelSignature TrilTriuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("diagonal");
  attrs.emplace_back("lower");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "tril_triu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TriuIndicesOpArgumentMapping:

return KernelSignature("triu_indices", {}, {"row", "col", "offset", "dtype"},
{"out"});
******************************************************************
*/

KernelSignature TriuIndicesOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("row");
  attrs.emplace_back("col");
  attrs.emplace_back("offset");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "triu_indices", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
TruncatedGaussianRandomOpArgumentMapping:

return KernelSignature("truncated_gaussian_random", {}, {"shape", "mean", "std",
"seed", "dtype"}, {"Out"});
******************************************************************
*/

KernelSignature TruncatedGaussianRandomOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("shape");
  attrs.emplace_back("mean");
  attrs.emplace_back("std");
  attrs.emplace_back("seed");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("truncated_gaussian_random",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnpoolOpArgumentMapping:

return KernelSignature("unpool", {"X", "Indices"}, {"ksize", "unpooling_type",
"strides", "paddings", "output_size", "data_format"}, {"Out"}); return
KernelSignature("unpool", {"X", "Indices"}, {"ksize", "unpooling_type",
"strides", "paddings", "OutputSizeTensor", "data_format"}, {"Out"}); return
KernelSignature("unpool", {"X", "Indices"}, {"ksize", "unpooling_type",
"strides", "paddings", "OutputSizeTensorList", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature UnpoolOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");

  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_size");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "unpool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AmaxGradOpArgumentMapping:

return KernelSignature("amax_grad", {"X", "Out", "Out@GRAD"}, {"dim",
"keep_dim", "reduce_all"}, {"X@GRAD"}); return KernelSignature("amax_grad",
{"X", "Out", "Out@GRAD"}, {"AxisTensor", "keep_dim", "reduce_all"}, {"X@GRAD"});
return KernelSignature("amax_grad", {"X", "Out", "Out@GRAD"}, {"AxisTensorList",
"keep_dim", "reduce_all"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReduceAmaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor")            ? "AxisTensor"
                     : ctx.InputSize("AxisTensorList") > 0 ? "AxisTensorList"
                                                           : "dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "amax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AminGradOpArgumentMapping:

return KernelSignature("amin_grad", {"X", "Out", "Out@GRAD"}, {"dim",
"keep_dim", "reduce_all"}, {"X@GRAD"}); return KernelSignature("amin_grad",
{"X", "Out", "Out@GRAD"}, {"AxisTensor", "keep_dim", "reduce_all"}, {"X@GRAD"});
return KernelSignature("amin_grad", {"X", "Out", "Out@GRAD"}, {"AxisTensorList",
"keep_dim", "reduce_all"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReduceAminGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor")            ? "AxisTensor"
                     : ctx.InputSize("AxisTensorList") > 0 ? "AxisTensorList"
                                                           : "dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "amin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
Conv2dTransposeDoubleGradOpArgumentMapping:

return KernelSignature("conv2d_transpose_double_grad", {"Input", "Filter",
"grad_out", "grad_x@GRAD", "grad_filter@GRAD"}, {"strides", "paddings",
"output_padding", "output_size", "padding_algorithm", "groups", "dilations",
"data_format"}, {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"}); return
KernelSignature("conv2d_transpose_double_grad", {"Input", "Filter", "grad_out",
"grad_x@GRAD", "grad_filter@GRAD"}, {"strides", "paddings", "output_padding",
"OutputSizeTensor", "padding_algorithm", "groups", "dilations", "data_format"},
{"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"}); return
KernelSignature("conv2d_transpose_double_grad", {"Input", "Filter", "grad_out",
"grad_x@GRAD", "grad_filter@GRAD"}, {"strides", "paddings", "output_padding",
"OutputSizeTensorList", "padding_algorithm", "groups", "dilations",
"data_format"}, {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature Conv2dTransposeGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "Filter", "grad_out", "grad_x@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{
      "Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("conv2d_transpose_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dTransposeGradOpArgumentMapping:

return KernelSignature("conv2d_transpose_grad", {"Input", "Filter",
"Output@GRAD"}, {"strides", "paddings", "output_padding", "output_size",
"padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD",
"Filter@GRAD"}); return KernelSignature("conv2d_transpose_grad", {"Input",
"Filter", "Output@GRAD"}, {"strides", "paddings", "output_padding",
"OutputSizeTensor", "padding_algorithm", "groups", "dilations", "data_format"},
{"Input@GRAD", "Filter@GRAD"}); return KernelSignature("conv2d_transpose_grad",
{"Input", "Filter", "Output@GRAD"}, {"strides", "paddings", "output_padding",
"OutputSizeTensorList", "padding_algorithm", "groups", "dilations",
"data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv2dTransposeGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("conv2d_transpose_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DeformableConvGradOpArgumentMapping:

return KernelSignature("deformable_conv_grad", {"Input", "Offset", "Filter",
"Mask", "Output@GRAD"}, {"strides", "paddings", "dilations",
"deformable_groups", "groups", "im2col_step"}, {"Input@GRAD", "Offset@GRAD",
"Filter@GRAD", "Mask@GRAD"});
******************************************************************
*/

KernelSignature DeformableConvGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "Offset", "Filter", "Mask", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("deformable_groups");
  attrs.emplace_back("groups");
  attrs.emplace_back("im2col_step");
  paddle::small_vector<const char*> outputs{
      "Input@GRAD", "Offset@GRAD", "Filter@GRAD", "Mask@GRAD"};
  return KernelSignature("deformable_conv_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
DepthwiseConv2dTransposeGradOpArgumentMapping:

return KernelSignature("depthwise_conv2d_transpose_grad", {"Input", "Filter",
"Output@GRAD"}, {"strides", "paddings", "output_padding", "output_size",
"padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD",
"Filter@GRAD"}); return KernelSignature("depthwise_conv2d_transpose_grad",
{"Input", "Filter", "Output@GRAD"}, {"strides", "paddings", "output_padding",
"OutputSizeTensor", "padding_algorithm", "groups", "dilations", "data_format"},
{"Input@GRAD", "Filter@GRAD"}); return
KernelSignature("depthwise_conv2d_transpose_grad", {"Input", "Filter",
"Output@GRAD"}, {"strides", "paddings", "output_padding",
"OutputSizeTensorList", "padding_algorithm", "groups", "dilations",
"data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature DepthwiseConv2dTransposeGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("depthwise_conv2d_transpose_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EinsumGradOpArgumentMapping:

return KernelSignature("einsum_grad", {"x_shape", "InnerCache", "Out@GRAD"},
{"equation"}, {"Operands@GRAD"});
******************************************************************
*/

KernelSignature EinsumGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x_shape", "InnerCache", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("equation");
  paddle::small_vector<const char*> outputs{"Operands@GRAD"};
  return KernelSignature(
      "einsum_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ElementwisePowGradOpArgumentMapping:

return KernelSignature("elementwise_pow_grad", {"X", "Y", "Out@GRAD"}, {},
{"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwisePowGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature("elementwise_pow_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FrobeniusNormGradOpArgumentMapping:

return KernelSignature("frobenius_norm_grad", {"X", "Out", "Out@GRAD"}, {"dim",
"keep_dim", "reduce_all", "in_dtype", "out_dtype"}, {"X@GRAD"}); return
KernelSignature("frobenius_norm_grad", {"X", "Out", "Out@GRAD"}, {"AxisTensor",
"keep_dim", "reduce_all", "in_dtype", "out_dtype"}, {"X@GRAD"}); return
KernelSignature("frobenius_norm_grad", {"X", "Out", "Out@GRAD"},
{"AxisTensorList", "keep_dim", "reduce_all", "in_dtype", "out_dtype"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature FrobeniusNormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");

  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("frobenius_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardswishGradOpArgumentMapping:

return KernelSignature("hardswish_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature HardSwishGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("hardswish_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatmulDoubleGradOpArgumentMapping:

return KernelSignature("matmul_double_grad", {"X", "Y", "grad_out",
"grad_x@GRAD", "grad_y@GRAD"}, {"trans_x", "trans_y"}, {"X@GRAD", "Y@GRAD",
"grad_out@GRAD"});
******************************************************************
*/

KernelSignature MatmulV2GradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Y", "grad_out", "grad_x@GRAD", "grad_y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("trans_x");
  attrs.emplace_back("trans_y");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "Y@GRAD", "grad_out@GRAD"};
  return KernelSignature("matmul_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatmulGradOpArgumentMapping:

return KernelSignature("matmul_grad", {"X", "Y", "Out@GRAD"}, {"trans_x",
"trans_y"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature MatmulV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("trans_x");
  attrs.emplace_back("trans_y");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "matmul_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatmulTripleGradOpArgumentMapping:

return KernelSignature("matmul_triple_grad", {"X", "Y", "grad_out",
"grad_grad_x", "grad_grad_y", "grad_x@GRAD", "grad_y@GRAD",
"grad_grad_out@GRAD"}, {"trans_x", "trans_y"}, {"X@GRAD", "Y@GRAD",
"grad_out@GRAD", "grad_grad_x@GRAD", "grad_grad_y@GRAD"});
******************************************************************
*/

KernelSignature MatmulV2TripleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X",
                                           "Y",
                                           "grad_out",
                                           "grad_grad_x",
                                           "grad_grad_y",
                                           "grad_x@GRAD",
                                           "grad_y@GRAD",
                                           "grad_grad_out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("trans_x");
  attrs.emplace_back("trans_y");
  paddle::small_vector<const char*> outputs{"X@GRAD",
                                            "Y@GRAD",
                                            "grad_out@GRAD",
                                            "grad_grad_x@GRAD",
                                            "grad_grad_y@GRAD"};
  return KernelSignature("matmul_triple_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxGradOpArgumentMapping:

return KernelSignature("max_grad", {"X", "Out", "Out@GRAD"}, {"dim", "keep_dim",
"reduce_all"}, {"X@GRAD"}); return KernelSignature("max_grad", {"X", "Out",
"Out@GRAD"}, {"AxisTensor", "keep_dim", "reduce_all"}, {"X@GRAD"}); return
KernelSignature("max_grad", {"X", "Out", "Out@GRAD"}, {"AxisTensorList",
"keep_dim", "reduce_all"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReduceMaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "max_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaximumGradOpArgumentMapping:

return KernelSignature("maximum_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseMaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "maximum_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MinGradOpArgumentMapping:

return KernelSignature("min_grad", {"X", "Out", "Out@GRAD"}, {"dim", "keep_dim",
"reduce_all"}, {"X@GRAD"}); return KernelSignature("min_grad", {"X", "Out",
"Out@GRAD"}, {"AxisTensor", "keep_dim", "reduce_all"}, {"X@GRAD"}); return
KernelSignature("min_grad", {"X", "Out", "Out@GRAD"}, {"AxisTensorList",
"keep_dim", "reduce_all"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReduceMinGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "min_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MinimumGradOpArgumentMapping:

return KernelSignature("minimum_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseMinGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "minimum_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NormGradOpArgumentMapping:

return KernelSignature("norm_grad", {"X", "Norm", "Out@GRAD"}, {"axis",
"epsilon", "is_test"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Norm", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pool2dDoubleGradOpArgumentMapping:

return KernelSignature("pool2d_double_grad", {"grad_x@GRAD"}, {"ksize",
"strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type",
"global_pooling", "adaptive", "padding_algorithm"}, {"grad_out@GRAD"}); return
KernelSignature("pool2d_double_grad", {"grad_x@GRAD"}, {"KernelSizeTensor",
"strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type",
"global_pooling", "adaptive", "padding_algorithm"}, {"grad_out@GRAD"}); return
KernelSignature("pool2d_double_grad", {"grad_x@GRAD"}, {"KernelSizeTensorList",
"strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type",
"global_pooling", "adaptive", "padding_algorithm"}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature Pool2dDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("ceil_mode");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("data_format");
  attrs.emplace_back("pooling_type");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  attrs.emplace_back("padding_algorithm");
  paddle::small_vector<const char*> outputs{"grad_out@GRAD"};
  return KernelSignature("pool2d_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pool2dGradOpArgumentMapping:

return KernelSignature("pool2d_grad", {"X", "Out", "Out@GRAD"}, {"ksize",
"strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type",
"global_pooling", "adaptive", "padding_algorithm"}, {"X@GRAD"}); return
KernelSignature("pool2d_grad", {"X", "Out", "Out@GRAD"}, {"KernelSizeTensor",
"strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type",
"global_pooling", "adaptive", "padding_algorithm"}, {"X@GRAD"}); return
KernelSignature("pool2d_grad", {"X", "Out", "Out@GRAD"},
{"KernelSizeTensorList", "strides", "paddings", "ceil_mode", "exclusive",
"data_format", "pooling_type", "global_pooling", "adaptive",
"padding_algorithm"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Pool2dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("ceil_mode");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("data_format");
  attrs.emplace_back("pooling_type");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  attrs.emplace_back("padding_algorithm");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "pool2d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pool3dGradOpArgumentMapping:

return KernelSignature("pool3d_grad", {"X", "Out", "Out@GRAD"}, {"ksize",
"strides", "paddings", "ceil_mode", "exclusive", "data_format", "pooling_type",
"global_pooling", "adaptive", "padding_algorithm"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Pool3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("ceil_mode");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("data_format");
  attrs.emplace_back("pooling_type");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  attrs.emplace_back("padding_algorithm");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "pool3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ProdGradOpArgumentMapping:

return KernelSignature("prod_grad", {"X", "Out", "Out@GRAD"}, {"dim",
"keep_dim", "reduce_all"}, {"X@GRAD"}); return KernelSignature("prod_grad",
{"X", "Out", "Out@GRAD"}, {"DimsTensor", "keep_dim", "reduce_all"}, {"X@GRAD"});
return KernelSignature("prod_grad", {"X", "Out", "Out@GRAD"}, {"DimsTensorList",
"keep_dim", "reduce_all"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReduceProdGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "prod_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RnnGradOpArgumentMapping:

return KernelSignature("rnn_grad", {"Input", "PreState", "WeightList",
"SequenceLength", "Out", "DropoutState", "Reserve", "Out@GRAD", "State@GRAD"},
{"dropout_prob", "is_bidirec", "input_size", "hidden_size", "num_layers",
"mode", "seed", "is_test"}, {"Input@GRAD", "PreState@GRAD", "WeightList@GRAD"});
******************************************************************
*/

KernelSignature RnnGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input",
                                           "PreState",
                                           "WeightList",
                                           "SequenceLength",
                                           "Out",
                                           "DropoutState",
                                           "Reserve",
                                           "Out@GRAD",
                                           "State@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout_prob");
  attrs.emplace_back("is_bidirec");
  attrs.emplace_back("input_size");
  attrs.emplace_back("hidden_size");
  attrs.emplace_back("num_layers");
  attrs.emplace_back("mode");
  attrs.emplace_back("seed");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{
      "Input@GRAD", "PreState@GRAD", "WeightList@GRAD"};
  return KernelSignature(
      "rnn_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftmaxGradOpArgumentMapping:

return KernelSignature("softmax_grad", {"Out", "Out@GRAD"}, {"axis"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature SoftmaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "softmax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SumGradOpArgumentMapping:

return KernelSignature("sum_grad", {"X", "Out@GRAD"}, {"dim", "keep_dim",
"reduce_all"}, {"X@GRAD"}); return KernelSignature("sum_grad", {"X",
"Out@GRAD"}, {"AxisTensor", "keep_dim", "reduce_all"}, {"X@GRAD"}); return
KernelSignature("sum_grad", {"X", "Out@GRAD"}, {"AxisTensorList", "keep_dim",
"reduce_all"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReduceSumGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("keep_dim");
  attrs.emplace_back("reduce_all");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "sum_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SwishGradOpArgumentMapping:

return KernelSignature("swish_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SwishGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "swish_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilTriuGradOpArgumentMapping:

return KernelSignature("tril_triu_grad", {"Out@GRAD"}, {"diagonal", "lower"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature TrilTriuGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("diagonal");
  attrs.emplace_back("lower");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("tril_triu_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnpoolGradOpArgumentMapping:

return KernelSignature("unpool_grad", {"X", "Indices", "Out", "Out@GRAD"},
{"ksize", "strides", "paddings", "output_size", "data_format"}, {"X@GRAD"});
return KernelSignature("unpool_grad", {"X", "Indices", "Out", "Out@GRAD"},
{"ksize", "strides", "paddings", "OutputSizeTensor", "data_format"},
{"X@GRAD"}); return KernelSignature("unpool_grad", {"X", "Indices", "Out",
"Out@GRAD"}, {"ksize", "strides", "paddings", "OutputSizeTensorList",
"data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UnpoolGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_size");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "unpool_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

}  // namespace phi

PD_REGISTER_ARG_MAPPING_FN(all_gather, phi::AllGatherOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(all_reduce, phi::AllReduceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(all_to_all, phi::AllToAllOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(range, arange);
PD_REGISTER_ARG_MAPPING_FN(range, phi::RangeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(broadcast, phi::BroadcastOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_transpose,
                           phi::Conv2dTransposeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(decode_jpeg, phi::DecodeJpegOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(deformable_conv,
                           phi::DeformableConvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d_transpose,
                           phi::DepthwiseConv2dTransposeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dist_concat, phi::DistConcatOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(einsum, phi::EinsumOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(lookup_table_v2, embedding);
PD_REGISTER_ARG_MAPPING_FN(lookup_table_v2,
                           phi::LookupTableV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(empty, phi::EmptyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(equal, phi::EqualOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(exponential, phi::ExponentialOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eye, phi::EyeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(frobenius_norm, phi::FrobeniusNormOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(fill_any_like, full_like);
PD_REGISTER_ARG_MAPPING_FN(fill_any_like, phi::FillAnyLikeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(greater_equal, phi::GreaterEqualOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(greater_than, phi::GreaterThanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(less_equal, phi::LessEqualOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(less_than, phi::LessThanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(linspace, phi::LinspaceOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(matmul_v2, matmul);
PD_REGISTER_ARG_MAPPING_FN(matmul_v2, phi::MatmulV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(norm, phi::NormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(not_equal, phi::NotEqualOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(one_hot_v2, one_hot);
PD_REGISTER_ARG_MAPPING_FN(one_hot_v2, phi::OneHotV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(p_recv, phi::PRecvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(p_recv_array, phi::PRecvArrayOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pool2d, phi::Pool2dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pool3d, phi::Pool3dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(quant_linear, phi::QuantLinearOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(randperm, phi::RandpermOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(reduce, phi::ReduceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(reduce_scatter, phi::ReduceScatterOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rnn, phi::RnnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(shadow_output, phi::ShadowOutputOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(share_buffer, phi::ShareBufferOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softmax, phi::SoftmaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(swish, phi::SwishOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tril_indices, phi::TrilIndicesOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tril_triu, phi::TrilTriuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(triu_indices, phi::TriuIndicesOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(truncated_gaussian_random,
                           phi::TruncatedGaussianRandomOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unpool, phi::UnpoolOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(reduce_amax_grad, amax_grad);
PD_REGISTER_ARG_MAPPING_FN(reduce_amax_grad,
                           phi::ReduceAmaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(reduce_amin_grad, amin_grad);
PD_REGISTER_ARG_MAPPING_FN(reduce_amin_grad,
                           phi::ReduceAminGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(conv2d_transpose_grad_grad,
                             conv2d_transpose_double_grad);
PD_REGISTER_ARG_MAPPING_FN(conv2d_transpose_grad_grad,
                           phi::Conv2dTransposeGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_transpose_grad,
                           phi::Conv2dTransposeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(deformable_conv_grad,
                           phi::DeformableConvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d_transpose_grad,
                           phi::DepthwiseConv2dTransposeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(einsum_grad, phi::EinsumGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(elementwise_pow_grad,
                           phi::ElementwisePowGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(exponential_grad, exponential__grad);
PD_REGISTER_ARG_MAPPING_FN(frobenius_norm_grad,
                           phi::FrobeniusNormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_swish_grad, hardswish_grad);
PD_REGISTER_ARG_MAPPING_FN(hard_swish_grad,
                           phi::HardSwishGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(matmul_v2_grad_grad, matmul_double_grad);
PD_REGISTER_ARG_MAPPING_FN(matmul_v2_grad_grad,
                           phi::MatmulV2GradGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(matmul_v2_grad, matmul_grad);
PD_REGISTER_ARG_MAPPING_FN(matmul_v2_grad, phi::MatmulV2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(matmul_v2_triple_grad, matmul_triple_grad);
PD_REGISTER_ARG_MAPPING_FN(matmul_v2_triple_grad,
                           phi::MatmulV2TripleGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(reduce_max_grad, max_grad);
PD_REGISTER_ARG_MAPPING_FN(reduce_max_grad,
                           phi::ReduceMaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_max_grad, maximum_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_max_grad,
                           phi::ElementwiseMaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(reduce_min_grad, min_grad);
PD_REGISTER_ARG_MAPPING_FN(reduce_min_grad,
                           phi::ReduceMinGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_min_grad, minimum_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_min_grad,
                           phi::ElementwiseMinGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(norm_grad, phi::NormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pool2d_double_grad,
                           phi::Pool2dDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pool2d_grad, phi::Pool2dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pool3d_grad, phi::Pool3dGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(reduce_prod_grad, prod_grad);
PD_REGISTER_ARG_MAPPING_FN(reduce_prod_grad,
                           phi::ReduceProdGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rnn_grad, phi::RnnGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softmax_grad, phi::SoftmaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(reduce_sum_grad, sum_grad);
PD_REGISTER_ARG_MAPPING_FN(reduce_sum_grad,
                           phi::ReduceSumGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(swish_grad, phi::SwishGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tril_triu_grad, phi::TrilTriuGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unpool_grad, phi::UnpoolGradOpArgumentMapping);
