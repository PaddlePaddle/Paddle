// Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// this file is generated by paddle/phi/op/yaml/generator/generate_op.py, do not
// edit.
#include "paddle/phi/core/compat/op_utils.h"
#include "paddle/utils/small_vector.h"

namespace phi {

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AbsOpArgumentMapping:

return KernelSignature("abs", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AbsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "abs", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AccuracyOpArgumentMapping:

return KernelSignature("accuracy", {"Out", "Indices", "Label"}, {}, {"Accuracy",
"Correct", "Total"});
******************************************************************
*/

KernelSignature AccuracyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Indices", "Label"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Accuracy", "Correct", "Total"};
  return KernelSignature(
      "accuracy", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcosOpArgumentMapping:

return KernelSignature("acos", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AcosOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "acos", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcoshOpArgumentMapping:

return KernelSignature("acosh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AcoshOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "acosh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AdagradOpArgumentMapping:

return KernelSignature("adagrad", {"Param", "Grad", "Moment", "LearningRate",
"MasterParam"}, {"epsilon", "multi_precision"}, {"ParamOut", "MomentOut",
"MasterParamOut"}); return KernelSignature("adagrad_dense_param_sparse_grad",
{"Param", "Grad", "Moment", "LearningRate", "MasterParam"}, {"epsilon",
"multi_precision"}, {"ParamOut", "MomentOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature AdagradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Param", "Grad", "Moment", "LearningRate", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs{
      "ParamOut", "MomentOut", "MasterParamOut"};
  if (ctx.IsDenseTensorInput("Param") && ctx.IsDenseTensorInput("Grad") &&
      ctx.IsDenseTensorInput("Moment") &&
      ctx.IsDenseTensorInput("LearningRate") &&
      ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) ||
       (!ctx.HasInput("MasterParam")))) {
    return KernelSignature(
        "adagrad", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsDenseTensorInput("Param") &&
             ctx.IsSelectedRowsInput("Grad") &&
             ctx.IsDenseTensorInput("Moment") &&
             ctx.IsDenseTensorInput("LearningRate") &&
             ((ctx.HasInput("MasterParam") &&
               ctx.IsDenseTensorInput("MasterParam")) ||
              (!ctx.HasInput("MasterParam")))) {
    return KernelSignature("adagrad_dense_param_sparse_grad",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AdamaxOpArgumentMapping:

return KernelSignature("adamax", {"Param", "Grad", "LearningRate", "Moment",
"InfNorm", "Beta1Pow", "MasterParam"}, {"beta1", "beta2", "epsilon",
"multi_precision"}, {"ParamOut", "MomentOut", "InfNormOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature AdamaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Param",
                                           "Grad",
                                           "LearningRate",
                                           "Moment",
                                           "InfNorm",
                                           "Beta1Pow",
                                           "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta1");
  attrs.emplace_back("beta2");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs{
      "ParamOut", "MomentOut", "InfNormOut", "MasterParamOut"};
  return KernelSignature(
      "adamax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AdamwOpArgumentMapping:

return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1",
"Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"beta1",
"beta2", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode",
"min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"},
{"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut",
"MasterParamOut"}); return KernelSignature("adamw", {"Param", "Grad",
"LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam",
"SkipUpdate"}, {"beta1", "beta2", "EpsilonTensor", "lr_ratio", "coeff",
"with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("adamw", {"Param",
"Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam", "SkipUpdate"}, {"beta1", "Beta2Tensor", "epsilon", "lr_ratio",
"coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread",
"multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out",
"Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"}); return
KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2",
"Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"beta1", "Beta2Tensor",
"EpsilonTensor", "lr_ratio", "coeff", "with_decay", "lazy_mode",
"min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"},
{"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut",
"MasterParamOut"}); return KernelSignature("adamw", {"Param", "Grad",
"LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam",
"SkipUpdate"}, {"Beta1Tensor", "beta2", "epsilon", "lr_ratio", "coeff",
"with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("adamw", {"Param",
"Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam", "SkipUpdate"}, {"Beta1Tensor", "beta2", "EpsilonTensor",
"lr_ratio", "coeff", "with_decay", "lazy_mode",
"min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"},
{"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut",
"MasterParamOut"}); return KernelSignature("adamw", {"Param", "Grad",
"LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam",
"SkipUpdate"}, {"Beta1Tensor", "Beta2Tensor", "epsilon", "lr_ratio", "coeff",
"with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("adamw", {"Param",
"Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam", "SkipUpdate"}, {"Beta1Tensor", "Beta2Tensor", "EpsilonTensor",
"lr_ratio", "coeff", "with_decay", "lazy_mode",
"min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"},
{"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut",
"MasterParamOut"});
******************************************************************
*/

KernelSignature AdamwOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Param",
                                           "Grad",
                                           "LearningRate",
                                           "Moment1",
                                           "Moment2",
                                           "Beta1Pow",
                                           "Beta2Pow",
                                           "MasterParam",
                                           "SkipUpdate"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Beta1Tensor") ? "Beta1Tensor" : "beta1");
  attrs.emplace_back(ctx.HasInput("Beta2Tensor") ? "Beta2Tensor" : "beta2");
  attrs.emplace_back(ctx.HasInput("EpsilonTensor") ? "EpsilonTensor"
                                                   : "epsilon");
  attrs.emplace_back("lr_ratio");
  attrs.emplace_back("coeff");
  attrs.emplace_back("with_decay");
  attrs.emplace_back("lazy_mode");
  attrs.emplace_back("min_row_size_to_use_multithread");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("use_global_beta_pow");
  paddle::small_vector<const char*> outputs{"ParamOut",
                                            "Moment1Out",
                                            "Moment2Out",
                                            "Beta1PowOut",
                                            "Beta2PowOut",
                                            "MasterParamOut"};
  return KernelSignature(
      "adamw", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddmmOpArgumentMapping:

return KernelSignature("addmm", {"Input", "X", "Y"}, {"Beta", "Alpha"},
{"Out"});
******************************************************************
*/

KernelSignature AddmmOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Beta");
  attrs.emplace_back("Alpha");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "addmm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AffineGridOpArgumentMapping:

return KernelSignature("affine_grid", {"Theta"}, {"output_shape",
"align_corners"}, {"Output"}); return KernelSignature("affine_grid", {"Theta"},
{"OutputShape", "align_corners"}, {"Output"});
******************************************************************
*/

KernelSignature AffineGridOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Theta"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("OutputShape") ? "OutputShape"
                                                 : "output_shape");

  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature(
      "affine_grid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AllcloseOpArgumentMapping:

return KernelSignature("allclose", {"Input", "Other"}, {"rtol", "atol",
"equal_nan"}, {"Out"}); return KernelSignature("allclose", {"Input", "Other"},
{"rtol", "Atol", "equal_nan"}, {"Out"}); return KernelSignature("allclose",
{"Input", "Other"}, {"Rtol", "atol", "equal_nan"}, {"Out"}); return
KernelSignature("allclose", {"Input", "Other"}, {"Rtol", "Atol", "equal_nan"},
{"Out"});
******************************************************************
*/

KernelSignature AllcloseOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Other"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Rtol") ? "Rtol" : "rtol");
  attrs.emplace_back(ctx.HasInput("Atol") ? "Atol" : "atol");
  attrs.emplace_back("equal_nan");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "allclose", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AngleOpArgumentMapping:

return KernelSignature("angle", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AngleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "angle", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgmaxOpArgumentMapping:

return KernelSignature("argmax", {"X"}, {"axis", "keepdims", "flatten",
"dtype"}, {"Out"});
******************************************************************
*/

KernelSignature ArgMaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdims");
  attrs.emplace_back("flatten");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "argmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgminOpArgumentMapping:

return KernelSignature("argmin", {"X"}, {"axis", "keepdims", "flatten",
"dtype"}, {"Out"});
******************************************************************
*/

KernelSignature ArgMinOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdims");
  attrs.emplace_back("flatten");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "argmin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgsortOpArgumentMapping:

return KernelSignature("argsort", {"X"}, {"axis", "descending"}, {"Out",
"Indices"});
******************************************************************
*/

KernelSignature ArgsortOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("descending");
  paddle::small_vector<const char*> outputs{"Out", "Indices"};
  return KernelSignature(
      "argsort", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsComplexOpArgumentMapping:

return KernelSignature("as_complex", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsComplexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "as_complex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsRealOpArgumentMapping:

return KernelSignature("as_real", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsRealOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "as_real", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsStridedOpArgumentMapping:

return KernelSignature("as_strided", {"input"}, {"dims", "stride", "offset"},
{"out"});
******************************************************************
*/

KernelSignature AsStridedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  attrs.emplace_back("stride");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "as_strided", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinOpArgumentMapping:

return KernelSignature("asin", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsinOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "asin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinhOpArgumentMapping:

return KernelSignature("asinh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsinhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "asinh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanOpArgumentMapping:

return KernelSignature("atan", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AtanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "atan", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Atan2OpArgumentMapping:

return KernelSignature("atan2", {"X1", "X2"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Atan2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X1", "X2"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "atan2", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanhOpArgumentMapping:

return KernelSignature("atanh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AtanhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "atanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AucOpArgumentMapping:

return KernelSignature("auc", {"Predict", "Label", "StatPos", "StatNeg",
"InsTagWeight"}, {"curve", "num_thresholds", "slide_steps"}, {"AUC",
"StatPosOut", "StatNegOut"});
******************************************************************
*/

KernelSignature AucOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Predict", "Label", "StatPos", "StatNeg", "InsTagWeight"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("curve");
  attrs.emplace_back("num_thresholds");
  attrs.emplace_back("slide_steps");
  paddle::small_vector<const char*> outputs{"AUC", "StatPosOut", "StatNegOut"};
  return KernelSignature(
      "auc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AverageAccumulatesOpArgumentMapping:

return KernelSignature("average_accumulates", {"param", "in_sum_1", "in_sum_2",
"in_sum_3", "in_num_accumulates", "in_old_num_accumulates", "in_num_updates"},
{"average_window", "max_average_window", "min_average_window"}, {"out_sum_1",
"out_sum_2", "out_sum_3", "out_num_accumulates", "out_old_num_accumulates",
"out_num_updates"});
******************************************************************
*/

KernelSignature AverageAccumulatesOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"param",
                                           "in_sum_1",
                                           "in_sum_2",
                                           "in_sum_3",
                                           "in_num_accumulates",
                                           "in_old_num_accumulates",
                                           "in_num_updates"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("average_window");
  attrs.emplace_back("max_average_window");
  attrs.emplace_back("min_average_window");
  paddle::small_vector<const char*> outputs{"out_sum_1",
                                            "out_sum_2",
                                            "out_sum_3",
                                            "out_num_accumulates",
                                            "out_old_num_accumulates",
                                            "out_num_updates"};
  return KernelSignature("average_accumulates",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BceLossOpArgumentMapping:

return KernelSignature("bce_loss", {"X", "Label"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BceLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Label"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bce_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BernoulliOpArgumentMapping:

return KernelSignature("bernoulli", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BernoulliOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bernoulli", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BicubicInterpOpArgumentMapping:

return KernelSignature("bicubic_interp", {"X", "OutSize", "SizeTensor",
"Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method",
"align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature BicubicInterpV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("bicubic_interp",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearOpArgumentMapping:

return KernelSignature("bilinear", {"X", "Y", "Weight", "Bias"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BilinearTensorProductOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Weight", "Bias"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bilinear", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearInterpOpArgumentMapping:

return KernelSignature("bilinear_interp", {"X", "OutSize", "SizeTensor",
"Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method",
"align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature BilinearInterpV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("bilinear_interp",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BincountOpArgumentMapping:

return KernelSignature("bincount", {"X", "Weights"}, {"minlength"}, {"Out"});
******************************************************************
*/

KernelSignature BincountOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Weights"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("minlength");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bincount", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseAndOpArgumentMapping:

return KernelSignature("bitwise_and", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseAndOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bitwise_and", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseNotOpArgumentMapping:

return KernelSignature("bitwise_not", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseNotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bitwise_not", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseOrOpArgumentMapping:

return KernelSignature("bitwise_or", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseOrOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bitwise_or", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseXorOpArgumentMapping:

return KernelSignature("bitwise_xor", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseXorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bitwise_xor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BmmOpArgumentMapping:

return KernelSignature("bmm", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BmmOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "bmm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BoxCoderOpArgumentMapping:

return KernelSignature("box_coder", {"PriorBox", "PriorBoxVar", "TargetBox"},
{"code_type", "box_normalized", "axis", "variance"}, {"OutputBox"});
******************************************************************
*/

KernelSignature BoxCoderOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "PriorBox", "PriorBoxVar", "TargetBox"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("code_type");
  attrs.emplace_back("box_normalized");
  attrs.emplace_back("axis");
  attrs.emplace_back("variance");
  paddle::small_vector<const char*> outputs{"OutputBox"};
  return KernelSignature(
      "box_coder", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BroadcastTensorsOpArgumentMapping:

return KernelSignature("broadcast_tensors", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BroadcastTensorsOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("broadcast_tensors",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeilOpArgumentMapping:

return KernelSignature("ceil", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature CeilOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "ceil", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeluOpArgumentMapping:

return KernelSignature("celu", {"X"}, {"alpha"}, {"Out"});
******************************************************************
*/

KernelSignature CeluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "celu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
CheckFiniteAndUnscaleOpArgumentMapping:

return KernelSignature("check_finite_and_unscale", {"X", "Scale"}, {}, {"Out",
"FoundInfinite"});
******************************************************************
*/

KernelSignature CheckFiniteAndUnscaleOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Scale"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out", "FoundInfinite"};
  return KernelSignature("check_finite_and_unscale",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CheckNumericsOpArgumentMapping:

return KernelSignature("check_numerics", {"tensor"}, {"op_type", "var_name",
"check_nan_inf_level", "stack_height_limit", "output_dir"}, {"stats",
"values"});
******************************************************************
*/

KernelSignature CheckNumericsOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"tensor"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("op_type");
  attrs.emplace_back("var_name");
  attrs.emplace_back("check_nan_inf_level");
  attrs.emplace_back("stack_height_limit");
  attrs.emplace_back("output_dir");
  paddle::small_vector<const char*> outputs{"stats", "values"};
  return KernelSignature("check_numerics",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskyOpArgumentMapping:

return KernelSignature("cholesky", {"X"}, {"upper"}, {"Out"});
******************************************************************
*/

KernelSignature CholeskyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "cholesky", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskySolveOpArgumentMapping:

return KernelSignature("cholesky_solve", {"X", "Y"}, {"upper"}, {"Out"});
******************************************************************
*/

KernelSignature CholeskySolveOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("cholesky_solve",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClassCenterSampleOpArgumentMapping:

return KernelSignature("class_center_sample", {"Label"}, {"num_classes",
"num_samples", "ring_id", "rank", "nranks", "fix_seed", "seed"},
{"RemappedLabel", "SampledLocalClassCenter"});
******************************************************************
*/

KernelSignature ClassCenterSampleOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Label"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("num_classes");
  attrs.emplace_back("num_samples");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("rank");
  attrs.emplace_back("nranks");
  attrs.emplace_back("fix_seed");
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs{"RemappedLabel",
                                            "SampledLocalClassCenter"};
  return KernelSignature("class_center_sample",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipOpArgumentMapping:

return KernelSignature("clip", {"X"}, {"min", "max"}, {"Out"});
******************************************************************
*/

KernelSignature ClipOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Min") ? "Min" : "min");
  attrs.emplace_back(ctx.HasInput("Max") ? "Max" : "max");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "clip", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipByNormOpArgumentMapping:

return KernelSignature("clip_by_norm", {"X"}, {"max_norm"}, {"Out"});
return KernelSignature("clip_by_norm_sr", {"X"}, {"max_norm"}, {"Out"});
******************************************************************
*/

KernelSignature ClipByNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_norm");
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature("clip_by_norm",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature("clip_by_norm_sr",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CoalesceTensorOpArgumentMapping:

return KernelSignature("coalesce_tensor", {"Input"}, {"dtype", "copy_data",
"set_constant", "persist_output", "constant", "use_align", "align_size",
"user_defined_size_of_dtype", "concated_shapes", "concated_ranks"}, {"Output",
"FusedOutput"});
******************************************************************
*/

KernelSignature CoalesceTensorOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  attrs.emplace_back("copy_data");
  attrs.emplace_back("set_constant");
  attrs.emplace_back("persist_output");
  attrs.emplace_back("constant");
  attrs.emplace_back("use_align");
  attrs.emplace_back("align_size");
  attrs.emplace_back("user_defined_size_of_dtype");
  attrs.emplace_back("concated_shapes");
  attrs.emplace_back("concated_ranks");
  paddle::small_vector<const char*> outputs{"Output", "FusedOutput"};
  return KernelSignature("coalesce_tensor",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ComplexOpArgumentMapping:

return KernelSignature("complex", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ComplexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "complex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ConcatOpArgumentMapping:

return KernelSignature("concat", {"X"}, {"axis"}, {"Out"});
return KernelSignature("concat", {"X"}, {"AxisTensor"}, {"Out"});
******************************************************************
*/

KernelSignature ConcatOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor") ? "AxisTensor" : "axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "concat", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ConjOpArgumentMapping:

return KernelSignature("conj", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ConjOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "conj", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dOpArgumentMapping:

return KernelSignature("conv2d", {"Input", "Filter"}, {"strides", "paddings",
"padding_algorithm", "dilations", "groups", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv2dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature(
      "conv2d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dOpArgumentMapping:

return KernelSignature("conv3d", {"Input", "Filter"}, {"strides", "paddings",
"padding_algorithm", "groups", "dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature(
      "conv3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dTransposeOpArgumentMapping:

return KernelSignature("conv3d_transpose", {"Input", "Filter"}, {"strides",
"paddings", "output_padding", "output_size", "padding_algorithm", "groups",
"dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv3dTransposeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature("conv3d_transpose",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosOpArgumentMapping:

return KernelSignature("cos", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature CosOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "cos", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CoshOpArgumentMapping:

return KernelSignature("cosh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature CoshOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "cosh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CropOpArgumentMapping:

return KernelSignature("crop", {"X"}, {"shape", "offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"shape", "Offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"shape", "OffsetsTensor"}, {"Out"});
return KernelSignature("crop", {"X"}, {"Shape", "offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"Shape", "Offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"Shape", "OffsetsTensor"}, {"Out"});
return KernelSignature("crop", {"X"}, {"ShapeTensor", "offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"ShapeTensor", "Offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"ShapeTensor", "OffsetsTensor"},
{"Out"});
******************************************************************
*/

KernelSignature CropTensorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Shape")              ? "Shape"
                     : ctx.InputSize("ShapeTensor") > 0 ? "ShapeTensor"
                                                        : "shape");
  attrs.emplace_back(ctx.HasInput("Offsets")              ? "Offsets"
                     : ctx.InputSize("OffsetsTensor") > 0 ? "OffsetsTensor"
                                                          : "offsets");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "crop", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossOpArgumentMapping:

return KernelSignature("cross", {"X", "Y"}, {"dim"}, {"Out"});
******************************************************************
*/

KernelSignature CrossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "cross", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
CrossEntropyWithSoftmaxOpArgumentMapping:

return KernelSignature("cross_entropy_with_softmax", {"Logits", "Label"},
{"soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"},
{"Softmax", "Loss"});
******************************************************************
*/

KernelSignature SoftmaxWithCrossEntropyOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Logits", "Label"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("soft_label");
  attrs.emplace_back("use_softmax");
  attrs.emplace_back("numeric_stable_mode");
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Softmax", "Loss"};
  return KernelSignature("cross_entropy_with_softmax",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CummaxOpArgumentMapping:

return KernelSignature("cummax", {"x"}, {"axis", "dtype"}, {"out", "indices"});
******************************************************************
*/

KernelSignature CummaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"out", "indices"};
  return KernelSignature(
      "cummax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumminOpArgumentMapping:

return KernelSignature("cummin", {"x"}, {"axis", "dtype"}, {"out", "indices"});
******************************************************************
*/

KernelSignature CumminOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"out", "indices"};
  return KernelSignature(
      "cummin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumprodOpArgumentMapping:

return KernelSignature("cumprod", {"X"}, {"dim"}, {"Out"});
******************************************************************
*/

KernelSignature CumprodOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "cumprod", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumsumOpArgumentMapping:

return KernelSignature("cumsum", {"X"}, {"axis", "flatten", "exclusive",
"reverse"}, {"Out"}); return KernelSignature("cumsum", {"X"}, {"AxisTensor",
"flatten", "exclusive", "reverse"}, {"Out"});
******************************************************************
*/

KernelSignature CumsumOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "cumsum", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DataOpArgumentMapping:

return KernelSignature("data", {}, {"name", "shape", "dtype", "place"},
{"out"}); return KernelSignature("data", {}, {"name", "ShapeTensor", "dtype",
"place"}, {"out"}); return KernelSignature("data", {}, {"name",
"ShapeTensorList", "dtype", "place"}, {"out"});
******************************************************************
*/

KernelSignature DataOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("name");
  attrs.emplace_back(ctx.HasInput("ShapeTensor")            ? "ShapeTensor"
                     : ctx.InputSize("ShapeTensorList") > 0 ? "ShapeTensorList"
                                                            : "shape");
  attrs.emplace_back("dtype");

  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "data", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DepthwiseConv2dOpArgumentMapping:

return KernelSignature("depthwise_conv2d", {"Input", "Filter"}, {"strides",
"paddings", "padding_algorithm", "groups", "dilations", "data_format"},
{"Output"});
******************************************************************
*/

KernelSignature DepthwiseConv2dOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature("depthwise_conv2d",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DetOpArgumentMapping:

return KernelSignature("determinant", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DeterminantOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "determinant", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagOpArgumentMapping:

return KernelSignature("diag", {"X"}, {"offset", "padding_value"}, {"Out"});
******************************************************************
*/

KernelSignature DiagV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("padding_value");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "diag", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagEmbedOpArgumentMapping:

return KernelSignature("diag_embed", {"Input"}, {"offset", "dim1", "dim2"},
{"Out"});
******************************************************************
*/

KernelSignature DiagEmbedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("dim1");
  attrs.emplace_back("dim2");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "diag_embed", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagonalOpArgumentMapping:

return KernelSignature("diagonal", {"Input"}, {"offset", "axis1", "axis2"},
{"Out"});
******************************************************************
*/

KernelSignature DiagonalOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "diagonal", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DigammaOpArgumentMapping:

return KernelSignature("digamma", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DigammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "digamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DirichletOpArgumentMapping:

return KernelSignature("dirichlet", {"Alpha"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DirichletOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Alpha"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "dirichlet", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DistOpArgumentMapping:

return KernelSignature("dist", {"X", "Y"}, {"p"}, {"Out"});
******************************************************************
*/

KernelSignature DistOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "dist", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DotOpArgumentMapping:

return KernelSignature("dot", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "dot", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EditDistanceOpArgumentMapping:

return KernelSignature("edit_distance", {"Hyps", "Refs", "HypsLength",
"RefsLength"}, {"normalized"}, {"SequenceNum", "Out"});
******************************************************************
*/

KernelSignature EditDistanceOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Hyps", "Refs", "HypsLength", "RefsLength"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("normalized");
  paddle::small_vector<const char*> outputs{"SequenceNum", "Out"};
  return KernelSignature(
      "edit_distance", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigOpArgumentMapping:

return KernelSignature("eig", {"X"}, {}, {"Eigenvalues", "Eigenvectors"});
******************************************************************
*/

KernelSignature EigOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Eigenvalues", "Eigenvectors"};
  return KernelSignature(
      "eig", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EighOpArgumentMapping:

return KernelSignature("eigh", {"X"}, {"UPLO"}, {"Eigenvalues",
"Eigenvectors"});
******************************************************************
*/

KernelSignature EighOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("UPLO");
  paddle::small_vector<const char*> outputs{"Eigenvalues", "Eigenvectors"};
  return KernelSignature(
      "eigh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigvalsOpArgumentMapping:

return KernelSignature("eigvals", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature EigvalsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "eigvals", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigvalshOpArgumentMapping:

return KernelSignature("eigvalsh", {"X"}, {"UPLO", "is_test"}, {"Eigenvalues",
"Eigenvectors"});
******************************************************************
*/

KernelSignature EigvalshOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("UPLO");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{"Eigenvalues", "Eigenvectors"};
  return KernelSignature(
      "eigvalsh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EluOpArgumentMapping:

return KernelSignature("elu", {"X"}, {"alpha"}, {"Out"});
******************************************************************
*/

KernelSignature EluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "elu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EqualAllOpArgumentMapping:

return KernelSignature("equal_all", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature EqualAllOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "equal_all", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfOpArgumentMapping:

return KernelSignature("erf", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ErfOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "erf", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfinvOpArgumentMapping:

return KernelSignature("erfinv", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ErfinvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "erfinv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpOpArgumentMapping:

return KernelSignature("exp", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ExpOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "exp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpandAsOpArgumentMapping:

return KernelSignature("expand_as", {"X", "Y"}, {"target_shape"}, {"Out"});
******************************************************************
*/

KernelSignature ExpandAsV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("target_shape");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "expand_as", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Expm1OpArgumentMapping:

return KernelSignature("expm1", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Expm1OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "expm1", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2cOpArgumentMapping:

return KernelSignature("fft_c2c", {"X"}, {"axes", "normalization", "forward"},
{"Out"});
******************************************************************
*/

KernelSignature FftC2cOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "fft_c2c", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2rOpArgumentMapping:

return KernelSignature("fft_c2r", {"X"}, {"axes", "normalization", "forward",
"last_dim_size"}, {"Out"});
******************************************************************
*/

KernelSignature FftC2rOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("last_dim_size");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "fft_c2r", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftR2cOpArgumentMapping:

return KernelSignature("fft_r2c", {"X"}, {"axes", "normalization", "forward",
"onesided"}, {"Out"});
******************************************************************
*/

KernelSignature FftR2cOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("onesided");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "fft_r2c", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillOpArgumentMapping:

return KernelSignature("fill", {"X"}, {"value"}, {"Out"});
return KernelSignature("fill", {"X"}, {"ValueTensor"}, {"Out"});
******************************************************************
*/

KernelSignature FillAnyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "fill", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalOpArgumentMapping:

return KernelSignature("fill_diagonal", {"X"}, {"value", "offset", "wrap"},
{"Out"});
******************************************************************
*/

KernelSignature FillDiagonalOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("offset");
  attrs.emplace_back("wrap");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "fill_diagonal", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalTensorOpArgumentMapping:

return KernelSignature("fill_diagonal_tensor", {"X", "Y"}, {"offset", "dim1",
"dim2"}, {"Out"});
******************************************************************
*/

KernelSignature FillDiagonalTensorOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("dim1");
  attrs.emplace_back("dim2");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("fill_diagonal_tensor",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnOpArgumentMapping:

return KernelSignature("flash_attn", {"q", "k", "v", "fixed_seed_offset",
"attn_mask"}, {"dropout", "causal", "return_softmax", "is_test", "rng_name"},
{"out", "softmax", "softmax_lse", "seed_offset"});
******************************************************************
*/

KernelSignature FlashAttnOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "q", "k", "v", "fixed_seed_offset", "attn_mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("is_test");
  attrs.emplace_back("rng_name");
  paddle::small_vector<const char*> outputs{
      "out", "softmax", "softmax_lse", "seed_offset"};
  return KernelSignature(
      "flash_attn", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnUnpaddedOpArgumentMapping:

return KernelSignature("flash_attn_unpadded", {"q", "k", "v", "cu_seqlens_q",
"cu_seqlens_k", "fixed_seed_offset", "attn_mask"}, {"max_seqlen_q",
"max_seqlen_k", "scale", "dropout", "causal", "return_softmax", "is_test",
"rng_name"}, {"out", "softmax", "softmax_lse", "seed_offset"});
******************************************************************
*/

KernelSignature FlashAttnUnpaddedOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"q",
                                           "k",
                                           "v",
                                           "cu_seqlens_q",
                                           "cu_seqlens_k",
                                           "fixed_seed_offset",
                                           "attn_mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_seqlen_q");
  attrs.emplace_back("max_seqlen_k");
  attrs.emplace_back("scale");
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("is_test");
  attrs.emplace_back("rng_name");
  paddle::small_vector<const char*> outputs{
      "out", "softmax", "softmax_lse", "seed_offset"};
  return KernelSignature("flash_attn_unpadded",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlipOpArgumentMapping:

return KernelSignature("flip", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature FlipOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "flip", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FloorOpArgumentMapping:

return KernelSignature("floor", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature FloorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "floor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FoldOpArgumentMapping:

return KernelSignature("fold", {"X"}, {"output_sizes", "kernel_sizes",
"strides", "paddings", "dilations"}, {"Y"});
******************************************************************
*/

KernelSignature FoldOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_sizes");
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs{"Y"};
  return KernelSignature(
      "fold", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FrameOpArgumentMapping:

return KernelSignature("frame", {"X"}, {"frame_length", "hop_length", "axis"},
{"Out"});
******************************************************************
*/

KernelSignature FrameOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("frame_length");
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "frame", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FullIntArrayOpArgumentMapping:

return KernelSignature("full_int_array", {}, {"value", "dtype", "place"},
{"out"});
******************************************************************
*/

KernelSignature FullIntArrayOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("dtype");

  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("full_int_array",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherOpArgumentMapping:

return KernelSignature("gather", {"X", "Index"}, {"axis"}, {"Out"});
return KernelSignature("gather", {"X", "Index"}, {"Axis"}, {"Out"});
******************************************************************
*/

KernelSignature GatherOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Axis") ? "Axis" : "axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "gather", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherNdOpArgumentMapping:

return KernelSignature("gather_nd", {"X", "Index"}, {}, {"Out"});
******************************************************************
*/

KernelSignature GatherNdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "gather_nd", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherTreeOpArgumentMapping:

return KernelSignature("gather_tree", {"Ids", "Parents"}, {}, {"Out"});
******************************************************************
*/

KernelSignature GatherTreeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Ids", "Parents"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "gather_tree", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GaussianInplaceOpArgumentMapping:

return KernelSignature("gaussian_inplace", {"x"}, {"mean", "std", "seed"},
{"out"});
******************************************************************
*/

KernelSignature GaussianInplaceOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mean");
  attrs.emplace_back("std");
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("gaussian_inplace",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GeluOpArgumentMapping:

return KernelSignature("gelu", {"X"}, {"approximate"}, {"Out"});
******************************************************************
*/

KernelSignature GeluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("approximate");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "gelu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GenerateProposalsOpArgumentMapping:

return KernelSignature("generate_proposals", {"Scores", "BboxDeltas", "ImShape",
"Anchors", "Variances"}, {"pre_nms_topN", "post_nms_topN", "nms_thresh",
"min_size", "eta", "pixel_offset"}, {"RpnRois", "RpnRoiProbs", "RpnRoisNum"});
******************************************************************
*/

KernelSignature GenerateProposalsV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Scores", "BboxDeltas", "ImShape", "Anchors", "Variances"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pre_nms_topN");
  attrs.emplace_back("post_nms_topN");
  attrs.emplace_back("nms_thresh");
  attrs.emplace_back("min_size");
  attrs.emplace_back("eta");
  attrs.emplace_back("pixel_offset");
  paddle::small_vector<const char*> outputs{
      "RpnRois", "RpnRoiProbs", "RpnRoisNum"};
  return KernelSignature("generate_proposals",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GridSampleOpArgumentMapping:

return KernelSignature("grid_sample", {"X", "Grid"}, {"mode", "padding_mode",
"align_corners"}, {"Output"});
******************************************************************
*/

KernelSignature GridSamplerOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Grid"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  attrs.emplace_back("padding_mode");
  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature(
      "grid_sample", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GroupNormOpArgumentMapping:

return KernelSignature("group_norm", {"X", "Scale", "Bias"}, {"epsilon",
"groups", "data_layout"}, {"Y", "Mean", "Variance"});
******************************************************************
*/

KernelSignature GroupNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_layout");
  paddle::small_vector<const char*> outputs{"Y", "Mean", "Variance"};
  return KernelSignature(
      "group_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GumbelSoftmaxOpArgumentMapping:

return KernelSignature("gumbel_softmax", {"X"}, {"temperature", "hard", "axis"},
{"Out"});
******************************************************************
*/

KernelSignature GumbelSoftmaxOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("temperature");
  attrs.emplace_back("hard");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("gumbel_softmax",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardshrinkOpArgumentMapping:

return KernelSignature("hard_shrink", {"X"}, {"threshold"}, {"Out"});
******************************************************************
*/

KernelSignature HardShrinkOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "hard_shrink", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardsigmoidOpArgumentMapping:

return KernelSignature("hardsigmoid", {"X"}, {"slope", "offset"}, {"Out"});
******************************************************************
*/

KernelSignature HardSigmoidOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("slope");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "hardsigmoid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardtanhOpArgumentMapping:

return KernelSignature("hardtanh", {"X"}, {"t_min", "t_max"}, {"Out"});
******************************************************************
*/

KernelSignature BreluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("t_min");
  attrs.emplace_back("t_max");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "hardtanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HeavisideOpArgumentMapping:

return KernelSignature("heaviside", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ElementwiseHeavisideOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "heaviside", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HistogramOpArgumentMapping:

return KernelSignature("histogram", {"X"}, {"bins", "min", "max"}, {"Out"});
******************************************************************
*/

KernelSignature HistogramOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("bins");
  attrs.emplace_back("min");
  attrs.emplace_back("max");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "histogram", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HuberLossOpArgumentMapping:

return KernelSignature("huber_loss", {"X", "Y"}, {"delta"}, {"Out",
"Residual"});
******************************************************************
*/

KernelSignature HuberLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("delta");
  paddle::small_vector<const char*> outputs{"Out", "Residual"};
  return KernelSignature(
      "huber_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0OpArgumentMapping:

return KernelSignature("i0", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I0OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "i0", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0eOpArgumentMapping:

return KernelSignature("i0e", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I0eOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "i0e", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1OpArgumentMapping:

return KernelSignature("i1", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I1OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "i1", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1eOpArgumentMapping:

return KernelSignature("i1e", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I1eOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "i1e", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IdentityLossOpArgumentMapping:

return KernelSignature("identity_loss", {"X"}, {"reduction"}, {"Out"});
******************************************************************
*/

KernelSignature IdentityLossOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "identity_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ImagOpArgumentMapping:

return KernelSignature("imag", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ImagOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "imag", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexAddOpArgumentMapping:

return KernelSignature("index_add", {"X", "Index", "AddValue"}, {"axis"},
{"Out"});
******************************************************************
*/

KernelSignature IndexAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index", "AddValue"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "index_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexPutOpArgumentMapping:

return KernelSignature("index_put", {"x", "indices", "value"}, {"accumulate"},
{"out"});
******************************************************************
*/

KernelSignature IndexPutOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "indices", "value"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("accumulate");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "index_put", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSampleOpArgumentMapping:

return KernelSignature("index_sample", {"X", "Index"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IndexSampleOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "index_sample", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectOpArgumentMapping:

return KernelSignature("index_select", {"X", "Index"}, {"dim"}, {"Out"});
******************************************************************
*/

KernelSignature IndexSelectOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "index_select", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectStridedOpArgumentMapping:

return KernelSignature("index_select_strided", {"x"}, {"index", "axis"},
{"out"});
******************************************************************
*/

KernelSignature IndexSelectStridedOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("index_select_strided",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InstanceNormOpArgumentMapping:

return KernelSignature("instance_norm", {"X", "Scale", "Bias"}, {"epsilon"},
{"Y", "SavedMean", "SavedVariance"});
******************************************************************
*/

KernelSignature InstanceNormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"Y", "SavedMean", "SavedVariance"};
  return KernelSignature(
      "instance_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InverseOpArgumentMapping:

return KernelSignature("inverse", {"Input"}, {}, {"Output"});
******************************************************************
*/

KernelSignature InverseOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Output"};
  return KernelSignature(
      "inverse", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsEmptyOpArgumentMapping:

return KernelSignature("is_empty", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsEmptyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "is_empty", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IscloseOpArgumentMapping:

return KernelSignature("isclose", {"Input", "Other"}, {"rtol", "atol",
"equal_nan"}, {"Out"});
******************************************************************
*/

KernelSignature IscloseOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Other"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Rtol") ? "Rtol" : "rtol");
  attrs.emplace_back(ctx.HasInput("Atol") ? "Atol" : "atol");
  attrs.emplace_back("equal_nan");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "isclose", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsfiniteOpArgumentMapping:

return KernelSignature("isfinite", {"X"}, {}, {"Out"});
return KernelSignature("isfinite_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsfiniteV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature(
        "isfinite", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature(
        "isfinite_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsinfOpArgumentMapping:

return KernelSignature("isinf", {"X"}, {}, {"Out"});
return KernelSignature("isinf_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsinfV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature(
        "isinf", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature(
        "isinf_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsnanOpArgumentMapping:

return KernelSignature("isnan", {"X"}, {}, {"Out"});
return KernelSignature("isnan_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsnanV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature(
        "isnan", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature(
        "isnan_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KldivLossOpArgumentMapping:

return KernelSignature("kldiv_loss", {"X", "Target"}, {"reduction"}, {"Loss"});
******************************************************************
*/

KernelSignature KldivLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Target"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs{"Loss"};
  return KernelSignature(
      "kldiv_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KronOpArgumentMapping:

return KernelSignature("kron", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature KronOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "kron", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KthvalueOpArgumentMapping:

return KernelSignature("kthvalue", {"X"}, {"k", "axis", "keepdim"}, {"Out",
"Indices"});
******************************************************************
*/

KernelSignature KthvalueOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("k");
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs{"Out", "Indices"};
  return KernelSignature(
      "kthvalue", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LabelSmoothOpArgumentMapping:

return KernelSignature("label_smooth", {"X", "PriorDist"}, {"epsilon"},
{"Out"});
******************************************************************
*/

KernelSignature LabelSmoothOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "PriorDist"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "label_smooth", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LambOpArgumentMapping:

return KernelSignature("lamb", {"Param", "Grad", "LearningRate", "Moment1",
"Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"},
{"weight_decay", "beta1", "beta2", "epsilon", "always_adapt",
"multi_precision"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("lamb_sr", {"Param",
"Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam", "SkipUpdate"}, {"weight_decay", "beta1", "beta2", "epsilon",
"always_adapt", "multi_precision"}, {"ParamOut", "Moment1Out", "Moment2Out",
"Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature LambOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Param",
                                           "Grad",
                                           "LearningRate",
                                           "Moment1",
                                           "Moment2",
                                           "Beta1Pow",
                                           "Beta2Pow",
                                           "MasterParam",
                                           "SkipUpdate"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("weight_decay");
  attrs.emplace_back("beta1");
  attrs.emplace_back("beta2");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("always_adapt");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs{"ParamOut",
                                            "Moment1Out",
                                            "Moment2Out",
                                            "Beta1PowOut",
                                            "Beta2PowOut",
                                            "MasterParamOut"};
  if (ctx.IsDenseTensorInput("Param") && ctx.IsDenseTensorInput("Grad") &&
      ctx.IsDenseTensorInput("LearningRate") &&
      ctx.IsDenseTensorInput("Moment1") && ctx.IsDenseTensorInput("Moment2") &&
      ctx.IsDenseTensorInput("Beta1Pow") &&
      ctx.IsDenseTensorInput("Beta2Pow") &&
      ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) ||
       (!ctx.HasInput("MasterParam"))) &&
      ((ctx.HasInput("SkipUpdate") && ctx.IsDenseTensorInput("SkipUpdate")) ||
       (!ctx.HasInput("SkipUpdate")))) {
    return KernelSignature(
        "lamb", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsDenseTensorInput("Param") &&
             ctx.IsSelectedRowsInput("Grad") &&
             ctx.IsDenseTensorInput("LearningRate") &&
             ctx.IsDenseTensorInput("Moment1") &&
             ctx.IsDenseTensorInput("Moment2") &&
             ctx.IsDenseTensorInput("Beta1Pow") &&
             ctx.IsDenseTensorInput("Beta2Pow") &&
             ((ctx.HasInput("MasterParam") &&
               ctx.IsDenseTensorInput("MasterParam")) ||
              (!ctx.HasInput("MasterParam"))) &&
             ((ctx.HasInput("SkipUpdate") &&
               ctx.IsDenseTensorInput("SkipUpdate")) ||
              (!ctx.HasInput("SkipUpdate")))) {
    return KernelSignature(
        "lamb_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LayerNormOpArgumentMapping:

return KernelSignature("layer_norm", {"X", "Scale", "Bias"}, {"epsilon",
"begin_norm_axis"}, {"Y", "Mean", "Variance"});
******************************************************************
*/

KernelSignature LayerNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs{"Y", "Mean", "Variance"};
  return KernelSignature(
      "layer_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LeakyReluOpArgumentMapping:

return KernelSignature("leaky_relu", {"X"}, {"alpha"}, {"Out"});
******************************************************************
*/

KernelSignature LeakyReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "leaky_relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LerpOpArgumentMapping:

return KernelSignature("lerp", {"X", "Y", "Weight"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LerpOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Weight"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "lerp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LgammaOpArgumentMapping:

return KernelSignature("lgamma", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LgammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "lgamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LinearInterpOpArgumentMapping:

return KernelSignature("linear_interp", {"X", "OutSize", "SizeTensor", "Scale"},
{"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method",
"align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature LinearInterpV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "linear_interp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LlmInt8LinearOpArgumentMapping:

return KernelSignature("llm_int8_linear", {"x", "weight", "bias",
"weight_scale"}, {"threshold"}, {"out"});
******************************************************************
*/

KernelSignature LlmInt8LinearOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "weight", "bias", "weight_scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("llm_int8_linear",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogOpArgumentMapping:

return KernelSignature("log", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "log", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log10OpArgumentMapping:

return KernelSignature("log10", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Log10OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "log10", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log1pOpArgumentMapping:

return KernelSignature("log1p", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Log1pOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "log1p", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log2OpArgumentMapping:

return KernelSignature("log2", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Log2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "log2", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogLossOpArgumentMapping:

return KernelSignature("log_loss", {"Predicted", "Labels"}, {"epsilon"},
{"Loss"});
******************************************************************
*/

KernelSignature LogLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Predicted", "Labels"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"Loss"};
  return KernelSignature(
      "log_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogSoftmaxOpArgumentMapping:

return KernelSignature("log_softmax", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature LogSoftmaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "log_softmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogcumsumexpOpArgumentMapping:

return KernelSignature("logcumsumexp", {"X"}, {"axis", "flatten", "exclusive",
"reverse"}, {"Out"});
******************************************************************
*/

KernelSignature LogcumsumexpOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logcumsumexp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalAndOpArgumentMapping:

return KernelSignature("logical_and", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalAndOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logical_and", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalNotOpArgumentMapping:

return KernelSignature("logical_not", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalNotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logical_not", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalOrOpArgumentMapping:

return KernelSignature("logical_or", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalOrOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logical_or", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalXorOpArgumentMapping:

return KernelSignature("logical_xor", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalXorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logical_xor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogitOpArgumentMapping:

return KernelSignature("logit", {"X"}, {"eps"}, {"Out"});
******************************************************************
*/

KernelSignature LogitOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logit", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogsigmoidOpArgumentMapping:

return KernelSignature("logsigmoid", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogsigmoidOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "logsigmoid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LstsqOpArgumentMapping:

return KernelSignature("lstsq", {"X", "Y"}, {"rcond", "driver"}, {"Solution",
"Residuals", "Rank", "SingularValues"}); return KernelSignature("lstsq", {"X",
"Y"}, {"RcondTensor", "driver"}, {"Solution", "Residuals", "Rank",
"SingularValues"});
******************************************************************
*/

KernelSignature LstsqOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("rcond");
  attrs.emplace_back("driver");
  paddle::small_vector<const char*> outputs{
      "Solution", "Residuals", "Rank", "SingularValues"};
  return KernelSignature(
      "lstsq", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuOpArgumentMapping:

return KernelSignature("lu", {"X"}, {"pivots"}, {"Out", "Pivots", "Infos"});
******************************************************************
*/

KernelSignature LuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pivots");
  paddle::small_vector<const char*> outputs{"Out", "Pivots", "Infos"};
  return KernelSignature(
      "lu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuUnpackOpArgumentMapping:

return KernelSignature("lu_unpack", {"X", "Pivots"}, {"unpack_ludata",
"unpack_pivots"}, {"Pmat", "L", "U"});
******************************************************************
*/

KernelSignature LuUnpackOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Pivots"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("unpack_ludata");
  attrs.emplace_back("unpack_pivots");
  paddle::small_vector<const char*> outputs{"Pmat", "L", "U"};
  return KernelSignature(
      "lu_unpack", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MarginCrossEntropyOpArgumentMapping:

return KernelSignature("margin_cross_entropy", {"Logits", "Label"},
{"return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3",
"scale"}, {"Softmax", "Loss"});
******************************************************************
*/

KernelSignature MarginCrossEntropyOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Logits", "Label"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("rank");
  attrs.emplace_back("nranks");
  attrs.emplace_back("margin1");
  attrs.emplace_back("margin2");
  attrs.emplace_back("margin3");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs{"Softmax", "Loss"};
  return KernelSignature("margin_cross_entropy",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
MaskedMultiheadAttentionOpArgumentMapping:

return KernelSignature("masked_multihead_attention", {"x", "cache_kv", "bias",
"src_mask", "cum_offsets", "sequence_lengths", "rotary_tensor",
"beam_cache_offset", "qkv_out_scale", "out_shift", "out_smooth"}, {"seq_len",
"rotary_emb_dims", "use_neox_rotary_style", "compute_dtype", "out_scale",
"quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out",
"cache_kv_out", "beam_cache_offset_out"});
******************************************************************
*/

KernelSignature MaskedMultiheadAttentionOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "cache_kv",
                                           "bias",
                                           "src_mask",
                                           "cum_offsets",
                                           "sequence_lengths",
                                           "rotary_tensor",
                                           "beam_cache_offset",
                                           "qkv_out_scale",
                                           "out_shift",
                                           "out_smooth"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seq_len");
  attrs.emplace_back("rotary_emb_dims");
  attrs.emplace_back("use_neox_rotary_style");
  attrs.emplace_back("compute_dtype");
  attrs.emplace_back("out_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs{
      "out", "cache_kv_out", "beam_cache_offset_out"};
  return KernelSignature("masked_multihead_attention",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaskedSelectOpArgumentMapping:

return KernelSignature("masked_select", {"X", "Mask"}, {}, {"Y"});
******************************************************************
*/

KernelSignature MaskedSelectOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Mask"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Y"};
  return KernelSignature(
      "masked_select", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatrixNmsOpArgumentMapping:

return KernelSignature("matrix_nms", {"BBoxes", "Scores"}, {"score_threshold",
"nms_top_k", "keep_top_k", "post_threshold", "use_gaussian", "gaussian_sigma",
"background_label", "normalized"}, {"Out", "Index", "RoisNum"});
******************************************************************
*/

KernelSignature MatrixNmsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"BBoxes", "Scores"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("score_threshold");
  attrs.emplace_back("nms_top_k");
  attrs.emplace_back("keep_top_k");
  attrs.emplace_back("post_threshold");
  attrs.emplace_back("use_gaussian");
  attrs.emplace_back("gaussian_sigma");
  attrs.emplace_back("background_label");
  attrs.emplace_back("normalized");
  paddle::small_vector<const char*> outputs{"Out", "Index", "RoisNum"};
  return KernelSignature(
      "matrix_nms", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatrixPowerOpArgumentMapping:

return KernelSignature("matrix_power", {"X"}, {"n"}, {"Out"});
******************************************************************
*/

KernelSignature MatrixPowerOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "matrix_power", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool2dWithIndexOpArgumentMapping:

return KernelSignature("max_pool2d_with_index", {"X"}, {"ksize", "strides",
"paddings", "global_pooling", "adaptive"}, {"Out", "Mask"});
******************************************************************
*/

KernelSignature MaxPool2dWithIndexOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs{"Out", "Mask"};
  return KernelSignature("max_pool2d_with_index",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool3dWithIndexOpArgumentMapping:

return KernelSignature("max_pool3d_with_index", {"X"}, {"ksize", "strides",
"paddings", "global_pooling", "adaptive"}, {"Out", "Mask"});
******************************************************************
*/

KernelSignature MaxPool3dWithIndexOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs{"Out", "Mask"};
  return KernelSignature("max_pool3d_with_index",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxoutOpArgumentMapping:

return KernelSignature("maxout", {"X"}, {"groups", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature MaxoutOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("groups");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "maxout", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeanAllOpArgumentMapping:

return KernelSignature("mean_all", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MeanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "mean_all", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
MemoryEfficientAttentionOpArgumentMapping:

return KernelSignature("memory_efficient_attention", {"query", "key", "value",
"bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"},
{"max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test"},
{"output", "logsumexp", "seed_and_offset"}); return
KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias",
"cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"}, {"max_seqlen_q",
"MaxSeqlenKTensor", "causal", "dropout_p", "scale", "is_test"}, {"output",
"logsumexp", "seed_and_offset"}); return
KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias",
"cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"},
{"MaxSeqlenQTensor", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test"},
{"output", "logsumexp", "seed_and_offset"}); return
KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias",
"cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"},
{"MaxSeqlenQTensor", "MaxSeqlenKTensor", "causal", "dropout_p", "scale",
"is_test"}, {"output", "logsumexp", "seed_and_offset"});
******************************************************************
*/

KernelSignature MemoryEfficientAttentionOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"query",
                                           "key",
                                           "value",
                                           "bias",
                                           "cu_seqlens_q",
                                           "cu_seqlens_k",
                                           "causal_diagonal",
                                           "seqlen_k"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("MaxSeqlenQTensor") ? "MaxSeqlenQTensor"
                                                      : "max_seqlen_q");
  attrs.emplace_back(ctx.HasInput("MaxSeqlenKTensor") ? "MaxSeqlenKTensor"
                                                      : "max_seqlen_k");
  attrs.emplace_back("causal");
  attrs.emplace_back("dropout_p");
  attrs.emplace_back("scale");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{
      "output", "logsumexp", "seed_and_offset"};
  return KernelSignature("memory_efficient_attention",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MergeSelectedRowsOpArgumentMapping:

return KernelSignature("merge_selected_rows", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MergeSelectedRowsOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("merge_selected_rows",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MergedAdamOpArgumentMapping:

return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate",
"Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1", "beta2",
"epsilon", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out",
"Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"}); return
KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1",
"Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1", "beta2",
"EpsilonTensor", "multi_precision", "use_global_beta_pow"}, {"ParamOut",
"Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate",
"Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1",
"Beta2Tensor", "epsilon", "multi_precision", "use_global_beta_pow"},
{"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut",
"MasterParamOut"}); return KernelSignature("merged_adam", {"Param", "Grad",
"LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"},
{"beta1", "Beta2Tensor", "EpsilonTensor", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("merged_adam",
{"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam"}, {"Beta1Tensor", "beta2", "epsilon", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("merged_adam",
{"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam"}, {"Beta1Tensor", "beta2", "EpsilonTensor", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("merged_adam",
{"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam"}, {"Beta1Tensor", "Beta2Tensor", "epsilon", "multi_precision",
"use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut",
"Beta2PowOut", "MasterParamOut"}); return KernelSignature("merged_adam",
{"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow",
"MasterParam"}, {"Beta1Tensor", "Beta2Tensor", "EpsilonTensor",
"multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out",
"Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature MergedAdamOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Param",
                                           "Grad",
                                           "LearningRate",
                                           "Moment1",
                                           "Moment2",
                                           "Beta1Pow",
                                           "Beta2Pow",
                                           "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta1");
  attrs.emplace_back("beta2");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("use_global_beta_pow");
  paddle::small_vector<const char*> outputs{"ParamOut",
                                            "Moment1Out",
                                            "Moment2Out",
                                            "Beta1PowOut",
                                            "Beta2PowOut",
                                            "MasterParamOut"};
  return KernelSignature(
      "merged_adam", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MergedMomentumOpArgumentMapping:

return KernelSignature("merged_momentum", {"Param", "Grad", "Velocity",
"LearningRate", "MasterParam"}, {"mu", "use_nesterov", "regularization_method",
"regularization_coeff", "multi_precision", "rescale_grad"}, {"ParamOut",
"VelocityOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature MergedMomentumOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Param", "Grad", "Velocity", "LearningRate", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mu");
  attrs.emplace_back("use_nesterov");
  attrs.emplace_back("regularization_method");
  attrs.emplace_back("regularization_coeff");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("rescale_grad");
  paddle::small_vector<const char*> outputs{
      "ParamOut", "VelocityOut", "MasterParamOut"};
  return KernelSignature("merged_momentum",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeshgridOpArgumentMapping:

return KernelSignature("meshgrid", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MeshgridOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "meshgrid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ModeOpArgumentMapping:

return KernelSignature("mode", {"X"}, {"axis", "keepdim"}, {"Out", "Indices"});
******************************************************************
*/

KernelSignature ModeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs{"Out", "Indices"};
  return KernelSignature(
      "mode", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MomentumOpArgumentMapping:

return KernelSignature("momentum", {"Param", "Grad", "Velocity", "LearningRate",
"MasterParam"}, {"mu", "use_nesterov", "regularization_method",
"regularization_coeff", "multi_precision", "rescale_grad"}, {"ParamOut",
"VelocityOut", "MasterParamOut"}); return
KernelSignature("momentum_dense_param_sparse_grad", {"Param", "Grad",
"Velocity", "LearningRate", "MasterParam"}, {"mu", "use_nesterov",
"regularization_method", "regularization_coeff", "multi_precision",
"rescale_grad"}, {"ParamOut", "VelocityOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature MomentumOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Param", "Grad", "Velocity", "LearningRate", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mu");
  attrs.emplace_back("use_nesterov");
  attrs.emplace_back("regularization_method");
  attrs.emplace_back("regularization_coeff");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("rescale_grad");
  paddle::small_vector<const char*> outputs{
      "ParamOut", "VelocityOut", "MasterParamOut"};
  if (ctx.IsDenseTensorInput("Param") && ctx.IsDenseTensorInput("Grad") &&
      ctx.IsDenseTensorInput("Velocity") &&
      ctx.IsDenseTensorInput("LearningRate") &&
      ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) ||
       (!ctx.HasInput("MasterParam")))) {
    return KernelSignature(
        "momentum", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsDenseTensorInput("Param") &&
             ctx.IsSelectedRowsInput("Grad") &&
             ctx.IsDenseTensorInput("Velocity") &&
             ctx.IsDenseTensorInput("LearningRate") &&
             ((ctx.HasInput("MasterParam") &&
               ctx.IsDenseTensorInput("MasterParam")) ||
              (!ctx.HasInput("MasterParam")))) {
    return KernelSignature("momentum_dense_param_sparse_grad",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiDotOpArgumentMapping:

return KernelSignature("multi_dot", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MultiDotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "multi_dot", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MulticlassNms3OpArgumentMapping:

return KernelSignature("multiclass_nms3", {"BBoxes", "Scores", "RoisNum"},
{"score_threshold", "nms_top_k", "keep_top_k", "nms_threshold", "normalized",
"nms_eta", "background_label"}, {"Out", "Index", "NmsRoisNum"});
******************************************************************
*/

KernelSignature MulticlassNms3OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"BBoxes", "Scores", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("score_threshold");
  attrs.emplace_back("nms_top_k");
  attrs.emplace_back("keep_top_k");
  attrs.emplace_back("nms_threshold");
  attrs.emplace_back("normalized");
  attrs.emplace_back("nms_eta");
  attrs.emplace_back("background_label");
  paddle::small_vector<const char*> outputs{"Out", "Index", "NmsRoisNum"};
  return KernelSignature("multiclass_nms3",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultinomialOpArgumentMapping:

return KernelSignature("multinomial", {"X"}, {"num_samples", "replacement"},
{"Out"});
******************************************************************
*/

KernelSignature MultinomialOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("num_samples");
  attrs.emplace_back("replacement");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "multinomial", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiplexOpArgumentMapping:

return KernelSignature("multiplex", {"X", "Ids"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MultiplexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Ids"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "multiplex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MvOpArgumentMapping:

return KernelSignature("mv", {"X", "Vec"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Vec"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "mv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NanmedianOpArgumentMapping:

return KernelSignature("nanmedian", {"X"}, {"axis", "keepdim"}, {"Out",
"MedianIndex"}); return KernelSignature("nanmedian", {"X"}, {"AxisTensorList",
"keepdim"}, {"Out", "MedianIndex"});
******************************************************************
*/

KernelSignature NanmedianOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor")            ? "AxisTensor"
                     : ctx.InputSize("AxisTensorList") > 0 ? "AxisTensorList"
                                                           : "axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs{"Out", "MedianIndex"};
  return KernelSignature(
      "nanmedian", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NearestInterpOpArgumentMapping:

return KernelSignature("nearest_interp", {"X", "OutSize", "SizeTensor",
"Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method",
"align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature NearestInterpV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("nearest_interp",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NextafterOpArgumentMapping:

return KernelSignature("nextafter", {"x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature NextafterOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "nextafter", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NllLossOpArgumentMapping:

return KernelSignature("nll_loss", {"X", "Label", "Weight"}, {"ignore_index",
"reduction"}, {"Out", "Total_weight"});
******************************************************************
*/

KernelSignature NllLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Label", "Weight"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs{"Out", "Total_weight"};
  return KernelSignature(
      "nll_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NmsOpArgumentMapping:

return KernelSignature("nms", {"Boxes"}, {"iou_threshold"}, {"KeepBoxesIdxs"});
******************************************************************
*/

KernelSignature NmsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Boxes"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("iou_threshold");
  paddle::small_vector<const char*> outputs{"KeepBoxesIdxs"};
  return KernelSignature(
      "nms", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NonzeroOpArgumentMapping:

return KernelSignature("nonzero", {"Condition"}, {}, {"Out"});
******************************************************************
*/

KernelSignature WhereIndexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Condition"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "nonzero", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NpuIdentityOpArgumentMapping:

return KernelSignature("npu_identity", {"x"}, {"format"}, {"out"});
******************************************************************
*/

KernelSignature NpuIdentityOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("format");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "npu_identity", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NumelOpArgumentMapping:

return KernelSignature("numel", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SizeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "numel", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by OverlapAddOpArgumentMapping:

return KernelSignature("overlap_add", {"X"}, {"hop_length", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature OverlapAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "overlap_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PNormOpArgumentMapping:

return KernelSignature("p_norm", {"X"}, {"porder", "axis", "epsilon", "keepdim",
"asvector"}, {"Out"});
******************************************************************
*/

KernelSignature PNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("porder");
  attrs.emplace_back("axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("keepdim");
  attrs.emplace_back("asvector");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "p_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pad3dOpArgumentMapping:

return KernelSignature("pad3d", {"X"}, {"paddings", "mode", "value",
"data_format"}, {"Out"}); return KernelSignature("pad3d", {"X"}, {"Paddings",
"mode", "value", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature Pad3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Paddings") ? "Paddings" : "paddings");

  attrs.emplace_back("mode");
  attrs.emplace_back("value");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "pad3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelShuffleOpArgumentMapping:

return KernelSignature("pixel_shuffle", {"X"}, {"upscale_factor",
"data_format"}, {"Out"});
******************************************************************
*/

KernelSignature PixelShuffleOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "pixel_shuffle", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelUnshuffleOpArgumentMapping:

return KernelSignature("pixel_unshuffle", {"X"}, {"downscale_factor",
"data_format"}, {"Out"});
******************************************************************
*/

KernelSignature PixelUnshuffleOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("downscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("pixel_unshuffle",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PoissonOpArgumentMapping:

return KernelSignature("poisson", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature PoissonOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "poisson", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PolygammaOpArgumentMapping:

return KernelSignature("polygamma", {"x"}, {"n"}, {"out"});
******************************************************************
*/

KernelSignature PolygammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "polygamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowOpArgumentMapping:

return KernelSignature("pow", {"X"}, {"factor"}, {"Out"});
return KernelSignature("pow", {"X"}, {"FactorTensor"}, {"Out"});
******************************************************************
*/

KernelSignature PowOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "pow", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PreluOpArgumentMapping:

return KernelSignature("prelu", {"X", "Alpha"}, {"data_format", "mode"},
{"Out"});
******************************************************************
*/

KernelSignature PreluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Alpha"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_format");
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "prelu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PriorBoxOpArgumentMapping:

return KernelSignature("prior_box", {"Input", "Image"}, {"min_sizes",
"max_sizes", "aspect_ratios", "variances", "flip", "clip", "step_w", "step_h",
"offset", "min_max_aspect_ratios_order"}, {"Boxes", "Variances"});
******************************************************************
*/

KernelSignature PriorBoxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Image"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("min_sizes");
  attrs.emplace_back("max_sizes");
  attrs.emplace_back("aspect_ratios");
  attrs.emplace_back("variances");
  attrs.emplace_back("flip");
  attrs.emplace_back("clip");
  attrs.emplace_back("step_w");
  attrs.emplace_back("step_h");
  attrs.emplace_back("offset");
  attrs.emplace_back("min_max_aspect_ratios_order");
  paddle::small_vector<const char*> outputs{"Boxes", "Variances"};
  return KernelSignature(
      "prior_box", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PsroiPoolOpArgumentMapping:

return KernelSignature("psroi_pool", {"X", "ROIs", "RoisNum"}, {"pooled_height",
"pooled_width", "output_channels", "spatial_scale"}, {"Out"});
******************************************************************
*/

KernelSignature PsroiPoolOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "ROIs", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("output_channels");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "psroi_pool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PutAlongAxisOpArgumentMapping:

return KernelSignature("put_along_axis", {"Input", "Index", "Value"}, {"Axis",
"Reduce"}, {"Result"});
******************************************************************
*/

KernelSignature PutAlongAxisOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Index", "Value"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  attrs.emplace_back("Reduce");
  paddle::small_vector<const char*> outputs{"Result"};
  return KernelSignature("put_along_axis",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QrOpArgumentMapping:

return KernelSignature("qr", {"X"}, {"mode"}, {"Q", "R"});
******************************************************************
*/

KernelSignature QrOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs{"Q", "R"};
  return KernelSignature(
      "qr", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RealOpArgumentMapping:

return KernelSignature("real", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RealOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "real", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReciprocalOpArgumentMapping:

return KernelSignature("reciprocal", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ReciprocalOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "reciprocal", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReindexGraphOpArgumentMapping:

return KernelSignature("graph_reindex", {"X", "Neighbors", "Count",
"HashTable_Value", "HashTable_Index"}, {}, {"Reindex_Src", "Reindex_Dst",
"Out_Nodes"});
******************************************************************
*/

KernelSignature GraphReindexOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Neighbors", "Count", "HashTable_Value", "HashTable_Index"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{
      "Reindex_Src", "Reindex_Dst", "Out_Nodes"};
  return KernelSignature(
      "graph_reindex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReluOpArgumentMapping:

return KernelSignature("relu", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Relu6OpArgumentMapping:

return KernelSignature("relu6", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Relu6OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "relu6", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RenormOpArgumentMapping:

return KernelSignature("renorm", {"X"}, {"p", "axis", "max_norm"}, {"Out"});
******************************************************************
*/

KernelSignature RenormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  attrs.emplace_back("axis");
  attrs.emplace_back("max_norm");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "renorm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RmsNormOpArgumentMapping:

return KernelSignature("rms_norm", {"x", "bias", "residual", "norm_weight",
"norm_bias"}, {"epsilon", "begin_norm_axis", "quant_scale", "quant_round_type",
"quant_max_bound", "quant_min_bound"}, {"out", "residual_out"});
******************************************************************
*/

KernelSignature RmsNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "bias", "residual", "norm_weight", "norm_bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("quant_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs{"out", "residual_out"};
  return KernelSignature(
      "rms_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RmspropOpArgumentMapping:

return KernelSignature("rmsprop", {"Param", "MeanSquare", "Grad", "Moment",
"LearningRate", "MeanGrad", "MasterParam"}, {"epsilon", "decay", "momentum",
"centered", "multi_precision"}, {"ParamOut", "MomentOut", "MeanSquareOut",
"MeanGradOut", "MasterParamOut"}); return
KernelSignature("rmsprop_dense_param_sparse_grad", {"Param", "MeanSquare",
"Grad", "Moment", "LearningRate", "MeanGrad", "MasterParam"}, {"epsilon",
"decay", "momentum", "centered", "multi_precision"}, {"ParamOut", "MomentOut",
"MeanSquareOut", "MeanGradOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature RmspropOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Param",
                                           "MeanSquare",
                                           "Grad",
                                           "Moment",
                                           "LearningRate",
                                           "MeanGrad",
                                           "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("decay");
  attrs.emplace_back("momentum");
  attrs.emplace_back("centered");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs{"ParamOut",
                                            "MomentOut",
                                            "MeanSquareOut",
                                            "MeanGradOut",
                                            "MasterParamOut"};
  if (ctx.IsDenseTensorInput("Param") && ctx.IsDenseTensorInput("MeanSquare") &&
      ctx.IsDenseTensorInput("Grad") && ctx.IsDenseTensorInput("Moment") &&
      ctx.IsDenseTensorInput("LearningRate") &&
      ((ctx.HasInput("MeanGrad") && ctx.IsDenseTensorInput("MeanGrad")) ||
       (!ctx.HasInput("MeanGrad"))) &&
      ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) ||
       (!ctx.HasInput("MasterParam")))) {
    return KernelSignature(
        "rmsprop", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsDenseTensorInput("Param") &&
             ctx.IsDenseTensorInput("MeanSquare") &&
             ctx.IsSelectedRowsInput("Grad") &&
             ctx.IsDenseTensorInput("Moment") &&
             ctx.IsDenseTensorInput("LearningRate") &&
             ((ctx.HasInput("MeanGrad") &&
               ctx.IsDenseTensorInput("MeanGrad")) ||
              (!ctx.HasInput("MeanGrad"))) &&
             ((ctx.HasInput("MasterParam") &&
               ctx.IsDenseTensorInput("MasterParam")) ||
              (!ctx.HasInput("MasterParam")))) {
    return KernelSignature("rmsprop_dense_param_sparse_grad",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiAlignOpArgumentMapping:

return KernelSignature("roi_align", {"X", "ROIs", "RoisNum"}, {"pooled_height",
"pooled_width", "spatial_scale", "sampling_ratio", "aligned"}, {"Out"});
******************************************************************
*/

KernelSignature RoiAlignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "ROIs", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  attrs.emplace_back("sampling_ratio");
  attrs.emplace_back("aligned");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "roi_align", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiPoolOpArgumentMapping:

return KernelSignature("roi_pool", {"X", "ROIs", "RoisNum"}, {"pooled_height",
"pooled_width", "spatial_scale"}, {"Out", "Argmax"});
******************************************************************
*/

KernelSignature RoiPoolOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "ROIs", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs{"Out", "Argmax"};
  return KernelSignature(
      "roi_pool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RollOpArgumentMapping:

return KernelSignature("roll", {"X"}, {"shifts", "axis"}, {"Out"});
return KernelSignature("roll", {"X"}, {"ShiftsTensor", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature RollOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("ShiftsTensor") ? "ShiftsTensor" : "shifts");

  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "roll", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoundOpArgumentMapping:

return KernelSignature("round", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RoundOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "round", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RsqrtOpArgumentMapping:

return KernelSignature("rsqrt", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RsqrtOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "rsqrt", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScaleOpArgumentMapping:

return KernelSignature("scale", {"X"}, {"scale", "bias", "bias_after_scale"},
{"Out"}); return KernelSignature("scale", {"X"}, {"ScaleTensor", "bias",
"bias_after_scale"}, {"Out"}); return KernelSignature("scale_sr", {"X"},
{"scale", "bias", "bias_after_scale"}, {"Out"}); return
KernelSignature("scale_sr", {"X"}, {"ScaleTensor", "bias", "bias_after_scale"},
{"Out"});
******************************************************************
*/

KernelSignature ScaleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("ScaleTensor") ? "ScaleTensor" : "scale");
  attrs.emplace_back("bias");
  attrs.emplace_back("bias_after_scale");
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature(
        "scale", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature(
        "scale_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterOpArgumentMapping:

return KernelSignature("scatter", {"X", "Ids", "Updates"}, {"overwrite"},
{"Out"});
******************************************************************
*/

KernelSignature ScatterOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Ids", "Updates"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("overwrite");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "scatter", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterNdAddOpArgumentMapping:

return KernelSignature("scatter_nd_add", {"X", "Index", "Updates"}, {},
{"Out"});
******************************************************************
*/

KernelSignature ScatterNdAddOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index", "Updates"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("scatter_nd_add",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SearchsortedOpArgumentMapping:

return KernelSignature("searchsorted", {"SortedSequence", "Values"},
{"out_int32", "right"}, {"Out"});
******************************************************************
*/

KernelSignature SearchsortedOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"SortedSequence", "Values"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("out_int32");
  attrs.emplace_back("right");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "searchsorted", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SegmentPoolOpArgumentMapping:

return KernelSignature("segment_pool", {"X", "SegmentIds"}, {"pooltype"},
{"Out", "SummedIds"});
******************************************************************
*/

KernelSignature SegmentPoolOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "SegmentIds"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooltype");
  paddle::small_vector<const char*> outputs{"Out", "SummedIds"};
  return KernelSignature(
      "segment_pool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SeluOpArgumentMapping:

return KernelSignature("selu", {"X"}, {"scale", "alpha"}, {"Out"});
******************************************************************
*/

KernelSignature SeluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale");
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "selu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendURecvOpArgumentMapping:

return KernelSignature("send_u_recv", {"X", "Src_index", "Dst_index"},
{"reduce_op", "out_size"}, {"Out", "Dst_count"}); return
KernelSignature("send_u_recv", {"X", "Src_index", "Dst_index"}, {"reduce_op",
"Out_size"}, {"Out", "Dst_count"});
******************************************************************
*/

KernelSignature GraphSendRecvOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Src_index", "Dst_index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduce_op");
  attrs.emplace_back(ctx.HasInput("Out_size") ? "Out_size" : "out_size");

  paddle::small_vector<const char*> outputs{"Out", "Dst_count"};
  return KernelSignature(
      "send_u_recv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUeRecvOpArgumentMapping:

return KernelSignature("send_ue_recv", {"X", "Y", "Src_index", "Dst_index"},
{"message_op", "reduce_op", "out_size"}, {"Out", "Dst_count"}); return
KernelSignature("send_ue_recv", {"X", "Y", "Src_index", "Dst_index"},
{"message_op", "reduce_op", "Out_size"}, {"Out", "Dst_count"});
******************************************************************
*/

KernelSignature GraphSendUeRecvOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Src_index", "Dst_index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  attrs.emplace_back("reduce_op");
  attrs.emplace_back(ctx.HasInput("Out_size") ? "Out_size" : "out_size");

  paddle::small_vector<const char*> outputs{"Out", "Dst_count"};
  return KernelSignature(
      "send_ue_recv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUvOpArgumentMapping:

return KernelSignature("send_uv", {"x", "y", "src_index", "dst_index"},
{"message_op"}, {"out"});
******************************************************************
*/

KernelSignature GraphSendUvOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "y", "src_index", "dst_index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "send_uv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SgdOpArgumentMapping:

return KernelSignature("sgd", {"Param", "LearningRate", "Grad", "MasterParam"},
{"multi_precision"}, {"ParamOut", "MasterParamOut"}); return
KernelSignature("sgd_dense_param_sparse_grad", {"Param", "LearningRate", "Grad",
"MasterParam"}, {"multi_precision"}, {"ParamOut", "MasterParamOut"}); return
KernelSignature("sgd_sparse_param_sparse_grad", {"Param", "LearningRate",
"Grad", "MasterParam"}, {"multi_precision"}, {"ParamOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature SgdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Param", "LearningRate", "Grad", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs{"ParamOut", "MasterParamOut"};
  if (ctx.IsDenseTensorInput("Param") &&
      ctx.IsDenseTensorInput("LearningRate") &&
      ctx.IsDenseTensorInput("Grad") &&
      ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) ||
       (!ctx.HasInput("MasterParam")))) {
    return KernelSignature(
        "sgd", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsDenseTensorInput("Param") &&
             ctx.IsDenseTensorInput("LearningRate") &&
             ctx.IsSelectedRowsInput("Grad") &&
             ((ctx.HasInput("MasterParam") &&
               ctx.IsDenseTensorInput("MasterParam")) ||
              (!ctx.HasInput("MasterParam")))) {
    return KernelSignature("sgd_dense_param_sparse_grad",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("Param") &&
             ctx.IsDenseTensorInput("LearningRate") &&
             ctx.IsSelectedRowsInput("Grad") &&
             ((ctx.HasInput("MasterParam") &&
               ctx.IsSelectedRowsInput("MasterParam")) ||
              (!ctx.HasInput("MasterParam")))) {
    return KernelSignature("sgd_sparse_param_sparse_grad",
                           std::move(inputs),
                           std::move(attrs),
                           std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ShapeOpArgumentMapping:

return KernelSignature("shape", {"Input"}, {}, {"Out"});
return KernelSignature("shape_sr", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ShapeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("Input")) {
    return KernelSignature(
        "shape", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("Input")) {
    return KernelSignature(
        "shape_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ShardIndexOpArgumentMapping:

return KernelSignature("shard_index", {"X"}, {"index_num", "nshards",
"shard_id", "ignore_value"}, {"Out"});
******************************************************************
*/

KernelSignature ShardIndexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("index_num");
  attrs.emplace_back("nshards");
  attrs.emplace_back("shard_id");
  attrs.emplace_back("ignore_value");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "shard_index", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidOpArgumentMapping:

return KernelSignature("sigmoid", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SigmoidOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "sigmoid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
SigmoidCrossEntropyWithLogitsOpArgumentMapping:

return KernelSignature("sigmoid_cross_entropy_with_logits", {"X", "Label",
"pos_weight"}, {"normalize", "ignore_index"}, {"Out"});
******************************************************************
*/

KernelSignature SigmoidCrossEntropyWithLogitsOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Label", "pos_weight"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("normalize");
  attrs.emplace_back("ignore_index");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("sigmoid_cross_entropy_with_logits",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SignOpArgumentMapping:

return KernelSignature("sign", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "sign", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SiluOpArgumentMapping:

return KernelSignature("silu", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SiluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "silu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinOpArgumentMapping:

return KernelSignature("sin", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SinOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "sin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinhOpArgumentMapping:

return KernelSignature("sinh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SinhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "sinh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SlogdetOpArgumentMapping:

return KernelSignature("slogdet", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SlogdeterminantOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "slogdet", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftplusOpArgumentMapping:

return KernelSignature("softplus", {"X"}, {"beta", "threshold"}, {"Out"});
******************************************************************
*/

KernelSignature SoftplusOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta");
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "softplus", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftshrinkOpArgumentMapping:

return KernelSignature("softshrink", {"X"}, {"lambda"}, {"Out"});
******************************************************************
*/

KernelSignature SoftshrinkOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("lambda");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "softshrink", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftsignOpArgumentMapping:

return KernelSignature("softsign", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SoftsignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "softsign", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SolveOpArgumentMapping:

return KernelSignature("solve", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SolveOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "solve", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SpectralNormOpArgumentMapping:

return KernelSignature("spectral_norm", {"Weight", "U", "V"}, {"dim",
"power_iters", "eps"}, {"Out"});
******************************************************************
*/

KernelSignature SpectralNormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Weight", "U", "V"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("power_iters");
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "spectral_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqrtOpArgumentMapping:

return KernelSignature("sqrt", {"X"}, {}, {"Out"});
return KernelSignature("sqrt_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SqrtOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature(
        "sqrt", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature(
        "sqrt_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquareOpArgumentMapping:

return KernelSignature("square", {"X"}, {}, {"Out"});
return KernelSignature("square_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SquareOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  if (ctx.IsDenseTensorInput("X")) {
    return KernelSignature(
        "square", std::move(inputs), std::move(attrs), std::move(outputs));
  } else if (ctx.IsSelectedRowsInput("X")) {
    return KernelSignature(
        "square_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  } else {
    return KernelSignature("unregistered", {}, {}, {});
  }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquaredL2NormOpArgumentMapping:

return KernelSignature("squared_l2_norm", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SquaredL2NormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("squared_l2_norm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqueezeOpArgumentMapping:

return KernelSignature("squeeze", {"X"}, {"axes"}, {"Out", "XShape"});
return KernelSignature("squeeze", {"X"}, {"AxisTensor"}, {"Out", "XShape"});
return KernelSignature("squeeze", {"X"}, {"AxisTensorList"}, {"Out", "XShape"});
******************************************************************
*/

KernelSignature Squeeze2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  paddle::small_vector<const char*> outputs{"Out", "XShape"};
  return KernelSignature(
      "squeeze", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StackOpArgumentMapping:

return KernelSignature("stack", {"X"}, {"axis"}, {"Y"});
******************************************************************
*/

KernelSignature StackOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Y"};
  return KernelSignature(
      "stack", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StanhOpArgumentMapping:

return KernelSignature("stanh", {"X"}, {"scale_a", "scale_b"}, {"Out"});
******************************************************************
*/

KernelSignature StanhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale_a");
  attrs.emplace_back("scale_b");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "stanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SvdOpArgumentMapping:

return KernelSignature("svd", {"X"}, {"full_matrices"}, {"U", "S", "VH"});
******************************************************************
*/

KernelSignature SvdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("full_matrices");
  paddle::small_vector<const char*> outputs{"U", "S", "VH"};
  return KernelSignature(
      "svd", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TakeAlongAxisOpArgumentMapping:

return KernelSignature("take_along_axis", {"Input", "Index"}, {"Axis"},
{"Result"});
******************************************************************
*/

KernelSignature TakeAlongAxisOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  paddle::small_vector<const char*> outputs{"Result"};
  return KernelSignature("take_along_axis",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanOpArgumentMapping:

return KernelSignature("tan", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "tan", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhOpArgumentMapping:

return KernelSignature("tanh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TanhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "tanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhShrinkOpArgumentMapping:

return KernelSignature("tanh_shrink", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TanhShrinkOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "tanh_shrink", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TemporalShiftOpArgumentMapping:

return KernelSignature("temporal_shift", {"X"}, {"seg_num", "shift_ratio",
"data_format"}, {"Out"});
******************************************************************
*/

KernelSignature TemporalShiftOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seg_num");
  attrs.emplace_back("shift_ratio");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("temporal_shift",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TensorUnfoldOpArgumentMapping:

return KernelSignature("tensor_unfold", {"input"}, {"axis", "size", "step"},
{"out"});
******************************************************************
*/

KernelSignature TensorUnfoldOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("size");
  attrs.emplace_back("step");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "tensor_unfold", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ThresholdedReluOpArgumentMapping:

return KernelSignature("thresholded_relu", {"X"}, {"threshold"}, {"Out"});
******************************************************************
*/

KernelSignature ThresholdedReluOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("thresholded_relu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TopPSamplingOpArgumentMapping:

return KernelSignature("top_p_sampling", {"x", "ps", "threshold"}, {"seed"},
{"out", "ids"});
******************************************************************
*/

KernelSignature TopPSamplingOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "ps", "threshold"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs{"out", "ids"};
  return KernelSignature("top_p_sampling",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TopkOpArgumentMapping:

return KernelSignature("topk", {"X"}, {"k", "axis", "largest", "sorted"},
{"Out", "Indices"});
******************************************************************
*/

KernelSignature TopKV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("K") ? "K" : "k");
  attrs.emplace_back("axis");
  attrs.emplace_back("largest");
  attrs.emplace_back("sorted");
  paddle::small_vector<const char*> outputs{"Out", "Indices"};
  return KernelSignature(
      "topk", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TraceOpArgumentMapping:

return KernelSignature("trace", {"Input"}, {"offset", "axis1", "axis2"},
{"Out"});
******************************************************************
*/

KernelSignature TraceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "trace", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TriangularSolveOpArgumentMapping:

return KernelSignature("triangular_solve", {"X", "Y"}, {"upper", "transpose",
"unitriangular"}, {"Out"});
******************************************************************
*/

KernelSignature TriangularSolveOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  attrs.emplace_back("transpose");
  attrs.emplace_back("unitriangular");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("triangular_solve",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilinearInterpOpArgumentMapping:

return KernelSignature("trilinear_interp", {"X", "OutSize", "SizeTensor",
"Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method",
"align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature TrilinearInterpV2OpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("trilinear_interp",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TruncOpArgumentMapping:

return KernelSignature("trunc", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TruncOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "trunc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnbindOpArgumentMapping:

return KernelSignature("unbind", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature UnbindOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "unbind", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnfoldOpArgumentMapping:

return KernelSignature("unfold", {"X"}, {"kernel_sizes", "strides", "paddings",
"dilations"}, {"Y"});
******************************************************************
*/

KernelSignature UnfoldOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs{"Y"};
  return KernelSignature(
      "unfold", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UniformInplaceOpArgumentMapping:

return KernelSignature("uniform_inplace", {"X"}, {"min", "max", "seed",
"diag_num", "diag_step", "diag_val"}, {"Out"});
******************************************************************
*/

KernelSignature UniformRandomInplaceOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("min");
  attrs.emplace_back("max");
  attrs.emplace_back("seed");
  attrs.emplace_back("diag_num");
  attrs.emplace_back("diag_step");
  attrs.emplace_back("diag_val");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("uniform_inplace",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UniqueConsecutiveOpArgumentMapping:

return KernelSignature("unique_consecutive", {"X"}, {"return_inverse",
"return_counts", "axis", "dtype"}, {"Out", "Index", "Counts"});
******************************************************************
*/

KernelSignature UniqueConsecutiveOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("return_inverse");
  attrs.emplace_back("return_counts");
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"Out", "Index", "Counts"};
  return KernelSignature("unique_consecutive",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Unpool3dOpArgumentMapping:

return KernelSignature("unpool3d", {"X", "Indices"}, {"ksize", "strides",
"paddings", "output_size", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature Unpool3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_size");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "unpool3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnsqueezeOpArgumentMapping:

return KernelSignature("unsqueeze", {"X"}, {"axes"}, {"Out", "XShape"});
return KernelSignature("unsqueeze", {"X"}, {"AxesTensor"}, {"Out", "XShape"});
return KernelSignature("unsqueeze", {"X"}, {"AxesTensorList"}, {"Out",
"XShape"});
******************************************************************
*/

KernelSignature Unsqueeze2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxesTensor")            ? "AxesTensor"
                     : ctx.InputSize("AxesTensorList") > 0 ? "AxesTensorList"
                                                           : "axes");
  paddle::small_vector<const char*> outputs{"Out", "XShape"};
  return KernelSignature(
      "unsqueeze", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnstackOpArgumentMapping:

return KernelSignature("unstack", {"X"}, {"axis", "num"}, {"Y"});
******************************************************************
*/

KernelSignature UnstackOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("num");
  paddle::small_vector<const char*> outputs{"Y"};
  return KernelSignature(
      "unstack", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UpdateLossScalingOpArgumentMapping:

return KernelSignature("update_loss_scaling", {"X", "FoundInfinite",
"PrevLossScaling", "InGoodSteps", "InBadSteps"}, {"incr_every_n_steps",
"decr_every_n_nan_or_inf", "incr_ratio", "decr_ratio", "stop_update"}, {"Out",
"LossScaling", "OutGoodSteps", "OutBadSteps"}); return
KernelSignature("update_loss_scaling", {"X", "FoundInfinite", "PrevLossScaling",
"InGoodSteps", "InBadSteps"}, {"incr_every_n_steps", "decr_every_n_nan_or_inf",
"incr_ratio", "decr_ratio", "StopUpdate"}, {"Out", "LossScaling",
"OutGoodSteps", "OutBadSteps"});
******************************************************************
*/

KernelSignature UpdateLossScalingOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "FoundInfinite", "PrevLossScaling", "InGoodSteps", "InBadSteps"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("incr_every_n_steps");
  attrs.emplace_back("decr_every_n_nan_or_inf");
  attrs.emplace_back("incr_ratio");
  attrs.emplace_back("decr_ratio");
  attrs.emplace_back(ctx.HasInput("StopUpdate") ? "StopUpdate" : "stop_update");
  paddle::small_vector<const char*> outputs{
      "Out", "LossScaling", "OutGoodSteps", "OutBadSteps"};
  return KernelSignature("update_loss_scaling",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewDtypeOpArgumentMapping:

return KernelSignature("view_dtype", {"input"}, {"dtype"}, {"out"});
******************************************************************
*/

KernelSignature ViewDtypeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "view_dtype", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewShapeOpArgumentMapping:

return KernelSignature("view_shape", {"input"}, {"dims"}, {"out"});
******************************************************************
*/

KernelSignature ViewShapeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "view_shape", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViterbiDecodeOpArgumentMapping:

return KernelSignature("viterbi_decode", {"Input", "Transition", "Length"},
{"include_bos_eos_tag"}, {"Scores", "Path"});
******************************************************************
*/

KernelSignature ViterbiDecodeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Transition", "Length"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("include_bos_eos_tag");
  paddle::small_vector<const char*> outputs{"Scores", "Path"};
  return KernelSignature("viterbi_decode",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarpctcOpArgumentMapping:

return KernelSignature("warpctc", {"Logits", "Label", "LogitsLength",
"LabelLength"}, {"blank", "norm_by_times"}, {"Loss", "WarpCTCGrad"});
******************************************************************
*/

KernelSignature WarpctcOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Logits", "Label", "LogitsLength", "LabelLength"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("norm_by_times");
  paddle::small_vector<const char*> outputs{"Loss", "WarpCTCGrad"};
  return KernelSignature(
      "warpctc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarprnntOpArgumentMapping:

return KernelSignature("warprnnt", {"input", "label", "input_lengths",
"label_lengths"}, {"blank", "fastemit_lambda"}, {"loss", "warprnntgrad"});
******************************************************************
*/

KernelSignature WarprnntOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "input", "label", "input_lengths", "label_lengths"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("fastemit_lambda");
  paddle::small_vector<const char*> outputs{"loss", "warprnntgrad"};
  return KernelSignature(
      "warprnnt", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightDequantizeOpArgumentMapping:

return KernelSignature("weight_dequantize", {"x", "scale"}, {"algo",
"out_dtype"}, {"out"});
******************************************************************
*/

KernelSignature WeightDequantizeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("algo");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("weight_dequantize",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightOnlyLinearOpArgumentMapping:

return KernelSignature("weight_only_linear", {"x", "weight", "bias",
"weight_scale"}, {"weight_dtype", "arch"}, {"out"});
******************************************************************
*/

KernelSignature WeightOnlyLinearOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "weight", "bias", "weight_scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("weight_dtype");
  attrs.emplace_back("arch");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("weight_only_linear",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightQuantizeOpArgumentMapping:

return KernelSignature("weight_quantize", {"x"}, {"algo", "arch"}, {"out",
"scale"});
******************************************************************
*/

KernelSignature WeightQuantizeOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("algo");
  attrs.emplace_back("arch");
  paddle::small_vector<const char*> outputs{"out", "scale"};
  return KernelSignature("weight_quantize",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
WeightedSampleNeighborsOpArgumentMapping:

return KernelSignature("weighted_sample_neighbors", {"row", "colptr",
"edge_weight", "input_nodes", "eids"}, {"sample_size", "return_eids"},
{"out_neighbors", "out_count", "out_eids"});
******************************************************************
*/

KernelSignature WeightedSampleNeighborsOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "row", "colptr", "edge_weight", "input_nodes", "eids"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("sample_size");
  attrs.emplace_back("return_eids");
  paddle::small_vector<const char*> outputs{
      "out_neighbors", "out_count", "out_eids"};
  return KernelSignature("weighted_sample_neighbors",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WhereOpArgumentMapping:

return KernelSignature("where", {"Condition", "X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature WhereOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Condition", "X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "where", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloBoxOpArgumentMapping:

return KernelSignature("yolo_box", {"X", "ImgSize"}, {"anchors", "class_num",
"conf_thresh", "downsample_ratio", "clip_bbox", "scale_x_y", "iou_aware",
"iou_aware_factor"}, {"Boxes", "Scores"});
******************************************************************
*/

KernelSignature YoloBoxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "ImgSize"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("anchors");
  attrs.emplace_back("class_num");
  attrs.emplace_back("conf_thresh");
  attrs.emplace_back("downsample_ratio");
  attrs.emplace_back("clip_bbox");
  attrs.emplace_back("scale_x_y");
  attrs.emplace_back("iou_aware");
  attrs.emplace_back("iou_aware_factor");
  paddle::small_vector<const char*> outputs{"Boxes", "Scores"};
  return KernelSignature(
      "yolo_box", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloLossOpArgumentMapping:

return KernelSignature("yolo_loss", {"X", "GTBox", "GTLabel", "GTScore"},
{"anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio",
"use_label_smooth", "scale_x_y"}, {"Loss", "ObjectnessMask", "GTMatchMask"});
******************************************************************
*/

KernelSignature Yolov3LossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "GTBox", "GTLabel", "GTScore"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("anchors");
  attrs.emplace_back("anchor_mask");
  attrs.emplace_back("class_num");
  attrs.emplace_back("ignore_thresh");
  attrs.emplace_back("downsample_ratio");
  attrs.emplace_back("use_label_smooth");
  attrs.emplace_back("scale_x_y");
  paddle::small_vector<const char*> outputs{
      "Loss", "ObjectnessMask", "GTMatchMask"};
  return KernelSignature(
      "yolo_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AbsDoubleGradOpArgumentMapping:

return KernelSignature("abs_double_grad", {"x", "grad_x@GRAD"}, {},
{"grad_out@GRAD"});
******************************************************************
*/

KernelSignature AbsDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"grad_out@GRAD"};
  return KernelSignature("abs_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AbsGradOpArgumentMapping:

return KernelSignature("abs_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AbsGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "abs_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcosGradOpArgumentMapping:

return KernelSignature("acos_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AcosGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "acos_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcoshGradOpArgumentMapping:

return KernelSignature("acosh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AcoshGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "acosh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddmmGradOpArgumentMapping:

return KernelSignature("addmm_grad", {"Input", "X", "Y", "Out@GRAD"}, {"Alpha",
"Beta"}, {"Input@GRAD", "X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature AddmmGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Alpha");
  attrs.emplace_back("Beta");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "addmm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AffineGridGradOpArgumentMapping:

return KernelSignature("affine_grid_grad", {"Output@GRAD"}, {"output_shape",
"align_corners"}, {"Theta@GRAD"}); return KernelSignature("affine_grid_grad",
{"Output@GRAD"}, {"OutputShape", "align_corners"}, {"Theta@GRAD"});
******************************************************************
*/

KernelSignature AffineGridGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("OutputShape") ? "OutputShape"
                                                 : "output_shape");

  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs{"Theta@GRAD"};
  return KernelSignature("affine_grid_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AngleGradOpArgumentMapping:

return KernelSignature("angle_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AngleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "angle_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgsortGradOpArgumentMapping:

return KernelSignature("argsort_grad", {"Indices", "X", "Out@GRAD"}, {"axis",
"descending"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ArgsortGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Indices", "X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("descending");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "argsort_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsStridedGradOpArgumentMapping:

return KernelSignature("as_strided_grad", {"input", "out@GRAD"}, {"dims",
"stride", "offset"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature AsStridedGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  attrs.emplace_back("stride");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs{"input@GRAD"};
  return KernelSignature("as_strided_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinGradOpArgumentMapping:

return KernelSignature("asin_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AsinGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "asin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinhGradOpArgumentMapping:

return KernelSignature("asinh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AsinhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "asinh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Atan2GradOpArgumentMapping:

return KernelSignature("atan2_grad", {"X1", "X2", "Out@GRAD"}, {}, {"X1@GRAD",
"X2@GRAD"});
******************************************************************
*/

KernelSignature Atan2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X1", "X2", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X1@GRAD", "X2@GRAD"};
  return KernelSignature(
      "atan2_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanGradOpArgumentMapping:

return KernelSignature("atan_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AtanGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "atan_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanhGradOpArgumentMapping:

return KernelSignature("atanh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AtanhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "atanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BceLossGradOpArgumentMapping:

return KernelSignature("bce_loss_grad", {"X", "Label", "Out@GRAD"}, {},
{"X@GRAD"});
******************************************************************
*/

KernelSignature BceLossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Label", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "bce_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BicubicInterpGradOpArgumentMapping:

return KernelSignature("bicubic_interp_grad", {"X", "OutSize", "SizeTensor",
"Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale",
"interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BicubicInterpV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("bicubic_interp_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearGradOpArgumentMapping:

return KernelSignature("bilinear_grad", {"X", "Y", "Weight", "Out@GRAD"}, {},
{"X@GRAD", "Y@GRAD", "Weight@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature BilinearTensorProductGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Weight", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "Y@GRAD", "Weight@GRAD", "Bias@GRAD"};
  return KernelSignature(
      "bilinear_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearInterpGradOpArgumentMapping:

return KernelSignature("bilinear_interp_grad", {"X", "OutSize", "SizeTensor",
"Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale",
"interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BilinearInterpV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("bilinear_interp_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BmmGradOpArgumentMapping:

return KernelSignature("bmm_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature BmmGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "bmm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BroadcastTensorsGradOpArgumentMapping:

return KernelSignature("broadcast_tensors_grad", {"X", "Out@GRAD"}, {},
{"X@GRAD"});
******************************************************************
*/

KernelSignature BroadcastTensorsGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("broadcast_tensors_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeilGradOpArgumentMapping:

return KernelSignature("ceil_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CeilGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "ceil_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeluDoubleGradOpArgumentMapping:

return KernelSignature("celu_double_grad", {"X", "grad_out", "grad_x@GRAD"},
{"alpha"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature CeluGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("celu_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeluGradOpArgumentMapping:

return KernelSignature("celu_grad", {"X", "Out@GRAD"}, {"alpha"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CeluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "celu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskyGradOpArgumentMapping:

return KernelSignature("cholesky_grad", {"Out", "Out@GRAD"}, {"upper"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature CholeskyGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "cholesky_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskySolveGradOpArgumentMapping:

return KernelSignature("cholesky_solve_grad", {"X", "Y", "Out", "Out@GRAD"},
{"upper"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature CholeskySolveGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature("cholesky_solve_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipDoubleGradOpArgumentMapping:

return KernelSignature("clip_grad", {"X", "grad_x@GRAD"}, {"min", "max"},
{"grad_out@GRAD"}); return KernelSignature("clip_grad", {"X", "grad_x@GRAD"},
{"min", "Max"}, {"grad_out@GRAD"}); return KernelSignature("clip_grad", {"X",
"grad_x@GRAD"}, {"Min", "max"}, {"grad_out@GRAD"}); return
KernelSignature("clip_grad", {"X", "grad_x@GRAD"}, {"Min", "Max"},
{"grad_out@GRAD"});
******************************************************************
*/

KernelSignature ClipDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Min") ? "Min" : "min");
  attrs.emplace_back(ctx.HasInput("Max") ? "Max" : "max");
  paddle::small_vector<const char*> outputs{"grad_out@GRAD"};
  return KernelSignature(
      "clip_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipGradOpArgumentMapping:

return KernelSignature("clip_grad", {"X", "Out@GRAD"}, {"min", "max"},
{"X@GRAD"}); return KernelSignature("clip_grad", {"X", "Out@GRAD"}, {"min",
"Max"}, {"X@GRAD"}); return KernelSignature("clip_grad", {"X", "Out@GRAD"},
{"Min", "max"}, {"X@GRAD"}); return KernelSignature("clip_grad", {"X",
"Out@GRAD"}, {"Min", "Max"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ClipGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Min") ? "Min" : "min");
  attrs.emplace_back(ctx.HasInput("Max") ? "Max" : "max");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "clip_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ComplexGradOpArgumentMapping:

return KernelSignature("complex_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature ComplexGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "complex_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ConcatGradOpArgumentMapping:

return KernelSignature("concat_grad", {"X", "Out@GRAD"}, {"axis"}, {"X@GRAD"});
return KernelSignature("concat_grad", {"X", "Out@GRAD"}, {"AxisTensor"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature ConcatGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor") ? "AxisTensor" : "axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "concat_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dGradOpArgumentMapping:

return KernelSignature("conv2d_grad", {"Input", "Filter", "Output@GRAD"},
{"strides", "paddings", "padding_algorithm", "dilations", "groups",
"data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv2dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Filter@GRAD"};
  return KernelSignature(
      "conv2d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dGradGradOpArgumentMapping:

return KernelSignature("conv2d_double_grad", {"Input", "Filter", "grad_out",
"grad_input@GRAD", "grad_filter@GRAD"}, {"strides", "paddings",
"padding_algorithm", "dilations", "groups", "data_format"}, {"Input@GRAD",
"Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature Conv2dGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{
      "Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("conv2d_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dDoubleGradOpArgumentMapping:

return KernelSignature("conv3d_double_grad", {"Input", "Filter", "grad_out",
"grad_input@GRAD", "grad_filter@GRAD"}, {"strides", "paddings",
"padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD",
"Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature Conv3dGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{
      "Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("conv3d_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dGradOpArgumentMapping:

return KernelSignature("conv3d_grad", {"Input", "Filter", "Output@GRAD"},
{"strides", "paddings", "padding_algorithm", "groups", "dilations",
"data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Filter@GRAD"};
  return KernelSignature(
      "conv3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dTransposeGradOpArgumentMapping:

return KernelSignature("conv3d_transpose_grad", {"Input", "Filter",
"Output@GRAD"}, {"strides", "paddings", "output_padding", "output_size",
"padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD",
"Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv3dTransposeGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("conv3d_transpose_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosDoubleGradOpArgumentMapping:

return KernelSignature("cos_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {},
{"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature CosDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("cos_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosGradOpArgumentMapping:

return KernelSignature("cos_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CosGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "cos_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosTripleGradOpArgumentMapping:

return KernelSignature("cos_triple_grad", {"X", "grad_out_forward",
"grad_x_grad_forward", "grad_x@GRAD", "grad_out_grad@GRAD"}, {}, {"X@GRAD",
"grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"});
******************************************************************
*/

KernelSignature CosTripleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X",
                                           "grad_out_forward",
                                           "grad_x_grad_forward",
                                           "grad_x@GRAD",
                                           "grad_out_grad@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"};
  return KernelSignature("cos_triple_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CoshGradOpArgumentMapping:

return KernelSignature("cosh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CoshGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "cosh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CropGradOpArgumentMapping:

return KernelSignature("crop_grad", {"X", "Out@GRAD"}, {"offsets"}, {"X@GRAD"});
return KernelSignature("crop_grad", {"X", "Out@GRAD"}, {"Offsets"}, {"X@GRAD"});
return KernelSignature("crop_grad", {"X", "Out@GRAD"}, {"OffsetsTensor"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature CropTensorGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Offsets")              ? "Offsets"
                     : ctx.InputSize("OffsetsTensor") > 0 ? "OffsetsTensor"
                                                          : "offsets");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "crop_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
CrossEntropyWithSoftmaxGradOpArgumentMapping:

return KernelSignature("cross_entropy_with_softmax_grad", {"Label", "Softmax",
"Loss@GRAD"}, {"soft_label", "use_softmax", "numeric_stable_mode",
"ignore_index", "axis"}, {"Logits@GRAD"});
******************************************************************
*/

KernelSignature SoftmaxWithCrossEntropyGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Label", "Softmax", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("soft_label");
  attrs.emplace_back("use_softmax");
  attrs.emplace_back("numeric_stable_mode");
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"Logits@GRAD"};
  return KernelSignature("cross_entropy_with_softmax_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossGradOpArgumentMapping:

return KernelSignature("cross_grad", {"X", "Y", "Out@GRAD"}, {"dim"}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature CrossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "cross_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CummaxGradOpArgumentMapping:

return KernelSignature("cummax_grad", {"x", "indices", "out@GRAD"}, {"axis",
"dtype"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature CummaxGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "indices", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature(
      "cummax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumminGradOpArgumentMapping:

return KernelSignature("cummin_grad", {"x", "indices", "out@GRAD"}, {"axis",
"dtype"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature CumminGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "indices", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature(
      "cummin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumprodGradOpArgumentMapping:

return KernelSignature("cumprod_grad", {"X", "Out", "Out@GRAD"}, {"dim"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature CumprodGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "cumprod_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumsumGradOpArgumentMapping:

return KernelSignature("cumsum_grad", {"X", "Out@GRAD"}, {"axis", "flatten",
"exclusive", "reverse"}, {"X@GRAD"}); return KernelSignature("cumsum_grad",
{"X", "Out@GRAD"}, {"AxisTensor", "flatten", "exclusive", "reverse"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature CumsumGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "cumsum_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
DepthwiseConv2dDoubleGradOpArgumentMapping:

return KernelSignature("depthwise_conv2d_double_grad", {"Input", "Filter",
"grad_out", "grad_input@GRAD", "grad_filter@GRAD"}, {"strides", "paddings",
"padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD",
"Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature DepthwiseConv2dGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{
      "Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("depthwise_conv2d_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DepthwiseConv2dGradOpArgumentMapping:

return KernelSignature("depthwise_conv2d_grad", {"Input", "Filter",
"Output@GRAD"}, {"strides", "paddings", "padding_algorithm", "groups",
"dilations", "data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature DepthwiseConv2dGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("depthwise_conv2d_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DetGradOpArgumentMapping:

return KernelSignature("determinant_grad", {"Input", "Out", "Out@GRAD"}, {},
{"Input@GRAD"});
******************************************************************
*/

KernelSignature DeterminantGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Input@GRAD"};
  return KernelSignature("determinant_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagGradOpArgumentMapping:

return KernelSignature("diag_grad", {"X", "Out@GRAD"}, {"offset"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature DiagV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "diag_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagonalGradOpArgumentMapping:

return KernelSignature("diagonal_grad", {"Input", "Out@GRAD"}, {"offset",
"axis1", "axis2"}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature DiagonalGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs{"Input@GRAD"};
  return KernelSignature(
      "diagonal_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DigammaGradOpArgumentMapping:

return KernelSignature("digamma_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature DigammaGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "digamma_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DistGradOpArgumentMapping:

return KernelSignature("dist_grad", {"X", "Y", "Out", "Out@GRAD"}, {"p"},
{"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature DistGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "dist_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DotGradOpArgumentMapping:

return KernelSignature("dot_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature DotGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "dot_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigGradOpArgumentMapping:

return KernelSignature("eig_grad", {"Eigenvalues", "Eigenvectors",
"Eigenvalues@GRAD", "Eigenvectors@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EigGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Eigenvalues", "Eigenvectors", "Eigenvalues@GRAD", "Eigenvectors@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "eig_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EighGradOpArgumentMapping:

return KernelSignature("eigh_grad", {"Eigenvalues", "Eigenvectors",
"Eigenvalues@GRAD", "Eigenvectors@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EighGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Eigenvalues", "Eigenvectors", "Eigenvalues@GRAD", "Eigenvectors@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "eigh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigvalshGradOpArgumentMapping:

return KernelSignature("eigvalsh_grad", {"Eigenvectors", "Eigenvalues@GRAD"},
{"UPLO", "is_test"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EigvalshGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Eigenvectors", "Eigenvalues@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("UPLO");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "eigvalsh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EluDoubleGradOpArgumentMapping:

return KernelSignature("elu_double_grad", {"X", "grad_out", "grad_x@GRAD"},
{"alpha"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature EluGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("elu_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EluGradOpArgumentMapping:

return KernelSignature("elu_grad", {"X", "Out", "Out@GRAD"}, {"alpha"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature EluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "elu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfGradOpArgumentMapping:

return KernelSignature("erf_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ErfGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "erf_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfinvGradOpArgumentMapping:

return KernelSignature("erfinv_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ErfinvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "erfinv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpGradOpArgumentMapping:

return KernelSignature("exp_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ExpGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "exp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpandAsGradOpArgumentMapping:

return KernelSignature("expand_as_grad", {"X", "Out@GRAD"}, {"target_shape"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature ExpandAsV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("target_shape");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("expand_as_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Expm1GradOpArgumentMapping:

return KernelSignature("expm1_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Expm1GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "expm1_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2cGradOpArgumentMapping:

return KernelSignature("fft_c2c_grad", {"Out@GRAD"}, {"axes", "normalization",
"forward"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FftC2cGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "fft_c2c_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2rGradOpArgumentMapping:

return KernelSignature("fft_c2r_grad", {"Out@GRAD"}, {"axes", "normalization",
"forward", "last_dim_size"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FftC2rGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("last_dim_size");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "fft_c2r_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftR2cGradOpArgumentMapping:

return KernelSignature("fft_r2c_grad", {"X", "Out@GRAD"}, {"axes",
"normalization", "forward", "onesided"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FftR2cGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("onesided");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "fft_r2c_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalGradOpArgumentMapping:

return KernelSignature("fill_diagonal_grad", {"Out@GRAD"}, {"value", "offset",
"wrap"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FillDiagonalGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("offset");
  attrs.emplace_back("wrap");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("fill_diagonal_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FillDiagonalTensorGradOpArgumentMapping:

return KernelSignature("fill_diagonal_tensor_grad", {"Out@GRAD"}, {"offset",
"dim1", "dim2"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FillDiagonalTensorGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("dim1");
  attrs.emplace_back("dim2");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("fill_diagonal_tensor_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillGradOpArgumentMapping:

return KernelSignature("fill_grad", {"Out@GRAD"}, {"value"}, {"X@GRAD"});
return KernelSignature("fill_grad", {"Out@GRAD"}, {"ValueTensor"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FillAnyGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "fill_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnGradOpArgumentMapping:

return KernelSignature("flash_attn_grad", {"q", "k", "v", "out", "softmax_lse",
"seed_offset", "attn_mask", "out@GRAD"}, {"dropout", "causal"}, {"q@GRAD",
"k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FlashAttnGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"q",
                                           "k",
                                           "v",
                                           "out",
                                           "softmax_lse",
                                           "seed_offset",
                                           "attn_mask",
                                           "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  paddle::small_vector<const char*> outputs{"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("flash_attn_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FlashAttnUnpaddedGradOpArgumentMapping:

return KernelSignature("flash_attn_unpadded_grad", {"q", "k", "v",
"cu_seqlens_q", "cu_seqlens_k", "out", "softmax_lse", "seed_offset",
"attn_mask", "out@GRAD"}, {"max_seqlen_q", "max_seqlen_k", "scale", "dropout",
"causal"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FlashAttnUnpaddedGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"q",
                                           "k",
                                           "v",
                                           "cu_seqlens_q",
                                           "cu_seqlens_k",
                                           "out",
                                           "softmax_lse",
                                           "seed_offset",
                                           "attn_mask",
                                           "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_seqlen_q");
  attrs.emplace_back("max_seqlen_k");
  attrs.emplace_back("scale");
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  paddle::small_vector<const char*> outputs{"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("flash_attn_unpadded_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FloorGradOpArgumentMapping:

return KernelSignature("floor_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FloorGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "floor_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FmaxGradOpArgumentMapping:

return KernelSignature("fmax_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseFmaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "fmax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FminGradOpArgumentMapping:

return KernelSignature("fmin_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseFminGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "fmin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FoldGradOpArgumentMapping:

return KernelSignature("fold_grad", {"X", "Y@GRAD"}, {"output_sizes",
"kernel_sizes", "strides", "paddings", "dilations"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FoldGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_sizes");
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "fold_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FrameGradOpArgumentMapping:

return KernelSignature("frame_grad", {"X", "Out@GRAD"}, {"frame_length",
"hop_length", "axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FrameGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("frame_length");
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "frame_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherGradOpArgumentMapping:

return KernelSignature("gather_grad", {"X", "Index", "Out@GRAD"}, {"axis"},
{"X@GRAD"}); return KernelSignature("gather_grad", {"X", "Index", "Out@GRAD"},
{"Axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GatherGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Axis") ? "Axis" : "axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "gather_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherNdGradOpArgumentMapping:

return KernelSignature("gather_nd_grad", {"X", "Index", "Out@GRAD"}, {},
{"X@GRAD"});
******************************************************************
*/

KernelSignature GatherNdGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("gather_nd_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GaussianInplaceGradOpArgumentMapping:

return KernelSignature("gaussian_inplace_grad", {"out@GRAD"}, {"mean", "std",
"seed"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature GaussianInplaceGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mean");
  attrs.emplace_back("std");
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature("gaussian_inplace_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GeluGradOpArgumentMapping:

return KernelSignature("gelu_grad", {"X", "Out@GRAD"}, {"approximate"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature GeluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("approximate");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "gelu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GridSampleGradOpArgumentMapping:

return KernelSignature("grid_sample_grad", {"X", "Grid", "Output@GRAD"},
{"mode", "padding_mode", "align_corners"}, {"X@GRAD", "Grid@GRAD"});
******************************************************************
*/

KernelSignature GridSamplerGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Grid", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  attrs.emplace_back("padding_mode");
  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Grid@GRAD"};
  return KernelSignature("grid_sample_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GroupNormGradOpArgumentMapping:

return KernelSignature("group_norm_grad", {"X", "Scale", "Bias", "Y", "Mean",
"Variance", "Y@GRAD"}, {"epsilon", "groups", "data_layout"}, {"X@GRAD",
"Scale@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature GroupNormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Scale", "Bias", "Y", "Mean", "Variance", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_layout");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "Scale@GRAD", "Bias@GRAD"};
  return KernelSignature("group_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GumbelSoftmaxGradOpArgumentMapping:

return KernelSignature("gumbel_softmax_grad", {"Out", "Out@GRAD"}, {"axis"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature GumbelSoftmaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("gumbel_softmax_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardshrinkGradOpArgumentMapping:

return KernelSignature("hard_shrink_grad", {"X", "Out@GRAD"}, {"threshold"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature HardShrinkGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("hard_shrink_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardsigmoidGradOpArgumentMapping:

return KernelSignature("hardsigmoid_grad", {"Out", "Out@GRAD"}, {"slope",
"offset"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature HardSigmoidGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("slope");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("hardsigmoid_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardtanhGradOpArgumentMapping:

return KernelSignature("hardtanh_grad", {"X", "Out@GRAD"}, {"t_min", "t_max"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature BreluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("t_min");
  attrs.emplace_back("t_max");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "hardtanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HeavisideGradOpArgumentMapping:

return KernelSignature("heaviside_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseHeavisideGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature("heaviside_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HuberLossGradOpArgumentMapping:

return KernelSignature("huber_loss_grad", {"Residual", "Out@GRAD"}, {"delta"},
{"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature HuberLossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Residual", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("delta");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature("huber_loss_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0GradOpArgumentMapping:

return KernelSignature("i0_grad", {"x", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I0GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature(
      "i0_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0eGradOpArgumentMapping:

return KernelSignature("i0e_grad", {"x", "out", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I0eGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "out", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature(
      "i0e_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1GradOpArgumentMapping:

return KernelSignature("i1_grad", {"x", "out", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I1GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "out", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature(
      "i1_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1eGradOpArgumentMapping:

return KernelSignature("i1e_grad", {"x", "out", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I1eGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "out", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature(
      "i1e_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IdentityLossGradOpArgumentMapping:

return KernelSignature("identity_loss_grad", {"X", "Out@GRAD"}, {"reduction"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature IdentityLossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("identity_loss_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ImagGradOpArgumentMapping:

return KernelSignature("imag_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ImagGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "imag_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexAddGradOpArgumentMapping:

return KernelSignature("index_add_grad", {"Index", "AddValue", "Out@GRAD"},
{"axis"}, {"X@GRAD", "AddValue@GRAD"});
******************************************************************
*/

KernelSignature IndexAddGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Index", "AddValue", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD", "AddValue@GRAD"};
  return KernelSignature("index_add_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexPutGradOpArgumentMapping:

return KernelSignature("index_put_grad", {"x", "indices", "value", "out@GRAD"},
{"accumulate"}, {"x@GRAD", "value@GRAD"});
******************************************************************
*/

KernelSignature IndexPutGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "indices", "value", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("accumulate");
  paddle::small_vector<const char*> outputs{"x@GRAD", "value@GRAD"};
  return KernelSignature("index_put_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSampleGradOpArgumentMapping:

return KernelSignature("index_sample_grad", {"X", "Index", "Out@GRAD"}, {},
{"X@GRAD"});
******************************************************************
*/

KernelSignature IndexSampleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("index_sample_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectGradOpArgumentMapping:

return KernelSignature("index_select_grad", {"X", "Index", "Out@GRAD"}, {"dim"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature IndexSelectGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("index_select_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
IndexSelectStridedGradOpArgumentMapping:

return KernelSignature("index_select_strided_grad", {"x", "out@GRAD"}, {"index",
"axis"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature IndexSelectStridedGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature("index_select_strided_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
InstanceNormDoubleGradOpArgumentMapping:

return KernelSignature("instance_norm_double_grad", {"x", "fwd_scale",
"saved_mean", "saved_variance", "grad_y", "grad_x@GRAD", "grad_scale@GRAD",
"grad_bias@GRAD"}, {"epsilon"}, {"x@GRAD", "fwd_scale@GRAD", "grad_y@GRAD"});
******************************************************************
*/

KernelSignature InstanceNormDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "fwd_scale",
                                           "saved_mean",
                                           "saved_variance",
                                           "grad_y",
                                           "grad_x@GRAD",
                                           "grad_scale@GRAD",
                                           "grad_bias@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{
      "x@GRAD", "fwd_scale@GRAD", "grad_y@GRAD"};
  return KernelSignature("instance_norm_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InstanceNormGradOpArgumentMapping:

return KernelSignature("instance_norm_grad", {"X", "Scale", "SavedMean",
"SavedVariance", "Y@GRAD"}, {"epsilon"}, {"X@GRAD", "Scale@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature InstanceNormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Scale", "SavedMean", "SavedVariance", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "Scale@GRAD", "Bias@GRAD"};
  return KernelSignature("instance_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InverseGradOpArgumentMapping:

return KernelSignature("inverse_grad", {"Output", "Output@GRAD"}, {},
{"Input@GRAD"});
******************************************************************
*/

KernelSignature InverseGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Output", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Input@GRAD"};
  return KernelSignature(
      "inverse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KldivLossGradOpArgumentMapping:

return KernelSignature("kldiv_loss_grad", {"X", "Target", "Loss@GRAD"},
{"reduction"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature KldivLossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Target", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("kldiv_loss_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KronGradOpArgumentMapping:

return KernelSignature("kron_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature KronGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "kron_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KthvalueGradOpArgumentMapping:

return KernelSignature("kthvalue_grad", {"X", "Indices", "Out@GRAD"}, {"k",
"axis", "keepdim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature KthvalueGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("k");
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "kthvalue_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LabelSmoothGradOpArgumentMapping:

return KernelSignature("label_smooth_grad", {"Out@GRAD"}, {"epsilon"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature LabelSmoothGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("label_smooth_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LayerNormGradOpArgumentMapping:

return KernelSignature("layer_norm_grad", {"X", "Scale", "Bias", "Mean",
"Variance", "Y@GRAD"}, {"epsilon", "begin_norm_axis"}, {"X@GRAD", "Scale@GRAD",
"Bias@GRAD"});
******************************************************************
*/

KernelSignature LayerNormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Scale", "Bias", "Mean", "Variance", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "Scale@GRAD", "Bias@GRAD"};
  return KernelSignature("layer_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LeakyReluDoubleGradOpArgumentMapping:

return KernelSignature("leaky_relu_double_grad", {"X", "grad_x@GRAD"},
{"alpha"}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature LeakyReluGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"grad_out@GRAD"};
  return KernelSignature("leaky_relu_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LeakyReluGradOpArgumentMapping:

return KernelSignature("leaky_relu_grad", {"X", "Out@GRAD"}, {"alpha"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature LeakyReluGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("leaky_relu_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LerpGradOpArgumentMapping:

return KernelSignature("lerp_grad", {"X", "Y", "Weight", "Out", "Out@GRAD"}, {},
{"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature LerpGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Y", "Weight", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "lerp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LgammaGradOpArgumentMapping:

return KernelSignature("lgamma_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LgammaGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "lgamma_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LinearInterpGradOpArgumentMapping:

return KernelSignature("linear_interp_grad", {"X", "OutSize", "SizeTensor",
"Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale",
"interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LinearInterpV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("linear_interp_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log10GradOpArgumentMapping:

return KernelSignature("log10_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Log10GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "log10_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log1pGradOpArgumentMapping:

return KernelSignature("log1p_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Log1pGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "log1p_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log2GradOpArgumentMapping:

return KernelSignature("log2_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Log2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "log2_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogDoubleGradOpArgumentMapping:

return KernelSignature("log_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {},
{"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature LogGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("log_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogGradOpArgumentMapping:

return KernelSignature("log_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "log_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogLossGradOpArgumentMapping:

return KernelSignature("log_loss_grad", {"Predicted", "Labels", "Loss@GRAD"},
{"epsilon"}, {"Predicted@GRAD"});
******************************************************************
*/

KernelSignature LogLossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Predicted", "Labels", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"Predicted@GRAD"};
  return KernelSignature(
      "log_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogSoftmaxGradOpArgumentMapping:

return KernelSignature("log_softmax_grad", {"Out", "Out@GRAD"}, {"axis"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature LogSoftmaxGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("log_softmax_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogcumsumexpGradOpArgumentMapping:

return KernelSignature("logcumsumexp_grad", {"X", "Out", "Out@GRAD"}, {"axis",
"flatten", "exclusive", "reverse"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogcumsumexpGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("logcumsumexp_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogitGradOpArgumentMapping:

return KernelSignature("logit_grad", {"X", "Out@GRAD"}, {"eps"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogitGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "logit_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogsigmoidGradOpArgumentMapping:

return KernelSignature("logsigmoid_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogsigmoidGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("logsigmoid_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuGradOpArgumentMapping:

return KernelSignature("lu_grad", {"X", "Out", "Pivots", "Out@GRAD"},
{"pivots"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LuGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Pivots", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pivots");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "lu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuUnpackGradOpArgumentMapping:

return KernelSignature("lu_unpack_grad", {"X", "Pivots", "L", "U", "Pmat",
"L@GRAD", "U@GRAD"}, {"unpack_ludata", "unpack_pivots"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LuUnpackGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Pivots", "L", "U", "Pmat", "L@GRAD", "U@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("unpack_ludata");
  attrs.emplace_back("unpack_pivots");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("lu_unpack_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
MarginCrossEntropyGradOpArgumentMapping:

return KernelSignature("margin_cross_entropy_grad", {"Logits", "Label",
"Softmax", "Loss@GRAD"}, {"return_softmax", "ring_id", "rank", "nranks",
"margin1", "margin2", "margin3", "scale"}, {"Logits@GRAD"});
******************************************************************
*/

KernelSignature MarginCrossEntropyGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Logits", "Label", "Softmax", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("rank");
  attrs.emplace_back("nranks");
  attrs.emplace_back("margin1");
  attrs.emplace_back("margin2");
  attrs.emplace_back("margin3");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs{"Logits@GRAD"};
  return KernelSignature("margin_cross_entropy_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaskedSelectGradOpArgumentMapping:

return KernelSignature("masked_select_grad", {"X", "Mask", "Y@GRAD"}, {},
{"X@GRAD"});
******************************************************************
*/

KernelSignature MaskedSelectGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Mask", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("masked_select_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatrixPowerGradOpArgumentMapping:

return KernelSignature("matrix_power_grad", {"X", "Out", "Out@GRAD"}, {"n"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature MatrixPowerGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("matrix_power_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
MaxPool2dWithIndexGradOpArgumentMapping:

return KernelSignature("max_pool2d_with_index_grad", {"X", "Mask", "Out@GRAD"},
{"ksize", "strides", "paddings", "global_pooling", "adaptive"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaxPool2dWithIndexGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Mask", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("max_pool2d_with_index_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
MaxPool3dWithIndexGradOpArgumentMapping:

return KernelSignature("max_pool3d_with_index_grad", {"X", "Mask", "Out@GRAD"},
{"ksize", "strides", "paddings", "global_pooling", "adaptive"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaxPool3dWithIndexGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Mask", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("max_pool3d_with_index_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxoutGradOpArgumentMapping:

return KernelSignature("maxout_grad", {"X", "Out", "Out@GRAD"}, {"groups",
"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaxoutGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("groups");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "maxout_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeanAllGradOpArgumentMapping:

return KernelSignature("mean_all_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MeanGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "mean_all_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
MemoryEfficientAttentionGradOpArgumentMapping:

return KernelSignature("memory_efficient_attention_grad", {"query", "key",
"value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp",
"seed_and_offset", "output@GRAD"}, {"max_seqlen_q", "max_seqlen_k", "causal",
"dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"});
return KernelSignature("memory_efficient_attention_grad", {"query", "key",
"value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp",
"seed_and_offset", "output@GRAD"}, {"max_seqlen_q", "MaxSeqlenKTensor",
"causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD",
"bias@GRAD"}); return KernelSignature("memory_efficient_attention_grad",
{"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output",
"logsumexp", "seed_and_offset", "output@GRAD"}, {"MaxSeqlenQTensor",
"max_seqlen_k", "causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD",
"value@GRAD", "bias@GRAD"}); return
KernelSignature("memory_efficient_attention_grad", {"query", "key", "value",
"bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp",
"seed_and_offset", "output@GRAD"}, {"MaxSeqlenQTensor", "MaxSeqlenKTensor",
"causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD",
"bias@GRAD"});
******************************************************************
*/

KernelSignature MemoryEfficientAttentionGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"query",
                                           "key",
                                           "value",
                                           "bias",
                                           "cu_seqlens_q",
                                           "cu_seqlens_k",
                                           "output",
                                           "logsumexp",
                                           "seed_and_offset",
                                           "output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("MaxSeqlenQTensor") ? "MaxSeqlenQTensor"
                                                      : "max_seqlen_q");
  attrs.emplace_back(ctx.HasInput("MaxSeqlenKTensor") ? "MaxSeqlenKTensor"
                                                      : "max_seqlen_k");
  attrs.emplace_back("causal");
  attrs.emplace_back("dropout_p");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs{
      "query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"};
  return KernelSignature("memory_efficient_attention_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeshgridGradOpArgumentMapping:

return KernelSignature("meshgrid_grad", {"X", "outputs@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MeshgridGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "outputs@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "meshgrid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ModeGradOpArgumentMapping:

return KernelSignature("mode_grad", {"X", "Indices", "Out@GRAD"}, {"axis",
"keepdim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ModeGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "mode_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiDotGradOpArgumentMapping:

return KernelSignature("multi_dot_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MultiDotGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("multi_dot_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiplexGradOpArgumentMapping:

return KernelSignature("multiplex_grad", {"Ids", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MultiplexGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Ids", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("multiplex_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MvGradOpArgumentMapping:

return KernelSignature("mv_grad", {"X", "Vec", "Out@GRAD"}, {}, {"X@GRAD",
"Vec@GRAD"});
******************************************************************
*/

KernelSignature MvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Vec", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Vec@GRAD"};
  return KernelSignature(
      "mv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NanmedianGradOpArgumentMapping:

return KernelSignature("nanmedian_grad", {"X", "MedianIndex", "Out@GRAD"},
{"axis", "keepdim"}, {"X@GRAD"}); return KernelSignature("nanmedian_grad", {"X",
"MedianIndex", "Out@GRAD"}, {"AxisTensorList", "keepdim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NanmedianGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "MedianIndex", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor")            ? "AxisTensor"
                     : ctx.InputSize("AxisTensorList") > 0 ? "AxisTensorList"
                                                           : "axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("nanmedian_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NearestInterpGradOpArgumentMapping:

return KernelSignature("nearest_interp_grad", {"X", "OutSize", "SizeTensor",
"Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale",
"interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NearestInterpV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("nearest_interp_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NllLossGradOpArgumentMapping:

return KernelSignature("nll_loss_grad", {"X", "Label", "Weight", "Total_weight",
"Out@GRAD"}, {"ignore_index", "reduction"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NllLossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Label", "Weight", "Total_weight", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "nll_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by OverlapAddGradOpArgumentMapping:

return KernelSignature("overlap_add_grad", {"X", "Out@GRAD"}, {"hop_length",
"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature OverlapAddGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("overlap_add_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PNormGradOpArgumentMapping:

return KernelSignature("p_norm_grad", {"X", "Out", "Out@GRAD"}, {"porder",
"axis", "epsilon", "keepdim", "asvector"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("porder");
  attrs.emplace_back("axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("keepdim");
  attrs.emplace_back("asvector");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "p_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pad3dDoubleGradOpArgumentMapping:

return KernelSignature("pad3d", {"grad_x@GRAD"}, {"paddings", "mode", "value",
"data_format"}, {"grad_out@GRAD"}); return KernelSignature("pad3d",
{"grad_x@GRAD"}, {"Paddings", "mode", "value", "data_format"},
{"grad_out@GRAD"});
******************************************************************
*/

KernelSignature Pad3dDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Paddings") ? "Paddings" : "paddings");

  attrs.emplace_back("mode");
  attrs.emplace_back("value");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"grad_out@GRAD"};
  return KernelSignature(
      "pad3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pad3dGradOpArgumentMapping:

return KernelSignature("pad3d_grad", {"X", "Out@GRAD"}, {"paddings", "mode",
"value", "data_format"}, {"X@GRAD"}); return KernelSignature("pad3d_grad", {"X",
"Out@GRAD"}, {"Paddings", "mode", "value", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Pad3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Paddings") ? "Paddings" : "paddings");

  attrs.emplace_back("mode");
  attrs.emplace_back("value");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "pad3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelShuffleGradOpArgumentMapping:

return KernelSignature("pixel_shuffle_grad", {"Out@GRAD"}, {"upscale_factor",
"data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PixelShuffleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("pixel_shuffle_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelUnshuffleGradOpArgumentMapping:

return KernelSignature("pixel_unshuffle_grad", {"Out@GRAD"},
{"downscale_factor", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PixelUnshuffleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("downscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("pixel_unshuffle_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PoissonGradOpArgumentMapping:

return KernelSignature("poisson_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PoissonGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "poisson_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PolygammaGradOpArgumentMapping:

return KernelSignature("polygamma_grad", {"x", "out@GRAD"}, {"n"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature PolygammaGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature("polygamma_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowDoubleGradOpArgumentMapping:

return KernelSignature("pow_double_grad", {"X", "grad_out", "grad_x@GRAD"},
{"factor"}, {"X@GRAD", "grad_out@GRAD"}); return
KernelSignature("pow_double_grad", {"X", "grad_out", "grad_x@GRAD"},
{"FactorTensor"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature PowDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("pow_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowGradOpArgumentMapping:

return KernelSignature("pow_grad", {"X", "Out@GRAD"}, {"factor"}, {"X@GRAD"});
return KernelSignature("pow_grad", {"X", "Out@GRAD"}, {"FactorTensor"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature PowGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "pow_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowTripleGradOpArgumentMapping:

return KernelSignature("pow_triple_grad", {"X", "grad_out", "grad_grad_x",
"grad_x@GRAD", "grad_grad_out@GRAD"}, {"factor"}, {"X@GRAD", "grad_out@GRAD",
"grad_grad_x@GRAD"}); return KernelSignature("pow_triple_grad", {"X",
"grad_out", "grad_grad_x", "grad_x@GRAD", "grad_grad_out@GRAD"},
{"FactorTensor"}, {"X@GRAD", "grad_out@GRAD", "grad_grad_x@GRAD"});
******************************************************************
*/

KernelSignature PowTripleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "grad_out", "grad_grad_x", "grad_x@GRAD", "grad_grad_out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "grad_out@GRAD", "grad_grad_x@GRAD"};
  return KernelSignature("pow_triple_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PreluGradOpArgumentMapping:

return KernelSignature("prelu_grad", {"X", "Alpha", "Out@GRAD"}, {"data_format",
"mode"}, {"X@GRAD", "Alpha@GRAD"});
******************************************************************
*/

KernelSignature PreluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Alpha", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_format");
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Alpha@GRAD"};
  return KernelSignature(
      "prelu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PsroiPoolGradOpArgumentMapping:

return KernelSignature("psroi_pool_grad", {"X", "ROIs", "RoisNum", "Out@GRAD"},
{"pooled_height", "pooled_width", "output_channels", "spatial_scale"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature PsroiPoolGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "ROIs", "RoisNum", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("output_channels");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("psroi_pool_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PutAlongAxisGradOpArgumentMapping:

return KernelSignature("put_along_axis_grad", {"Input", "Index", "Result@GRAD"},
{"Axis", "Reduce"}, {"Input@GRAD", "Value@GRAD"});
******************************************************************
*/

KernelSignature PutAlongAxisGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Index", "Result@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  attrs.emplace_back("Reduce");
  paddle::small_vector<const char*> outputs{"Input@GRAD", "Value@GRAD"};
  return KernelSignature("put_along_axis_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QrGradOpArgumentMapping:

return KernelSignature("qr_grad", {"X", "Q", "R", "Q@GRAD", "R@GRAD"}, {"mode"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature QrGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Q", "R", "Q@GRAD", "R@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "qr_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RealGradOpArgumentMapping:

return KernelSignature("real_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RealGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "real_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReciprocalGradOpArgumentMapping:

return KernelSignature("reciprocal_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReciprocalGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("reciprocal_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Relu6GradOpArgumentMapping:

return KernelSignature("relu6_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Relu6GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "relu6_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReluDoubleGradOpArgumentMapping:

return KernelSignature("relu_double_grad", {"Out", "grad_x@GRAD"}, {},
{"grad_out@GRAD"});
******************************************************************
*/

KernelSignature ReluGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"grad_out@GRAD"};
  return KernelSignature("relu_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReluGradOpArgumentMapping:

return KernelSignature("relu_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "relu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RenormGradOpArgumentMapping:

return KernelSignature("renorm_grad", {"X", "Out@GRAD"}, {"p", "axis",
"max_norm"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RenormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  attrs.emplace_back("axis");
  attrs.emplace_back("max_norm");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "renorm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiAlignGradOpArgumentMapping:

return KernelSignature("roi_align_grad", {"X", "ROIs", "RoisNum", "Out@GRAD"},
{"pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature RoiAlignGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "ROIs", "RoisNum", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  attrs.emplace_back("sampling_ratio");
  attrs.emplace_back("aligned");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("roi_align_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiPoolGradOpArgumentMapping:

return KernelSignature("roi_pool_grad", {"X", "ROIs", "RoisNum", "Argmax",
"Out@GRAD"}, {"pooled_height", "pooled_width", "spatial_scale"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RoiPoolGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "ROIs", "RoisNum", "Argmax", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "roi_pool_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RollGradOpArgumentMapping:

return KernelSignature("roll_grad", {"X", "Out@GRAD"}, {"shifts", "axis"},
{"X@GRAD"}); return KernelSignature("roll_grad", {"X", "Out@GRAD"},
{"ShiftsTensor", "axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RollGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("ShiftsTensor") ? "ShiftsTensor" : "shifts");

  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "roll_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoundGradOpArgumentMapping:

return KernelSignature("round_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RoundGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "round_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RsqrtDoubleGradOpArgumentMapping:

return KernelSignature("rsqrt_double_grad", {"Out", "grad_x", "grad_x@GRAD"},
{}, {"Out@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature RsqrtGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "grad_x", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out@GRAD", "grad_out@GRAD"};
  return KernelSignature("rsqrt_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RsqrtGradOpArgumentMapping:

return KernelSignature("rsqrt_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RsqrtGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "rsqrt_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterGradOpArgumentMapping:

return KernelSignature("scatter_grad", {"Ids", "Updates", "Out@GRAD"},
{"overwrite"}, {"X@GRAD", "Updates@GRAD"});
******************************************************************
*/

KernelSignature ScatterGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Ids", "Updates", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("overwrite");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Updates@GRAD"};
  return KernelSignature(
      "scatter_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterNdAddGradOpArgumentMapping:

return KernelSignature("scatter_nd_add_grad", {"Index", "Updates", "Out@GRAD"},
{}, {"X@GRAD", "Updates@GRAD"});
******************************************************************
*/

KernelSignature ScatterNdAddGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Index", "Updates", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Updates@GRAD"};
  return KernelSignature("scatter_nd_add_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SegmentPoolGradOpArgumentMapping:

return KernelSignature("segment_pool_grad", {"X", "SegmentIds", "Out",
"SummedIds", "Out@GRAD"}, {"pooltype"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SegmentPoolGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "SegmentIds", "Out", "SummedIds", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooltype");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("segment_pool_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SeluGradOpArgumentMapping:

return KernelSignature("selu_grad", {"Out", "Out@GRAD"}, {"scale", "alpha"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature SeluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale");
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "selu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendURecvGradOpArgumentMapping:

return KernelSignature("send_u_recv_grad", {"X", "Src_index", "Dst_index",
"Out", "Dst_count", "Out@GRAD"}, {"reduce_op"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GraphSendRecvGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Src_index", "Dst_index", "Out", "Dst_count", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduce_op");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("send_u_recv_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUeRecvGradOpArgumentMapping:

return KernelSignature("send_ue_recv_grad", {"X", "Y", "Src_index", "Dst_index",
"Out", "Dst_count", "Out@GRAD"}, {"message_op", "reduce_op"}, {"X@GRAD",
"Y@GRAD"});
******************************************************************
*/

KernelSignature GraphSendUeRecvGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Y", "Src_index", "Dst_index", "Out", "Dst_count", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  attrs.emplace_back("reduce_op");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature("send_ue_recv_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUvGradOpArgumentMapping:

return KernelSignature("send_uv_grad", {"x", "y", "src_index", "dst_index",
"out@GRAD"}, {"message_op"}, {"x@GRAD", "y@GRAD"});
******************************************************************
*/

KernelSignature GraphSendUvGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "y", "src_index", "dst_index", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  paddle::small_vector<const char*> outputs{"x@GRAD", "y@GRAD"};
  return KernelSignature(
      "send_uv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
SigmoidCrossEntropyWithLogitsGradOpArgumentMapping:

return KernelSignature("sigmoid_cross_entropy_with_logits_grad", {"X", "Label",
"pos_weight", "Out@GRAD"}, {"normalize", "ignore_index"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SigmoidCrossEntropyWithLogitsGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Label", "pos_weight", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("normalize");
  attrs.emplace_back("ignore_index");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("sigmoid_cross_entropy_with_logits_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidDoubleGradOpArgumentMapping:

return KernelSignature("sigmoid_double_grad", {"Out", "fwd_grad_out",
"grad_x@GRAD"}, {}, {"Out@GRAD", "fwd_grad_out@GRAD"});
******************************************************************
*/

KernelSignature SigmoidGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Out", "fwd_grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out@GRAD", "fwd_grad_out@GRAD"};
  return KernelSignature("sigmoid_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidGradOpArgumentMapping:

return KernelSignature("sigmoid_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SigmoidGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "sigmoid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidTripleGradOpArgumentMapping:

return KernelSignature("sigmoid_triple_grad", {"Out", "fwd_grad_out",
"grad_grad_x", "grad_out@GRAD", "grad_grad_out@GRAD"}, {}, {"Out@GRAD",
"fwd_grad_out@GRAD", "grad_grad_x@GRAD"});
******************************************************************
*/

KernelSignature SigmoidTripleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out",
                                           "fwd_grad_out",
                                           "grad_grad_x",
                                           "grad_out@GRAD",
                                           "grad_grad_out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{
      "Out@GRAD", "fwd_grad_out@GRAD", "grad_grad_x@GRAD"};
  return KernelSignature("sigmoid_triple_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SiluGradOpArgumentMapping:

return KernelSignature("silu_grad", {"X", "Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SiluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "silu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinDoubleGradOpArgumentMapping:

return KernelSignature("sin_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {},
{"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SinDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("sin_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinGradOpArgumentMapping:

return KernelSignature("sin_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SinGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "sin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinTripleGradOpArgumentMapping:

return KernelSignature("sin_triple_grad", {"X", "grad_out_forward",
"grad_x_grad_forward", "grad_x@GRAD", "grad_out_grad@GRAD"}, {}, {"X@GRAD",
"grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"});
******************************************************************
*/

KernelSignature SinTripleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X",
                                           "grad_out_forward",
                                           "grad_x_grad_forward",
                                           "grad_x@GRAD",
                                           "grad_out_grad@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"};
  return KernelSignature("sin_triple_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinhGradOpArgumentMapping:

return KernelSignature("sinh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SinhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "sinh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SlogdetGradOpArgumentMapping:

return KernelSignature("slogdet_grad", {"Input", "Out", "Out@GRAD"}, {},
{"Input@GRAD"});
******************************************************************
*/

KernelSignature SlogdeterminantGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Input@GRAD"};
  return KernelSignature(
      "slogdet_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftplusDoubleGradOpArgumentMapping:

return KernelSignature("softplus_double_grad", {"X", "grad_out", "grad_x@GRAD"},
{"beta", "threshold"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SoftplusDoubleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta");
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("softplus_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftplusGradOpArgumentMapping:

return KernelSignature("softplus_grad", {"X", "Out@GRAD"}, {"beta",
"threshold"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SoftplusGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta");
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "softplus_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftshrinkGradOpArgumentMapping:

return KernelSignature("softshrink_grad", {"X", "Out@GRAD"}, {"lambda"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature SoftshrinkGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("lambda");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("softshrink_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftsignGradOpArgumentMapping:

return KernelSignature("softsign_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SoftsignGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "softsign_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SolveGradOpArgumentMapping:

return KernelSignature("solve_grad", {"X", "Y", "Out", "Out@GRAD"}, {},
{"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature SolveGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "solve_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SpectralNormGradOpArgumentMapping:

return KernelSignature("spectral_norm_grad", {"Weight", "U", "V", "Out@GRAD"},
{"dim", "power_iters", "eps"}, {"Weight@GRAD"});
******************************************************************
*/

KernelSignature SpectralNormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Weight", "U", "V", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("power_iters");
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs{"Weight@GRAD"};
  return KernelSignature("spectral_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqrtDoubleGradOpArgumentMapping:

return KernelSignature("sqrt_double_grad", {"Out", "grad_x", "grad_x@GRAD"}, {},
{"Out@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SqrtGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "grad_x", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out@GRAD", "grad_out@GRAD"};
  return KernelSignature("sqrt_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqrtGradOpArgumentMapping:

return KernelSignature("sqrt_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SqrtGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "sqrt_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquareDoubleGradOpArgumentMapping:

return KernelSignature("square_double_grad", {"X", "grad_out", "grad_x@GRAD"},
{}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SquareGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("square_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquareGradOpArgumentMapping:

return KernelSignature("square_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SquareGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "square_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquaredL2NormGradOpArgumentMapping:

return KernelSignature("squared_l2_norm_grad", {"X", "Out@GRAD"}, {},
{"X@GRAD"});
******************************************************************
*/

KernelSignature SquaredL2NormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("squared_l2_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqueezeGradOpArgumentMapping:

return KernelSignature("squeeze_grad", {"XShape", "Out@GRAD"}, {"axes"},
{"X@GRAD"}); return KernelSignature("squeeze_grad", {"XShape", "Out@GRAD"},
{"AxisTensor"}, {"X@GRAD"}); return KernelSignature("squeeze_grad", {"XShape",
"Out@GRAD"}, {"AxisTensorList"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Squeeze2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"XShape", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "squeeze_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StackGradOpArgumentMapping:

return KernelSignature("stack_grad", {"Y@GRAD"}, {"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature StackGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "stack_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StanhGradOpArgumentMapping:

return KernelSignature("stanh_grad", {"X", "Out@GRAD"}, {"scale_a", "scale_b"},
{"X@GRAD"});
******************************************************************
*/

KernelSignature StanhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale_a");
  attrs.emplace_back("scale_b");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "stanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SvdGradOpArgumentMapping:

return KernelSignature("svd_grad", {"X", "U", "VH", "S", "U@GRAD", "VH@GRAD",
"S@GRAD"}, {"full_matrices"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SvdGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "U", "VH", "S", "U@GRAD", "VH@GRAD", "S@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("full_matrices");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "svd_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TakeAlongAxisGradOpArgumentMapping:

return KernelSignature("take_along_axis_grad", {"Input", "Index",
"Result@GRAD"}, {"Axis"}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature TakeAlongAxisGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Index", "Result@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  paddle::small_vector<const char*> outputs{"Input@GRAD"};
  return KernelSignature("take_along_axis_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanGradOpArgumentMapping:

return KernelSignature("tan_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TanGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "tan_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhDoubleGradOpArgumentMapping:

return KernelSignature("tanh_double_grad", {"Out", "grad_out", "grad_x@GRAD"},
{}, {"Out@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature TanhGradGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"Out@GRAD", "grad_out@GRAD"};
  return KernelSignature("tanh_double_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhGradOpArgumentMapping:

return KernelSignature("tanh_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TanhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "tanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhShrinkGradOpArgumentMapping:

return KernelSignature("tanh_shrink_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TanhShrinkGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("tanh_shrink_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhTripleGradOpArgumentMapping:

return KernelSignature("tanh_triple_grad", {"Out", "grad_out_forward",
"grad_x_grad_forward", "grad_out_new@GRAD", "grad_out_grad@GRAD"}, {},
{"Out@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"});
******************************************************************
*/

KernelSignature TanhTripleGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out",
                                           "grad_out_forward",
                                           "grad_x_grad_forward",
                                           "grad_out_new@GRAD",
                                           "grad_out_grad@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{
      "Out@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"};
  return KernelSignature("tanh_triple_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TemporalShiftGradOpArgumentMapping:

return KernelSignature("temporal_shift_grad", {"Out@GRAD"}, {"seg_num",
"shift_ratio", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TemporalShiftGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seg_num");
  attrs.emplace_back("shift_ratio");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("temporal_shift_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TensorUnfoldGradOpArgumentMapping:

return KernelSignature("tensor_unfold_grad", {"input", "out@GRAD"}, {"axis",
"size", "step"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature TensorUnfoldGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("size");
  attrs.emplace_back("step");
  paddle::small_vector<const char*> outputs{"input@GRAD"};
  return KernelSignature("tensor_unfold_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ThresholdedReluGradOpArgumentMapping:

return KernelSignature("thresholded_relu_grad", {"X", "Out@GRAD"},
{"threshold"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ThresholdedReluGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("thresholded_relu_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TopkGradOpArgumentMapping:

return KernelSignature("topk_grad", {"X", "Indices", "Out@GRAD"}, {"k", "axis",
"largest", "sorted"}, {"X@GRAD"}); return KernelSignature("topk_grad", {"X",
"Indices", "Out@GRAD"}, {"K", "axis", "largest", "sorted"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TopKV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("K") ? "K" : "k");
  attrs.emplace_back("axis");
  attrs.emplace_back("largest");
  attrs.emplace_back("sorted");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "topk_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TraceGradOpArgumentMapping:

return KernelSignature("trace_grad", {"Input", "Out@GRAD"}, {"offset", "axis1",
"axis2"}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature TraceGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs{"Input@GRAD"};
  return KernelSignature(
      "trace_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TriangularSolveGradOpArgumentMapping:

return KernelSignature("triangular_solve_grad", {"X", "Y", "Out", "Out@GRAD"},
{"upper", "transpose", "unitriangular"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature TriangularSolveGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  attrs.emplace_back("transpose");
  attrs.emplace_back("unitriangular");
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature("triangular_solve_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilinearInterpGradOpArgumentMapping:

return KernelSignature("trilinear_interp_grad", {"X", "OutSize", "SizeTensor",
"Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale",
"interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TrilinearInterpV2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("trilinear_interp_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TruncGradOpArgumentMapping:

return KernelSignature("trunc_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TruncGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "trunc_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnfoldGradOpArgumentMapping:

return KernelSignature("unfold_grad", {"X", "Y@GRAD"}, {"kernel_sizes",
"strides", "paddings", "dilations"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UnfoldGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "unfold_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UniformInplaceGradOpArgumentMapping:

return KernelSignature("uniform_inplace_grad", {"Out@GRAD"}, {"min", "max",
"seed", "diag_num", "diag_step", "diag_val"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UniformRandomInplaceGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("min");
  attrs.emplace_back("max");
  attrs.emplace_back("seed");
  attrs.emplace_back("diag_num");
  attrs.emplace_back("diag_step");
  attrs.emplace_back("diag_val");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("uniform_inplace_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnsqueezeGradOpArgumentMapping:

return KernelSignature("unsqueeze_grad", {"XShape", "Out@GRAD"}, {"axes"},
{"X@GRAD"}); return KernelSignature("unsqueeze_grad", {"XShape", "Out@GRAD"},
{"AxesTensor"}, {"X@GRAD"}); return KernelSignature("unsqueeze_grad", {"XShape",
"Out@GRAD"}, {"AxesTensorList"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Unsqueeze2GradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"XShape", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;

  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature("unsqueeze_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnstackGradOpArgumentMapping:

return KernelSignature("unstack_grad", {"Y@GRAD"}, {"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UnstackGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "unstack_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewDtypeGradOpArgumentMapping:

return KernelSignature("view_dtype_grad", {"input", "out@GRAD"}, {"dtype"},
{"input@GRAD"});
******************************************************************
*/

KernelSignature ViewDtypeGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"input@GRAD"};
  return KernelSignature("view_dtype_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewShapeGradOpArgumentMapping:

return KernelSignature("view_shape_grad", {"input", "out@GRAD"}, {"dims"},
{"input@GRAD"});
******************************************************************
*/

KernelSignature ViewShapeGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  paddle::small_vector<const char*> outputs{"input@GRAD"};
  return KernelSignature("view_shape_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarpctcGradOpArgumentMapping:

return KernelSignature("warpctc_grad", {"Logits", "LogitsLength", "WarpCTCGrad",
"Loss@GRAD"}, {"blank", "norm_by_times"}, {"Logits@GRAD"});
******************************************************************
*/

KernelSignature WarpctcGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Logits", "LogitsLength", "WarpCTCGrad", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("norm_by_times");
  paddle::small_vector<const char*> outputs{"Logits@GRAD"};
  return KernelSignature(
      "warpctc_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarprnntGradOpArgumentMapping:

return KernelSignature("warprnnt_grad", {"input", "input_lengths",
"warprnntgrad", "loss@GRAD"}, {"blank", "fastemit_lambda"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature WarprnntGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "input", "input_lengths", "warprnntgrad", "loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("fastemit_lambda");
  paddle::small_vector<const char*> outputs{"input@GRAD"};
  return KernelSignature(
      "warprnnt_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightOnlyLinearGradOpArgumentMapping:

return KernelSignature("weight_only_linear_grad", {"x", "weight", "bias",
"weight_scale", "out@GRAD"}, {"weight_dtype", "arch"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature WeightOnlyLinearGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "weight", "bias", "weight_scale", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("weight_dtype");
  attrs.emplace_back("arch");
  paddle::small_vector<const char*> outputs{"x@GRAD"};
  return KernelSignature("weight_only_linear_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WhereGradOpArgumentMapping:

return KernelSignature("where_grad", {"Condition", "X", "Y", "Out@GRAD"}, {},
{"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature WhereGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Condition", "X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"X@GRAD", "Y@GRAD"};
  return KernelSignature(
      "where_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloLossGradOpArgumentMapping:

return KernelSignature("yolo_loss_grad", {"X", "GTBox", "GTLabel", "GTScore",
"ObjectnessMask", "GTMatchMask", "Loss@GRAD"}, {"anchors", "anchor_mask",
"class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth",
"scale_x_y"}, {"X@GRAD", "GTBox@GRAD", "GTLabel@GRAD", "GTScore@GRAD"});
******************************************************************
*/

KernelSignature Yolov3LossGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X",
                                           "GTBox",
                                           "GTLabel",
                                           "GTScore",
                                           "ObjectnessMask",
                                           "GTMatchMask",
                                           "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("anchors");
  attrs.emplace_back("anchor_mask");
  attrs.emplace_back("class_num");
  attrs.emplace_back("ignore_thresh");
  attrs.emplace_back("downsample_ratio");
  attrs.emplace_back("use_label_smooth");
  attrs.emplace_back("scale_x_y");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "GTBox@GRAD", "GTLabel@GRAD", "GTScore@GRAD"};
  return KernelSignature("yolo_loss_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Unpool3dGradOpArgumentMapping:

return KernelSignature("unpool3d_grad", {"X", "Indices", "Out", "Out@GRAD"},
{"ksize", "strides", "paddings", "output_size", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Unpool3dGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Indices", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_size");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs{"X@GRAD"};
  return KernelSignature(
      "unpool3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

}  // namespace phi

PD_REGISTER_ARG_MAPPING_FN(abs, phi::AbsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(accuracy, phi::AccuracyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acos, phi::AcosOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acosh, phi::AcoshOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(adagrad, phi::AdagradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(adamax, phi::AdamaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(adamw, phi::AdamwOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(addmm, phi::AddmmOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(affine_grid, phi::AffineGridOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(allclose, phi::AllcloseOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(angle, phi::AngleOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(arg_max, argmax);
PD_REGISTER_ARG_MAPPING_FN(arg_max, phi::ArgMaxOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(arg_min, argmin);
PD_REGISTER_ARG_MAPPING_FN(arg_min, phi::ArgMinOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(argsort, phi::ArgsortOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_complex, phi::AsComplexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_real, phi::AsRealOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_strided, phi::AsStridedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asin, phi::AsinOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asinh, phi::AsinhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan, phi::AtanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan2, phi::Atan2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atanh, phi::AtanhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(auc, phi::AucOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(average_accumulates,
                           phi::AverageAccumulatesOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bce_loss, phi::BceLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bernoulli, phi::BernoulliOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bicubic_interp_v2, bicubic_interp);
PD_REGISTER_ARG_MAPPING_FN(bicubic_interp_v2,
                           phi::BicubicInterpV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_tensor_product, bilinear);
PD_REGISTER_ARG_MAPPING_FN(bilinear_tensor_product,
                           phi::BilinearTensorProductOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_interp_v2, bilinear_interp);
PD_REGISTER_ARG_MAPPING_FN(bilinear_interp_v2,
                           phi::BilinearInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bincount, phi::BincountOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_and, phi::BitwiseAndOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_not, phi::BitwiseNotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_or, phi::BitwiseOrOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_xor, phi::BitwiseXorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bmm, phi::BmmOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(box_coder, phi::BoxCoderOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(broadcast_tensors,
                           phi::BroadcastTensorsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(ceil, phi::CeilOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(celu, phi::CeluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(check_finite_and_unscale,
                           phi::CheckFiniteAndUnscaleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(check_numerics, phi::CheckNumericsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky, phi::CholeskyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky_solve, phi::CholeskySolveOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(class_center_sample,
                           phi::ClassCenterSampleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip, phi::ClipOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip_by_norm, phi::ClipByNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(coalesce_tensor,
                           phi::CoalesceTensorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(complex, phi::ComplexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(concat, phi::ConcatOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conj, phi::ConjOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d, phi::Conv2dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d, phi::Conv3dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d_transpose,
                           phi::Conv3dTransposeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos, phi::CosOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cosh, phi::CoshOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(crop_tensor, crop);
PD_REGISTER_ARG_MAPPING_FN(crop_tensor, phi::CropTensorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cross, phi::CrossOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(softmax_with_cross_entropy,
                             cross_entropy_with_softmax);
PD_REGISTER_ARG_MAPPING_FN(softmax_with_cross_entropy,
                           phi::SoftmaxWithCrossEntropyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummax, phi::CummaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummin, phi::CumminOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumprod, phi::CumprodOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumsum, phi::CumsumOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(data, phi::DataOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d,
                           phi::DepthwiseConv2dOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(determinant, det);
PD_REGISTER_ARG_MAPPING_FN(determinant, phi::DeterminantOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(diag_v2, diag);
PD_REGISTER_ARG_MAPPING_FN(diag_v2, phi::DiagV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(diag_embed, phi::DiagEmbedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(diagonal, phi::DiagonalOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(digamma, phi::DigammaOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dirichlet, phi::DirichletOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dist, phi::DistOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dot, phi::DotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(edit_distance, phi::EditDistanceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eig, phi::EigOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigh, phi::EighOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigvals, phi::EigvalsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigvalsh, phi::EigvalshOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(elu, phi::EluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(equal_all, phi::EqualAllOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erf, phi::ErfOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erfinv, phi::ErfinvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(exp, phi::ExpOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(expand_as_v2, expand_as);
PD_REGISTER_ARG_MAPPING_FN(expand_as_v2, phi::ExpandAsV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(expm1, phi::Expm1OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2c, phi::FftC2cOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2r, phi::FftC2rOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_r2c, phi::FftR2cOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(fill_any, fill);
PD_REGISTER_ARG_MAPPING_FN(fill_any, phi::FillAnyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal, phi::FillDiagonalOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal_tensor,
                           phi::FillDiagonalTensorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn, phi::FlashAttnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_unpadded,
                           phi::FlashAttnUnpaddedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flip, phi::FlipOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(floor, phi::FloorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fold, phi::FoldOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(frame, phi::FrameOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(full_int_array, phi::FullIntArrayOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather, phi::GatherOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_nd, phi::GatherNdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_tree, phi::GatherTreeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gaussian_inplace,
                           phi::GaussianInplaceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gelu, phi::GeluOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(generate_proposals_v2, generate_proposals);
PD_REGISTER_ARG_MAPPING_FN(generate_proposals_v2,
                           phi::GenerateProposalsV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(grid_sampler, grid_sample);
PD_REGISTER_ARG_MAPPING_FN(grid_sampler, phi::GridSamplerOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(group_norm, phi::GroupNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gumbel_softmax, phi::GumbelSoftmaxOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_shrink, hardshrink);
PD_REGISTER_ARG_MAPPING_FN(hard_shrink, phi::HardShrinkOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_sigmoid, hardsigmoid);
PD_REGISTER_ARG_MAPPING_FN(hard_sigmoid, phi::HardSigmoidOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(brelu, hardtanh);
PD_REGISTER_ARG_MAPPING_FN(brelu, phi::BreluOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_heaviside, heaviside);
PD_REGISTER_ARG_MAPPING_FN(elementwise_heaviside,
                           phi::ElementwiseHeavisideOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(histogram, phi::HistogramOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(huber_loss, phi::HuberLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0, phi::I0OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0e, phi::I0eOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1, phi::I1OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1e, phi::I1eOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(identity_loss, phi::IdentityLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(imag, phi::ImagOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_add, phi::IndexAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_put, phi::IndexPutOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_sample, phi::IndexSampleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select, phi::IndexSelectOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select_strided,
                           phi::IndexSelectStridedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(instance_norm, phi::InstanceNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(inverse, phi::InverseOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(is_empty, phi::IsEmptyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(isclose, phi::IscloseOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(isfinite_v2, isfinite);
PD_REGISTER_ARG_MAPPING_FN(isfinite_v2, phi::IsfiniteV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(isinf_v2, isinf);
PD_REGISTER_ARG_MAPPING_FN(isinf_v2, phi::IsinfV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(isnan_v2, isnan);
PD_REGISTER_ARG_MAPPING_FN(isnan_v2, phi::IsnanV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kldiv_loss, phi::KldivLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kron, phi::KronOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kthvalue, phi::KthvalueOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(label_smooth, phi::LabelSmoothOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lamb, phi::LambOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(layer_norm, phi::LayerNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(leaky_relu, phi::LeakyReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lerp, phi::LerpOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lgamma, phi::LgammaOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(linear_interp_v2, linear_interp);
PD_REGISTER_ARG_MAPPING_FN(linear_interp_v2,
                           phi::LinearInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(llm_int8_linear,
                           phi::LlmInt8LinearOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log, phi::LogOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log10, phi::Log10OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log1p, phi::Log1pOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log2, phi::Log2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_loss, phi::LogLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_softmax, phi::LogSoftmaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logcumsumexp, phi::LogcumsumexpOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_and, phi::LogicalAndOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_not, phi::LogicalNotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_or, phi::LogicalOrOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_xor, phi::LogicalXorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logit, phi::LogitOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logsigmoid, phi::LogsigmoidOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lstsq, phi::LstsqOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu, phi::LuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu_unpack, phi::LuUnpackOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(margin_cross_entropy,
                           phi::MarginCrossEntropyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(masked_multihead_attention,
                           phi::MaskedMultiheadAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(masked_select, phi::MaskedSelectOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(matrix_nms, phi::MatrixNmsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(matrix_power, phi::MatrixPowerOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool2d_with_index,
                           phi::MaxPool2dWithIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool3d_with_index,
                           phi::MaxPool3dWithIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(maxout, phi::MaxoutOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(mean, mean_all);
PD_REGISTER_ARG_MAPPING_FN(mean, phi::MeanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(memory_efficient_attention,
                           phi::MemoryEfficientAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(merge_selected_rows,
                           phi::MergeSelectedRowsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(merged_adam, phi::MergedAdamOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(merged_momentum,
                           phi::MergedMomentumOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(meshgrid, phi::MeshgridOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mode, phi::ModeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(momentum, phi::MomentumOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multi_dot, phi::MultiDotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multiclass_nms3,
                           phi::MulticlassNms3OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multinomial, phi::MultinomialOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multiplex, phi::MultiplexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mv, phi::MvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nanmedian, phi::NanmedianOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(nearest_interp_v2, nearest_interp);
PD_REGISTER_ARG_MAPPING_FN(nearest_interp_v2,
                           phi::NearestInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nextafter, phi::NextafterOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nll_loss, phi::NllLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nms, phi::NmsOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(where_index, nonzero);
PD_REGISTER_ARG_MAPPING_FN(where_index, phi::WhereIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(npu_identity, phi::NpuIdentityOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(size, numel);
PD_REGISTER_ARG_MAPPING_FN(size, phi::SizeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(overlap_add, phi::OverlapAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(p_norm, phi::PNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pad3d, phi::Pad3dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_shuffle, phi::PixelShuffleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_unshuffle,
                           phi::PixelUnshuffleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(poisson, phi::PoissonOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(polygamma, phi::PolygammaOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow, phi::PowOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(prelu, phi::PreluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(prior_box, phi::PriorBoxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(psroi_pool, phi::PsroiPoolOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(put_along_axis, phi::PutAlongAxisOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(qr, phi::QrOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(real, phi::RealOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(reciprocal, phi::ReciprocalOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_reindex, reindex_graph);
PD_REGISTER_ARG_MAPPING_FN(graph_reindex, phi::GraphReindexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu, phi::ReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu6, phi::Relu6OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(renorm, phi::RenormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rms_norm, phi::RmsNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rmsprop, phi::RmspropOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_align, phi::RoiAlignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_pool, phi::RoiPoolOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roll, phi::RollOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(round, phi::RoundOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rsqrt, phi::RsqrtOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scale, phi::ScaleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter, phi::ScatterOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter_nd_add, phi::ScatterNdAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(searchsorted, phi::SearchsortedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(segment_pool, phi::SegmentPoolOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(selu, phi::SeluOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_recv, send_u_recv);
PD_REGISTER_ARG_MAPPING_FN(graph_send_recv,
                           phi::GraphSendRecvOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_ue_recv, send_ue_recv);
PD_REGISTER_ARG_MAPPING_FN(graph_send_ue_recv,
                           phi::GraphSendUeRecvOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_uv, send_uv);
PD_REGISTER_ARG_MAPPING_FN(graph_send_uv, phi::GraphSendUvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sgd, phi::SgdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(shape, phi::ShapeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(shard_index, phi::ShardIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid, phi::SigmoidOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_cross_entropy_with_logits,
                           phi::SigmoidCrossEntropyWithLogitsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sign, phi::SignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(silu, phi::SiluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin, phi::SinOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sinh, phi::SinhOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(slogdeterminant, slogdet);
PD_REGISTER_ARG_MAPPING_FN(slogdeterminant,
                           phi::SlogdeterminantOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softplus, phi::SoftplusOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softshrink, phi::SoftshrinkOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softsign, phi::SoftsignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(solve, phi::SolveOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(spectral_norm, phi::SpectralNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sqrt, phi::SqrtOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(square, phi::SquareOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(squared_l2_norm,
                           phi::SquaredL2NormOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(squeeze2, squeeze);
PD_REGISTER_ARG_MAPPING_FN(squeeze2, phi::Squeeze2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stack, phi::StackOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stanh, phi::StanhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(svd, phi::SvdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(take_along_axis,
                           phi::TakeAlongAxisOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tan, phi::TanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh, phi::TanhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_shrink, phi::TanhShrinkOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(temporal_shift, phi::TemporalShiftOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tensor_unfold, phi::TensorUnfoldOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(thresholded_relu,
                           phi::ThresholdedReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(top_p_sampling, phi::TopPSamplingOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(top_k_v2, topk);
PD_REGISTER_ARG_MAPPING_FN(top_k_v2, phi::TopKV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trace, phi::TraceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(triangular_solve,
                           phi::TriangularSolveOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(trilinear_interp_v2, trilinear_interp);
PD_REGISTER_ARG_MAPPING_FN(trilinear_interp_v2,
                           phi::TrilinearInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trunc, phi::TruncOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unbind, phi::UnbindOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unfold, phi::UnfoldOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(uniform_random_inplace, uniform_inplace);
PD_REGISTER_ARG_MAPPING_FN(uniform_random_inplace,
                           phi::UniformRandomInplaceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unique_consecutive,
                           phi::UniqueConsecutiveOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unpool3d, phi::Unpool3dOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(unsqueeze2, unsqueeze);
PD_REGISTER_ARG_MAPPING_FN(unsqueeze2, phi::Unsqueeze2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unstack, phi::UnstackOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(update_loss_scaling,
                           phi::UpdateLossScalingOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_dtype, phi::ViewDtypeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_shape, phi::ViewShapeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(viterbi_decode, phi::ViterbiDecodeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warpctc, phi::WarpctcOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warprnnt, phi::WarprnntOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_dequantize,
                           phi::WeightDequantizeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_only_linear,
                           phi::WeightOnlyLinearOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_quantize,
                           phi::WeightQuantizeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weighted_sample_neighbors,
                           phi::WeightedSampleNeighborsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(where, phi::WhereOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(yolo_box, phi::YoloBoxOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(yolov3_loss, yolo_loss);
PD_REGISTER_ARG_MAPPING_FN(yolov3_loss, phi::Yolov3LossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(abs_double_grad,
                           phi::AbsDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(abs_grad, phi::AbsGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acos_grad, phi::AcosGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acosh_grad, phi::AcoshGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(addmm_grad, phi::AddmmGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(affine_grid_grad,
                           phi::AffineGridGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(angle_grad, phi::AngleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(argsort_grad, phi::ArgsortGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_strided_grad,
                           phi::AsStridedGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asin_grad, phi::AsinGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asinh_grad, phi::AsinhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan2_grad, phi::Atan2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan_grad, phi::AtanGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atanh_grad, phi::AtanhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bce_loss_grad, phi::BceLossGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bicubic_interp_v2_grad, bicubic_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(bicubic_interp_v2_grad,
                           phi::BicubicInterpV2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_tensor_product_grad, bilinear_grad);
PD_REGISTER_ARG_MAPPING_FN(bilinear_tensor_product_grad,
                           phi::BilinearTensorProductGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_interp_v2_grad, bilinear_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(bilinear_interp_v2_grad,
                           phi::BilinearInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bmm_grad, phi::BmmGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(broadcast_tensors_grad,
                           phi::BroadcastTensorsGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(ceil_grad, phi::CeilGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(celu_grad_grad, celu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(celu_grad_grad, phi::CeluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(celu_grad, phi::CeluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky_grad, phi::CholeskyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky_solve_grad,
                           phi::CholeskySolveGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip_double_grad,
                           phi::ClipDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip_grad, phi::ClipGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(complex_grad, phi::ComplexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(concat_grad, phi::ConcatGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_grad, phi::Conv2dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_grad_grad,
                           phi::Conv2dGradGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(conv3d_grad_grad, conv3d_double_grad);
PD_REGISTER_ARG_MAPPING_FN(conv3d_grad_grad,
                           phi::Conv3dGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d_grad, phi::Conv3dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d_transpose_grad,
                           phi::Conv3dTransposeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos_double_grad,
                           phi::CosDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos_grad, phi::CosGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos_triple_grad,
                           phi::CosTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cosh_grad, phi::CoshGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(crop_tensor_grad, crop_grad);
PD_REGISTER_ARG_MAPPING_FN(crop_tensor_grad,
                           phi::CropTensorGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(softmax_with_cross_entropy_grad,
                             cross_entropy_with_softmax_grad);
PD_REGISTER_ARG_MAPPING_FN(softmax_with_cross_entropy_grad,
                           phi::SoftmaxWithCrossEntropyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cross_grad, phi::CrossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummax_grad, phi::CummaxGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummin_grad, phi::CumminGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumprod_grad, phi::CumprodGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumsum_grad, phi::CumsumGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(depthwise_conv2d_grad_grad,
                             depthwise_conv2d_double_grad);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d_grad_grad,
                           phi::DepthwiseConv2dGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d_grad,
                           phi::DepthwiseConv2dGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(determinant_grad, det_grad);
PD_REGISTER_ARG_MAPPING_FN(determinant_grad,
                           phi::DeterminantGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(diag_v2_grad, diag_grad);
PD_REGISTER_ARG_MAPPING_FN(diag_v2_grad, phi::DiagV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(diagonal_grad, phi::DiagonalGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(digamma_grad, phi::DigammaGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dist_grad, phi::DistGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dot_grad, phi::DotGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eig_grad, phi::EigGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigh_grad, phi::EighGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigvalsh_grad, phi::EigvalshGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elu_grad_grad, elu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(elu_grad_grad, phi::EluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(elu_grad, phi::EluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erf_grad, phi::ErfGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erfinv_grad, phi::ErfinvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(exp_grad, phi::ExpGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(expand_as_v2_grad, expand_as_grad);
PD_REGISTER_ARG_MAPPING_FN(expand_as_v2_grad,
                           phi::ExpandAsV2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(expand_v2_double_grad, expand_double_grad);
PD_REGISTER_ARG_MAPPING_FN(expm1_grad, phi::Expm1GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2c_grad, phi::FftC2cGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2r_grad, phi::FftC2rGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_r2c_grad, phi::FftR2cGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal_grad,
                           phi::FillDiagonalGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal_tensor_grad,
                           phi::FillDiagonalTensorGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(fill_any_grad, fill_grad);
PD_REGISTER_ARG_MAPPING_FN(fill_any_grad, phi::FillAnyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_grad,
                           phi::FlashAttnGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_unpadded_grad,
                           phi::FlashAttnUnpaddedGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(floor_grad, phi::FloorGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_fmax_grad, fmax_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_fmax_grad,
                           phi::ElementwiseFmaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_fmin_grad, fmin_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_fmin_grad,
                           phi::ElementwiseFminGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fold_grad, phi::FoldGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(frame_grad, phi::FrameGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_grad, phi::GatherGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_nd_grad, phi::GatherNdGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gaussian_inplace_grad,
                           phi::GaussianInplaceGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gelu_grad, phi::GeluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(grid_sampler_grad, grid_sample_grad);
PD_REGISTER_ARG_MAPPING_FN(grid_sampler_grad,
                           phi::GridSamplerGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(group_norm_grad,
                           phi::GroupNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gumbel_softmax_grad,
                           phi::GumbelSoftmaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_shrink_grad, hardshrink_grad);
PD_REGISTER_ARG_MAPPING_FN(hard_shrink_grad,
                           phi::HardShrinkGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_sigmoid_grad, hardsigmoid_grad);
PD_REGISTER_ARG_MAPPING_FN(hard_sigmoid_grad,
                           phi::HardSigmoidGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(brelu_grad, hardtanh_grad);
PD_REGISTER_ARG_MAPPING_FN(brelu_grad, phi::BreluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_heaviside_grad, heaviside_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_heaviside_grad,
                           phi::ElementwiseHeavisideGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(huber_loss_grad,
                           phi::HuberLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0_grad, phi::I0GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0e_grad, phi::I0eGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1_grad, phi::I1GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1e_grad, phi::I1eGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(identity_loss_grad,
                           phi::IdentityLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(imag_grad, phi::ImagGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_add_grad, phi::IndexAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_put_grad, phi::IndexPutGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_sample_grad,
                           phi::IndexSampleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select_grad,
                           phi::IndexSelectGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select_strided_grad,
                           phi::IndexSelectStridedGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(instance_norm_double_grad,
                           phi::InstanceNormDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(instance_norm_grad,
                           phi::InstanceNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(inverse_grad, phi::InverseGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kldiv_loss_grad,
                           phi::KldivLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kron_grad, phi::KronGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kthvalue_grad, phi::KthvalueGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(label_smooth_grad,
                           phi::LabelSmoothGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(layer_norm_grad,
                           phi::LayerNormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(leaky_relu_grad_grad, leaky_relu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(leaky_relu_grad_grad,
                           phi::LeakyReluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(leaky_relu_grad,
                           phi::LeakyReluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lerp_grad, phi::LerpGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lgamma_grad, phi::LgammaGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(linear_interp_v2_grad, linear_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(linear_interp_v2_grad,
                           phi::LinearInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log10_grad, phi::Log10GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log1p_grad, phi::Log1pGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log2_grad, phi::Log2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(log_grad_grad, log_double_grad);
PD_REGISTER_ARG_MAPPING_FN(log_grad_grad, phi::LogGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_grad, phi::LogGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_loss_grad, phi::LogLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_softmax_grad,
                           phi::LogSoftmaxGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logcumsumexp_grad,
                           phi::LogcumsumexpGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logit_grad, phi::LogitGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logsigmoid_grad,
                           phi::LogsigmoidGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu_grad, phi::LuGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu_unpack_grad, phi::LuUnpackGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(margin_cross_entropy_grad,
                           phi::MarginCrossEntropyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(masked_select_grad,
                           phi::MaskedSelectGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(matrix_power_grad,
                           phi::MatrixPowerGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool2d_with_index_grad,
                           phi::MaxPool2dWithIndexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool3d_with_index_grad,
                           phi::MaxPool3dWithIndexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(maxout_grad, phi::MaxoutGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(mean_grad, mean_all_grad);
PD_REGISTER_ARG_MAPPING_FN(mean_grad, phi::MeanGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(memory_efficient_attention_grad,
                           phi::MemoryEfficientAttentionGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(meshgrid_grad, phi::MeshgridGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mode_grad, phi::ModeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multi_dot_grad, phi::MultiDotGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multiplex_grad, phi::MultiplexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mv_grad, phi::MvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nanmedian_grad, phi::NanmedianGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(nearest_interp_v2_grad, nearest_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(nearest_interp_v2_grad,
                           phi::NearestInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nll_loss_grad, phi::NllLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(overlap_add_grad,
                           phi::OverlapAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(p_norm_grad, phi::PNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pad3d_double_grad,
                           phi::Pad3dDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pad3d_grad, phi::Pad3dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_shuffle_grad,
                           phi::PixelShuffleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_unshuffle_grad,
                           phi::PixelUnshuffleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(poisson_grad, phi::PoissonGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(polygamma_grad, phi::PolygammaGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow_double_grad,
                           phi::PowDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow_grad, phi::PowGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow_triple_grad,
                           phi::PowTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(prelu_grad, phi::PreluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(psroi_pool_grad,
                           phi::PsroiPoolGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(put_along_axis_grad,
                           phi::PutAlongAxisGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(qr_grad, phi::QrGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(real_grad, phi::RealGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(reciprocal_grad,
                           phi::ReciprocalGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu6_grad, phi::Relu6GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(relu_grad_grad, relu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(relu_grad_grad, phi::ReluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu_grad, phi::ReluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(renorm_grad, phi::RenormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_align_grad, phi::RoiAlignGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_pool_grad, phi::RoiPoolGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roll_grad, phi::RollGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(round_grad, phi::RoundGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(rsqrt_grad_grad, rsqrt_double_grad);
PD_REGISTER_ARG_MAPPING_FN(rsqrt_grad_grad,
                           phi::RsqrtGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rsqrt_grad, phi::RsqrtGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter_grad, phi::ScatterGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter_nd_add_grad,
                           phi::ScatterNdAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(segment_pool_grad,
                           phi::SegmentPoolGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(selu_grad, phi::SeluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_recv_grad, send_u_recv_grad);
PD_REGISTER_ARG_MAPPING_FN(graph_send_recv_grad,
                           phi::GraphSendRecvGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_ue_recv_grad, send_ue_recv_grad);
PD_REGISTER_ARG_MAPPING_FN(graph_send_ue_recv_grad,
                           phi::GraphSendUeRecvGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_uv_grad, send_uv_grad);
PD_REGISTER_ARG_MAPPING_FN(graph_send_uv_grad,
                           phi::GraphSendUvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(
    sigmoid_cross_entropy_with_logits_grad,
    phi::SigmoidCrossEntropyWithLogitsGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(sigmoid_grad_grad, sigmoid_double_grad);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_grad_grad,
                           phi::SigmoidGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_grad, phi::SigmoidGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_triple_grad,
                           phi::SigmoidTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(silu_grad, phi::SiluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin_double_grad,
                           phi::SinDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin_grad, phi::SinGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin_triple_grad,
                           phi::SinTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sinh_grad, phi::SinhGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(slogdeterminant_grad, slogdet_grad);
PD_REGISTER_ARG_MAPPING_FN(slogdeterminant_grad,
                           phi::SlogdeterminantGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softplus_double_grad,
                           phi::SoftplusDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softplus_grad, phi::SoftplusGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softshrink_grad,
                           phi::SoftshrinkGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softsign_grad, phi::SoftsignGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(solve_grad, phi::SolveGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(spectral_norm_grad,
                           phi::SpectralNormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(sqrt_grad_grad, sqrt_double_grad);
PD_REGISTER_ARG_MAPPING_FN(sqrt_grad_grad, phi::SqrtGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sqrt_grad, phi::SqrtGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(square_grad_grad, square_double_grad);
PD_REGISTER_ARG_MAPPING_FN(square_grad_grad,
                           phi::SquareGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(square_grad, phi::SquareGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(squared_l2_norm_grad,
                           phi::SquaredL2NormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(squeeze2_double_grad, squeeze_double_grad);
PD_REGISTER_BASE_KERNEL_NAME(squeeze2_grad, squeeze_grad);
PD_REGISTER_ARG_MAPPING_FN(squeeze2_grad, phi::Squeeze2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stack_grad, phi::StackGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stanh_grad, phi::StanhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(svd_grad, phi::SvdGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(take_along_axis_grad,
                           phi::TakeAlongAxisGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tan_grad, phi::TanGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(tanh_grad_grad, tanh_double_grad);
PD_REGISTER_ARG_MAPPING_FN(tanh_grad_grad, phi::TanhGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_grad, phi::TanhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_shrink_grad,
                           phi::TanhShrinkGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_triple_grad,
                           phi::TanhTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(temporal_shift_grad,
                           phi::TemporalShiftGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tensor_unfold_grad,
                           phi::TensorUnfoldGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(thresholded_relu_grad,
                           phi::ThresholdedReluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(top_k_v2_grad, topk_grad);
PD_REGISTER_ARG_MAPPING_FN(top_k_v2_grad, phi::TopKV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trace_grad, phi::TraceGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(triangular_solve_grad,
                           phi::TriangularSolveGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(trilinear_interp_v2_grad, trilinear_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(trilinear_interp_v2_grad,
                           phi::TrilinearInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trunc_grad, phi::TruncGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unfold_grad, phi::UnfoldGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(uniform_random_inplace_grad, uniform_inplace_grad);
PD_REGISTER_ARG_MAPPING_FN(uniform_random_inplace_grad,
                           phi::UniformRandomInplaceGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(unsqueeze2_double_grad, unsqueeze_double_grad);
PD_REGISTER_BASE_KERNEL_NAME(unsqueeze2_grad, unsqueeze_grad);
PD_REGISTER_ARG_MAPPING_FN(unsqueeze2_grad,
                           phi::Unsqueeze2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unstack_grad, phi::UnstackGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_dtype_grad,
                           phi::ViewDtypeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_shape_grad,
                           phi::ViewShapeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warpctc_grad, phi::WarpctcGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warprnnt_grad, phi::WarprnntGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_only_linear_grad,
                           phi::WeightOnlyLinearGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(where_grad, phi::WhereGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(yolov3_loss_grad, yolo_loss_grad);
PD_REGISTER_ARG_MAPPING_FN(yolov3_loss_grad,
                           phi::Yolov3LossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unpool3d_grad, phi::Unpool3dGradOpArgumentMapping);
