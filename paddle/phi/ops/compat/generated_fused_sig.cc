// Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

// this file is generated by paddle/phi/op/yaml/generator/generate_op.py, do not
// edit.
#include "paddle/phi/core/compat/op_utils.h"
#include "paddle/utils/small_vector.h"

namespace phi {

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddActXpuOpArgumentMapping:

return KernelSignature("add_act_xpu", {"x", "x_max", "y", "y_max"},
{"act_type"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature AddActXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "x_max", "y", "y_max"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("act_type");
  paddle::small_vector<const char*> outputs{"out", "out_max"};
  return KernelSignature(
      "add_act_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddLayernormXpuOpArgumentMapping:

return KernelSignature("add_layernorm_xpu", {"x", "y", "scale", "bias"},
{"begin_norm_axis", "epsilon"}, {"out"});
******************************************************************
*/

KernelSignature AddLayernormXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "y", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("add_layernorm_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddcmulXpuOpArgumentMapping:

return KernelSignature("addcmul_xpu", {"x", "y", "w"}, {}, {"out"});
******************************************************************
*/

KernelSignature AddcmulXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "y", "w"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "addcmul_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
BlockMultiheadAttentionOpArgumentMapping:

return KernelSignature("block_multihead_attention", {"qkv", "key_cache",
"value_cache", "seq_lens_encoder", "seq_lens_decoder", "seq_lens_this_time",
"padding_offsets", "cum_offsets", "cu_seqlens_q", "cu_seqlens_k",
"block_tables", "pre_key_cache", "pre_value_cache", "rope_emb", "mask",
"tgt_mask"}, {"max_seq_len", "block_size", "use_neox_style"}, {"fmha_out",
"qkv_out", "key_cache_out", "value_cache_out"});
******************************************************************
*/

KernelSignature BlockMultiheadAttentionOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"qkv",
                                           "key_cache",
                                           "value_cache",
                                           "seq_lens_encoder",
                                           "seq_lens_decoder",
                                           "seq_lens_this_time",
                                           "padding_offsets",
                                           "cum_offsets",
                                           "cu_seqlens_q",
                                           "cu_seqlens_k",
                                           "block_tables",
                                           "pre_key_cache",
                                           "pre_value_cache",
                                           "rope_emb",
                                           "mask",
                                           "tgt_mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_seq_len");
  attrs.emplace_back("block_size");
  attrs.emplace_back("use_neox_style");
  paddle::small_vector<const char*> outputs{
      "fmha_out", "qkv_out", "key_cache_out", "value_cache_out"};
  return KernelSignature("block_multihead_attention",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BnActXpuOpArgumentMapping:

return KernelSignature("bn_act_xpu", {"x", "mean", "variance", "scale", "bias"},
{"momentum", "epsilon", "data_layout", "act_type"}, {"out"});
******************************************************************
*/

KernelSignature BnActXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "mean", "variance", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("momentum");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("data_layout");
  attrs.emplace_back("act_type");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature(
      "bn_act_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv1dXpuOpArgumentMapping:

return KernelSignature("conv1d_xpu", {"x", "x_max", "filter", "filter_max",
"bias", "branch", "branch_max"}, {"paddings", "padding_algorithm", "dilations",
"strides", "groups", "act_type", "act_param"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature Conv1dXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("groups");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  paddle::small_vector<const char*> outputs{"out", "out_max"};
  return KernelSignature(
      "conv1d_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dTransposeXpuOpArgumentMapping:

return KernelSignature("conv2d_transpose_xpu", {"x", "x_max", "filter",
"filter_max", "bias"}, {"strides", "paddings", "output_padding", "output_size",
"padding_algorithm", "groups", "dilations", "data_format", "has_bias",
"with_act", "act_type"}, {"out", "out_max"}); return
KernelSignature("conv2d_transpose_xpu", {"x", "x_max", "filter", "filter_max",
"bias"}, {"strides", "paddings", "output_padding", "OutputSizeTensor",
"padding_algorithm", "groups", "dilations", "data_format", "has_bias",
"with_act", "act_type"}, {"out", "out_max"}); return
KernelSignature("conv2d_transpose_xpu", {"x", "x_max", "filter", "filter_max",
"bias"}, {"strides", "paddings", "output_padding", "OutputSizeTensorList",
"padding_algorithm", "groups", "dilations", "data_format", "has_bias",
"with_act", "act_type"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature Conv2dTransposeXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "x_max", "filter", "filter_max", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back(ctx.HasInput("OutputSizeTensor") ? "OutputSizeTensor"
                     : ctx.InputSize("OutputSizeTensorList") > 0
                         ? "OutputSizeTensorList"
                         : "output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  attrs.emplace_back("has_bias");
  attrs.emplace_back("with_act");
  attrs.emplace_back("act_type");
  paddle::small_vector<const char*> outputs{"out", "out_max"};
  return KernelSignature("conv2d_transpose_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dXpuOpArgumentMapping:

return KernelSignature("conv2d_xpu", {"x", "x_max", "filter", "filter_max",
"bias", "branch", "branch_max", "scale_max", "out_max_in"}, {"paddings",
"dilations", "strides", "padding_algorithm", "groups", "act_type", "act_param",
"out_dtype"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature Conv2dXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "x_max",
                                           "filter",
                                           "filter_max",
                                           "bias",
                                           "branch",
                                           "branch_max",
                                           "scale_max",
                                           "out_max_in"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs{"out", "out_max"};
  return KernelSignature(
      "conv2d_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DequantizeXpuOpArgumentMapping:

return KernelSignature("dequantize_xpu", {"x"}, {"out_dtype", "scale"}, {"y"});
******************************************************************
*/

KernelSignature DequantizeXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("out_dtype");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs{"y"};
  return KernelSignature("dequantize_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
EmbeddingWithEltwiseAddXpuOpArgumentMapping:

return KernelSignature("embedding_with_eltwise_add_xpu", {"ids", "tables",
"mask"}, {"padding_idx"}, {"out", "seq_lod", "max_seq_len"});
******************************************************************
*/

KernelSignature EmbeddingWithEltwiseAddXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"ids", "tables", "mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("padding_idx");
  paddle::small_vector<const char*> outputs{"out", "seq_lod", "max_seq_len"};
  return KernelSignature("embedding_with_eltwise_add_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FastLayernormXpuOpArgumentMapping:

return KernelSignature("fast_layernorm_xpu", {"x", "scale", "bias"},
{"begin_norm_axis", "epsilon"}, {"out"});
******************************************************************
*/

KernelSignature FastLayernormXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("fast_layernorm_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FastWhereXpuOpArgumentMapping:

return KernelSignature("fast_where_xpu", {"condition", "x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature FastWhereXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"condition", "x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("fast_where_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FcOpArgumentMapping:

return KernelSignature("fc", {"Input", "W", "Bias"}, {"in_num_col_dims",
"activation_type", "use_mkldnn", "padding_weights", "use_quantizer",
"mkldnn_data_type", "Scale_in", "Scale_weights", "Scale_out",
"force_fp32_output"}, {"Out"});
******************************************************************
*/

KernelSignature FcOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "W", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("in_num_col_dims");
  attrs.emplace_back("activation_type");
  attrs.emplace_back("use_mkldnn");
  attrs.emplace_back("padding_weights");
  attrs.emplace_back("use_quantizer");
  attrs.emplace_back("mkldnn_data_type");
  attrs.emplace_back("Scale_in");
  attrs.emplace_back("Scale_weights");
  attrs.emplace_back("Scale_out");
  attrs.emplace_back("force_fp32_output");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature(
      "fc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FcXpuOpArgumentMapping:

return KernelSignature("fc_xpu", {"x", "x_max", "w", "w_max", "bias",
"scale_max", "out_max_in"}, {"in_num_col_dims", "transpose_x", "alpha", "beta",
"act_type", "act_alpha", "out_dtype"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature FcXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "x_max", "w", "w_max", "bias", "scale_max", "out_max_in"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("in_num_col_dims");
  attrs.emplace_back("transpose_x");
  attrs.emplace_back("alpha");
  attrs.emplace_back("beta");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_alpha");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs{"out", "out_max"};
  return KernelSignature(
      "fc_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedBiasActOpArgumentMapping:

return KernelSignature("fused_bias_act", {"x", "bias", "dequant_scales",
"shift", "smooth"}, {"act_method", "compute_dtype", "quant_scale",
"quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out"});
******************************************************************
*/

KernelSignature FusedBiasActOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "bias", "dequant_scales", "shift", "smooth"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("act_method");
  attrs.emplace_back("compute_dtype");
  attrs.emplace_back("quant_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("fused_bias_act",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedBiasDropoutResidualLayerNormOpArgumentMapping:

return KernelSignature("fused_bias_dropout_residual_layer_norm", {"X",
"Residual", "Bias", "LnScale", "LnBias"}, {"dropout_rate", "is_test",
"dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"},
{"Y", "BiasDropoutResidualOut", "DropoutMaskOut", "LnMean", "LnVariance"});
******************************************************************
*/

KernelSignature FusedBiasDropoutResidualLayerNormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "Residual", "Bias", "LnScale", "LnBias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_fix_seed");
  attrs.emplace_back("dropout_seed");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("ln_epsilon");
  paddle::small_vector<const char*> outputs{
      "Y", "BiasDropoutResidualOut", "DropoutMaskOut", "LnMean", "LnVariance"};
  return KernelSignature("fused_bias_dropout_residual_layer_norm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedBiasResidualLayernormOpArgumentMapping:

return KernelSignature("fused_bias_residual_layernorm", {"x", "bias",
"residual", "norm_weight", "norm_bias"}, {"epsilon", "residual_alpha",
"begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound",
"quant_min_bound"}, {"out", "residual_out", "mean", "variance"});
******************************************************************
*/

KernelSignature FusedBiasResidualLayernormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "bias", "residual", "norm_weight", "norm_bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("residual_alpha");
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("quant_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs{
      "out", "residual_out", "mean", "variance"};
  return KernelSignature("fused_bias_residual_layernorm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedConv2dAddActOpArgumentMapping:

return KernelSignature("fused_conv2d_add_act", {"Input", "Filter", "Bias",
"ResidualData"}, {"strides", "paddings", "padding_algorithm", "dilations",
"groups", "data_format", "activation", "split_channels", "exhaustive_search",
"workspace_size_MB", "fuse_alpha"}, {"Output", "Outputs"});
******************************************************************
*/

KernelSignature FusedConv2dAddActOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "Input", "Filter", "Bias", "ResidualData"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  attrs.emplace_back("activation");
  attrs.emplace_back("split_channels");
  attrs.emplace_back("exhaustive_search");
  attrs.emplace_back("workspace_size_MB");
  attrs.emplace_back("fuse_alpha");
  paddle::small_vector<const char*> outputs{"Output", "Outputs"};
  return KernelSignature("fused_conv2d_add_act",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDconvDreluDbnOpArgumentMapping:

return KernelSignature("fused_dconv_drelu_dbn", {"grad_output", "weight",
"grad_output_add", "residual_input", "bn1_eqscale", "bn1_eqbias", "conv_input",
"bn1_mean", "bn1_inv_std", "bn1_gamma", "bn1_beta", "bn1_input", "bn2_mean",
"bn2_inv_std", "bn2_gamma", "bn2_beta", "bn2_input"}, {"paddings", "dilations",
"strides", "padding_algorithm", "groups", "data_format", "fuse_shortcut",
"fuse_dual", "fuse_add", "exhaustive_search"}, {"grad_weight", "grad_bn1_input",
"grad_bn1_gamma", "grad_bn1_beta", "grad_bn2_input", "grad_bn2_gamma",
"grad_bn2_beta"});
******************************************************************
*/

KernelSignature FusedDconvDreluDbnOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"grad_output",
                                           "weight",
                                           "grad_output_add",
                                           "residual_input",
                                           "bn1_eqscale",
                                           "bn1_eqbias",
                                           "conv_input",
                                           "bn1_mean",
                                           "bn1_inv_std",
                                           "bn1_gamma",
                                           "bn1_beta",
                                           "bn1_input",
                                           "bn2_mean",
                                           "bn2_inv_std",
                                           "bn2_gamma",
                                           "bn2_beta",
                                           "bn2_input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  attrs.emplace_back("fuse_shortcut");
  attrs.emplace_back("fuse_dual");
  attrs.emplace_back("fuse_add");
  attrs.emplace_back("exhaustive_search");
  paddle::small_vector<const char*> outputs{"grad_weight",
                                            "grad_bn1_input",
                                            "grad_bn1_gamma",
                                            "grad_bn1_beta",
                                            "grad_bn2_input",
                                            "grad_bn2_gamma",
                                            "grad_bn2_beta"};
  return KernelSignature("fused_dconv_drelu_dbn",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDropoutAddOpArgumentMapping:

return KernelSignature("fused_dropout_add", {"x", "y", "seed_tensor"}, {"p",
"is_test", "mode", "seed", "fix_seed"}, {"out", "seed_offset"}); return
KernelSignature("fused_dropout_add", {"x", "y", "seed_tensor"}, {"PTensor",
"is_test", "mode", "seed", "fix_seed"}, {"out", "seed_offset"});
******************************************************************
*/

KernelSignature FusedDropoutAddOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "y", "seed_tensor"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("PTensor") ? "PTensor" : "p");
  attrs.emplace_back("is_test");
  attrs.emplace_back("mode");
  attrs.emplace_back("seed");
  attrs.emplace_back("fix_seed");
  paddle::small_vector<const char*> outputs{"out", "seed_offset"};
  return KernelSignature("fused_dropout_add",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedEmbeddingEltwiseLayernormOpArgumentMapping:

return KernelSignature("fused_embedding_eltwise_layernorm", {"Ids", "Embs",
"Bias", "Scale"}, {"epsilon"}, {"Out"});
******************************************************************
*/

KernelSignature FusedEmbeddingEltwiseLayernormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Ids", "Embs", "Bias", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("fused_embedding_eltwise_layernorm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedFcElementwiseLayernormOpArgumentMapping:

return KernelSignature("fused_fc_elementwise_layernorm", {"X", "W", "Y",
"Bias0", "Scale", "Bias1"}, {"x_num_col_dims", "activation_type", "epsilon",
"begin_norm_axis"}, {"Out", "Mean", "Variance"});
******************************************************************
*/

KernelSignature FusedFcElementwiseLayernormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "W", "Y", "Bias0", "Scale", "Bias1"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("x_num_col_dims");
  attrs.emplace_back("activation_type");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs{"Out", "Mean", "Variance"};
  return KernelSignature("fused_fc_elementwise_layernorm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedLinearParamGradAddOpArgumentMapping:

return KernelSignature("fused_linear_param_grad_add", {"x", "dout", "dweight",
"dbias"}, {"multi_precision", "has_bias"}, {"dweight_out", "dbias_out"});
******************************************************************
*/

KernelSignature FusedLinearParamGradAddOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "dout", "dweight", "dbias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("has_bias");
  paddle::small_vector<const char*> outputs{"dweight_out", "dbias_out"};
  return KernelSignature("fused_linear_param_grad_add",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedMultiTransformerInt8XpuOpArgumentMapping:

return KernelSignature("fused_multi_transformer_int8_xpu", {"x", "ln_scale",
"ln_bias", "qkv_in_max", "qkvw", "qkv_bias", "qkv_scales", "out_linear_in_max",
"out_linear_w", "out_linear_bias", "out_linear_scales", "ffn_ln_scale",
"ffn_ln_bias", "ffn1_in_max", "ffn1_weight", "ffn1_bias", "ffn1_scales",
"ffn2_in_max", "ffn2_weight", "ffn2_bias", "ffn2_scales", "cache_kv",
"pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask",
"gather_index", "max_buffer"}, {"pre_layer_norm", "rotary_emb_dims", "epsilon",
"dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw",
"ring_id", "gather_axis"}, {"out", "cache_kv_out"});
******************************************************************
*/

KernelSignature FusedMultiTransformerInt8XpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "ln_scale",
                                           "ln_bias",
                                           "qkv_in_max",
                                           "qkvw",
                                           "qkv_bias",
                                           "qkv_scales",
                                           "out_linear_in_max",
                                           "out_linear_w",
                                           "out_linear_bias",
                                           "out_linear_scales",
                                           "ffn_ln_scale",
                                           "ffn_ln_bias",
                                           "ffn1_in_max",
                                           "ffn1_weight",
                                           "ffn1_bias",
                                           "ffn1_scales",
                                           "ffn2_in_max",
                                           "ffn2_weight",
                                           "ffn2_bias",
                                           "ffn2_scales",
                                           "cache_kv",
                                           "pre_caches",
                                           "rotary_pos_emb",
                                           "time_step",
                                           "seq_lengths",
                                           "src_mask",
                                           "gather_index",
                                           "max_buffer"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pre_layer_norm");
  attrs.emplace_back("rotary_emb_dims");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("act_method");
  attrs.emplace_back("trans_qkvw");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("gather_axis");
  paddle::small_vector<const char*> outputs{"out", "cache_kv_out"};
  return KernelSignature("fused_multi_transformer_int8_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedMultiTransformerXpuOpArgumentMapping:

return KernelSignature("fused_multi_transformer_xpu", {"x", "ln_scale",
"ln_bias", "qkvw", "qkvw_max", "qkv_bias", "out_linear_w", "out_linear_wmax",
"out_linear_bias", "ffn_ln_scale", "ffn_ln_bias", "ffn1_weight",
"ffn1_weight_max", "ffn1_bias", "ffn2_weight", "ffn2_weight_max", "ffn2_bias",
"cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths",
"src_mask", "gather_index", "max_buffer"}, {"pre_layer_norm", "rotary_emb_dims",
"epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method",
"trans_qkvw", "ring_id", "gather_axis"}, {"out", "cache_kv_out"});
******************************************************************
*/

KernelSignature FusedMultiTransformerXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "ln_scale",
                                           "ln_bias",
                                           "qkvw",
                                           "qkvw_max",
                                           "qkv_bias",
                                           "out_linear_w",
                                           "out_linear_wmax",
                                           "out_linear_bias",
                                           "ffn_ln_scale",
                                           "ffn_ln_bias",
                                           "ffn1_weight",
                                           "ffn1_weight_max",
                                           "ffn1_bias",
                                           "ffn2_weight",
                                           "ffn2_weight_max",
                                           "ffn2_bias",
                                           "cache_kv",
                                           "pre_caches",
                                           "rotary_pos_emb",
                                           "time_step",
                                           "seq_lengths",
                                           "src_mask",
                                           "gather_index",
                                           "max_buffer"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pre_layer_norm");
  attrs.emplace_back("rotary_emb_dims");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("act_method");
  attrs.emplace_back("trans_qkvw");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("gather_axis");
  paddle::small_vector<const char*> outputs{"out", "cache_kv_out"};
  return KernelSignature("fused_multi_transformer_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedRotaryPositionEmbeddingOpArgumentMapping:

return KernelSignature("fused_rotary_position_embedding", {"q", "k", "v", "sin",
"cos", "position_ids"}, {"use_neox_rotary_style"}, {"out_q", "out_k", "out_v"});
******************************************************************
*/

KernelSignature FusedRotaryPositionEmbeddingOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "q", "k", "v", "sin", "cos", "position_ids"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("use_neox_rotary_style");
  paddle::small_vector<const char*> outputs{"out_q", "out_k", "out_v"};
  return KernelSignature("fused_rotary_position_embedding",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedScaleBiasAddReluOpArgumentMapping:

return KernelSignature("fused_scale_bias_add_relu", {"x1", "scale1", "bias1",
"x2", "scale2", "bias2"}, {"fuse_dual", "exhaustive_search"}, {"out"});
******************************************************************
*/

KernelSignature FusedScaleBiasAddReluOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x1", "scale1", "bias1", "x2", "scale2", "bias2"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("fuse_dual");
  attrs.emplace_back("exhaustive_search");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("fused_scale_bias_add_relu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedScaleBiasReluConvBnOpArgumentMapping:

return KernelSignature("fused_scale_bias_relu_conv_bn", {"x", "w", "scale",
"bias", "bn_scale", "bn_bias", "input_running_mean", "input_running_var"},
{"paddings", "dilations", "strides", "padding_algorithm", "groups",
"data_format", "momentum", "epsilon", "fuse_prologue", "exhaustive_search",
"accumulation_count"}, {"out", "out_running_mean", "out_running_var",
"saved_mean", "saved_var", "eq_scale", "eq_bias"});
******************************************************************
*/

KernelSignature FusedScaleBiasReluConvBnOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "w",
                                           "scale",
                                           "bias",
                                           "bn_scale",
                                           "bn_bias",
                                           "input_running_mean",
                                           "input_running_var"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  attrs.emplace_back("momentum");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("fuse_prologue");
  attrs.emplace_back("exhaustive_search");
  attrs.emplace_back("accumulation_count");
  paddle::small_vector<const char*> outputs{"out",
                                            "out_running_mean",
                                            "out_running_var",
                                            "saved_mean",
                                            "saved_var",
                                            "eq_scale",
                                            "eq_bias"};
  return KernelSignature("fused_scale_bias_relu_conv_bn",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionGruOpArgumentMapping:

return KernelSignature("fusion_gru", {"X", "H0", "WeightX", "WeightH", "Bias"},
{"activation", "gate_activation", "is_reverse", "use_seq", "origin_mode",
"use_mkldnn", "mkldnn_data_type", "Scale_data", "Shift_data", "Scale_weights",
"force_fp32_output"}, {"ReorderedH0", "XX", "BatchedInput", "BatchedOut",
"Hidden"});
******************************************************************
*/

KernelSignature FusionGruOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "X", "H0", "WeightX", "WeightH", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("activation");
  attrs.emplace_back("gate_activation");
  attrs.emplace_back("is_reverse");
  attrs.emplace_back("use_seq");
  attrs.emplace_back("origin_mode");
  attrs.emplace_back("use_mkldnn");
  attrs.emplace_back("mkldnn_data_type");
  attrs.emplace_back("Scale_data");
  attrs.emplace_back("Shift_data");
  attrs.emplace_back("Scale_weights");
  attrs.emplace_back("force_fp32_output");
  paddle::small_vector<const char*> outputs{
      "ReorderedH0", "XX", "BatchedInput", "BatchedOut", "Hidden"};
  return KernelSignature(
      "fusion_gru", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionRepeatedFcReluOpArgumentMapping:

return KernelSignature("fusion_repeated_fc_relu", {"X", "W", "Bias"}, {},
{"ReluOut", "Out"});
******************************************************************
*/

KernelSignature FusionRepeatedFcReluOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "W", "Bias"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs{"ReluOut", "Out"};
  return KernelSignature("fusion_repeated_fc_relu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusionSeqconvEltaddReluOpArgumentMapping:

return KernelSignature("fusion_seqconv_eltadd_relu", {"X", "Filter", "Bias"},
{"contextLength", "contextStart", "contextStride"}, {"Out", "ColMat"});
******************************************************************
*/

KernelSignature FusionSeqconvEltaddReluOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Filter", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("contextLength");
  attrs.emplace_back("contextStart");
  attrs.emplace_back("contextStride");
  paddle::small_vector<const char*> outputs{"Out", "ColMat"};
  return KernelSignature("fusion_seqconv_eltadd_relu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusionSeqexpandConcatFcOpArgumentMapping:

return KernelSignature("fusion_seqexpand_concat_fc", {"X", "FCWeight",
"FCBias"}, {"fc_activation"}, {"Out", "FCOut"});
******************************************************************
*/

KernelSignature FusionSeqexpandConcatFcOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "FCWeight", "FCBias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("fc_activation");
  paddle::small_vector<const char*> outputs{"Out", "FCOut"};
  return KernelSignature("fusion_seqexpand_concat_fc",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionSquaredMatSubOpArgumentMapping:

return KernelSignature("fusion_squared_mat_sub", {"X", "Y"}, {"scalar"},
{"SquaredX", "SquaredY", "SquaredXY", "Out"});
******************************************************************
*/

KernelSignature FusionSquaredMatSubOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scalar");
  paddle::small_vector<const char*> outputs{
      "SquaredX", "SquaredY", "SquaredXY", "Out"};
  return KernelSignature("fusion_squared_mat_sub",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusionTransposeFlattenConcatOpArgumentMapping:

return KernelSignature("fusion_transpose_flatten_concat", {"X"}, {"trans_axis",
"flatten_axis", "concat_axis"}, {"Out"});
******************************************************************
*/

KernelSignature FusionTransposeFlattenConcatOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("trans_axis");
  attrs.emplace_back("flatten_axis");
  attrs.emplace_back("concat_axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("fusion_transpose_flatten_concat",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GenerateSequenceXpuOpArgumentMapping:

return KernelSignature("generate_sequence_xpu", {"x"}, {"dtype"}, {"out"});
******************************************************************
*/

KernelSignature GenerateSequenceXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("generate_sequence_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LayerNormActXpuOpArgumentMapping:

return KernelSignature("layer_norm_act_xpu", {"x", "scale", "bias"},
{"begin_norm_axis", "epsilon", "act_type", "act_param"}, {"out"});
******************************************************************
*/

KernelSignature LayerNormActXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("layer_norm_act_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiEncoderXpuOpArgumentMapping:

return KernelSignature("multi_encoder_xpu", {"x", "fc_weight", "fc_weight_max",
"fc_bias", "ln_scale", "ln_bias", "mask", "seq_lod", "max_seq_len"},
{"layer_num", "norm_before", "hidden_dim", "head_num", "size_per_head",
"ffn_hidden_dim_scale", "act_type", "relative_type", "slice_idx"}, {"out",
"x_fp16", "out_fp16"});
******************************************************************
*/

KernelSignature MultiEncoderXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x",
                                           "fc_weight",
                                           "fc_weight_max",
                                           "fc_bias",
                                           "ln_scale",
                                           "ln_bias",
                                           "mask",
                                           "seq_lod",
                                           "max_seq_len"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("layer_num");
  attrs.emplace_back("norm_before");
  attrs.emplace_back("hidden_dim");
  attrs.emplace_back("head_num");
  attrs.emplace_back("size_per_head");
  attrs.emplace_back("ffn_hidden_dim_scale");
  attrs.emplace_back("act_type");
  attrs.emplace_back("relative_type");
  attrs.emplace_back("slice_idx");
  paddle::small_vector<const char*> outputs{"out", "x_fp16", "out_fp16"};
  return KernelSignature("multi_encoder_xpu",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiheadMatmulOpArgumentMapping:

return KernelSignature("multihead_matmul", {"Input", "W", "Bias", "BiasQK"},
{"transpose_Q", "transpose_K", "transpose_V", "alpha", "head_number"}, {"Out"});
******************************************************************
*/

KernelSignature MultiheadMatmulOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Input", "W", "Bias", "BiasQK"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("transpose_Q");
  attrs.emplace_back("transpose_K");
  attrs.emplace_back("transpose_V");
  attrs.emplace_back("alpha");
  attrs.emplace_back("head_number");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("multihead_matmul",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QuantizeXpuOpArgumentMapping:

return KernelSignature("quantize_xpu", {"x"}, {"out_dtype", "scale"}, {"y"});
******************************************************************
*/

KernelSignature QuantizeXpuOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("out_dtype");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs{"y"};
  return KernelSignature(
      "quantize_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SelfDpAttentionOpArgumentMapping:

return KernelSignature("self_dp_attention", {"X"}, {"alpha", "head_number"},
{"Out"});
******************************************************************
*/

KernelSignature SelfDpAttentionOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  attrs.emplace_back("head_number");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("self_dp_attention",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SkipLayernormOpArgumentMapping:

return KernelSignature("skip_layernorm", {"X", "Y", "Scale", "Bias"},
{"epsilon", "begin_norm_axis"}, {"Out"});
******************************************************************
*/

KernelSignature SkipLayernormOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"X", "Y", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs{"Out"};
  return KernelSignature("skip_layernorm",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
SqueezeExcitationBlockOpArgumentMapping:

return KernelSignature("squeeze_excitation_block", {"x", "filter", "filter_max",
"bias", "branch"}, {"act_type", "act_param", "filter_dims"}, {"out"});
******************************************************************
*/

KernelSignature SqueezeExcitationBlockOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "filter", "filter_max", "bias", "branch"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  attrs.emplace_back("filter_dims");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("squeeze_excitation_block",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
VariableLengthMemoryEfficientAttentionOpArgumentMapping:

return KernelSignature("variable_length_memory_efficient_attention", {"query",
"key", "value", "seq_lens", "kv_seq_lens", "mask"}, {"scale", "causal",
"pre_cache_length"}, {"out"});
******************************************************************
*/

KernelSignature VariableLengthMemoryEfficientAttentionOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "query", "key", "value", "seq_lens", "kv_seq_lens", "mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale");
  attrs.emplace_back("causal");
  attrs.emplace_back("pre_cache_length");
  paddle::small_vector<const char*> outputs{"out"};
  return KernelSignature("variable_length_memory_efficient_attention",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloBoxXpuOpArgumentMapping:

return KernelSignature("yolo_box_xpu", {"x", "x_max", "grid", "stride",
"anchor_grid"}, {"offset"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature YoloBoxXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "x", "x_max", "grid", "stride", "anchor_grid"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs{"out", "out_max"};
  return KernelSignature(
      "yolo_box_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedBiasDropoutResidualLayerNormGradOpArgumentMapping:

return KernelSignature("fused_bias_dropout_residual_layer_norm_grad", {"Y@GRAD",
"X", "Residual", "Bias", "LnScale", "LnBias", "LnMean", "LnVariance",
"BiasDropoutResidualOut", "DropoutMaskOut"}, {"dropout_rate", "is_test",
"dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"},
{"X@GRAD", "Residual@GRAD", "Bias@GRAD", "LnScale@GRAD", "LnBias@GRAD"});
******************************************************************
*/

KernelSignature FusedBiasDropoutResidualLayerNormGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"Y@GRAD",
                                           "X",
                                           "Residual",
                                           "Bias",
                                           "LnScale",
                                           "LnBias",
                                           "LnMean",
                                           "LnVariance",
                                           "BiasDropoutResidualOut",
                                           "DropoutMaskOut"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_fix_seed");
  attrs.emplace_back("dropout_seed");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("ln_epsilon");
  paddle::small_vector<const char*> outputs{
      "X@GRAD", "Residual@GRAD", "Bias@GRAD", "LnScale@GRAD", "LnBias@GRAD"};
  return KernelSignature("fused_bias_dropout_residual_layer_norm_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDropoutAddGradOpArgumentMapping:

return KernelSignature("fused_dropout_add_grad", {"seed_offset", "out@GRAD"},
{"p", "is_test", "mode", "fix_seed"}, {"x@GRAD", "y@GRAD"}); return
KernelSignature("fused_dropout_add_grad", {"seed_offset", "out@GRAD"},
{"PTensor", "is_test", "mode", "fix_seed"}, {"x@GRAD", "y@GRAD"});
******************************************************************
*/

KernelSignature FusedDropoutAddGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{"seed_offset", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("PTensor") ? "PTensor" : "p");
  attrs.emplace_back("is_test");
  attrs.emplace_back("mode");
  attrs.emplace_back("fix_seed");
  paddle::small_vector<const char*> outputs{"x@GRAD", "y@GRAD"};
  return KernelSignature("fused_dropout_add_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by
FusedRotaryPositionEmbeddingGradOpArgumentMapping:

return KernelSignature("fused_rotary_position_embedding_grad", {"sin", "cos",
"position_ids", "out_q@GRAD", "out_k@GRAD", "out_v@GRAD"},
{"use_neox_rotary_style"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FusedRotaryPositionEmbeddingGradOpArgumentMapping(
    const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs{
      "sin", "cos", "position_ids", "out_q@GRAD", "out_k@GRAD", "out_v@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("use_neox_rotary_style");
  paddle::small_vector<const char*> outputs{"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("fused_rotary_position_embedding_grad",
                         std::move(inputs),
                         std::move(attrs),
                         std::move(outputs));
}

}  // namespace phi

PD_REGISTER_ARG_MAPPING_FN(add_act_xpu, phi::AddActXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(add_layernorm_xpu,
                           phi::AddLayernormXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(addcmul_xpu, phi::AddcmulXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(block_multihead_attention,
                           phi::BlockMultiheadAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bn_act_xpu, phi::BnActXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv1d_xpu, phi::Conv1dXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_transpose_xpu,
                           phi::Conv2dTransposeXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_xpu, phi::Conv2dXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dequantize_xpu, phi::DequantizeXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(embedding_with_eltwise_add_xpu,
                           phi::EmbeddingWithEltwiseAddXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fast_layernorm_xpu,
                           phi::FastLayernormXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fast_where_xpu, phi::FastWhereXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fc, phi::FcOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fc_xpu, phi::FcXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_bias_act, phi::FusedBiasActOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(
    fused_bias_dropout_residual_layer_norm,
    phi::FusedBiasDropoutResidualLayerNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_bias_residual_layernorm,
                           phi::FusedBiasResidualLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_conv2d_add_act,
                           phi::FusedConv2dAddActOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dconv_drelu_dbn,
                           phi::FusedDconvDreluDbnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dropout_add,
                           phi::FusedDropoutAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(
    fused_embedding_eltwise_layernorm,
    phi::FusedEmbeddingEltwiseLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_fc_elementwise_layernorm,
                           phi::FusedFcElementwiseLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_linear_param_grad_add,
                           phi::FusedLinearParamGradAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_multi_transformer_int8_xpu,
                           phi::FusedMultiTransformerInt8XpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_multi_transformer_xpu,
                           phi::FusedMultiTransformerXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_rotary_position_embedding,
                           phi::FusedRotaryPositionEmbeddingOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_scale_bias_add_relu,
                           phi::FusedScaleBiasAddReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_scale_bias_relu_conv_bn,
                           phi::FusedScaleBiasReluConvBnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_gru, phi::FusionGruOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_repeated_fc_relu,
                           phi::FusionRepeatedFcReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_seqconv_eltadd_relu,
                           phi::FusionSeqconvEltaddReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_seqexpand_concat_fc,
                           phi::FusionSeqexpandConcatFcOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_squared_mat_sub,
                           phi::FusionSquaredMatSubOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_transpose_flatten_concat,
                           phi::FusionTransposeFlattenConcatOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(generate_sequence_xpu,
                           phi::GenerateSequenceXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(layer_norm_act_xpu,
                           phi::LayerNormActXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multi_encoder_xpu,
                           phi::MultiEncoderXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multihead_matmul,
                           phi::MultiheadMatmulOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(quantize_xpu, phi::QuantizeXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(self_dp_attention,
                           phi::SelfDpAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(skip_layernorm, phi::SkipLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(squeeze_excitation_block,
                           phi::SqueezeExcitationBlockOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(
    variable_length_memory_efficient_attention,
    phi::VariableLengthMemoryEfficientAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(yolo_box_xpu, phi::YoloBoxXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(
    fused_bias_dropout_residual_layer_norm_grad,
    phi::FusedBiasDropoutResidualLayerNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dropout_add_grad,
                           phi::FusedDropoutAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(
    fused_rotary_position_embedding_grad,
    phi::FusedRotaryPositionEmbeddingGradOpArgumentMapping);
