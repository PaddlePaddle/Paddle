# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys

# the cutlass python script path
sys.path.append(str(sys.argv[1]))

from conv2d_operation import *
from library import *

# this is a file's header part

cba_header = '''
// Generated by conv2d_bias_act.py - Do not edit.

#include <mutex>
#include "cutlass/conv/kernel/default_conv2d_fprop.h"
#include "cutlass/epilogue/thread/linear_combination_silu.h"
#include "cutlass/epilogue/thread/linear_combination_bias_relu.h"
#include "paddle/phi/kernels/fusion/cutlass/conv2d/conv2d_util.h"

namespace phi {
namespace fusion {
namespace cutlass_internal {
'''

# This is a cutlass kernel, will be many these like kernels

cba_kernel = '''
cutlass::Status ${kernel_function_name}(ConvAllParams params) {
  ${kenel_tmplate_instantiate}
  using ImplicitGemm =
      cutlass::conv::device::ImplicitGemmConvolution<${kernel_sig}_base>;

  const half *input = params.input;
  const half *weight = params.weight;
  const half *bias = params.bias;
  half *output = params.output;
  int batch = params.batch;
  int ic = params.ic;
  int ih = params.ih;
  int iw = params.iw;
  int kh = params.kh;
  int kw = params.kw;
  int oc = params.oc;
  int pad_h0 = params.pad_h0;
  int pad_w0 = params.pad_w0;
  int stride_h = params.stride_h;
  int stride_w = params.stride_w;

  int oh = params.oh;
  int ow = params.ow;
  int dilation_h = params.dilation_h;
  int dilation_w = params.dilation_w;

  cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation;
  cutlass::conv::Conv2dProblemSize problem_size({batch, ih, iw, ic},
                                                {oc, kh, kw, ic},
                                                {pad_h0, 0, pad_w0, 0},
                                                {stride_h, stride_w},
                                                {dilation_h, dilation_w},
                                                {batch, oh, ow, oc},
                                                mode,
                                                1);

  typename ImplicitGemm::Arguments arguments{
      problem_size,
      {(cutlass::half_t *)(input), {ic, ic * iw, ic * iw * ih}},
      {(cutlass::half_t *)(weight), {ic, ic * kw, ic * kw * kh}},
      {(cutlass::half_t *)(bias), {0, 0, 0}},
      {(cutlass::half_t *)(output), {oc, oc * ow, oc * ow * oh}},
      {1.f, 1.f}};

  ImplicitGemm implicit_gemm_op;
  size_t bytes = implicit_gemm_op.get_workspace_size(arguments);

  auto ctx = params.ctx;
  auto stream = ctx->stream();
  paddle::memory::allocation::AllocationPtr tmp_gpu_ptrs_data =
      paddle::memory::Alloc(
          ctx->GetPlace(),
          bytes,
          phi::Stream(reinterpret_cast<phi::StreamId>(stream)));
  void *workspace = tmp_gpu_ptrs_data->ptr();

  cutlass::Status status = implicit_gemm_op.can_implement(arguments);
  CUTLASS_CHECK(status);
  status = implicit_gemm_op.initialize(arguments, workspace);
  CUTLASS_CHECK(status);
  status = implicit_gemm_op(stream);
  CUTLASS_CHECK(status);
  return status;
}
'''


# conv2d_bias_act_function
# a function name is like such as conv2d_bias_silu_sm75
# it has many kernels, we should pick up a performence-best
# ${cba_name} is like conv2d_bias_silu_sm75
# ${enum_op_name} is like CONV2D_BIAS_SILU

cba_function = """
std::vector<std::function<cutlass::Status(ConvAllParams)>>
    ${cba_name}_all_func =  {${all_kernel_function_name}};

std::map<std::vector<int>, int> map_problem_${cba_name};
std::mutex ${cba_name}_mutex;

void ${cba_name}(ConvAllParams params) {
  int batch = params.batch;
  int ic = params.ic;
  int ih = params.ih;
  int iw = params.iw;
  int kh = params.kh;
  int kw = params.kw;
  int oc = params.oc;
  int pad_h0 = params.pad_h0;
  int pad_w0 = params.pad_w0;
  int stride_h = params.stride_h;
  int stride_w = params.stride_w;

  std::vector<int> problem_size = {
      batch, ic, ih, iw, kh, kw, oc, pad_h0, pad_w0, stride_h, stride_w};

  if (map_problem_${cba_name}.count(problem_size)) {
    ${cba_name}_all_func[map_problem_${cba_name}.at(problem_size)](
        params);
    return;
  }

  int best_config_index = ProfileToGetBestConfig(
      ${cba_name}_all_func, params, ${enum_op_name});

  std::lock_guard<std::mutex> guard(${cba_name}_mutex);

  map_problem_${cba_name}[problem_size] = best_config_index;
  ${cba_name}_all_func[best_config_index](params);
}
"""


# We should wrapper all op_name_with_sm_version into a function
# like : wrapper conv2d_bias_silu_sm75,  conv2d_bias_silu_sm80,  conv2d_bias_silu_sm86 into conv2d_bias_silu for phi kernel
# this function is invoked by phi kernel

cba_for_phi = """
void ${op_name}(ConvAllParams params) {
    ${dispatch_body}
}
"""


# this is a file's ending part

cba_tail = '''
}  // namespace cutlass_internal
}  // namespace fusion
}  // namespace phi
'''


class CbaAct(enum.Enum):
    Identity = enum_auto()
    Relu = enum_auto()
    Silu = enum_auto()


# EpilogueFunctorTag is from library.py

EpilogueFunctorTag[
    CbaAct.Identity
] = 'cutlass::epilogue::thread::LinearCombination'
EpilogueFunctorTag[
    CbaAct.Silu
] = 'cutlass::epilogue::thread::LinearCombinationSilu'
EpilogueFunctorTag[
    CbaAct.Relu
] = 'cutlass::epilogue::thread::LinearCombinationRelu'

UnderScoreName = {
    CbaAct.Identity: "conv2d_bias",
    CbaAct.Silu: "conv2d_bias_silu",
    CbaAct.Relu: "conv2d_bias_relu",
}

CamelName = {
    CbaAct.Identity: "Conv2dBias",
    CbaAct.Silu: "Conv2dBiasSilu",
    CbaAct.Relu: "Conv2dBiasRelu",
}

# some global variables used, now we only support these activations
epilogue_functors = [
    CbaAct.Identity,
    CbaAct.Relu,
    CbaAct.Silu,
]
# now we only support these algorithms
iterator_algorithms = [
    IteratorAlgorithm.Optimized,
    IteratorAlgorithm.Analytic,
]

# iterate over iterator_algorithms and tiles
# cba_name is like conv2d_bias_silu_sm75, it's added at the end of kernel_signature to make kernel function name unique
def generate_conv_kernels(
    conv_kind,
    iterator_algorithms,
    tiles,
    A,
    B,
    C,
    element_epilogue,
    stride_kind,
    epilogue_functor,
    swizzling_functor,
    cba_name,
):
    all_kernel_code = ""
    all_kernel_names = ""
    for iterator_algorithm in iterator_algorithms:
        for tile in tiles:
            conv2d_operation = Conv2dOperation(
                conv_kind,
                iterator_algorithm,
                tile.minimum_compute_capability,
                tile,
                A,
                B,
                C,
                element_epilogue,
                stride_kind,
                epilogue_functor,
                swizzling_functor,
            )
            kernel_dict = {}
            kernel_dict["kernel_sig"] = conv2d_operation.configuration_name()
            kernel_dict["kernel_function_name"] = (
                kernel_dict["kernel_sig"] + cba_name
            )
            kernel_dict[
                "kenel_tmplate_instantiate"
            ] = EmitConv2dInstance().emit(conv2d_operation)
            all_kernel_names += kernel_dict["kernel_function_name"] + ", \n"
            # Generate kernel code
            all_kernel_code += SubstituteTemplate(cba_kernel, kernel_dict)
    return (all_kernel_code, all_kernel_names)


def generate_sm75_1688():
    conv_kind = ConvKind.Fprop
    stride_kind = StrideSupport.Strided
    # alpha data type
    element_epilogue = DataType.f32
    min_cc = 75
    max_cc = 1024
    swizzling_functor = SwizzlingFunctor.Identity4
    layout = LayoutType.TensorNHWC

    math_instructions = [
        MathInstruction(
            [16, 8, 8],
            DataType.f16,
            DataType.f16,
            DataType.f32,
            OpcodeClass.TensorOp,
            MathOperation.multiply_add,
        ),
        MathInstruction(
            [16, 8, 8],
            DataType.f16,
            DataType.f16,
            DataType.f16,
            OpcodeClass.TensorOp,
            MathOperation.multiply_add,
        ),
    ]

    alignments = [8]

    sm75_code = ""
    for epilogue_functor in epilogue_functors:
        op_dict = {}
        op_dict["cba_name"] = UnderScoreName[epilogue_functor].lower() + "_sm75"
        op_dict["enum_op_name"] = UnderScoreName[epilogue_functor].upper()
        # for a op, we record all its kernels into a std::vector in C++ code
        all_kernel_names = ""
        for alignment in alignments:
            A = TensorDescription(DataType.f16, layout, alignment)
            B = TensorDescription(DataType.f16, layout, alignment)
            C = TensorDescription(DataType.f16, layout, alignment)
            for math_inst in math_instructions:
                tiles = [
                    TileDescription(
                        [64, 64, 64], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 32, 64], 2, [2, 1, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [128, 32, 64], 2, [4, 1, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [128, 64, 64], 2, [4, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 64, 32], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 128, 32], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 128, 64], 2, [1, 2, 2], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 256, 32], 2, [1, 4, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [128, 64, 32], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                ]

                # Generate kernel code
                # iterate over tils and iterator_algorithms
                return_tuple = generate_conv_kernels(
                    conv_kind,
                    iterator_algorithms,
                    tiles,
                    A,
                    B,
                    C,
                    element_epilogue,
                    stride_kind,
                    epilogue_functor,
                    swizzling_functor,
                    op_dict["cba_name"],
                )
                sm75_code += return_tuple[0]
                all_kernel_names += return_tuple[1]

        # Generate op code
        op_dict["all_kernel_function_name"] = all_kernel_names
        sm75_code += SubstituteTemplate(cba_function, op_dict)
    return sm75_code


# wrap different sm versions into a function
# now only support sm75
def generate_cba_for_phi():
    sm_versions = [
        "75",
    ]
    generated_code = ""
    dispatch_template = '''
    if (params.sm_version == ${sm_code})
    {
        ${op_name_with_sm}(params);
    }
    '''
    for epilogue_functor in epilogue_functors:
        dispatch_body = ""
        for sm_version in sm_versions:
            sm_dicts = {}
            sm_dicts["sm_code"] = sm_version
            sm_dicts["op_name_with_sm"] = (
                UnderScoreName[epilogue_functor].lower() + "_sm" + sm_version
            )
            dispatch_body += SubstituteTemplate(dispatch_template, sm_dicts)
        op_dicts = {}
        op_dicts["dispatch_body"] = dispatch_body
        op_dicts["op_name"] = CamelName[epilogue_functor]
        generated_code += SubstituteTemplate(cba_for_phi, op_dicts)
    return generated_code


if __name__ == "__main__":
    all_code = cba_header
    all_code += generate_sm75_1688()
    all_code += generate_cba_for_phi()
    all_code += cba_tail
    print(all_code)
