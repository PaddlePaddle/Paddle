# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys

# the cutlass python script path
sys.path.append(str(sys.argv[1]))

# this is a file's header part

cba_header = '''
// Generated by conv2d_bias_residual.py - Do not edit.

#include <mutex>
#include "cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h"
#include "cutlass/epilogue/thread/linear_combination_residual_block.h"
#include "paddle/phi/kernels/fusion/cutlass/conv2d/conv2d_util.h"

namespace phi {
namespace fusion {
namespace cutlass_internal {
'''

# This is a cutlass kernel, will be many these like kernels

cba_kernel = '''
cutlass::Status ${kernel_function_name}(ConvAllParams params) {
  ${kenel_tmplate_instantiate}
  using ImplicitGemm =
      cutlass::conv::device::ImplicitGemmConvolution<${kernel_sig}_base>;

  const half *input = params.input;
  const half *weight = params.weight;
  const half *bias = params.bias;
  half *output = params.output;
  int batch = params.batch;
  int ic = params.ic;
  int ih = params.ih;
  int iw = params.iw;
  int kh = params.kh;
  int kw = params.kw;
  int oc = params.oc;
  int pad_h0 = params.pad_h0;
  int pad_w0 = params.pad_w0;
  int stride_h = params.stride_h;
  int stride_w = params.stride_w;
  const half *residual = params.residual;

  int oh = params.oh;
  int ow = params.ow;
  int dilation_h = params.dilation_h;
  int dilation_w = params.dilation_w;

  cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation;
  cutlass::conv::Conv2dProblemSize problem_size(
      {batch, ih, iw, ic},
      {oc, kh, kw, ic},
      {pad_h0, 0, pad_w0, 0},
      {stride_h, stride_w},
      {dilation_h, dilation_w},
      {batch, oh, ow, oc},
      cutlass::conv::Mode::kCrossCorrelation,
      1);

  typename ImplicitGemm::Arguments arguments{
      problem_size,
      {(cutlass::half_t *)input, {ic, ic * iw, ic * iw * ih}},
      {(cutlass::half_t *)weight, {ic, ic * kw, ic * kw * kh}},
      {(cutlass::half_t *)residual, {oc, oc * ow, oc * ow * oh}},
      {(cutlass::half_t *)output, {oc, oc * ow, oc * ow * oh}},
      {1.f, 1.f},
      cutlass::conv::SplitKMode::kSerial,
      (cutlass::half_t *)(bias),
      nullptr,
      0,
      oc};

  ImplicitGemm implicit_gemm_op;
  size_t bytes = implicit_gemm_op.get_workspace_size(arguments);

  auto ctx = params.ctx;
  auto stream = ctx->stream();
  paddle::memory::allocation::AllocationPtr tmp_gpu_ptrs_data =
      paddle::memory::Alloc(
          ctx->GetPlace(),
          bytes,
          phi::Stream(reinterpret_cast<phi::StreamId>(stream)));
  void *workspace = tmp_gpu_ptrs_data->ptr();

  cutlass::Status status = implicit_gemm_op.can_implement(arguments);
  CUTLASS_CHECK(status);
  status = implicit_gemm_op.initialize(arguments, workspace);
  CUTLASS_CHECK(status);
  status = implicit_gemm_op(stream);
  CUTLASS_CHECK(status);
  return status;
}
'''


# conv2d_bias_act_function
# a function name is like such as conv2d_bias_silu_sm75
# it has many kernels, we should pick up a performence-best
# ${cba_name} is like conv2d_bias_silu_sm75
# ${enum_op_name} is like CONV2D_BIAS_SILU

cba_function = """
std::vector<std::function<cutlass::Status(ConvAllParams)>>
    ${cba_name}_all_func =  {${all_kernel_function_name}};

std::map<std::vector<int>, int> map_problem_${cba_name};
std::mutex ${cba_name}_mutex;

void ${cba_name}(ConvAllParams params) {
  int batch = params.batch;
  int ic = params.ic;
  int ih = params.ih;
  int iw = params.iw;
  int kh = params.kh;
  int kw = params.kw;
  int oc = params.oc;
  int pad_h0 = params.pad_h0;
  int pad_w0 = params.pad_w0;
  int stride_h = params.stride_h;
  int stride_w = params.stride_w;

  std::vector<int> problem_size = {
      batch, ic, ih, iw, kh, kw, oc, pad_h0, pad_w0, stride_h, stride_w};

  if (map_problem_${cba_name}.count(problem_size)) {
    ${cba_name}_all_func[map_problem_${cba_name}.at(problem_size)](
        params);
    return;
  }

  int best_config_index = ProfileToGetBestConfig(
      ${cba_name}_all_func, params, ${enum_op_name});

  std::lock_guard<std::mutex> guard(${cba_name}_mutex);

  map_problem_${cba_name}[problem_size] = best_config_index;
  ${cba_name}_all_func[best_config_index](params);
}
"""


# We should wrapper all op_name_with_sm_version into a function
# like : wrapper conv2d_bias_silu_sm75,  conv2d_bias_silu_sm80,  conv2d_bias_silu_sm86 into conv2d_bias_silu for phi kernel
# this function is invoked by phi kernel

cba_for_phi = """
void ${op_name}(ConvAllParams params) {
    ${dispatch_body}
}
"""


# this is a file's ending part

cba_tail = '''
}  // namespace cutlass_internal
}  // namespace fusion
}  // namespace phi
'''

import enum

from library import *


class Conv2dResidualOperation:
    #
    def __init__(
        self,
        conv_kind,
        iterator_algorithm,
        arch,
        tile_description,
        A,
        B,
        C,
        element_epilogue,
        stride_support,
        epilogue_functor_struct,
        swizzling_functor=SwizzlingFunctor.Identity1,
    ):

        self.operation_kind = OperationKind.Conv2d
        self.arch = arch
        self.tile_description = tile_description
        self.conv_kind = conv_kind
        self.A = A
        self.B = B
        self.C = C
        self.element_epilogue = element_epilogue
        self.epilogue_functor_struct = epilogue_functor_struct
        self.iterator_algorithm = iterator_algorithm
        self.stride_support = stride_support
        self.swizzling_functor = swizzling_functor

    #
    def is_complex(self):
        complex_operators = [
            MathOperation.multiply_add_complex,
            MathOperation.multiply_add_complex_gaussian,
        ]
        return (
            self.tile_description.math_instruction.math_operation
            in complex_operators
        )

    #
    def accumulator_type(self):
        accum = self.tile_description.math_instruction.element_accumulator

        if self.is_complex():
            return get_complex_from_real(accum)

        return accum

    #
    def core_name(self):
        '''The basic operation kind is prefixed with a letter indicating the accumulation type.'''

        intermediate_type = ''

        if (
            self.tile_description.math_instruction.opcode_class
            == OpcodeClass.TensorOp
        ):
            inst_shape = "%d%d%d" % tuple(
                self.tile_description.math_instruction.instruction_shape
            )
            if (
                self.tile_description.math_instruction.element_a
                != self.A.element
                and self.tile_description.math_instruction.element_a
                != self.accumulator_type()
            ):
                intermediate_type = DataTypeNames[
                    self.tile_description.math_instruction.element_a
                ]
        else:
            inst_shape = ''

        return "%s%s%s%s_%s" % (
            ShortDataTypeNames[self.accumulator_type()],
            inst_shape,
            intermediate_type,
            ConvKindNames[self.conv_kind],
            IteratorAlgorithmNames[self.iterator_algorithm],
        )

    #
    def extended_name(self):
        '''Append data types if they differ from compute type.'''
        if (
            self.C.element
            != self.tile_description.math_instruction.element_accumulator
            and self.A.element
            != self.tile_description.math_instruction.element_accumulator
        ):
            extended_name = "${element_c}_${core_name}_${element_a}"
        elif (
            self.C.element
            == self.tile_description.math_instruction.element_accumulator
            and self.A.element
            != self.tile_description.math_instruction.element_accumulator
        ):
            extended_name = "${core_name}_${element_a}"
        else:
            extended_name = "${core_name}"

        extended_name = SubstituteTemplate(
            extended_name,
            {
                'element_a': DataTypeNames[self.A.element],
                'element_c': DataTypeNames[self.C.element],
                'core_name': self.core_name(),
            },
        )

        return extended_name

    #
    def layout_name(self):
        return "%s" % (ShortLayoutTypeNames[self.A.layout])

    #
    def configuration_name(self):
        '''The full procedural name indicates architecture, extended name, tile size, and layout.'''

        opcode_class_name = OpcodeClassNames[
            self.tile_description.math_instruction.opcode_class
        ]

        threadblock = "%dx%d_%dx%d" % (
            self.tile_description.threadblock_shape[0],
            self.tile_description.threadblock_shape[1],
            self.tile_description.threadblock_shape[2],
            self.tile_description.stages,
        )

        if self.stride_support == StrideSupport.Unity:
            configuration_name = "cutlass_${opcode_class}_${extended_name}_${threadblock}_${layout}_unity_stride_align${alignment}"
        else:
            configuration_name = "cutlass_${opcode_class}_${extended_name}_${threadblock}_${layout}_align${alignment}"

        return SubstituteTemplate(
            configuration_name,
            {
                'opcode_class': opcode_class_name,
                'extended_name': self.extended_name(),
                'threadblock': threadblock,
                'layout': self.layout_name(),
                'alignment': "%d" % self.A.alignment,
            },
        )

    #
    def procedural_name(self):
        '''The full procedural name indicates architecture, extended name, tile size, and layout.'''
        return self.configuration_name()


class EmitConv2dInstance:
    def __init__(self):
        self.template = """
  // Conv2d${conv_kind_name} ${iterator_algorithm_name} kernel instance "${operation_name}"
  using ${operation_name}_base =
  typename cutlass::conv::kernel::DefaultConv2dFpropWithBroadcast<
    ${element_a},
    ${layout_a},
    ${element_b},
    ${layout_b},
    ${element_c},
    ${layout_c},
    ${element_accumulator},
    ${opcode_class},
    ${arch},
    cutlass::gemm::GemmShape<${threadblock_shape_m}, ${threadblock_shape_n}, ${threadblock_shape_k}>,
    cutlass::gemm::GemmShape<${warp_shape_m}, ${warp_shape_n}, ${warp_shape_k} >,
    cutlass::gemm::GemmShape<${instruction_shape_m}, ${instruction_shape_n}, ${instruction_shape_k}>,
    cutlass::epilogue::thread::LinearCombinationResidualBlock<
      ${element_c},
      ${element_accumulator},
      ${element_epilogue},
      ${element_residul},
      ${epilogue_vector_length},
      ${element_act1},
      ${element_binary},
      ${element_act2}
    >,
    ${swizzling_functor}, // cutlass::gemm::threadblock::GemmSplitKIdentityThreadblockSwizzle<>,
    ${stages},
    ${math_operator},
    ${iterator_algorithm},
    ${stride_support},
    ${align_a},
    ${align_b}
  >::Kernel;
"""

    def emit(self, operation):

        warp_shape = [
            int(
                operation.tile_description.threadblock_shape[idx]
                / operation.tile_description.warp_count[idx]
            )
            for idx in range(3)
        ]

        epilogue_vector_length = int(
            min(operation.C.alignment * DataTypeSize[operation.C.element], 128)
            / DataTypeSize[operation.C.element]
        )

        values = {
            'operation_name': operation.procedural_name(),
            'conv_kind': ConvKindTag[operation.conv_kind],
            'conv_kind_name': ConvKindNames[operation.conv_kind].capitalize(),
            'element_a': DataTypeTag[operation.A.element],
            'layout_a': LayoutTag[operation.A.layout],
            'element_b': DataTypeTag[operation.B.element],
            'layout_b': LayoutTag[operation.B.layout],
            'element_c': DataTypeTag[operation.C.element],
            'layout_c': LayoutTag[operation.C.layout],
            'element_accumulator': DataTypeTag[operation.accumulator_type()],
            'opcode_class': OpcodeClassTag[
                operation.tile_description.math_instruction.opcode_class
            ],
            'arch': "cutlass::arch::Sm%d" % operation.arch,
            'threadblock_shape_m': str(
                operation.tile_description.threadblock_shape[0]
            ),
            'threadblock_shape_n': str(
                operation.tile_description.threadblock_shape[1]
            ),
            'threadblock_shape_k': str(
                operation.tile_description.threadblock_shape[2]
            ),
            'warp_shape_m': str(warp_shape[0]),
            'warp_shape_n': str(warp_shape[1]),
            'warp_shape_k': str(warp_shape[2]),
            'instruction_shape_m': str(
                operation.tile_description.math_instruction.instruction_shape[0]
            ),
            'instruction_shape_n': str(
                operation.tile_description.math_instruction.instruction_shape[1]
            ),
            'instruction_shape_k': str(
                operation.tile_description.math_instruction.instruction_shape[2]
            ),
            'epilogue_vector_length': str(epilogue_vector_length),
            'element_residul': DataTypeTag[operation.C.element],
            'element_act1': ActCutlassTag[operation.epilogue_functor_struct[0]],
            'element_binary': operation.epilogue_functor_struct[1],
            'element_act2': ActCutlassTag[operation.epilogue_functor_struct[2]],
            'element_epilogue': str(DataTypeTag[operation.element_epilogue]),
            'swizzling_functor': SwizzlingFunctorTag[
                operation.swizzling_functor
            ],
            'stages': str(operation.tile_description.stages),
            'iterator_algorithm': IteratorAlgorithmTag[
                operation.iterator_algorithm
            ],
            'iterator_algorithm_name': IteratorAlgorithmNames[
                operation.iterator_algorithm
            ].capitalize(),
            'stride_support': StrideSupportTag[operation.stride_support],
            'math_operator': 'cutlass::arch::OpMultiplyAddComplex'
            if operation.is_complex()
            else MathOperationTag[
                operation.tile_description.math_instruction.math_operation
            ],
            'align_a': str(operation.A.alignment),
            'align_b': str(operation.B.alignment),
        }

        return SubstituteTemplate(self.template, values)


class CbrAct(enum.Enum):
    Identity = enum_auto()
    Relu = enum_auto()
    Silu = enum_auto()


ActCutlassTag = {
    CbrAct.Identity: 'cutlass::epilogue::thread::Identity',
    CbrAct.Silu: 'cutlass::epilogue::thread::SiLu',
    CbrAct.Relu: 'cutlass::epilogue::thread::ReLu',
}

# some global variables used, now we only support these activations
epi_res_blocks = [
    (CbrAct.Silu, "cutlass::plus", CbrAct.Identity),
]

UnderScoreName = {
    epi_res_blocks[0]: "conv2d_bias_silu_add",
}

CamelName = {
    epi_res_blocks[0]: "Conv2dBiasSiluAdd",
}

# now we only support these algorithms
iterator_algorithms = [
    IteratorAlgorithm.Optimized,
    IteratorAlgorithm.Analytic,
]

# iterate over iterator_algorithms and tiles
# cba_name is like conv2d_bias_silu_sm75, it's added at the end of kernel_signature to make kernel function name unique
def generate_conv_kernels(
    conv_kind,
    iterator_algorithms,
    tiles,
    A,
    B,
    C,
    element_epilogue,
    stride_kind,
    epilogue_functor,
    swizzling_functor,
    cba_name,
):
    all_kernel_code = ""
    all_kernel_names = ""
    for iterator_algorithm in iterator_algorithms:
        for tile in tiles:
            conv2d_operation = Conv2dResidualOperation(
                conv_kind,
                iterator_algorithm,
                tile.minimum_compute_capability,
                tile,
                A,
                B,
                C,
                element_epilogue,
                stride_kind,
                epilogue_functor,
                swizzling_functor,
            )
            kernel_dict = {}
            kernel_dict["kernel_sig"] = conv2d_operation.configuration_name()
            kernel_dict["kernel_function_name"] = (
                kernel_dict["kernel_sig"] + cba_name
            )
            kernel_dict[
                "kenel_tmplate_instantiate"
            ] = EmitConv2dInstance().emit(conv2d_operation)
            all_kernel_names += kernel_dict["kernel_function_name"] + ", \n"
            # Generate kernel code
            all_kernel_code += SubstituteTemplate(cba_kernel, kernel_dict)
    return (all_kernel_code, all_kernel_names)


def generate_sm75_1688():
    conv_kind = ConvKind.Fprop
    stride_kind = StrideSupport.Strided
    # alpha data type
    element_epilogue = DataType.f32
    min_cc = 75
    max_cc = 1024
    swizzling_functor = SwizzlingFunctor.Identity4
    layout = LayoutType.TensorNHWC

    math_instructions = [
        MathInstruction(
            [16, 8, 8],
            DataType.f16,
            DataType.f16,
            DataType.f32,
            OpcodeClass.TensorOp,
            MathOperation.multiply_add,
        ),
        MathInstruction(
            [16, 8, 8],
            DataType.f16,
            DataType.f16,
            DataType.f16,
            OpcodeClass.TensorOp,
            MathOperation.multiply_add,
        ),
    ]

    alignments = [8]

    sm75_code = ""
    for epi_res_block in epi_res_blocks:
        op_dict = {}
        op_dict["cba_name"] = UnderScoreName[epi_res_block].lower() + "_sm75"
        op_dict["enum_op_name"] = UnderScoreName[epi_res_block].upper()
        # for a op, we record all its kernels into a std::vector in C++ code
        all_kernel_names = ""
        for alignment in alignments:
            A = TensorDescription(DataType.f16, layout, alignment)
            B = TensorDescription(DataType.f16, layout, alignment)
            C = TensorDescription(DataType.f16, layout, alignment)
            for math_inst in math_instructions:
                tiles = [
                    TileDescription(
                        [64, 64, 64], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 32, 64], 2, [2, 1, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [128, 32, 64], 2, [4, 1, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [128, 64, 64], 2, [4, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 64, 32], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 128, 32], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 128, 64], 2, [1, 2, 2], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [64, 256, 32], 2, [1, 4, 1], math_inst, min_cc, max_cc
                    ),
                    TileDescription(
                        [128, 64, 32], 2, [2, 2, 1], math_inst, min_cc, max_cc
                    ),
                ]

                # Generate kernel code
                # iterate over tils and iterator_algorithms
                return_tuple = generate_conv_kernels(
                    conv_kind,
                    iterator_algorithms,
                    tiles,
                    A,
                    B,
                    C,
                    element_epilogue,
                    stride_kind,
                    epi_res_block,
                    swizzling_functor,
                    op_dict["cba_name"],
                )
                sm75_code += return_tuple[0]
                all_kernel_names += return_tuple[1]

        # Generate op code
        op_dict["all_kernel_function_name"] = all_kernel_names
        sm75_code += SubstituteTemplate(cba_function, op_dict)
    return sm75_code


# wrap different sm versions into a function
# now only support sm75
def generate_cbr_for_phi():
    sm_versions = [
        "75",
    ]
    generated_code = ""
    dispatch_template = '''
    if (params.sm_version == ${sm_code})
    {
        ${op_name_with_sm}(params);
    }
    '''
    for epi_res_block in epi_res_blocks:
        dispatch_body = ""
        for sm_version in sm_versions:
            sm_dicts = {}
            sm_dicts["sm_code"] = sm_version
            sm_dicts["op_name_with_sm"] = (
                UnderScoreName[epi_res_block].lower() + "_sm" + sm_version
            )
            dispatch_body += SubstituteTemplate(dispatch_template, sm_dicts)
        op_dicts = {}
        op_dicts["dispatch_body"] = dispatch_body
        op_dicts["op_name"] = CamelName[epi_res_block]
        generated_code += SubstituteTemplate(cba_for_phi, op_dicts)
    return generated_code


if __name__ == "__main__":
    all_code = cba_header
    all_code += generate_sm75_1688()
    all_code += generate_cbr_for_phi()
    all_code += cba_tail
    print(all_code)
