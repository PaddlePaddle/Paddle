# Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import sys

sys.path.append("../")
import enum

from conv2d_common import (
    common_conv_function,
    common_dispatch_temp,
    common_tail,
    common_wrapper_for_phi,
)
from util import SubstituteTemplate, TileDesc

# this is a file's header part

cba_header = '''
// Generated by conv2d_bias_act.py - Do not edit.

#include <mutex>
#include "cutlass/conv/kernel/default_conv2d_fprop.h"
#include "cutlass/epilogue/thread/linear_combination_silu.h"
#include "cutlass/epilogue/thread/linear_combination_bias_relu.h"
#include "paddle/phi/kernels/fusion/cutlass/conv2d/conv2d_util.h"

namespace phi {
namespace fusion {
namespace cutlass_internal {
'''

# This is a cutlass kernel, will be many these like kernels

cba_kernel = '''
cutlass::Status ${kernel_func_name}(ConvAllParams params) {

  using kernel_base =
  typename cutlass::conv::kernel::DefaultConv2d${conv_kind_name}<
    ${element_a},
    ${layout_a},
    ${element_b},
    ${layout_b},
    ${element_c},
    ${layout_c},
    ${element_accum},
    ${opcode_class},
    ${arch},
    cutlass::gemm::GemmShape<${Tshape}>,
    cutlass::gemm::GemmShape<${Wshape}>,
    cutlass::gemm::GemmShape<${Ishape}>,
    ${epi_func}<
      ${element_c},
      ${epilogue_vector_length},
      ${element_accum},
      ${element_epilogue}
    >,
    ${swizzling_functor}, // cutlass::gemm::threadblock::GemmSplitKIdentityThreadblockSwizzle<>,
    ${stages},
    ${math_operator},
    ${iterator_algorithm},
    ${stride_support},
    ${align_a},
    ${align_b}
  >::Kernel;

  using ImplicitGemm =
      cutlass::conv::device::ImplicitGemmConvolution<kernel_base>;
  const half *input = params.input;
  const half *weight = params.weight;
  const half *bias = params.bias;
  half *output = params.output;
  int batch = params.batch;
  int ic = params.ic;
  int ih = params.ih;
  int iw = params.iw;
  int kh = params.kh;
  int kw = params.kw;
  int oc = params.oc;
  int pad_h0 = params.pad_h0;
  int pad_w0 = params.pad_w0;
  int stride_h = params.stride_h;
  int stride_w = params.stride_w;

  int oh = params.oh;
  int ow = params.ow;
  int dilation_h = params.dilation_h;
  int dilation_w = params.dilation_w;

  cutlass::conv::Mode mode = cutlass::conv::Mode::kCrossCorrelation;
  cutlass::conv::Conv2dProblemSize problem_size({batch, ih, iw, ic},
                                                {oc, kh, kw, ic},
                                                {pad_h0, 0, pad_w0, 0},
                                                {stride_h, stride_w},
                                                {dilation_h, dilation_w},
                                                {batch, oh, ow, oc},
                                                mode,
                                                1);

  typename ImplicitGemm::Arguments arguments{
      problem_size,
      {(cutlass::half_t *)(input), {ic, ic * iw, ic * iw * ih}},
      {(cutlass::half_t *)(weight), {ic, ic * kw, ic * kw * kh}},
      {(cutlass::half_t *)(bias), {0, 0, 0}},
      {(cutlass::half_t *)(output), {oc, oc * ow, oc * ow * oh}},
      {1.f, 1.f}};

  ImplicitGemm implicit_gemm_op;
  size_t bytes = implicit_gemm_op.get_workspace_size(arguments);

  auto ctx = params.ctx;
  auto stream = ctx->stream();
  paddle::memory::allocation::AllocationPtr tmp_gpu_ptrs_data =
      paddle::memory::Alloc(
          ctx->GetPlace(),
          bytes,
          phi::Stream(reinterpret_cast<phi::StreamId>(stream)));
  void *workspace = tmp_gpu_ptrs_data->ptr();

  cutlass::Status status = implicit_gemm_op.can_implement(arguments);
  CUTLASS_CHECK(status);
  status = implicit_gemm_op.initialize(arguments, workspace);
  CUTLASS_CHECK(status);
  status = implicit_gemm_op(stream);
  CUTLASS_CHECK(status);
  return status;
}
'''


class CbaAct(enum.Enum):
    Identity = 1
    Relu = 2
    Silu = 3


ActCutlassTag = {
    CbaAct.Identity: 'cutlass::epilogue::thread::LinearCombination',
    CbaAct.Silu: 'cutlass::epilogue::thread::LinearCombinationSilu',
    CbaAct.Relu: 'cutlass::epilogue::thread::LinearCombinationRelu',
}

UnderScoreName = {
    CbaAct.Identity: "conv2d_bias",
    CbaAct.Silu: "conv2d_bias_silu",
    CbaAct.Relu: "conv2d_bias_relu",
}

CamelName = {
    CbaAct.Identity: "Conv2dBias",
    CbaAct.Silu: "Conv2dBiasSilu",
    CbaAct.Relu: "Conv2dBiasRelu",
}

# some global variables used, now we only support these activations
epi_funcs = [
    CbaAct.Identity,
    CbaAct.Relu,
    CbaAct.Silu,
]


def generate_sm75_1688():
    kernel_dict = {
        "conv_kind_name": "Fprop",
        "element_a": "cutlass::half_t",
        "layout_a": "cutlass::layout::TensorNHWC",
        "element_b": "cutlass::half_t",
        "layout_b": "cutlass::layout::TensorNHWC",
        "element_c": "cutlass::half_t",
        "layout_c": "cutlass::layout::TensorNHWC",
        "opcode_class": "cutlass::arch::OpClassTensorOp",
        "arch": "cutlass::arch::Sm75",
        "stages": "2",
        "swizzling_functor": "cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<4>",
        # alpha is always float!
        "element_epilogue": "float",
        "math_operator": "cutlass::arch::OpMultiplyAdd",
    }

    kernel_dict["stride_support"] = "cutlass::conv::StrideSupport::kStrided"

    # iterate over this loop
    element_accums = ["cutlass::half_t", "float"]
    iterator_algorithms = [
        "cutlass::conv::IteratorAlgorithm::kOptimized",
        "cutlass::conv::IteratorAlgorithm::kAnalytic",
    ]

    math_instructions = [
        (
            "16,8,8",
            "cutlass::half_t",
            "cutlass::half_t",
            "cutlass::half_t",
        ),
        (
            "16,8,8",
            "cutlass::half_t",
            "cutlass::half_t",
            "float",
        ),
    ]

    alignments = [8]

    kernel_dict["align_a"] = "8"
    kernel_dict["align_b"] = "8"
    kernel_dict["epilogue_vector_length"] = "8"

    sm75_code = ""
    for epi_func in epi_funcs:
        op_dict = {}
        op_dict["func_name"] = UnderScoreName[epi_func].lower() + "_sm75"
        op_dict["enum_op_name"] = UnderScoreName[epi_func].upper()
        # for a op, we record all its kernels into a std::vector in C++ code
        all_kernel_names = ""
        kernel_dict["epi_func"] = ActCutlassTag[epi_func]
        suffix = 0
        for iterator_algorithm in iterator_algorithms:
            for alignment in alignments:
                for math_inst in math_instructions:
                    tiles = [
                        TileDesc("64, 64, 64", 2, "32, 32, 64", math_inst),
                        TileDesc("64, 32, 64", 2, "32, 32, 64", math_inst),
                        TileDesc("128, 32, 64", 2, "32, 32, 64", math_inst),
                        TileDesc("128, 64, 64", 2, "32, 32, 64", math_inst),
                        TileDesc("64, 64, 32", 2, "32, 32, 32", math_inst),
                        TileDesc("64, 128, 32", 2, "32, 64, 32", math_inst),
                        TileDesc("64, 128, 64", 2, "64, 64, 32", math_inst),
                        TileDesc("64, 256, 32", 2, "64, 64, 32", math_inst),
                        TileDesc("128, 64, 32", 2, "64, 32, 32", math_inst),
                    ]
                    for tile in tiles:
                        kernel_dict["iterator_algorithm"] = iterator_algorithm
                        kernel_dict["Tshape"] = tile.Tshape
                        kernel_dict["Wshape"] = tile.Wshape
                        kernel_dict["Ishape"] = tile.math_inst[0]
                        kernel_dict["element_accum"] = tile.math_inst[3]
                        kernel_dict["kernel_func_name"] = op_dict[
                            "func_name"
                        ] + str(suffix)
                        suffix += 1

                        sm75_code += SubstituteTemplate(cba_kernel, kernel_dict)
                        all_kernel_names += (
                            kernel_dict["kernel_func_name"] + ", \n"
                        )

        # Generate op code
        op_dict["all_kernel_func_name"] = all_kernel_names
        sm75_code += SubstituteTemplate(common_conv_function, op_dict)
    return sm75_code


# wrap different sm versions into a function
def generate_cba_for_phi():
    sm_versions = ["75"]
    generated_code = ""
    for epi_func in epi_funcs:
        dispatch_body = ""
        for sm_version in sm_versions:
            sm_dicts = {}
            sm_dicts["sm_code"] = sm_version
            sm_dicts["op_name_with_sm"] = (
                UnderScoreName[epi_func].lower() + "_sm" + sm_version
            )
            dispatch_body += SubstituteTemplate(common_dispatch_temp, sm_dicts)
        op_dicts = {}
        op_dicts["dispatch_body"] = dispatch_body
        op_dicts["op_name"] = CamelName[epi_func]
        generated_code += SubstituteTemplate(common_wrapper_for_phi, op_dicts)
    return generated_code


if __name__ == "__main__":
    all_code = cba_header
    all_code += generate_sm75_1688()
    all_code += generate_cba_for_phi()
    all_code += common_tail
    print(all_code)
