// Copyright (c) 2024 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

#include "paddle/phi/core/dense_tensor.h"
#include "paddle/phi/core/kernel_registry.h"
#include "paddle/phi/kernels/funcs/correlation_funcs.cu.h"

namespace phi {

template <typename T>
__global__ void correlation_backward_input1(int item,
                                            T *grad_input1,
                                            const int input_channel,
                                            const int input_height,
                                            const int input_width,
                                            const T *grad_output,
                                            const int output_channel,
                                            const int output_height,
                                            const int output_width,
                                            const T *rinput2,
                                            const int pad_size,
                                            const int kernel_size,
                                            const int max_displacement,
                                            const int stride1,
                                            const int stride2) {
  int n = item;
  int h = blockIdx.x * stride1 + pad_size;
  int w = blockIdx.y * stride1 + pad_size;
  int c = blockIdx.z;
  int tch_off = threadIdx.x;

  int kernel_rad = (kernel_size - 1) / 2;
  int displacement_rad = max_displacement / stride2;
  int displacement_size = 2 * displacement_rad + 1;

  int xmin = (w - kernel_rad - max_displacement) / stride1;
  int ymin = (h - kernel_rad - max_displacement) / stride1;

  int xmax = (w + kernel_rad - max_displacement) / stride1;
  int ymax = (h + kernel_rad - max_displacement) / stride1;

  if (xmax < 0 || ymax < 0 || xmin >= output_width || ymin >= output_height) {
    return;
  }

  if (xmin > xmax || ymin > ymax) {
    return;
  }

  xmin = max(0, xmin);
  xmax = min(output_width - 1, xmax);

  ymin = max(0, ymin);
  ymax = min(output_height - 1, ymax);

  int p_input_width = input_width + 2 * pad_size;
  int p_input_height = input_height + 2 * pad_size;
  int p_dimchw = input_channel * p_input_height * p_input_width;
  int p_dimcw = input_channel * p_input_width;
  int p_dimc = input_channel;

  int t_dimchw = output_channel * output_height * output_width;
  int t_dimhw = output_height * output_width;
  int t_dimw = output_width;

  int o_dimchw = input_channel * input_height * input_width;
  int o_dimhw = input_height * input_width;
  int o_dimw = input_width;

  int nelems = kernel_size * kernel_size * input_channel;

  __shared__ T prod_sum[THREADS_PER_BLOCK];
  prod_sum[tch_off] = 0;

  for (int tc = tch_off; tc < output_channel; tc += THREADS_PER_BLOCK) {
    int i2 = (tc % displacement_size - displacement_rad) * stride2;
    int j2 = (tc / displacement_size - displacement_rad) * stride2;

    int index2 = n * p_dimchw + (h + j2) * p_dimcw + (w + i2) * p_dimc + c;

    T val2 = rinput2[index2];
    for (int j = ymin; j <= ymax; ++j) {
      for (int i = xmin; i <= xmax; ++i) {
        int t_index = n * t_dimchw + tc * t_dimhw + j * t_dimw + i;
        prod_sum[tch_off] += grad_output[t_index] * val2;
      }
    }
  }

  __syncthreads();

  if (tch_off == 0) {
    T reduce_sum = 0;
    for (int index = 0; index < THREADS_PER_BLOCK; index++) {
      reduce_sum += prod_sum[index];
    }
    const int index1 =
        n * o_dimchw + c * o_dimhw + (h - pad_size) * o_dimw + (w - pad_size);
    grad_input1[index1] = static_cast<T>(reduce_sum / nelems);
  }
}

template <typename T>
__global__ void correlation_backward_input2(int item,
                                            T *grad_input2,
                                            const int input_channel,
                                            const int input_height,
                                            const int input_width,
                                            const T *grad_output,
                                            const int output_channel,
                                            const int output_height,
                                            const int output_width,
                                            const T *rinput1,
                                            const int pad_size,
                                            const int kernel_size,
                                            const int max_displacement,
                                            const int stride1,
                                            const int stride2) {
  int n = item;
  int h = blockIdx.x * stride1 + pad_size;
  int w = blockIdx.y * stride1 + pad_size;
  int c = blockIdx.z;

  int tch_off = threadIdx.x;

  int kernel_rad = (kernel_size - 1) / 2;
  int displacement_rad = max_displacement / stride2;
  int displacement_size = 2 * displacement_rad + 1;

  int p_input_width = input_width + 2 * pad_size;
  int p_input_height = input_height + 2 * pad_size;
  int p_dimchw = input_channel * p_input_height * p_input_width;
  int p_dimcw = input_channel * p_input_width;
  int p_dimc = input_channel;

  int t_dimchw = output_channel * output_height * output_width;
  int t_dimhw = output_height * output_width;
  int t_dimw = output_width;

  int o_dimchw = input_channel * input_height * input_width;
  int o_dimhw = input_height * input_width;
  int o_dimw = input_width;

  int nelems = kernel_size * kernel_size * input_channel;

  __shared__ T prod_sum[THREADS_PER_BLOCK];
  prod_sum[tch_off] = 0;

  for (int tc = tch_off; tc < output_channel; tc += THREADS_PER_BLOCK) {
    int i2 = (tc % displacement_size - displacement_rad) * stride2;
    int j2 = (tc / displacement_size - displacement_rad) * stride2;

    int xmin = (w - kernel_rad - max_displacement - i2) / stride1;
    int ymin = (h - kernel_rad - max_displacement - j2) / stride1;

    int xmax = (w + kernel_rad - max_displacement - i2) / stride1;
    int ymax = (h + kernel_rad - max_displacement - j2) / stride1;

    if (xmax < 0 || ymax < 0 || xmin >= output_width || ymin >= output_height) {
      continue;
    }

    if (xmin > xmax || ymin > ymax) {
      continue;
    }

    xmin = max(0, xmin);
    xmax = min(output_width - 1, xmax);

    ymin = max(0, ymin);
    ymax = min(output_height - 1, ymax);

    int index1 = n * p_dimchw + (h - j2) * p_dimcw + (w - i2) * p_dimc + c;
    T val1 = rinput1[index1];
    for (int j = ymin; j <= ymax; ++j) {
      for (int i = xmin; i <= xmax; ++i) {
        int t_index = n * t_dimchw + tc * t_dimhw + j * t_dimw + i;
        prod_sum[tch_off] += grad_output[t_index] * val1;
      }
    }
  }

  __syncthreads();

  if (tch_off == 0) {
    T reduce_sum = 0;
    for (int index = 0; index < THREADS_PER_BLOCK; index++) {
      reduce_sum += prod_sum[index];
    }
    const int index2 =
        n * o_dimchw + c * o_dimhw + (h - pad_size) * o_dimw + (w - pad_size);
    grad_input2[index2] = static_cast<T>(reduce_sum / nelems);
  }
}

template <typename T, typename Context>
void CorrelationCUDAGradKernel(const Context &dev_ctx,
                               const DenseTensor &input1,
                               const DenseTensor &input2,
                               const DenseTensor &out_grad,
                               int pad_size,
                               int kernel_size,
                               int max_displacement,
                               int stride1,
                               int stride2,
                               int corr_type_multiply,
                               DenseTensor *input1_grad,
                               DenseTensor *input2_grad) {
  bool is_gpu_place = dev_ctx.GetPlace().GetType() == phi::AllocationType::GPU;
  PADDLE_ENFORCE_EQ(
      is_gpu_place,
      true,
      common::errors::InvalidArgument("Correlation only supports GPU now."));
  const auto *grad_output = &out_grad;

  auto *grad_input1 = input1_grad;
  dev_ctx.template Alloc<T>(grad_input1);
  auto *grad_input2 = input2_grad;
  dev_ctx.template Alloc<T>(grad_input2);

  auto in_dims = input1.dims();
  int N = in_dims[0];
  int C = in_dims[1];
  int H = in_dims[2];
  int W = in_dims[3];

  int padded_input_height = H + 2 * pad_size;
  int padded_input_width = W + 2 * pad_size;

  phi::DenseTensor rinput1;
  rinput1.Resize({N, padded_input_height, padded_input_width, C});
  dev_ctx.template Alloc<T>(&rinput1);

  phi::DenseTensor rinput2;
  rinput2.Resize({N, padded_input_height, padded_input_width, C});
  dev_ctx.template Alloc<T>(&rinput2);

  set_zero<<<(rinput1.numel() + 512 - 1) / 512, 512, 0, dev_ctx.stream()>>>(
      rinput1.data<T>(), rinput1.numel());
  set_zero<<<(rinput2.numel() + 512 - 1) / 512, 512, 0, dev_ctx.stream()>>>(
      rinput2.data<T>(), rinput2.numel());
  set_zero<<<(grad_input1->numel() + 512 - 1) / 512,
             512,
             0,
             dev_ctx.stream()>>>(grad_input1->data<T>(), grad_input1->numel());
  set_zero<<<(grad_input2->numel() + 512 - 1) / 512,
             512,
             0,
             dev_ctx.stream()>>>(grad_input2->data<T>(), grad_input2->numel());

  auto grad_out_dims = grad_output->dims();
  int GOC = grad_out_dims[1];
  int GOH = grad_out_dims[2];
  int GOW = grad_out_dims[3];

  dim3 blocks_grid(N, H, W);
  dim3 threads_block(THREADS_PER_BLOCK);

  channel_first<T><<<blocks_grid, threads_block, 0, dev_ctx.stream()>>>(
      input1.data<T>(), rinput1.data<T>(), C, H, W, pad_size);
  channel_first<T><<<blocks_grid, threads_block, 0, dev_ctx.stream()>>>(
      input2.data<T>(), rinput2.data<T>(), C, H, W, pad_size);

  dim3 threadsPerBlock(THREADS_PER_BLOCK);
  dim3 totalBlocksCorr(H, W, C);

  for (int n = 0; n < N; n++) {
    correlation_backward_input1<T>
        <<<totalBlocksCorr, threadsPerBlock, 0, dev_ctx.stream()>>>(
            n,
            grad_input1->data<T>(),
            C,
            H,
            W,
            grad_output->data<T>(),
            GOC,
            GOH,
            GOW,
            rinput2.data<T>(),
            pad_size,
            kernel_size,
            max_displacement,
            stride1,
            stride2);
  }

  for (int n = 0; n < N; n++) {
    correlation_backward_input2<T>
        <<<totalBlocksCorr, threadsPerBlock, 0, dev_ctx.stream()>>>(
            n,
            grad_input2->data<T>(),
            C,
            H,
            W,
            grad_output->data<T>(),
            GOC,
            GOH,
            GOW,
            rinput1.data<T>(),
            pad_size,
            kernel_size,
            max_displacement,
            stride1,
            stride2);
  }
}
}  // namespace phi

PD_REGISTER_KERNEL(correlation_grad,
                   GPU,
                   ALL_LAYOUT,
                   phi::CorrelationCUDAGradKernel,
                   float,
                   double) {}
