// Copyright (c) 2023 PaddlePaddle Authors. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
/*
 * copyright (C) 2022 KUNLUNXIN, Inc
 */

#include "xpu/kernel/cluster.h"
#include "xpu/kernel/cluster_partition.h"
#include "xpu/kernel/cluster_primitive.h"

namespace xpu2 {
namespace plugin {

template <typename TID>
__global__ void fast_gather1d(const int8_t* x,
                              const TID* index,
                              int64_t count,
                              int64_t x_dim0,
                              int64_t x_stride0,
                              int8_t* y) {
  int cid = core_id();
  int tid = core_id() * cluster_num() + cluster_id();
  int nthreads = core_num() * cluster_num();
  const int index_len = 320 / sizeof(TID);
  __simd__ TID local_index[index_len];
  const int buf_len = 5824 / sizeof(int8_t);
  __simd__ int8_t local_x[buf_len];
  if (x_stride0 > buf_len) {
    for (int64_t i = tid; i < count; i += nthreads) {
      GM2LM(index + i, local_index, sizeof(TID));
      int64_t offset = ((local_index[0] + x_dim0) % x_dim0) * x_stride0;
      for (int64_t j = 0; j < x_stride0; j += buf_len) {
        int read_len = min(static_cast<int64_t>(x_stride0), x_stride0 - j);
        GM2LM(x + offset + j, local_x, read_len);
        LM2GM(local_x, y + i * x_stride0 + j, read_len);
      }
    }
  } else {
    int64_t count_per_thread = min(index_len, buf_len / x_stride0);
    for (int64_t i = tid * count_per_thread; i < count;
         i += nthreads * count_per_thread) {
      int count_in_thread =
          min(static_cast<int64_t>(count_per_thread), count - i);
      GM2LM(index + i, local_index, count_in_thread * sizeof(TID));
      for (int64_t j = 0; j < count_in_thread; j++) {
        int64_t offset = ((local_index[j] + x_dim0) % x_dim0) * x_stride0;
        GM2LM_ASYNC(x + offset, local_x + j * x_stride0, x_stride0);
      }
      mfence_lm();
      LM2GM(local_x, y + i * x_stride0, x_stride0 * count_in_thread);
    }
  }
}

template <typename TID>
__global__ void fast_gather2d(const int8_t* x,
                              const TID* index,
                              int64_t count,
                              int64_t x_dim0,
                              int64_t x_dim1,
                              int64_t x_stride0,
                              int64_t x_stride1,
                              int8_t* y) {
  int cid = core_id();
  int tid = core_id() * cluster_num() + cluster_id();
  int nthreads = core_num() * cluster_num();
  const int index_len = 640 / sizeof(TID);
  __simd__ TID local_index[index_len];
  const int buf_len = 5504 / sizeof(int8_t);
  __simd__ int8_t local_x[buf_len];
  if (x_stride1 > buf_len) {
    for (int64_t i = tid; i < count; i += nthreads) {
      GM2LM(index + i * 2, local_index, 2 * sizeof(TID));
      int64_t offset = ((local_index[0] + x_dim0) % x_dim0) * x_stride0 +
                       ((local_index[1] + x_dim1) % x_dim1) * x_stride1;
      for (int64_t j = 0; j < x_stride1; j += buf_len) {
        int read_len = min(static_cast<int64_t>(x_stride1), x_stride1 - j);
        GM2LM(x + offset + j, local_x, read_len);
        LM2GM(local_x, y + i * x_stride1 + j, read_len);
      }
    }
  } else {
    int64_t count_per_thread = min(index_len / 2, buf_len / x_stride1);
    for (int64_t i = tid * count_per_thread; i < count;
         i += nthreads * count_per_thread) {
      int count_in_thread =
          min(static_cast<int64_t>(count_per_thread), count - i);
      GM2LM(index + i * 2, local_index, 2 * count_in_thread * sizeof(TID));
      for (int64_t j = 0; j < count_in_thread; j++) {
        int64_t offset =
            ((local_index[j * 2] + x_dim0) % x_dim0) * x_stride0 +
            ((local_index[j * 2 + 1] + x_dim1) % x_dim1) * x_stride1;
        GM2LM_ASYNC(x + offset, local_x + j * x_stride1, x_stride1);
      }
      mfence_lm();
      LM2GM(local_x, y + i * x_stride1, x_stride1 * count_in_thread);
    }
  }
}

template <typename TID>
__global__ void fast_gather3d(const int8_t* x,
                              const TID* index,
                              int64_t count,
                              int64_t x_dim0,
                              int64_t x_dim1,
                              int64_t x_dim2,
                              int64_t x_stride0,
                              int64_t x_stride1,
                              int64_t x_stride2,
                              int8_t* y) {
  int cid = core_id();
  int tid = core_id() * cluster_num() + cluster_id();
  int nthreads = core_num() * cluster_num();
  const int index_len = 960 / sizeof(TID);
  __simd__ TID local_index[index_len];
  const int buf_len = 5184 / sizeof(int8_t);
  __simd__ int8_t local_x[buf_len];
  if (x_stride2 > buf_len) {
    for (int64_t i = tid; i < count; i += nthreads) {
      GM2LM(index + i * 3, local_index, 3 * sizeof(TID));
      int64_t offset = ((local_index[0] + x_dim0) % x_dim0) * x_stride0 +
                       ((local_index[1] + x_dim1) % x_dim1) * x_stride1 +
                       ((local_index[2] + x_dim2) % x_dim2) * x_stride2;
      for (int64_t j = 0; j < x_stride2; j += buf_len) {
        int read_len = min(static_cast<int64_t>(x_stride2), x_stride2 - j);
        GM2LM(x + offset + j, local_x, read_len);
        LM2GM(local_x, y + i * x_stride2 + j, read_len);
      }
    }
  } else {
    int64_t count_per_thread = min(index_len / 3, buf_len / x_stride2);
    for (int64_t i = tid * count_per_thread; i < count;
         i += nthreads * count_per_thread) {
      int count_in_thread =
          min(static_cast<int64_t>(count_per_thread), count - i);
      GM2LM(index + i * 3, local_index, 3 * count_in_thread * sizeof(TID));
      for (int64_t j = 0; j < count_in_thread; j++) {
        int64_t offset =
            ((local_index[j * 3] + x_dim0) % x_dim0) * x_stride0 +
            ((local_index[j * 3 + 1] + x_dim1) % x_dim1) * x_stride1 +
            ((local_index[j * 3 + 2] + x_dim2) % x_dim2) * x_stride2;
        GM2LM_ASYNC(x + offset, local_x + j * x_stride2, x_stride2);
      }
      mfence_lm();
      LM2GM(local_x, y + i * x_stride2, x_stride2 * count_in_thread);
    }
  }
}

template <typename TID>
__global__ void fast_gather4d(const int8_t* x,
                              const TID* index,
                              int64_t count,
                              int64_t x_dim0,
                              int64_t x_dim1,
                              int64_t x_dim2,
                              int64_t x_dim3,
                              int64_t x_stride0,
                              int64_t x_stride1,
                              int64_t x_stride2,
                              int64_t x_stride3,
                              int8_t* y) {
  int cid = core_id();
  int tid = core_id() * cluster_num() + cluster_id();
  int nthreads = core_num() * cluster_num();
  const int index_len = 1280 / sizeof(TID);
  __simd__ TID local_index[index_len];
  const int buf_len = 4864 / sizeof(int8_t);
  __simd__ int8_t local_x[buf_len];
  if (x_stride3 > buf_len) {
    for (int64_t i = tid; i < count; i += nthreads) {
      GM2LM(index + i * 4, local_index, 4 * sizeof(TID));
      int64_t offset = ((local_index[0] + x_dim0) % x_dim0) * x_stride0 +
                       ((local_index[1] + x_dim1) % x_dim1) * x_stride1 +
                       ((local_index[2] + x_dim2) % x_dim2) * x_stride2 +
                       ((local_index[3] + x_dim3) % x_dim3) * x_stride3;
      for (int64_t j = 0; j < x_stride3; j += buf_len) {
        int read_len = min(static_cast<int64_t>(x_stride3), x_stride3 - j);
        GM2LM(x + offset + j, local_x, read_len);
        LM2GM(local_x, y + i * x_stride3 + j, read_len);
      }
    }
  } else {
    int64_t count_per_thread = min(index_len / 4, buf_len / x_stride3);
    for (int64_t i = tid * count_per_thread; i < count;
         i += nthreads * count_per_thread) {
      int count_in_thread =
          min(static_cast<int64_t>(count_per_thread), count - i);
      GM2LM(index + i * 4, local_index, 4 * count_in_thread * sizeof(TID));
      for (int64_t j = 0; j < count_in_thread; j++) {
        int64_t offset =
            ((local_index[j * 4] + x_dim0) % x_dim0) * x_stride0 +
            ((local_index[j * 4 + 1] + x_dim1) % x_dim1) * x_stride1 +
            ((local_index[j * 4 + 2] + x_dim2) % x_dim2) * x_stride2 +
            ((local_index[j * 4 + 3] + x_dim3) % x_dim3) * x_stride3;
        GM2LM_ASYNC(x + offset, local_x + j * x_stride3, x_stride3);
      }
      mfence_lm();
      LM2GM(local_x, y + i * x_stride3, x_stride3 * count_in_thread);
    }
  }
}

#define _XPU_DEF__FAST_GATHERND_(IDTYPE)                              \
  template __global__ void fast_gather1d<IDTYPE>(const int8_t* x,     \
                                                 const IDTYPE* index, \
                                                 int64_t count,       \
                                                 int64_t x_dim0,      \
                                                 int64_t x_stride0,   \
                                                 int8_t* y);          \
  template __global__ void fast_gather2d<IDTYPE>(const int8_t* x,     \
                                                 const IDTYPE* index, \
                                                 int64_t count,       \
                                                 int64_t x_dim0,      \
                                                 int64_t x_dim1,      \
                                                 int64_t x_stride0,   \
                                                 int64_t x_stride1,   \
                                                 int8_t* y);          \
  template __global__ void fast_gather3d<IDTYPE>(const int8_t* x,     \
                                                 const IDTYPE* index, \
                                                 int64_t count,       \
                                                 int64_t x_dim0,      \
                                                 int64_t x_dim1,      \
                                                 int64_t x_dim2,      \
                                                 int64_t x_stride0,   \
                                                 int64_t x_stride1,   \
                                                 int64_t x_stride2,   \
                                                 int8_t* y);          \
  template __global__ void fast_gather4d<IDTYPE>(const int8_t* x,     \
                                                 const IDTYPE* index, \
                                                 int64_t count,       \
                                                 int64_t x_dim0,      \
                                                 int64_t x_dim1,      \
                                                 int64_t x_dim2,      \
                                                 int64_t x_dim3,      \
                                                 int64_t x_stride0,   \
                                                 int64_t x_stride1,   \
                                                 int64_t x_stride2,   \
                                                 int64_t x_stride3,   \
                                                 int8_t* y);
_XPU_DEF__FAST_GATHERND_(int);
_XPU_DEF__FAST_GATHERND_(int8_t);
_XPU_DEF__FAST_GATHERND_(int64_t);
_XPU_DEF__FAST_GATHERND_(bool);

}  // namespace plugin
}  // namespace xpu2
