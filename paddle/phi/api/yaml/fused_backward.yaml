# This file is designed for fusion C++ backward operators, which manages the
# generated code for static mode and dynamic mode (when `support_dygraph_mode` is true).
# "support_dygraph_mode" is an extra configuration item in this file,
# if one operator have "support_dygraph_mode : true", it supports dygraph mode,
# otherwise the operator only could be used in static mode.

- backward_op : fused_bias_dropout_residual_layer_norm_grad
  forward: fused_bias_dropout_residual_layer_norm (Tensor x, Tensor residual, Tensor bias, Tensor ln_scale, Tensor ln_bias, float dropout_rate, bool is_test, bool dropout_fix_seed, int dropout_seed, str dropout_implementation, float ln_epsilon) -> Tensor(y), Tensor(bias_dropout_residual_out), Tensor(dropout_mask_out), Tensor(ln_mean), Tensor(ln_variance)
  args : (Tensor y_grad, Tensor x, Tensor residual, Tensor bias, Tensor ln_scale, Tensor ln_bias, Tensor ln_mean, Tensor ln_variance, Tensor bias_dropout_residual_out, Tensor dropout_mask_out, float dropout_rate = 0.5f, bool is_test = false, bool dropout_fix_seed = true, int dropout_seed = true, str dropout_implementation = "downgrade_in_infer", float ln_epsilon = 1e-5)
  output : Tensor(x_grad), Tensor(residual_grad), Tensor(bias_grad), Tensor(ln_scale_grad), Tensor(ln_bias_grad)
  optional :  bias, ln_scale, ln_bias, bias_grad, ln_scale_grad, ln_bias_grad
  infer_meta :
    func : FusedBiasDropoutResidualLnGradInferMeta
  kernel :
    func : fused_bias_dropout_residual_layer_norm_grad
    data_type : y_grad

- backward_op : fused_dot_product_attention_grad
  forward : fused_dot_product_attention (Tensor q, Tensor k, Tensor v, Tensor mask, float scaling_factor, float dropout_probability, bool is_training, bool is_causal_masking) -> Tensor(out), Tensor(softmax_out), Tensor(rng_state)
  args : (Tensor q, Tensor k, Tensor v, Tensor out, Tensor softmax_out, Tensor rng_state, Tensor mask, Tensor out_grad, float scaling_factor, float dropout_probability, bool is_causal_masking = false)
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  infer_meta :
    func : FusedDotProductAttentionGradInferMeta
    param: [q, k, v]
  kernel :
    func : fused_dot_product_attention_grad
    data_type : q
  support_dygraph_mode : true

- backward_op : fused_dropout_add_grad
  forward : fused_dropout_add (Tensor x, Tensor y, Tensor seed_tensor, Scalar p, bool is_test, str mode, int seed, bool fix_seed) -> Tensor(out), Tensor(seed_offset)
  args : (Tensor seed_offset, Tensor out_grad, Scalar p, bool is_test, str mode, bool fix_seed)
  output : Tensor(x_grad), Tensor(y_grad)
  infer_meta :
    func : FusedDropoutAddGradInferMeta
    param : [seed_offset, out_grad]
  kernel :
    func : fused_dropout_add_grad
    data_type : out_grad
  support_dygraph_mode : true

- backward_op : fused_rotary_position_embedding_grad
  forward: fused_rotary_position_embedding (Tensor q, Tensor k, Tensor v, Tensor sin, Tensor cos, Tensor position_ids, bool use_neox_rotary_style) -> Tensor(out_q), Tensor(out_k), Tensor(out_v)
  args : (Tensor sin, Tensor cos, Tensor position_ids, Tensor out_q_grad, Tensor out_k_grad,Tensor out_v_grad, bool use_neox_rotary_style)
  output : Tensor(q_grad), Tensor(k_grad), Tensor(v_grad)
  optional :  sin, cos, position_ids, out_k_grad, out_v_grad, k_grad, v_grad
  infer_meta :
    func : FusedRopeGradInferMeta
    spmd_rule : FusedRopeGradInferSpmd
  kernel :
    func : fused_rotary_position_embedding_grad
    data_type : out_q_grad
  support_dygraph_mode : true
