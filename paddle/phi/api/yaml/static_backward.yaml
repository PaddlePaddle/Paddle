# This file is to support those static ops different the dynamic.

- backward_op : assign_grad
  forward : assign (Tensor x) -> Tensor(out)
  args : (Tensor out_grad)
  output : Tensor(x_grad)
  composite: assign_grad(out_grad, x_grad)
  invoke : assign(out_grad)

- backward_op : batch_norm_grad
  forward : batch_norm (Tensor x, Tensor mean, Tensor variance, Tensor scale, Tensor bias, Tensor MomentumTensor, bool is_test, float momentum, float epsilon, str data_layout, bool use_global_stats, bool trainable_statistics) -> Tensor(out), Tensor(mean_out), Tensor(variance_out), Tensor(saved_mean), Tensor(saved_variance), Tensor(reserve_space)
  args : (Tensor x, Tensor scale, Tensor bias, Tensor mean_out, Tensor variance_out, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor out_grad, float momentum, float epsilon, str data_layout, bool is_test, bool use_global_stats, bool trainable_statistics)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(bias_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, scale, bias]
  kernel :
    func : batch_norm_grad
    data_type : out_grad
  optional : mean_out, variance_out, reserve_space
  composite: batch_norm_grad(x, scale, bias, mean_out, variance_out, saved_mean, saved_variance, reserve_space, out_grad, momentum, epsilon, data_layout, is_test, use_global_stats, trainable_statistics, x_grad, scale_grad, bias_grad)
  backward : batch_norm_grad_grad

- backward_op : batch_norm_grad_grad
  forward : batch_norm_grad (Tensor x, Tensor scale, Tensor bias, Tensor out_mean, Tensor out_variance, Tensor saved_mean, Tensor saved_variance, Tensor reserve_space, Tensor grad_out, float momentum, float epsilon, str data_layout, bool is_test, bool use_global_stats, bool trainable_statistics) -> Tensor(grad_x), Tensor(grad_scale), Tensor(grad_bias)
  args : (Tensor x, Tensor scale, Tensor out_mean, Tensor out_variance, Tensor saved_mean, Tensor saved_variance, Tensor grad_out,  Tensor grad_x_grad, Tensor grad_scale_grad, Tensor grad_bias_grad, float momentum, float epsilon, str data_layout, bool is_test, bool use_global_stats, bool trainable_statistics)
  output : Tensor(x_grad), Tensor(scale_grad), Tensor(grad_out_grad)
  infer_meta :
    func : GeneralTernaryGradInferMeta
    param : [x, scale, x]
  kernel :
    func : batch_norm_grad_grad
    data_type : x
  optional : out_mean, out_variance, grad_x_grad, grad_scale_grad, grad_bias_grad
  inplace : (grad_out -> grad_out_grad)

- backward_op : deformable_conv_grad
  forward : deformable_conv (Tensor x, Tensor offset, Tensor filter, Tensor mask, int[] strides={1, 1}, int[] paddings={0, 0}, int[] dilations={1, 1}, int deformable_groups=1, int groups=1, int im2col_step=64) -> Tensor(out)
  args : (Tensor x, Tensor offset, Tensor filter, Tensor mask, Tensor out_grad, int[] strides, int[] paddings, int[] dilations, int deformable_groups, int groups, int im2col_step)
  output : Tensor(x_grad), Tensor(offset_grad), Tensor(filter_grad), Tensor(mask_grad)
  infer_meta :
    func : DeformableConvGradInferMeta
  kernel :
    func : deformable_conv_grad
    data_type : x

- backward_op : embedding_grad
  forward : embedding (Tensor x, Tensor weight, int64_t padding_idx=-1) -> Tensor(out)
  args : (Tensor x, Tensor weight, Tensor out_grad, int64_t padding_idx=-1)
  output : Tensor(weight_grad)
  infer_meta :
    func : EmbeddingGradInferMeta
    param : [x,weght]
  kernel :
    func : embedding_grad {dense, dense, dense -> dense}
           embedding_sparse_grad {dense, dense, dense -> selected_rows}
           sparse_weight_embedding_grad {selected_rows, dense, dense -> dense}
           sparse_weight_embedding_sparse_grad {selected_rows, dense, dense -> selected_rows}
    data_type : out_grad
  no_need_buffer : weight

- backward_op : frobenius_norm_grad
  forward: frobenius_norm (Tensor x, IntArray axis={0}, bool keepdim=false, bool reduce_all=false, int in_dtype=-1, int out_dtype=-1) -> Tensor(out)
  args : (Tensor x, Tensor out, Tensor out_grad, IntArray axis={0}, bool keepdim=false, bool reduce_all=false, int in_dtype=-1, int out_dtype=-1)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [x]
  kernel :
    func : frobenius_norm_grad
    param : [x, out, out_grad, axis, keepdim, reduce_all]

- backward_op : rnn_grad
  forward : rnn (Tensor x, Tensor[] pre_state, Tensor[] weight_list, Tensor sequence_length, float dropout_prob=0.0, bool is_bidirec=false, int input_size=10, int hidden_size=100, int num_layers=1, str mode="RNN_TANH", int seed=0, bool is_test=false) -> Tensor(out), Tensor(dropout_state_out), Tensor[](state), Tensor(reserve)
  args : (Tensor x, Tensor[] pre_state, Tensor[] weight_list, Tensor sequence_length, Tensor out, Tensor dropout_state_out, Tensor reserve, Tensor out_grad, Tensor[] state_grad, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, str mode, int seed, bool is_test)
  output : Tensor(x_grad), Tensor[](pre_state_grad){pre_state.size()}, Tensor[](weight_list_grad){weight_list.size()}
  infer_meta :
    func : RnnGradInferMeta
    param : [x, pre_state, weight_list]
  kernel :
    func : rnn_grad
    data_type: out_grad

- backward_op : softmax_grad
  forward : softmax (Tensor x, int axis=-1) -> Tensor(out)
  args : (Tensor out, Tensor out_grad, int axis)
  output : Tensor(x_grad)
  infer_meta :
    func : UnchangedInferMeta
    param : [out]
  kernel :
    func : softmax_grad
  composite : softmax_grad(out, out_grad, axis, x_grad)

- backward_op: unpool_grad
  forward: unpool (Tensor x, Tensor indices, int[] ksize, str unpooling_type, int[] strides = {1,1}, int[] paddings ={0,0} ,IntArray output_size = {0,0}, str data_format="NCHW") -> Tensor(out)
  args: (Tensor x, Tensor indices, Tensor out, Tensor out_grad, int[] ksize, int[] strides, int[] paddings, IntArray output_size, str data_format)
  output: Tensor(x_grad)
  infer_meta:
    func: UnchangedInferMeta
    param : [x]
  kernel:
    func: unpool_grad
    data_type: x
