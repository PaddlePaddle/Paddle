// this file is generated by paddle/phi/api/yaml/generator/generate_op.py, do not edit.
#include <string>
#include "paddle/fluid/framework/convert_utils.h"
#include "paddle/fluid/framework/infershape_utils.h"
#include "paddle/fluid/framework/op_registry.h"
#include "paddle/fluid/framework/op_version_registry.h"
#include "paddle/fluid/prim/api/composite_backward/composite_backward_api.h"
#include "paddle/fluid/prim/utils/static/composite_grad_desc_maker.h"
#include "paddle/fluid/prim/utils/static/desc_tensor.h"
#include "paddle/fluid/operators/generator/get_expected_kernel_func.h"
#include "paddle/phi/core/infermeta_utils.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"

namespace paddle {
namespace operators {

using paddle::framework::GradVarName;


class AddActXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of add_act_xpu op.");
    AddInput("x_max", "(Tensor), input 1 of add_act_xpu op.")
        .AsDispensable();
    AddInput("y", "(Tensor), input 2 of add_act_xpu op.");
    AddInput("y_max", "(Tensor), input 3 of add_act_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of add_act_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of add_act_xpu op.");
    AddAttr<int>("act_type", "(int), attribute 0 for add_act_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of add_act_xpu op.
)DOC");
  }
};


class AddActXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(add_act_xpu, AddActXpuInferShapeFunctor,
                            PD_INFER_META(phi::AddActXPUInferMeta));



class AddLayernormXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of add_layernorm_xpu op.");
    AddInput("y", "(Tensor), input 1 of add_layernorm_xpu op.");
    AddInput("scale", "(Tensor), input 2 of add_layernorm_xpu op.");
    AddInput("bias", "(Tensor), input 3 of add_layernorm_xpu op.");
    AddOutput("out", "(Tensor), output 0 of add_layernorm_xpu op.");
    AddAttr<int>("begin_norm_axis", "(int), attribute 0 for add_layernorm_xpu op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 1 for add_layernorm_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of add_layernorm_xpu op.
)DOC");
  }
};


class AddLayernormXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(add_layernorm_xpu, AddLayernormXpuInferShapeFunctor,
                            PD_INFER_META(phi::AddLayernormXPUInferMeta));



class AddcmulXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of addcmul_xpu op.");
    AddInput("y", "(Tensor), input 1 of addcmul_xpu op.");
    AddInput("w", "(Tensor), input 2 of addcmul_xpu op.");
    AddOutput("out", "(Tensor), output 0 of addcmul_xpu op.");
    AddComment(R"DOC(
TODO: Documentation of addcmul_xpu op.
)DOC");
  }
};


class AddcmulXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(addcmul_xpu, AddcmulXpuInferShapeFunctor,
                            PD_INFER_META(phi::AddCMulXPUInferMeta));



class BlockMultiheadAttentionOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("qkv", "(Tensor), input 0 of block_multihead_attention op.");
    AddInput("key_cache", "(Tensor), input 1 of block_multihead_attention op.");
    AddInput("value_cache", "(Tensor), input 2 of block_multihead_attention op.");
    AddInput("seq_lens_encoder", "(Tensor), input 3 of block_multihead_attention op.");
    AddInput("seq_lens_decoder", "(Tensor), input 4 of block_multihead_attention op.");
    AddInput("seq_lens_this_time", "(Tensor), input 5 of block_multihead_attention op.");
    AddInput("padding_offsets", "(Tensor), input 6 of block_multihead_attention op.");
    AddInput("cum_offsets", "(Tensor), input 7 of block_multihead_attention op.");
    AddInput("cu_seqlens_q", "(Tensor), input 8 of block_multihead_attention op.");
    AddInput("cu_seqlens_k", "(Tensor), input 9 of block_multihead_attention op.");
    AddInput("block_tables", "(Tensor), input 10 of block_multihead_attention op.");
    AddInput("pre_key_cache", "(Tensor), input 11 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("pre_value_cache", "(Tensor), input 12 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("rope_emb", "(Tensor), input 13 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("mask", "(Tensor), input 14 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("tgt_mask", "(Tensor), input 15 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("cache_k_quant_scales", "(Tensor), input 16 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("cache_v_quant_scales", "(Tensor), input 17 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("cache_k_dequant_scales", "(Tensor), input 18 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("cache_v_dequant_scales", "(Tensor), input 19 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("qkv_out_scale", "(Tensor), input 20 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("qkv_bias", "(Tensor), input 21 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("out_shift", "(Tensor), input 22 of block_multihead_attention op.")
        .AsDispensable();
    AddInput("out_smooth", "(Tensor), input 23 of block_multihead_attention op.")
        .AsDispensable();
    AddOutput("fmha_out", "(Tensor), output 0 of block_multihead_attention op.");
    AddOutput("qkv_out", "(Tensor), output 1 of block_multihead_attention op.");
    AddOutput("key_cache_out", "(Tensor), output 2 of block_multihead_attention op.");
    AddOutput("value_cache_out", "(Tensor), output 3 of block_multihead_attention op.");
    AddAttr<int>("max_seq_len", "(int), attribute 0 for block_multihead_attention op.")
    ;
    AddAttr<int>("block_size", "(int), attribute 1 for block_multihead_attention op.")
    ;
    AddAttr<bool>("use_neox_style", "(bool), attribute 2 for block_multihead_attention op.")
    ;
    AddAttr<bool>("dynamic_cachekv_quant", "(bool), attribute 3 for block_multihead_attention op.")
        .SetDefault(false);
    AddAttr<int>("quant_round_type", "(int), attribute 4 for block_multihead_attention op.")
        .SetDefault(1);
    AddAttr<float>("quant_max_bound", "(float), attribute 5 for block_multihead_attention op.")
        .SetDefault(127.0);
    AddAttr<float>("quant_min_bound", "(float), attribute 6 for block_multihead_attention op.")
        .SetDefault(-127.0);
    AddAttr<float>("out_scale", "(float), attribute 7 for block_multihead_attention op.")
        .SetDefault(-1);
    AddAttr<std::string>("compute_dtype", "(std::string), attribute 8 for block_multihead_attention op.")
        .SetDefault("default");
    AddComment(R"DOC(
TODO: Documentation of block_multihead_attention op.
)DOC");
  }
};


class BlockMultiheadAttentionOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "qkv");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(block_multihead_attention, BlockMultiheadAttentionInferShapeFunctor,
                            PD_INFER_META(phi::BlockMultiheadAttentionInferMeta));



class BnActXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of bn_act_xpu op.");
    AddInput("mean", "(Tensor), input 1 of bn_act_xpu op.");
    AddInput("variance", "(Tensor), input 2 of bn_act_xpu op.");
    AddInput("scale", "(Tensor), input 3 of bn_act_xpu op.");
    AddInput("bias", "(Tensor), input 4 of bn_act_xpu op.");
    AddOutput("out", "(Tensor), output 0 of bn_act_xpu op.");
    AddAttr<float>("momentum", "(float), attribute 0 for bn_act_xpu op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 1 for bn_act_xpu op.")
    ;
    AddAttr<std::string>("data_layout", "(std::string), attribute 2 for bn_act_xpu op.")
    ;
    AddAttr<int>("act_type", "(int), attribute 3 for bn_act_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of bn_act_xpu op.
)DOC");
  }
};


class BnActXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(bn_act_xpu, BnActXpuInferShapeFunctor,
                            PD_INFER_META(phi::BNActXPUInferMeta));



class Conv1dXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of conv1d_xpu op.");
    AddInput("x_max", "(Tensor), input 1 of conv1d_xpu op.")
        .AsDispensable();
    AddInput("filter", "(Tensor), input 2 of conv1d_xpu op.");
    AddInput("filter_max", "(Tensor), input 3 of conv1d_xpu op.");
    AddInput("bias", "(Tensor), input 4 of conv1d_xpu op.")
        .AsDispensable();
    AddInput("branch", "(Tensor), input 5 of conv1d_xpu op.")
        .AsDispensable();
    AddInput("branch_max", "(Tensor), input 6 of conv1d_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of conv1d_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of conv1d_xpu op.");
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 0 for conv1d_xpu op.")
    ;
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 1 for conv1d_xpu op.")
    ;
    AddAttr<int>("dilations", "(int), attribute 2 for conv1d_xpu op.")
    ;
    AddAttr<int>("strides", "(int), attribute 3 for conv1d_xpu op.")
    ;
    AddAttr<int>("groups", "(int), attribute 4 for conv1d_xpu op.")
    ;
    AddAttr<int>("act_type", "(int), attribute 5 for conv1d_xpu op.")
    ;
    AddAttr<float>("act_param", "(float), attribute 6 for conv1d_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of conv1d_xpu op.
)DOC");
  }
};


class Conv1dXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(conv1d_xpu, Conv1dXpuInferShapeFunctor,
                            PD_INFER_META(phi::Conv1dXPUInferMeta));



class Conv2dTransposeXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of conv2d_transpose_xpu op.");
    AddInput("x_max", "(Tensor), input 1 of conv2d_transpose_xpu op.")
        .AsDispensable();
    AddInput("filter", "(Tensor), input 2 of conv2d_transpose_xpu op.");
    AddInput("filter_max", "(Tensor), input 3 of conv2d_transpose_xpu op.");
    AddInput("bias", "(Tensor), input 4 of conv2d_transpose_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of conv2d_transpose_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of conv2d_transpose_xpu op.");
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 0 for conv2d_transpose_xpu op.")
    ;
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 1 for conv2d_transpose_xpu op.")
    ;
    AddAttr<std::vector<int>>("output_padding", "(std::vector<int>), attribute 2 for conv2d_transpose_xpu op.")
    ;
    AddInput("OutputSizeTensor", "attribute 3 for conv2d_transpose_xpu op from 1D integer Tensor.")
        .AsDispensable();
    AddInput("OutputSizeTensorList", "attribute 3 for conv2d_transpose_xpu op from list fo 0D integer Tensors.")
        .AsDuplicable()
        .AsDispensable();
      AddAttr<std::vector<int64_t>>("output_size", "(std::vector<int64_t>), attribute 3 for conv2d_transpose_xpu op.")
    ;
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 4 for conv2d_transpose_xpu op.")
    ;
    AddAttr<int>("groups", "(int), attribute 5 for conv2d_transpose_xpu op.")
    ;
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 6 for conv2d_transpose_xpu op.")
    ;
    AddAttr<std::string>("data_format", "(std::string), attribute 7 for conv2d_transpose_xpu op.")
    ;
    AddAttr<bool>("has_bias", "(bool), attribute 8 for conv2d_transpose_xpu op.")
    ;
    AddAttr<bool>("with_act", "(bool), attribute 9 for conv2d_transpose_xpu op.")
    ;
    AddAttr<std::string>("act_type", "(std::string), attribute 10 for conv2d_transpose_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of conv2d_transpose_xpu op.
)DOC");
  }
};


class Conv2dTransposeXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(conv2d_transpose_xpu, Conv2dTransposeXpuInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dTransposeXPUInferMeta));



class Conv2dXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of conv2d_xpu op.");
    AddInput("x_max", "(Tensor), input 1 of conv2d_xpu op.")
        .AsDispensable();
    AddInput("filter", "(Tensor), input 2 of conv2d_xpu op.");
    AddInput("filter_max", "(Tensor), input 3 of conv2d_xpu op.");
    AddInput("bias", "(Tensor), input 4 of conv2d_xpu op.")
        .AsDispensable();
    AddInput("branch", "(Tensor), input 5 of conv2d_xpu op.")
        .AsDispensable();
    AddInput("branch_max", "(Tensor), input 6 of conv2d_xpu op.")
        .AsDispensable();
    AddInput("scale_max", "(Tensor), input 7 of conv2d_xpu op.")
        .AsDispensable();
    AddInput("out_max_in", "(Tensor), input 8 of conv2d_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of conv2d_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of conv2d_xpu op.");
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 0 for conv2d_xpu op.")
    ;
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 1 for conv2d_xpu op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 2 for conv2d_xpu op.")
    ;
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 3 for conv2d_xpu op.")
    ;
    AddAttr<int>("groups", "(int), attribute 4 for conv2d_xpu op.")
    ;
    AddAttr<int>("act_type", "(int), attribute 5 for conv2d_xpu op.")
    ;
    AddAttr<float>("act_param", "(float), attribute 6 for conv2d_xpu op.")
    ;
    AddAttr<int>("out_dtype", "(int), attribute 7 for conv2d_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of conv2d_xpu op.
)DOC");
  }
};


class Conv2dXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(conv2d_xpu, Conv2dXpuInferShapeFunctor,
                            PD_INFER_META(phi::Conv2dXPUInferMeta));



class CrossAttentionXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("input_q", "(Tensor), input 0 of cross_attention_xpu op.");
    AddInput("input_kv", "(Tensor), input 1 of cross_attention_xpu op.");
    AddInput("fc_weight", "(Tensor[]), input 2 of cross_attention_xpu op.")
        .AsDuplicable();
    AddInput("fc_weight_max", "(Tensor[]), input 3 of cross_attention_xpu op.")
        .AsDuplicable();
    AddInput("fc_bias", "(Tensor[]), input 4 of cross_attention_xpu op.")
        .AsDuplicable();
    AddInput("mask", "(Tensor), input 5 of cross_attention_xpu op.");
    AddOutput("qkv", "(Tensor), output 0 of cross_attention_xpu op.");
    AddOutput("qkv_max", "(Tensor), output 1 of cross_attention_xpu op.");
    AddAttr<int>("head_num", "(int), attribute 0 for cross_attention_xpu op.")
    ;
    AddAttr<int>("head_dim", "(int), attribute 1 for cross_attention_xpu op.")
    ;
    AddAttr<float>("alpha", "(float), attribute 2 for cross_attention_xpu op.")
    ;
    AddAttr<int>("out_dtype", "(int), attribute 3 for cross_attention_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of cross_attention_xpu op.
)DOC");
  }
};


class CrossAttentionXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "input_q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(cross_attention_xpu, CrossAttentionXpuInferShapeFunctor,
                            PD_INFER_META(phi::CrossAttentionXPUInferMeta));



class DequantizeXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of dequantize_xpu op.");
    AddOutput("y", "(Tensor), output 0 of dequantize_xpu op.");
    AddAttr<int>("out_dtype", "(int), attribute 0 for dequantize_xpu op.")
    ;
    AddAttr<float>("scale", "(float), attribute 1 for dequantize_xpu op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of dequantize_xpu op.
)DOC");
  }
};


class DequantizeXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(dequantize_xpu, DequantizeXpuInferShapeFunctor,
                            PD_INFER_META(phi::DeQuantizeXPUInferMeta));



class EmbeddingWithEltwiseAddXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("ids", "(Tensor[]), input 0 of embedding_with_eltwise_add_xpu op.")
        .AsDuplicable();
    AddInput("tables", "(Tensor[]), input 1 of embedding_with_eltwise_add_xpu op.")
        .AsDuplicable();
    AddInput("mask", "(Tensor), input 2 of embedding_with_eltwise_add_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of embedding_with_eltwise_add_xpu op.");
    AddOutput("seq_lod", "(Tensor), output 1 of embedding_with_eltwise_add_xpu op.")
        .AsDispensable();
    AddOutput("max_seq_len", "(Tensor), output 2 of embedding_with_eltwise_add_xpu op.")
        .AsDispensable();
    AddAttr<int64_t>("padding_idx", "(int64_t), attribute 0 for embedding_with_eltwise_add_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of embedding_with_eltwise_add_xpu op.
)DOC");
  }
};


class EmbeddingWithEltwiseAddXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "tables");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(embedding_with_eltwise_add_xpu, EmbeddingWithEltwiseAddXpuInferShapeFunctor,
                            PD_INFER_META(phi::EmbeddingWithEltwiseAddXPUInferMeta));



class FastLayernormXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fast_layernorm_xpu op.");
    AddInput("scale", "(Tensor), input 1 of fast_layernorm_xpu op.");
    AddInput("bias", "(Tensor), input 2 of fast_layernorm_xpu op.");
    AddOutput("out", "(Tensor), output 0 of fast_layernorm_xpu op.");
    AddAttr<int>("begin_norm_axis", "(int), attribute 0 for fast_layernorm_xpu op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 1 for fast_layernorm_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fast_layernorm_xpu op.
)DOC");
  }
};


class FastLayernormXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fast_layernorm_xpu, FastLayernormXpuInferShapeFunctor,
                            PD_INFER_META(phi::FastLayernormXPUInferMeta));



class FastWhereXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("condition", "(Tensor), input 0 of fast_where_xpu op.");
    AddInput("x", "(Tensor), input 1 of fast_where_xpu op.");
    AddInput("y", "(Tensor), input 2 of fast_where_xpu op.");
    AddOutput("out", "(Tensor), output 0 of fast_where_xpu op.");
    AddComment(R"DOC(
TODO: Documentation of fast_where_xpu op.
)DOC");
  }
};


class FastWhereXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fast_where_xpu, FastWhereXpuInferShapeFunctor,
                            PD_INFER_META(phi::FastWhereXPUInferMeta));



class FcOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of fc op.");
    AddInput("W", "(Tensor), input 1 of fc op.");
    AddInput("Bias", "(Tensor), input 2 of fc op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of fc op.");
    AddAttr<int>("in_num_col_dims", "(int), attribute 0 for fc op.")
        .SetDefault(1);
    AddAttr<std::string>("activation_type", "(std::string), attribute 1 for fc op.")
        .SetDefault("");
    AddAttr<bool>("padding_weights", "(bool), attribute 2 for fc op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of fc op.
)DOC");
  }
};


class FcOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fc, FcInferShapeFunctor,
                            PD_INFER_META(phi::FCInferMeta));



class FcXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fc_xpu op.");
    AddInput("x_max", "(Tensor), input 1 of fc_xpu op.")
        .AsDispensable();
    AddInput("w", "(Tensor), input 2 of fc_xpu op.");
    AddInput("w_max", "(Tensor), input 3 of fc_xpu op.");
    AddInput("bias", "(Tensor), input 4 of fc_xpu op.")
        .AsDispensable();
    AddInput("scale_max", "(Tensor), input 5 of fc_xpu op.")
        .AsDispensable();
    AddInput("out_max_in", "(Tensor), input 6 of fc_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of fc_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of fc_xpu op.");
    AddAttr<int>("in_num_col_dims", "(int), attribute 0 for fc_xpu op.")
    ;
    AddAttr<bool>("transpose_x", "(bool), attribute 1 for fc_xpu op.")
    ;
    AddAttr<float>("alpha", "(float), attribute 2 for fc_xpu op.")
    ;
    AddAttr<float>("beta", "(float), attribute 3 for fc_xpu op.")
    ;
    AddAttr<int>("act_type", "(int), attribute 4 for fc_xpu op.")
    ;
    AddAttr<float>("act_alpha", "(float), attribute 5 for fc_xpu op.")
    ;
    AddAttr<int>("out_dtype", "(int), attribute 6 for fc_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fc_xpu op.
)DOC");
  }
};


class FcXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fc_xpu, FcXpuInferShapeFunctor,
                            PD_INFER_META(phi::FcXPUInferMeta));



class FusedBiasActOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_bias_act op.");
    AddInput("bias", "(Tensor), input 1 of fused_bias_act op.")
        .AsDispensable();
    AddInput("dequant_scales", "(Tensor), input 2 of fused_bias_act op.")
        .AsDispensable();
    AddInput("shift", "(Tensor), input 3 of fused_bias_act op.")
        .AsDispensable();
    AddInput("smooth", "(Tensor), input 4 of fused_bias_act op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of fused_bias_act op.");
    AddAttr<std::string>("act_method", "(std::string), attribute 0 for fused_bias_act op.")
        .SetDefault("gelu");
    AddAttr<std::string>("compute_dtype", "(std::string), attribute 1 for fused_bias_act op.")
        .SetDefault("default");
    AddAttr<float>("quant_scale", "(float), attribute 2 for fused_bias_act op.")
        .SetDefault(-1);
    AddAttr<int>("quant_round_type", "(int), attribute 3 for fused_bias_act op.")
        .SetDefault(1);
    AddAttr<float>("quant_max_bound", "(float), attribute 4 for fused_bias_act op.")
        .SetDefault(127.0);
    AddAttr<float>("quant_min_bound", "(float), attribute 5 for fused_bias_act op.")
        .SetDefault(-127.0);
    AddComment(R"DOC(
TODO: Documentation of fused_bias_act op.
)DOC");
  }
};


class FusedBiasActOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_bias_act, FusedBiasActInferShapeFunctor,
                            PD_INFER_META(phi::FusedBiasActInferMeta));



class FusedBiasDropoutResidualLayerNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fused_bias_dropout_residual_layer_norm op.");
    AddInput("Residual", "(Tensor), input 1 of fused_bias_dropout_residual_layer_norm op.");
    AddInput("Bias", "(Tensor), input 2 of fused_bias_dropout_residual_layer_norm op.")
        .AsDispensable();
    AddInput("LnScale", "(Tensor), input 3 of fused_bias_dropout_residual_layer_norm op.")
        .AsDispensable();
    AddInput("LnBias", "(Tensor), input 4 of fused_bias_dropout_residual_layer_norm op.")
        .AsDispensable();
    AddOutput("Y", "(Tensor), output 0 of fused_bias_dropout_residual_layer_norm op.");
    AddOutput("BiasDropoutResidualOut", "(Tensor), output 1 of fused_bias_dropout_residual_layer_norm op.")
        .AsIntermediate();
    AddOutput("DropoutMaskOut", "(Tensor), output 2 of fused_bias_dropout_residual_layer_norm op.")
        .AsIntermediate();
    AddOutput("LnMean", "(Tensor), output 3 of fused_bias_dropout_residual_layer_norm op.")
        .AsIntermediate();
    AddOutput("LnVariance", "(Tensor), output 4 of fused_bias_dropout_residual_layer_norm op.")
        .AsIntermediate();
    AddAttr<float>("dropout_rate", "(float), attribute 0 for fused_bias_dropout_residual_layer_norm op.")
        .SetDefault(0.5f);
    AddAttr<bool>("is_test", "(bool), attribute 1 for fused_bias_dropout_residual_layer_norm op.")
        .SetDefault(false);
    AddAttr<bool>("dropout_fix_seed", "(bool), attribute 2 for fused_bias_dropout_residual_layer_norm op.")
        .SetDefault(true);
    AddAttr<int>("dropout_seed", "(int), attribute 3 for fused_bias_dropout_residual_layer_norm op.")
        .SetDefault(true);
    AddAttr<std::string>("dropout_implementation", "(std::string), attribute 4 for fused_bias_dropout_residual_layer_norm op.")
        .SetDefault("downgrade_in_infer");
    AddAttr<float>("ln_epsilon", "(float), attribute 5 for fused_bias_dropout_residual_layer_norm op.")
        .SetDefault(1e-5);
    AddComment(R"DOC(
TODO: Documentation of fused_bias_dropout_residual_layer_norm op.
)DOC");
  }
};


class FusedBiasDropoutResidualLayerNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_bias_dropout_residual_layer_norm, FusedBiasDropoutResidualLayerNormInferShapeFunctor,
                            PD_INFER_META(phi::FusedBiasDropoutResidualLnInferMeta));



class FusedBiasResidualLayernormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_bias_residual_layernorm op.");
    AddInput("bias", "(Tensor), input 1 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddInput("residual", "(Tensor), input 2 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddInput("norm_weight", "(Tensor), input 3 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddInput("norm_bias", "(Tensor), input 4 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of fused_bias_residual_layernorm op.");
    AddOutput("residual_out", "(Tensor), output 1 of fused_bias_residual_layernorm op.")
        .AsDispensable();
    AddOutput("mean", "(Tensor), output 2 of fused_bias_residual_layernorm op.");
    AddOutput("variance", "(Tensor), output 3 of fused_bias_residual_layernorm op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("residual_alpha", "(float), attribute 1 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<int>("begin_norm_axis", "(int), attribute 2 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("quant_scale", "(float), attribute 3 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<int>("quant_round_type", "(int), attribute 4 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("quant_max_bound", "(float), attribute 5 for fused_bias_residual_layernorm op.")
    ;
    AddAttr<float>("quant_min_bound", "(float), attribute 6 for fused_bias_residual_layernorm op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fused_bias_residual_layernorm op.
)DOC");
  }
};


class FusedBiasResidualLayernormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_bias_residual_layernorm, FusedBiasResidualLayernormInferShapeFunctor,
                            PD_INFER_META(phi::FusedLayerNormInferMeta));



class FusedConv2dAddActOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of fused_conv2d_add_act op.");
    AddInput("Filter", "(Tensor), input 1 of fused_conv2d_add_act op.");
    AddInput("Bias", "(Tensor), input 2 of fused_conv2d_add_act op.")
        .AsDispensable();
    AddInput("ResidualData", "(Tensor), input 3 of fused_conv2d_add_act op.")
        .AsDispensable();
    AddOutput("Output", "(Tensor), output 0 of fused_conv2d_add_act op.");
    AddOutput("Outputs", "(Tensor[]), output 1 of fused_conv2d_add_act op.")
        .AsDuplicable()
        .AsDispensable();
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 0 for fused_conv2d_add_act op.")
        .SetDefault({1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 1 for fused_conv2d_add_act op.")
        .SetDefault({0, 0});
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 2 for fused_conv2d_add_act op.")
        .SetDefault("EXPLICIT");
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 3 for fused_conv2d_add_act op.")
        .SetDefault({1, 1});
    AddAttr<int>("groups", "(int), attribute 4 for fused_conv2d_add_act op.")
        .SetDefault(1);
    AddAttr<std::string>("data_format", "(std::string), attribute 5 for fused_conv2d_add_act op.")
        .SetDefault("NCHW");
    AddAttr<std::string>("activation", "(std::string), attribute 6 for fused_conv2d_add_act op.")
        .SetDefault("relu");
    AddAttr<std::vector<int>>("split_channels", "(std::vector<int>), attribute 7 for fused_conv2d_add_act op.")
        .SetDefault({});
    AddAttr<bool>("exhaustive_search", "(bool), attribute 8 for fused_conv2d_add_act op.")
        .SetDefault(false);
    AddAttr<int>("workspace_size_MB", "(int), attribute 9 for fused_conv2d_add_act op.")
        .SetDefault(32);
    AddAttr<float>("fuse_alpha", "(float), attribute 10 for fused_conv2d_add_act op.")
        .SetDefault(0.0f);
    AddComment(R"DOC(
TODO: Documentation of fused_conv2d_add_act op.
)DOC");
  }
};


class FusedConv2dAddActOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetConvExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_conv2d_add_act, FusedConv2dAddActInferShapeFunctor,
                            PD_INFER_META(phi::FusedConv2dAddActInferMeta));



class FusedDconvDreluDbnOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("grad_output", "(Tensor), input 0 of fused_dconv_drelu_dbn op.");
    AddInput("weight", "(Tensor), input 1 of fused_dconv_drelu_dbn op.");
    AddInput("grad_output_add", "(Tensor), input 2 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("residual_input", "(Tensor), input 3 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn1_eqscale", "(Tensor), input 4 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn1_eqbias", "(Tensor), input 5 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("conv_input", "(Tensor), input 6 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn1_mean", "(Tensor), input 7 of fused_dconv_drelu_dbn op.");
    AddInput("bn1_inv_std", "(Tensor), input 8 of fused_dconv_drelu_dbn op.");
    AddInput("bn1_gamma", "(Tensor), input 9 of fused_dconv_drelu_dbn op.");
    AddInput("bn1_beta", "(Tensor), input 10 of fused_dconv_drelu_dbn op.");
    AddInput("bn1_input", "(Tensor), input 11 of fused_dconv_drelu_dbn op.");
    AddInput("bn2_mean", "(Tensor), input 12 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn2_inv_std", "(Tensor), input 13 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn2_gamma", "(Tensor), input 14 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn2_beta", "(Tensor), input 15 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddInput("bn2_input", "(Tensor), input 16 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddOutput("grad_weight", "(Tensor), output 0 of fused_dconv_drelu_dbn op.");
    AddOutput("grad_bn1_input", "(Tensor), output 1 of fused_dconv_drelu_dbn op.");
    AddOutput("grad_bn1_gamma", "(Tensor), output 2 of fused_dconv_drelu_dbn op.");
    AddOutput("grad_bn1_beta", "(Tensor), output 3 of fused_dconv_drelu_dbn op.");
    AddOutput("grad_bn2_input", "(Tensor), output 4 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddOutput("grad_bn2_gamma", "(Tensor), output 5 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddOutput("grad_bn2_beta", "(Tensor), output 6 of fused_dconv_drelu_dbn op.")
        .AsDispensable();
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 0 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 1 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 2 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 3 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<int>("groups", "(int), attribute 4 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<std::string>("data_format", "(std::string), attribute 5 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<bool>("fuse_shortcut", "(bool), attribute 6 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<bool>("fuse_dual", "(bool), attribute 7 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<bool>("fuse_add", "(bool), attribute 8 for fused_dconv_drelu_dbn op.")
    ;
    AddAttr<bool>("exhaustive_search", "(bool), attribute 9 for fused_dconv_drelu_dbn op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fused_dconv_drelu_dbn op.
)DOC");
  }
};


class FusedDconvDreluDbnOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "grad_output");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_dconv_drelu_dbn, FusedDconvDreluDbnInferShapeFunctor,
                            PD_INFER_META(phi::FusedDconvDreluDbnInferMeta));



class FusedDotProductAttentionOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("q", "(Tensor), input 0 of fused_dot_product_attention op.");
    AddInput("k", "(Tensor), input 1 of fused_dot_product_attention op.");
    AddInput("v", "(Tensor), input 2 of fused_dot_product_attention op.");
    AddInput("mask", "(Tensor), input 3 of fused_dot_product_attention op.");
    AddOutput("out", "(Tensor), output 0 of fused_dot_product_attention op.");
    AddOutput("softmax_out", "(Tensor), output 1 of fused_dot_product_attention op.");
    AddOutput("rng_state", "(Tensor), output 2 of fused_dot_product_attention op.");
    AddAttr<float>("scaling_factor", "(float), attribute 0 for fused_dot_product_attention op.")
    ;
    AddAttr<float>("dropout_probability", "(float), attribute 1 for fused_dot_product_attention op.")
    ;
    AddAttr<bool>("is_training", "(bool), attribute 2 for fused_dot_product_attention op.")
        .SetDefault(false);
    AddAttr<bool>("is_causal_masking", "(bool), attribute 3 for fused_dot_product_attention op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of fused_dot_product_attention op.
)DOC");
  }
};


class FusedDotProductAttentionOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_dot_product_attention, FusedDotProductAttentionInferShapeFunctor,
                            PD_INFER_META(phi::FusedDotProductAttentionInferMeta));



class FusedDropoutAddOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_dropout_add op.");
    AddInput("y", "(Tensor), input 1 of fused_dropout_add op.");
    AddInput("seed_tensor", "(Tensor), input 2 of fused_dropout_add op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of fused_dropout_add op.");
    AddOutput("seed_offset", "(Tensor), output 1 of fused_dropout_add op.");
    AddInput("PTensor", "attribute 0 for fused_dropout_add op from 0D Tensor.")
        .AsDispensable();
    AddAttr<float>("p", "(float), attribute 0 for fused_dropout_add op.")
    ;
    AddAttr<bool>("is_test", "(bool), attribute 1 for fused_dropout_add op.")
    ;
    AddAttr<std::string>("mode", "(std::string), attribute 2 for fused_dropout_add op.")
    ;
    AddAttr<int>("seed", "(int), attribute 3 for fused_dropout_add op.")
        .SetDefault(0);
    AddAttr<bool>("fix_seed", "(bool), attribute 4 for fused_dropout_add op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of fused_dropout_add op.
)DOC");
  }
};


class FusedDropoutAddOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_dropout_add, FusedDropoutAddInferShapeFunctor,
                            PD_INFER_META(phi::FusedDropoutAddInferMeta));



class FusedEmbeddingEltwiseLayernormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Ids", "(Tensor[]), input 0 of fused_embedding_eltwise_layernorm op.")
        .AsDuplicable();
    AddInput("Embs", "(Tensor[]), input 1 of fused_embedding_eltwise_layernorm op.")
        .AsDuplicable();
    AddInput("Bias", "(Tensor), input 2 of fused_embedding_eltwise_layernorm op.");
    AddInput("Scale", "(Tensor), input 3 of fused_embedding_eltwise_layernorm op.");
    AddOutput("Out", "(Tensor), output 0 of fused_embedding_eltwise_layernorm op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for fused_embedding_eltwise_layernorm op.")
        .SetDefault(0.00001f);
    AddComment(R"DOC(
TODO: Documentation of fused_embedding_eltwise_layernorm op.
)DOC");
  }
};


class FusedEmbeddingEltwiseLayernormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Embs");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_embedding_eltwise_layernorm, FusedEmbeddingEltwiseLayernormInferShapeFunctor,
                            PD_INFER_META(phi::FusedEmbeddingEltWiseLayerNormInferMeta));



class FusedFcElementwiseLayernormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fused_fc_elementwise_layernorm op.");
    AddInput("W", "(Tensor), input 1 of fused_fc_elementwise_layernorm op.");
    AddInput("Y", "(Tensor), input 2 of fused_fc_elementwise_layernorm op.");
    AddInput("Bias0", "(Tensor), input 3 of fused_fc_elementwise_layernorm op.")
        .AsDispensable();
    AddInput("Scale", "(Tensor), input 4 of fused_fc_elementwise_layernorm op.")
        .AsDispensable();
    AddInput("Bias1", "(Tensor), input 5 of fused_fc_elementwise_layernorm op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of fused_fc_elementwise_layernorm op.");
    AddOutput("Mean", "(Tensor), output 1 of fused_fc_elementwise_layernorm op.")
        .AsDispensable();
    AddOutput("Variance", "(Tensor), output 2 of fused_fc_elementwise_layernorm op.")
        .AsDispensable();
    AddAttr<int>("x_num_col_dims", "(int), attribute 0 for fused_fc_elementwise_layernorm op.")
        .SetDefault(1);
    AddAttr<std::string>("activation_type", "(std::string), attribute 1 for fused_fc_elementwise_layernorm op.")
        .SetDefault("");
    AddAttr<float>("epsilon", "(float), attribute 2 for fused_fc_elementwise_layernorm op.")
        .SetDefault(0.00001f);
    AddAttr<int>("begin_norm_axis", "(int), attribute 3 for fused_fc_elementwise_layernorm op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of fused_fc_elementwise_layernorm op.
)DOC");
  }
};


class FusedFcElementwiseLayernormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_fc_elementwise_layernorm, FusedFcElementwiseLayernormInferShapeFunctor,
                            PD_INFER_META(phi::FusedFCElementwiseLayerNormInferMeta));



class FusedLinearParamGradAddOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_linear_param_grad_add op.");
    AddInput("dout", "(Tensor), input 1 of fused_linear_param_grad_add op.");
    AddInput("dweight", "(Tensor), input 2 of fused_linear_param_grad_add op.")
        .AsDispensable();
    AddInput("dbias", "(Tensor), input 3 of fused_linear_param_grad_add op.")
        .AsDispensable();
    AddOutput("dweight_out", "(Tensor), output 0 of fused_linear_param_grad_add op.");
    AddOutput("dbias_out", "(Tensor), output 1 of fused_linear_param_grad_add op.");
    AddAttr<bool>("multi_precision", "(bool), attribute 0 for fused_linear_param_grad_add op.")
        .SetDefault(true);
    AddAttr<bool>("has_bias", "(bool), attribute 1 for fused_linear_param_grad_add op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of fused_linear_param_grad_add op.
)DOC");
  }
};


class FusedLinearParamGradAddOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "dout");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_linear_param_grad_add, FusedLinearParamGradAddInferShapeFunctor,
                            PD_INFER_META(phi::FusedLinearParamGradAddInferMeta));



class FusedMultiTransformerInt8XpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_multi_transformer_int8_xpu op.");
    AddInput("ln_scale", "(Tensor[]), input 1 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ln_bias", "(Tensor[]), input 2 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("qkv_in_max", "(Tensor[]), input 3 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("qkvw", "(Tensor[]), input 4 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("qkv_bias", "(Tensor[]), input 5 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("qkv_scales", "(Tensor[]), input 6 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_in_max", "(Tensor[]), input 7 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_w", "(Tensor[]), input 8 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_bias", "(Tensor[]), input 9 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_scales", "(Tensor[]), input 10 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn_ln_scale", "(Tensor[]), input 11 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn_ln_bias", "(Tensor[]), input 12 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_in_max", "(Tensor[]), input 13 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_weight", "(Tensor[]), input 14 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_bias", "(Tensor[]), input 15 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_scales", "(Tensor[]), input 16 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_in_max", "(Tensor[]), input 17 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_weight", "(Tensor[]), input 18 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_bias", "(Tensor[]), input 19 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_scales", "(Tensor[]), input 20 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddInput("cache_kv", "(Tensor[]), input 21 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable()
        .AsDispensable();
    AddInput("pre_caches", "(Tensor[]), input 22 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable()
        .AsDispensable();
    AddInput("rotary_pos_emb", "(Tensor), input 23 of fused_multi_transformer_int8_xpu op.")
        .AsDispensable();
    AddInput("time_step", "(Tensor), input 24 of fused_multi_transformer_int8_xpu op.")
        .AsDispensable();
    AddInput("seq_lengths", "(Tensor), input 25 of fused_multi_transformer_int8_xpu op.")
        .AsDispensable();
    AddInput("src_mask", "(Tensor), input 26 of fused_multi_transformer_int8_xpu op.")
        .AsDispensable();
    AddInput("gather_index", "(Tensor), input 27 of fused_multi_transformer_int8_xpu op.")
        .AsDispensable();
    AddInput("max_buffer", "(Tensor), input 28 of fused_multi_transformer_int8_xpu op.");
    AddOutput("out", "(Tensor), output 0 of fused_multi_transformer_int8_xpu op.");
    AddOutput("cache_kv_out", "(Tensor[]), output 1 of fused_multi_transformer_int8_xpu op.")
        .AsDuplicable();
    AddAttr<bool>("pre_layer_norm", "(bool), attribute 0 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<int>("rotary_emb_dims", "(int), attribute 1 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 2 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<float>("dropout_rate", "(float), attribute 3 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<bool>("is_test", "(bool), attribute 4 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<std::string>("dropout_implementation", "(std::string), attribute 5 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<std::string>("act_method", "(std::string), attribute 6 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<bool>("trans_qkvw", "(bool), attribute 7 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<int>("ring_id", "(int), attribute 8 for fused_multi_transformer_int8_xpu op.")
    ;
    AddAttr<int>("gather_axis", "(int), attribute 9 for fused_multi_transformer_int8_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fused_multi_transformer_int8_xpu op.
)DOC");
  }
};


class FusedMultiTransformerInt8XpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_multi_transformer_int8_xpu, FusedMultiTransformerInt8XpuInferShapeFunctor,
                            PD_INFER_META(phi::FusedMultiTransformerInt8XpuInferMeta));



class FusedMultiTransformerXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_multi_transformer_xpu op.");
    AddInput("ln_scale", "(Tensor[]), input 1 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ln_bias", "(Tensor[]), input 2 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("qkvw", "(Tensor[]), input 3 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("qkvw_max", "(Tensor[]), input 4 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("qkv_bias", "(Tensor[]), input 5 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_w", "(Tensor[]), input 6 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_wmax", "(Tensor[]), input 7 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("out_linear_bias", "(Tensor[]), input 8 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn_ln_scale", "(Tensor[]), input 9 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn_ln_bias", "(Tensor[]), input 10 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_weight", "(Tensor[]), input 11 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_weight_max", "(Tensor[]), input 12 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn1_bias", "(Tensor[]), input 13 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_weight", "(Tensor[]), input 14 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_weight_max", "(Tensor[]), input 15 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("ffn2_bias", "(Tensor[]), input 16 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddInput("cache_kv", "(Tensor[]), input 17 of fused_multi_transformer_xpu op.")
        .AsDuplicable()
        .AsDispensable();
    AddInput("pre_caches", "(Tensor[]), input 18 of fused_multi_transformer_xpu op.")
        .AsDuplicable()
        .AsDispensable();
    AddInput("rotary_pos_emb", "(Tensor), input 19 of fused_multi_transformer_xpu op.")
        .AsDispensable();
    AddInput("time_step", "(Tensor), input 20 of fused_multi_transformer_xpu op.")
        .AsDispensable();
    AddInput("seq_lengths", "(Tensor), input 21 of fused_multi_transformer_xpu op.")
        .AsDispensable();
    AddInput("src_mask", "(Tensor), input 22 of fused_multi_transformer_xpu op.")
        .AsDispensable();
    AddInput("gather_index", "(Tensor), input 23 of fused_multi_transformer_xpu op.")
        .AsDispensable();
    AddInput("max_buffer", "(Tensor), input 24 of fused_multi_transformer_xpu op.");
    AddOutput("out", "(Tensor), output 0 of fused_multi_transformer_xpu op.");
    AddOutput("cache_kv_out", "(Tensor[]), output 1 of fused_multi_transformer_xpu op.")
        .AsDuplicable();
    AddAttr<bool>("pre_layer_norm", "(bool), attribute 0 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<int>("rotary_emb_dims", "(int), attribute 1 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 2 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<float>("dropout_rate", "(float), attribute 3 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<bool>("is_test", "(bool), attribute 4 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<std::string>("dropout_implementation", "(std::string), attribute 5 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<std::string>("act_method", "(std::string), attribute 6 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<bool>("trans_qkvw", "(bool), attribute 7 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<int>("ring_id", "(int), attribute 8 for fused_multi_transformer_xpu op.")
    ;
    AddAttr<int>("gather_axis", "(int), attribute 9 for fused_multi_transformer_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fused_multi_transformer_xpu op.
)DOC");
  }
};


class FusedMultiTransformerXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_multi_transformer_xpu, FusedMultiTransformerXpuInferShapeFunctor,
                            PD_INFER_META(phi::FusedMultiTransformerXpuInferMeta));



class FusedRotaryPositionEmbeddingOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("q", "(Tensor), input 0 of fused_rotary_position_embedding op.");
    AddInput("k", "(Tensor), input 1 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddInput("v", "(Tensor), input 2 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddInput("sin", "(Tensor), input 3 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddInput("cos", "(Tensor), input 4 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddInput("position_ids", "(Tensor), input 5 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddOutput("out_q", "(Tensor), output 0 of fused_rotary_position_embedding op.");
    AddOutput("out_k", "(Tensor), output 1 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddOutput("out_v", "(Tensor), output 2 of fused_rotary_position_embedding op.")
        .AsDispensable();
    AddAttr<bool>("use_neox_rotary_style", "(bool), attribute 0 for fused_rotary_position_embedding op.")
        .SetDefault(true);
    AddAttr<bool>("time_major", "(bool), attribute 1 for fused_rotary_position_embedding op.")
        .SetDefault(false);
    AddAttr<float>("rotary_emb_base", "(float), attribute 2 for fused_rotary_position_embedding op.")
        .SetDefault(10000.0);
    AddComment(R"DOC(
TODO: Documentation of fused_rotary_position_embedding op.
)DOC");
  }
};


class FusedRotaryPositionEmbeddingOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_rotary_position_embedding, FusedRotaryPositionEmbeddingInferShapeFunctor,
                            PD_INFER_META(phi::FusedRopeInferMeta));



class FusedScaleBiasAddReluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x1", "(Tensor), input 0 of fused_scale_bias_add_relu op.");
    AddInput("scale1", "(Tensor), input 1 of fused_scale_bias_add_relu op.");
    AddInput("bias1", "(Tensor), input 2 of fused_scale_bias_add_relu op.");
    AddInput("x2", "(Tensor), input 3 of fused_scale_bias_add_relu op.");
    AddInput("scale2", "(Tensor), input 4 of fused_scale_bias_add_relu op.")
        .AsDispensable();
    AddInput("bias2", "(Tensor), input 5 of fused_scale_bias_add_relu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of fused_scale_bias_add_relu op.");
    AddAttr<bool>("fuse_dual", "(bool), attribute 0 for fused_scale_bias_add_relu op.")
    ;
    AddAttr<bool>("exhaustive_search", "(bool), attribute 1 for fused_scale_bias_add_relu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fused_scale_bias_add_relu op.
)DOC");
  }
};


class FusedScaleBiasAddReluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x1");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_scale_bias_add_relu, FusedScaleBiasAddReluInferShapeFunctor,
                            PD_INFER_META(phi::FusedScaleBiasAddReluInferMeta));



class FusedScaleBiasReluConvBnOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of fused_scale_bias_relu_conv_bn op.");
    AddInput("w", "(Tensor), input 1 of fused_scale_bias_relu_conv_bn op.");
    AddInput("scale", "(Tensor), input 2 of fused_scale_bias_relu_conv_bn op.")
        .AsDispensable();
    AddInput("bias", "(Tensor), input 3 of fused_scale_bias_relu_conv_bn op.")
        .AsDispensable();
    AddInput("bn_scale", "(Tensor), input 4 of fused_scale_bias_relu_conv_bn op.");
    AddInput("bn_bias", "(Tensor), input 5 of fused_scale_bias_relu_conv_bn op.");
    AddInput("input_running_mean", "(Tensor), input 6 of fused_scale_bias_relu_conv_bn op.");
    AddInput("input_running_var", "(Tensor), input 7 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("out", "(Tensor), output 0 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("out_running_mean", "(Tensor), output 1 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("out_running_var", "(Tensor), output 2 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("saved_mean", "(Tensor), output 3 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("saved_var", "(Tensor), output 4 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("eq_scale", "(Tensor), output 5 of fused_scale_bias_relu_conv_bn op.");
    AddOutput("eq_bias", "(Tensor), output 6 of fused_scale_bias_relu_conv_bn op.");
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 0 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 1 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 2 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<std::string>("padding_algorithm", "(std::string), attribute 3 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<int>("groups", "(int), attribute 4 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<std::string>("data_format", "(std::string), attribute 5 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<float>("momentum", "(float), attribute 6 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 7 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<bool>("fuse_prologue", "(bool), attribute 8 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<bool>("exhaustive_search", "(bool), attribute 9 for fused_scale_bias_relu_conv_bn op.")
    ;
    AddAttr<int64_t>("accumulation_count", "(int64_t), attribute 10 for fused_scale_bias_relu_conv_bn op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of fused_scale_bias_relu_conv_bn op.
)DOC");
  }
};


class FusedScaleBiasReluConvBnOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_scale_bias_relu_conv_bn, FusedScaleBiasReluConvBnInferShapeFunctor,
                            PD_INFER_META(phi::FusedScaleBiasReluConvBnInferMeta));



class FusionGruOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fusion_gru op.");
    AddInput("H0", "(Tensor), input 1 of fusion_gru op.")
        .AsDispensable();
    AddInput("WeightX", "(Tensor), input 2 of fusion_gru op.");
    AddInput("WeightH", "(Tensor), input 3 of fusion_gru op.");
    AddInput("Bias", "(Tensor), input 4 of fusion_gru op.")
        .AsDispensable();
    AddOutput("ReorderedH0", "(Tensor), output 0 of fusion_gru op.")
        .AsIntermediate();
    AddOutput("XX", "(Tensor), output 1 of fusion_gru op.")
        .AsIntermediate();
    AddOutput("BatchedInput", "(Tensor), output 2 of fusion_gru op.")
        .AsIntermediate();
    AddOutput("BatchedOut", "(Tensor), output 3 of fusion_gru op.")
        .AsIntermediate();
    AddOutput("Hidden", "(Tensor), output 4 of fusion_gru op.");
    AddAttr<std::string>("activation", "(std::string), attribute 0 for fusion_gru op.")
        .SetDefault("tanh");
    AddAttr<std::string>("gate_activation", "(std::string), attribute 1 for fusion_gru op.")
        .SetDefault("sigmoid");
    AddAttr<bool>("is_reverse", "(bool), attribute 2 for fusion_gru op.")
        .SetDefault(false);
    AddAttr<bool>("use_seq", "(bool), attribute 3 for fusion_gru op.")
        .SetDefault(true);
    AddAttr<bool>("origin_mode", "(bool), attribute 4 for fusion_gru op.")
        .SetDefault(false);
    AddAttr<bool>("force_fp32_output", "(bool), attribute 5 for fusion_gru op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of fusion_gru op.
)DOC");
  }
};


class FusionGruOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fusion_gru, FusionGruInferShapeFunctor,
                            PD_INFER_META(phi::FusionGRUInferMeta));



class FusionRepeatedFcReluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fusion_repeated_fc_relu op.");
    AddInput("W", "(Tensor[]), input 1 of fusion_repeated_fc_relu op.")
        .AsDuplicable();
    AddInput("Bias", "(Tensor[]), input 2 of fusion_repeated_fc_relu op.")
        .AsDuplicable();
    AddOutput("ReluOut", "(Tensor[]), output 0 of fusion_repeated_fc_relu op.")
        .AsDuplicable()
        .AsIntermediate();
    AddOutput("Out", "(Tensor), output 1 of fusion_repeated_fc_relu op.");
    AddComment(R"DOC(
TODO: Documentation of fusion_repeated_fc_relu op.
)DOC");
  }
};


class FusionRepeatedFcReluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fusion_repeated_fc_relu, FusionRepeatedFcReluInferShapeFunctor,
                            PD_INFER_META(phi::FusionRepeatedFCReluInferMeta));



class FusionSeqconvEltaddReluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fusion_seqconv_eltadd_relu op.");
    AddInput("Filter", "(Tensor), input 1 of fusion_seqconv_eltadd_relu op.");
    AddInput("Bias", "(Tensor), input 2 of fusion_seqconv_eltadd_relu op.");
    AddOutput("Out", "(Tensor), output 0 of fusion_seqconv_eltadd_relu op.");
    AddOutput("ColMat", "(Tensor), output 1 of fusion_seqconv_eltadd_relu op.")
        .AsIntermediate();
    AddAttr<int>("contextLength", "(int), attribute 0 for fusion_seqconv_eltadd_relu op.")
    ;
    AddAttr<int>("contextStart", "(int), attribute 1 for fusion_seqconv_eltadd_relu op.")
        .SetDefault(0);
    AddAttr<int>("contextStride", "(int), attribute 2 for fusion_seqconv_eltadd_relu op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of fusion_seqconv_eltadd_relu op.
)DOC");
  }
};


class FusionSeqconvEltaddReluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fusion_seqconv_eltadd_relu, FusionSeqconvEltaddReluInferShapeFunctor,
                            PD_INFER_META(phi::FusionSeqConvEltAddReluInferMeta));



class FusionSeqexpandConcatFcOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor[]), input 0 of fusion_seqexpand_concat_fc op.")
        .AsDuplicable();
    AddInput("FCWeight", "(Tensor), input 1 of fusion_seqexpand_concat_fc op.");
    AddInput("FCBias", "(Tensor), input 2 of fusion_seqexpand_concat_fc op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of fusion_seqexpand_concat_fc op.");
    AddOutput("FCOut", "(Tensor), output 1 of fusion_seqexpand_concat_fc op.")
        .AsIntermediate();
    AddAttr<std::string>("fc_activation", "(std::string), attribute 0 for fusion_seqexpand_concat_fc op.")
        .SetDefault("identity");
    AddComment(R"DOC(
TODO: Documentation of fusion_seqexpand_concat_fc op.
)DOC");
  }
};


class FusionSeqexpandConcatFcOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fusion_seqexpand_concat_fc, FusionSeqexpandConcatFcInferShapeFunctor,
                            PD_INFER_META(phi::FusionSeqExpandConcatFCInferMeta));



class FusionSquaredMatSubOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of fusion_squared_mat_sub op.");
    AddInput("Y", "(Tensor), input 1 of fusion_squared_mat_sub op.");
    AddOutput("SquaredX", "(Tensor), output 0 of fusion_squared_mat_sub op.")
        .AsIntermediate();
    AddOutput("SquaredY", "(Tensor), output 1 of fusion_squared_mat_sub op.")
        .AsIntermediate();
    AddOutput("SquaredXY", "(Tensor), output 2 of fusion_squared_mat_sub op.")
        .AsIntermediate();
    AddOutput("Out", "(Tensor), output 3 of fusion_squared_mat_sub op.");
    AddAttr<float>("scalar", "(float), attribute 0 for fusion_squared_mat_sub op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of fusion_squared_mat_sub op.
)DOC");
  }
};


class FusionSquaredMatSubOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fusion_squared_mat_sub, FusionSquaredMatSubInferShapeFunctor,
                            PD_INFER_META(phi::FusionSquaredMatSubInferMeta));



class FusionTransposeFlattenConcatOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor[]), input 0 of fusion_transpose_flatten_concat op.")
        .AsDuplicable();
    AddOutput("Out", "(Tensor), output 0 of fusion_transpose_flatten_concat op.");
    AddAttr<std::vector<int>>("trans_axis", "(std::vector<int>), attribute 0 for fusion_transpose_flatten_concat op.")
    ;
    AddAttr<int>("flatten_axis", "(int), attribute 1 for fusion_transpose_flatten_concat op.")
    ;
    AddAttr<int>("concat_axis", "(int), attribute 2 for fusion_transpose_flatten_concat op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of fusion_transpose_flatten_concat op.
)DOC");
  }
};


class FusionTransposeFlattenConcatOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fusion_transpose_flatten_concat, FusionTransposeFlattenConcatInferShapeFunctor,
                            PD_INFER_META(phi::FusionTransposeFlattenConcatInferMeta));



class GenerateSequenceXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of generate_sequence_xpu op.");
    AddOutput("out", "(Tensor), output 0 of generate_sequence_xpu op.");
    AddAttr<int>("dtype", "(int), attribute 0 for generate_sequence_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of generate_sequence_xpu op.
)DOC");
  }
};


class GenerateSequenceXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::proto::VarType::Type(ctx.Attr<int>("dtype"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(generate_sequence_xpu, GenerateSequenceXpuInferShapeFunctor,
                            PD_INFER_META(phi::GenerateSequenceXPUInferMeta));



class GroupNormSiluXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of group_norm_silu_xpu op.");
    AddInput("scale", "(Tensor), input 1 of group_norm_silu_xpu op.");
    AddInput("bias", "(Tensor), input 2 of group_norm_silu_xpu op.");
    AddOutput("out", "(Tensor), output 0 of group_norm_silu_xpu op.");
    AddAttr<int>("groups", "(int), attribute 0 for group_norm_silu_xpu op.")
        .SetDefault(-1);
    AddAttr<float>("epsilon", "(float), attribute 1 for group_norm_silu_xpu op.")
        .SetDefault(1e-5);
    AddComment(R"DOC(
TODO: Documentation of group_norm_silu_xpu op.
)DOC");
  }
};


class GroupNormSiluXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(group_norm_silu_xpu, GroupNormSiluXpuInferShapeFunctor,
                            PD_INFER_META(phi::GroupNormalizeSiluXPUInferMeta));



class LayerNormActXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of layer_norm_act_xpu op.");
    AddInput("scale", "(Tensor), input 1 of layer_norm_act_xpu op.");
    AddInput("bias", "(Tensor), input 2 of layer_norm_act_xpu op.");
    AddOutput("out", "(Tensor), output 0 of layer_norm_act_xpu op.");
    AddAttr<int>("begin_norm_axis", "(int), attribute 0 for layer_norm_act_xpu op.")
    ;
    AddAttr<float>("epsilon", "(float), attribute 1 for layer_norm_act_xpu op.")
    ;
    AddAttr<int>("act_type", "(int), attribute 2 for layer_norm_act_xpu op.")
    ;
    AddAttr<float>("act_param", "(float), attribute 3 for layer_norm_act_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of layer_norm_act_xpu op.
)DOC");
  }
};


class LayerNormActXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(layer_norm_act_xpu, LayerNormActXpuInferShapeFunctor,
                            PD_INFER_META(phi::LayerNormActXPUInferMeta));



class MaxPool2dV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of max_pool2d_v2 op.");
    AddOutput("out", "(Tensor), output 0 of max_pool2d_v2 op.");
    AddOutput("saved_idx", "(Tensor), output 1 of max_pool2d_v2 op.")
        .AsIntermediate();
    AddAttr<std::vector<int>>("kernel_size", "(std::vector<int>), attribute 0 for max_pool2d_v2 op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 1 for max_pool2d_v2 op.")
        .SetDefault({1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 2 for max_pool2d_v2 op.")
        .SetDefault({0, 0});
    AddAttr<std::string>("data_format", "(std::string), attribute 3 for max_pool2d_v2 op.")
        .SetDefault("NCHW");
    AddAttr<bool>("global_pooling", "(bool), attribute 4 for max_pool2d_v2 op.")
        .SetDefault(false);
    AddAttr<bool>("adaptive", "(bool), attribute 5 for max_pool2d_v2 op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of max_pool2d_v2 op.
)DOC");
  }
};


class MaxPool2dV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(max_pool2d_v2, MaxPool2dV2InferShapeFunctor,
                            PD_INFER_META(phi::MaxPoolV2InferMeta));



class MultiEncoderXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of multi_encoder_xpu op.");
    AddInput("fc_input_max", "(Tensor[]), input 1 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("fc_weight", "(Tensor[]), input 2 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("fc_weight_max", "(Tensor[]), input 3 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("fc_bias", "(Tensor[]), input 4 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("ln_scale", "(Tensor[]), input 5 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("ln_bias", "(Tensor[]), input 6 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("smooth_scale_weight", "(Tensor[]), input 7 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("roformer_embedding", "(Tensor[]), input 8 of multi_encoder_xpu op.")
        .AsDuplicable();
    AddInput("mask", "(Tensor), input 9 of multi_encoder_xpu op.")
        .AsDispensable();
    AddInput("seq_lod", "(Tensor), input 10 of multi_encoder_xpu op.")
        .AsDispensable();
    AddInput("max_seq_len", "(Tensor), input 11 of multi_encoder_xpu op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of multi_encoder_xpu op.");
    AddOutput("x_fp16", "(Tensor), output 1 of multi_encoder_xpu op.")
        .AsDispensable();
    AddOutput("out_fp16", "(Tensor), output 2 of multi_encoder_xpu op.")
        .AsDispensable();
    AddAttr<int>("layer_num", "(int), attribute 0 for multi_encoder_xpu op.")
    ;
    AddAttr<bool>("norm_before", "(bool), attribute 1 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("hidden_dim", "(int), attribute 2 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("head_num", "(int), attribute 3 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("size_per_head", "(int), attribute 4 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("ffn_hidden_dim_scale", "(int), attribute 5 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("act_type", "(int), attribute 6 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("relative_type", "(int), attribute 7 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("slice_idx", "(int), attribute 8 for multi_encoder_xpu op.")
    ;
    AddAttr<bool>("is_per_channel", "(bool), attribute 9 for multi_encoder_xpu op.")
    ;
    AddAttr<int>("max_pos_len", "(int), attribute 10 for multi_encoder_xpu op.")
    ;
    AddAttr<std::vector<float>>("softmax_max_value", "(std::vector<float>), attribute 11 for multi_encoder_xpu op.")
    ;
    AddAttr<std::vector<std::string>>("quant_types", "(std::vector<std::string>), attribute 12 for multi_encoder_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of multi_encoder_xpu op.
)DOC");
  }
};


class MultiEncoderXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(multi_encoder_xpu, MultiEncoderXpuInferShapeFunctor,
                            PD_INFER_META(phi::MultiEncoderXPUInferMeta));



class MultiheadMatmulOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of multihead_matmul op.");
    AddInput("W", "(Tensor), input 1 of multihead_matmul op.");
    AddInput("Bias", "(Tensor), input 2 of multihead_matmul op.");
    AddInput("BiasQK", "(Tensor), input 3 of multihead_matmul op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of multihead_matmul op.");
    AddAttr<bool>("transpose_Q", "(bool), attribute 0 for multihead_matmul op.")
        .SetDefault(false);
    AddAttr<bool>("transpose_K", "(bool), attribute 1 for multihead_matmul op.")
        .SetDefault(true);
    AddAttr<bool>("transpose_V", "(bool), attribute 2 for multihead_matmul op.")
        .SetDefault(false);
    AddAttr<float>("alpha", "(float), attribute 3 for multihead_matmul op.")
        .SetDefault(1.0f);
    AddAttr<int>("head_number", "(int), attribute 4 for multihead_matmul op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of multihead_matmul op.
)DOC");
  }
};


class MultiheadMatmulOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(multihead_matmul, MultiheadMatmulInferShapeFunctor,
                            PD_INFER_META(phi::MultiheadMatmulInferMeta));



class QkvAttentionXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("q", "(Tensor), input 0 of qkv_attention_xpu op.");
    AddInput("k", "(Tensor), input 1 of qkv_attention_xpu op.");
    AddInput("v", "(Tensor), input 2 of qkv_attention_xpu op.");
    AddInput("q_max", "(Tensor), input 3 of qkv_attention_xpu op.")
        .AsDispensable();
    AddInput("k_max", "(Tensor), input 4 of qkv_attention_xpu op.")
        .AsDispensable();
    AddInput("v_max", "(Tensor), input 5 of qkv_attention_xpu op.")
        .AsDispensable();
    AddInput("qk_max", "(Tensor), input 6 of qkv_attention_xpu op.")
        .AsDispensable();
    AddInput("qkv_max", "(Tensor), input 7 of qkv_attention_xpu op.")
        .AsDispensable();
    AddOutput("qkv", "(Tensor), output 0 of qkv_attention_xpu op.");
    AddAttr<float>("alpha", "(float), attribute 0 for qkv_attention_xpu op.")
    ;
    AddAttr<int>("head_num", "(int), attribute 1 for qkv_attention_xpu op.")
    ;
    AddAttr<int>("head_dim", "(int), attribute 2 for qkv_attention_xpu op.")
    ;
    AddAttr<bool>("qkv_fc_fusion", "(bool), attribute 3 for qkv_attention_xpu op.")
    ;
    AddAttr<int>("out_dtype", "(int), attribute 4 for qkv_attention_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of qkv_attention_xpu op.
)DOC");
  }
};


class QkvAttentionXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(qkv_attention_xpu, QkvAttentionXpuInferShapeFunctor,
                            PD_INFER_META(phi::QKVAttentionXPUInferMeta));



class QuantizeXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of quantize_xpu op.");
    AddOutput("y", "(Tensor), output 0 of quantize_xpu op.");
    AddAttr<int>("out_dtype", "(int), attribute 0 for quantize_xpu op.")
    ;
    AddAttr<float>("scale", "(float), attribute 1 for quantize_xpu op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of quantize_xpu op.
)DOC");
  }
};


class QuantizeXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(quantize_xpu, QuantizeXpuInferShapeFunctor,
                            PD_INFER_META(phi::QuantizeXPUInferMeta));



class RoformerRelativeEmbeddingXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of roformer_relative_embedding_xpu op.");
    AddInput("sin_emb", "(Tensor), input 1 of roformer_relative_embedding_xpu op.");
    AddInput("cos_emb", "(Tensor), input 2 of roformer_relative_embedding_xpu op.");
    AddOutput("out", "(Tensor), output 0 of roformer_relative_embedding_xpu op.");
    AddAttr<int>("max_pos_len", "(int), attribute 0 for roformer_relative_embedding_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of roformer_relative_embedding_xpu op.
)DOC");
  }
};


class RoformerRelativeEmbeddingXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(roformer_relative_embedding_xpu, RoformerRelativeEmbeddingXpuInferShapeFunctor,
                            PD_INFER_META(phi::RoformerRelativePosXPUInferMeta));



class SelfDpAttentionOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of self_dp_attention op.");
    AddOutput("Out", "(Tensor), output 0 of self_dp_attention op.");
    AddAttr<float>("alpha", "(float), attribute 0 for self_dp_attention op.")
        .SetDefault(1.0f);
    AddAttr<int>("head_number", "(int), attribute 1 for self_dp_attention op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of self_dp_attention op.
)DOC");
  }
};


class SelfDpAttentionOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(self_dp_attention, SelfDpAttentionInferShapeFunctor,
                            PD_INFER_META(phi::SelfDPAttenInferMeta));



class SinePosXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of sine_pos_xpu op.");
    AddInput("y", "(Tensor), input 1 of sine_pos_xpu op.");
    AddOutput("out", "(Tensor), output 0 of sine_pos_xpu op.");
    AddComment(R"DOC(
TODO: Documentation of sine_pos_xpu op.
)DOC");
  }
};


class SinePosXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(sine_pos_xpu, SinePosXpuInferShapeFunctor,
                            PD_INFER_META(phi::SinePosXPUInferMeta));



class SkipLayernormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of skip_layernorm op.");
    AddInput("Y", "(Tensor), input 1 of skip_layernorm op.");
    AddInput("Scale", "(Tensor), input 2 of skip_layernorm op.");
    AddInput("Bias", "(Tensor), input 3 of skip_layernorm op.");
    AddOutput("Out", "(Tensor), output 0 of skip_layernorm op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for skip_layernorm op.")
    ;
    AddAttr<int>("begin_norm_axis", "(int), attribute 1 for skip_layernorm op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of skip_layernorm op.
)DOC");
  }
};


class SkipLayernormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(skip_layernorm, SkipLayernormInferShapeFunctor,
                            PD_INFER_META(phi::SkipLayerNormInferMeta));



class SpatialTransformerResblockXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of spatial_transformer_resblock_xpu op.");
    AddInput("x_max", "(Tensor[]), input 1 of spatial_transformer_resblock_xpu op.")
        .AsDuplicable();
    AddInput("conv_bias", "(Tensor[]), input 2 of spatial_transformer_resblock_xpu op.")
        .AsDuplicable();
    AddInput("conv_filter", "(Tensor[]), input 3 of spatial_transformer_resblock_xpu op.")
        .AsDuplicable();
    AddInput("conv_filter_max", "(Tensor[]), input 4 of spatial_transformer_resblock_xpu op.")
        .AsDuplicable();
    AddInput("gn_bias", "(Tensor[]), input 5 of spatial_transformer_resblock_xpu op.")
        .AsDuplicable();
    AddInput("gn_scale", "(Tensor[]), input 6 of spatial_transformer_resblock_xpu op.")
        .AsDuplicable();
    AddOutput("out", "(Tensor), output 0 of spatial_transformer_resblock_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of spatial_transformer_resblock_xpu op.");
    AddAttr<std::vector<int>>("dilations", "(std::vector<int>), attribute 0 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 1 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 2 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<std::vector<float>>("gn_eps", "(std::vector<float>), attribute 3 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<std::vector<int>>("gn_groups", "(std::vector<int>), attribute 4 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<std::vector<int>>("groups", "(std::vector<int>), attribute 5 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<bool>("conv_fix", "(bool), attribute 6 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<bool>("has_silu_fc_input", "(bool), attribute 7 for spatial_transformer_resblock_xpu op.")
    ;
    AddAttr<bool>("include_silu", "(bool), attribute 8 for spatial_transformer_resblock_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of spatial_transformer_resblock_xpu op.
)DOC");
  }
};


class SpatialTransformerResblockXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(spatial_transformer_resblock_xpu, SpatialTransformerResblockXpuInferShapeFunctor,
                            PD_INFER_META(phi::SpatialTransformerResblockXPUInferMeta));



class SqueezeExcitationBlockOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of squeeze_excitation_block op.");
    AddInput("filter", "(Tensor), input 1 of squeeze_excitation_block op.");
    AddInput("filter_max", "(Tensor), input 2 of squeeze_excitation_block op.");
    AddInput("bias", "(Tensor), input 3 of squeeze_excitation_block op.")
        .AsDispensable();
    AddInput("branch", "(Tensor), input 4 of squeeze_excitation_block op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of squeeze_excitation_block op.");
    AddAttr<std::vector<int>>("act_type", "(std::vector<int>), attribute 0 for squeeze_excitation_block op.")
    ;
    AddAttr<std::vector<float>>("act_param", "(std::vector<float>), attribute 1 for squeeze_excitation_block op.")
    ;
    AddAttr<std::vector<int>>("filter_dims", "(std::vector<int>), attribute 2 for squeeze_excitation_block op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of squeeze_excitation_block op.
)DOC");
  }
};


class SqueezeExcitationBlockOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(squeeze_excitation_block, SqueezeExcitationBlockInferShapeFunctor,
                            PD_INFER_META(phi::SqueezeExcitationInferMeta));



class VariableLengthMemoryEfficientAttentionOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("query", "(Tensor), input 0 of variable_length_memory_efficient_attention op.");
    AddInput("key", "(Tensor), input 1 of variable_length_memory_efficient_attention op.");
    AddInput("value", "(Tensor), input 2 of variable_length_memory_efficient_attention op.");
    AddInput("seq_lens", "(Tensor), input 3 of variable_length_memory_efficient_attention op.");
    AddInput("kv_seq_lens", "(Tensor), input 4 of variable_length_memory_efficient_attention op.");
    AddInput("mask", "(Tensor), input 5 of variable_length_memory_efficient_attention op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of variable_length_memory_efficient_attention op.");
    AddAttr<float>("scale", "(float), attribute 0 for variable_length_memory_efficient_attention op.")
    ;
    AddAttr<bool>("causal", "(bool), attribute 1 for variable_length_memory_efficient_attention op.")
    ;
    AddAttr<int>("pre_cache_length", "(int), attribute 2 for variable_length_memory_efficient_attention op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of variable_length_memory_efficient_attention op.
)DOC");
  }
};


class VariableLengthMemoryEfficientAttentionOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "query");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(variable_length_memory_efficient_attention, VariableLengthMemoryEfficientAttentionInferShapeFunctor,
                            PD_INFER_META(phi::VariableLengthMemoryEfficientAttentionInferMeta));



class YoloBoxXpuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of yolo_box_xpu op.");
    AddInput("x_max", "(Tensor), input 1 of yolo_box_xpu op.")
        .AsDispensable();
    AddInput("grid", "(Tensor), input 2 of yolo_box_xpu op.");
    AddInput("stride", "(Tensor), input 3 of yolo_box_xpu op.");
    AddInput("anchor_grid", "(Tensor), input 4 of yolo_box_xpu op.");
    AddOutput("out", "(Tensor), output 0 of yolo_box_xpu op.");
    AddOutput("out_max", "(Tensor), output 1 of yolo_box_xpu op.");
    AddAttr<float>("offset", "(float), attribute 0 for yolo_box_xpu op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of yolo_box_xpu op.
)DOC");
  }
};


class YoloBoxXpuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(yolo_box_xpu, YoloBoxXpuInferShapeFunctor,
                            PD_INFER_META(phi::YoloBoxXPUInferMeta));




template <typename T>
class FusedBiasDropoutResidualLayerNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fused_bias_dropout_residual_layer_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Residual", this->Input("Residual"));
    if (this->HasInput("Bias")) {
      grad_op->SetInput("Bias", this->Input("Bias"));
    }
    if (this->HasInput("LnScale")) {
      grad_op->SetInput("LnScale", this->Input("LnScale"));
    }
    if (this->HasInput("LnBias")) {
      grad_op->SetInput("LnBias", this->Input("LnBias"));
    }
    grad_op->SetInput("LnMean", this->Output("LnMean"));
    grad_op->SetInput("LnVariance", this->Output("LnVariance"));
    grad_op->SetInput("BiasDropoutResidualOut", this->Output("BiasDropoutResidualOut"));
    grad_op->SetInput("DropoutMaskOut", this->Output("DropoutMaskOut"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Residual"), this->InputGrad("Residual"));
    if (this->HasInput("Bias")) {
      grad_op->SetOutput(GradVarName("Bias"), this->InputGrad("Bias"));
    }
    if (this->HasInput("LnScale")) {
      grad_op->SetOutput(GradVarName("LnScale"), this->InputGrad("LnScale"));
    }
    if (this->HasInput("LnBias")) {
      grad_op->SetOutput(GradVarName("LnBias"), this->InputGrad("LnBias"));
    }

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FusedBiasDropoutResidualLayerNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Y"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_bias_dropout_residual_layer_norm_grad, FusedBiasDropoutResidualLayerNormGradInferShapeFunctor,
                            PD_INFER_META(phi::FusedBiasDropoutResidualLnGradInferMeta));



template <typename T>
class FusedDotProductAttentionGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fused_dot_product_attention_grad");

    grad_op->SetInput("q", this->Input("q"));
    grad_op->SetInput("k", this->Input("k"));
    grad_op->SetInput("v", this->Input("v"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput("softmax_out", this->Output("softmax_out"));
    grad_op->SetInput("rng_state", this->Output("rng_state"));
    grad_op->SetInput("mask", this->Input("mask"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("q"), this->InputGrad("q"));
    grad_op->SetOutput(GradVarName("k"), this->InputGrad("k"));
    grad_op->SetOutput(GradVarName("v"), this->InputGrad("v"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FusedDotProductAttentionGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "q");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_dot_product_attention_grad, FusedDotProductAttentionGradInferShapeFunctor,
                            PD_INFER_META(phi::FusedDotProductAttentionGradInferMeta));



template <typename T>
class FusedDropoutAddGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fused_dropout_add_grad");

    grad_op->SetInput("seed_offset", this->Output("seed_offset"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));
    grad_op->SetOutput(GradVarName("y"), this->InputGrad("y"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("PTensor")) {
      grad_op->SetInput("PTensor", this->Input("PTensor"));
    }
  }
};


class FusedDropoutAddGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_dropout_add_grad, FusedDropoutAddGradInferShapeFunctor,
                            PD_INFER_META(phi::FusedDropoutAddGradInferMeta));



template <typename T>
class FusedRotaryPositionEmbeddingGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("fused_rotary_position_embedding_grad");

    if (this->HasInput("sin")) {
      grad_op->SetInput("sin", this->Input("sin"));
    }
    if (this->HasInput("cos")) {
      grad_op->SetInput("cos", this->Input("cos"));
    }
    if (this->HasInput("position_ids")) {
      grad_op->SetInput("position_ids", this->Input("position_ids"));
    }
    grad_op->SetInput(GradVarName("out_q"), this->OutputGrad("out_q"));
    if (this->HasOutput("out_k")) {
      grad_op->SetInput(GradVarName("out_k"), this->OutputGrad("out_k"));
    }
    if (this->HasOutput("out_v")) {
      grad_op->SetInput(GradVarName("out_v"), this->OutputGrad("out_v"));
    }

    grad_op->SetOutput(GradVarName("q"), this->InputGrad("q"));
    if (this->HasInput("k")) {
      grad_op->SetOutput(GradVarName("k"), this->InputGrad("k"));
    }
    if (this->HasInput("v")) {
      grad_op->SetOutput(GradVarName("v"), this->InputGrad("v"));
    }

    grad_op->SetAttrMap(this->Attrs());
  }
};


class FusedRotaryPositionEmbeddingGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("out_q"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(fused_rotary_position_embedding_grad, FusedRotaryPositionEmbeddingGradInferShapeFunctor,
                            PD_INFER_META(phi::FusedRopeGradInferMeta));



template <typename T>
class MaxPool2dV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("max_pool2d_v2_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput("out", this->Output("out"));
    grad_op->SetInput("saved_idx", this->Output("saved_idx"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MaxPool2dV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(max_pool2d_v2_grad, MaxPool2dV2GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


}  // namespace operators
}  // namespace paddle

namespace ops = paddle::operators;
REGISTER_OPERATOR(add_act_xpu, ops::AddActXpuOp,
                  ops::AddActXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AddActXpuInferShapeFunctor);


REGISTER_OPERATOR(add_layernorm_xpu, ops::AddLayernormXpuOp,
                  ops::AddLayernormXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AddLayernormXpuInferShapeFunctor);


REGISTER_OPERATOR(addcmul_xpu, ops::AddcmulXpuOp,
                  ops::AddcmulXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::AddcmulXpuInferShapeFunctor);


REGISTER_OPERATOR(block_multihead_attention, ops::BlockMultiheadAttentionOp,
                  ops::BlockMultiheadAttentionOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::BlockMultiheadAttentionInferShapeFunctor);


REGISTER_OPERATOR(bn_act_xpu, ops::BnActXpuOp,
                  ops::BnActXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::BnActXpuInferShapeFunctor);


REGISTER_OPERATOR(conv1d_xpu, ops::Conv1dXpuOp,
                  ops::Conv1dXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::Conv1dXpuInferShapeFunctor);


REGISTER_OPERATOR(conv2d_transpose_xpu, ops::Conv2dTransposeXpuOp,
                  ops::Conv2dTransposeXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::Conv2dTransposeXpuInferShapeFunctor);


REGISTER_OPERATOR(conv2d_xpu, ops::Conv2dXpuOp,
                  ops::Conv2dXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::Conv2dXpuInferShapeFunctor);


REGISTER_OPERATOR(cross_attention_xpu, ops::CrossAttentionXpuOp,
                  ops::CrossAttentionXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::CrossAttentionXpuInferShapeFunctor);


REGISTER_OPERATOR(dequantize_xpu, ops::DequantizeXpuOp,
                  ops::DequantizeXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::DequantizeXpuInferShapeFunctor);


REGISTER_OPERATOR(embedding_with_eltwise_add_xpu, ops::EmbeddingWithEltwiseAddXpuOp,
                  ops::EmbeddingWithEltwiseAddXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::EmbeddingWithEltwiseAddXpuInferShapeFunctor);


REGISTER_OPERATOR(fast_layernorm_xpu, ops::FastLayernormXpuOp,
                  ops::FastLayernormXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FastLayernormXpuInferShapeFunctor);


REGISTER_OPERATOR(fast_where_xpu, ops::FastWhereXpuOp,
                  ops::FastWhereXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FastWhereXpuInferShapeFunctor);


REGISTER_OPERATOR(fc, ops::FcOp,
                  ops::FcOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FcInferShapeFunctor);


REGISTER_OPERATOR(fc_xpu, ops::FcXpuOp,
                  ops::FcXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FcXpuInferShapeFunctor);


REGISTER_OPERATOR(fused_bias_act, ops::FusedBiasActOp,
                  ops::FusedBiasActOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedBiasActInferShapeFunctor);


REGISTER_OPERATOR(fused_bias_dropout_residual_layer_norm, ops::FusedBiasDropoutResidualLayerNormOp,
                  ops::FusedBiasDropoutResidualLayerNormOpMaker,
                  ops::FusedBiasDropoutResidualLayerNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::FusedBiasDropoutResidualLayerNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedBiasDropoutResidualLayerNormInferShapeFunctor);


REGISTER_OPERATOR(fused_bias_residual_layernorm, ops::FusedBiasResidualLayernormOp,
                  ops::FusedBiasResidualLayernormOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedBiasResidualLayernormInferShapeFunctor);


REGISTER_OPERATOR(fused_conv2d_add_act, ops::FusedConv2dAddActOp,
                  ops::FusedConv2dAddActOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedConv2dAddActInferShapeFunctor);


REGISTER_OPERATOR(fused_dconv_drelu_dbn, ops::FusedDconvDreluDbnOp,
                  ops::FusedDconvDreluDbnOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedDconvDreluDbnInferShapeFunctor);


REGISTER_OPERATOR(fused_dot_product_attention, ops::FusedDotProductAttentionOp,
                  ops::FusedDotProductAttentionOpMaker,
                  ops::FusedDotProductAttentionGradOpMaker<paddle::framework::OpDesc>,
                  ops::FusedDotProductAttentionGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedDotProductAttentionInferShapeFunctor);


REGISTER_OPERATOR(fused_dropout_add, ops::FusedDropoutAddOp,
                  ops::FusedDropoutAddOpMaker,
                  ops::FusedDropoutAddGradOpMaker<paddle::framework::OpDesc>,
                  ops::FusedDropoutAddGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedDropoutAddInferShapeFunctor);


REGISTER_OPERATOR(fused_embedding_eltwise_layernorm, ops::FusedEmbeddingEltwiseLayernormOp,
                  ops::FusedEmbeddingEltwiseLayernormOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedEmbeddingEltwiseLayernormInferShapeFunctor);


REGISTER_OPERATOR(fused_fc_elementwise_layernorm, ops::FusedFcElementwiseLayernormOp,
                  ops::FusedFcElementwiseLayernormOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedFcElementwiseLayernormInferShapeFunctor);


REGISTER_OPERATOR(fused_linear_param_grad_add, ops::FusedLinearParamGradAddOp,
                  ops::FusedLinearParamGradAddOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedLinearParamGradAddInferShapeFunctor);


REGISTER_OPERATOR(fused_multi_transformer_int8_xpu, ops::FusedMultiTransformerInt8XpuOp,
                  ops::FusedMultiTransformerInt8XpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedMultiTransformerInt8XpuInferShapeFunctor);


REGISTER_OPERATOR(fused_multi_transformer_xpu, ops::FusedMultiTransformerXpuOp,
                  ops::FusedMultiTransformerXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedMultiTransformerXpuInferShapeFunctor);


REGISTER_OPERATOR(fused_rotary_position_embedding, ops::FusedRotaryPositionEmbeddingOp,
                  ops::FusedRotaryPositionEmbeddingOpMaker,
                  ops::FusedRotaryPositionEmbeddingGradOpMaker<paddle::framework::OpDesc>,
                  ops::FusedRotaryPositionEmbeddingGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedRotaryPositionEmbeddingInferShapeFunctor);


REGISTER_OPERATOR(fused_scale_bias_add_relu, ops::FusedScaleBiasAddReluOp,
                  ops::FusedScaleBiasAddReluOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedScaleBiasAddReluInferShapeFunctor);


REGISTER_OPERATOR(fused_scale_bias_relu_conv_bn, ops::FusedScaleBiasReluConvBnOp,
                  ops::FusedScaleBiasReluConvBnOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusedScaleBiasReluConvBnInferShapeFunctor);


REGISTER_OPERATOR(fusion_gru, ops::FusionGruOp,
                  ops::FusionGruOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusionGruInferShapeFunctor);


REGISTER_OPERATOR(fusion_repeated_fc_relu, ops::FusionRepeatedFcReluOp,
                  ops::FusionRepeatedFcReluOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusionRepeatedFcReluInferShapeFunctor);


REGISTER_OPERATOR(fusion_seqconv_eltadd_relu, ops::FusionSeqconvEltaddReluOp,
                  ops::FusionSeqconvEltaddReluOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusionSeqconvEltaddReluInferShapeFunctor);


REGISTER_OPERATOR(fusion_seqexpand_concat_fc, ops::FusionSeqexpandConcatFcOp,
                  ops::FusionSeqexpandConcatFcOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusionSeqexpandConcatFcInferShapeFunctor);


REGISTER_OPERATOR(fusion_squared_mat_sub, ops::FusionSquaredMatSubOp,
                  ops::FusionSquaredMatSubOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusionSquaredMatSubInferShapeFunctor);


REGISTER_OPERATOR(fusion_transpose_flatten_concat, ops::FusionTransposeFlattenConcatOp,
                  ops::FusionTransposeFlattenConcatOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::FusionTransposeFlattenConcatInferShapeFunctor);


REGISTER_OPERATOR(generate_sequence_xpu, ops::GenerateSequenceXpuOp,
                  ops::GenerateSequenceXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GenerateSequenceXpuInferShapeFunctor);


REGISTER_OPERATOR(group_norm_silu_xpu, ops::GroupNormSiluXpuOp,
                  ops::GroupNormSiluXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GroupNormSiluXpuInferShapeFunctor);


REGISTER_OPERATOR(layer_norm_act_xpu, ops::LayerNormActXpuOp,
                  ops::LayerNormActXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LayerNormActXpuInferShapeFunctor);


REGISTER_OPERATOR(max_pool2d_v2, ops::MaxPool2dV2Op,
                  ops::MaxPool2dV2OpMaker,
                  ops::MaxPool2dV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::MaxPool2dV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::MaxPool2dV2InferShapeFunctor);


REGISTER_OPERATOR(multi_encoder_xpu, ops::MultiEncoderXpuOp,
                  ops::MultiEncoderXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MultiEncoderXpuInferShapeFunctor);


REGISTER_OPERATOR(multihead_matmul, ops::MultiheadMatmulOp,
                  ops::MultiheadMatmulOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MultiheadMatmulInferShapeFunctor);


REGISTER_OPERATOR(qkv_attention_xpu, ops::QkvAttentionXpuOp,
                  ops::QkvAttentionXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::QkvAttentionXpuInferShapeFunctor);


REGISTER_OPERATOR(quantize_xpu, ops::QuantizeXpuOp,
                  ops::QuantizeXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::QuantizeXpuInferShapeFunctor);


REGISTER_OPERATOR(roformer_relative_embedding_xpu, ops::RoformerRelativeEmbeddingXpuOp,
                  ops::RoformerRelativeEmbeddingXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::RoformerRelativeEmbeddingXpuInferShapeFunctor);


REGISTER_OPERATOR(self_dp_attention, ops::SelfDpAttentionOp,
                  ops::SelfDpAttentionOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::SelfDpAttentionInferShapeFunctor);


REGISTER_OPERATOR(sine_pos_xpu, ops::SinePosXpuOp,
                  ops::SinePosXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::SinePosXpuInferShapeFunctor);


REGISTER_OPERATOR(skip_layernorm, ops::SkipLayernormOp,
                  ops::SkipLayernormOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::SkipLayernormInferShapeFunctor);


REGISTER_OPERATOR(spatial_transformer_resblock_xpu, ops::SpatialTransformerResblockXpuOp,
                  ops::SpatialTransformerResblockXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::SpatialTransformerResblockXpuInferShapeFunctor);


REGISTER_OPERATOR(squeeze_excitation_block, ops::SqueezeExcitationBlockOp,
                  ops::SqueezeExcitationBlockOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::SqueezeExcitationBlockInferShapeFunctor);


REGISTER_OPERATOR(variable_length_memory_efficient_attention, ops::VariableLengthMemoryEfficientAttentionOp,
                  ops::VariableLengthMemoryEfficientAttentionOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::VariableLengthMemoryEfficientAttentionInferShapeFunctor);


REGISTER_OPERATOR(yolo_box_xpu, ops::YoloBoxXpuOp,
                  ops::YoloBoxXpuOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::YoloBoxXpuInferShapeFunctor);


REGISTER_OPERATOR(fused_bias_dropout_residual_layer_norm_grad, ops::FusedBiasDropoutResidualLayerNormGradOp,
                  ops::FusedBiasDropoutResidualLayerNormGradInferShapeFunctor);


REGISTER_OPERATOR(fused_dot_product_attention_grad, ops::FusedDotProductAttentionGradOp,
                  ops::FusedDotProductAttentionGradInferShapeFunctor);


REGISTER_OPERATOR(fused_dropout_add_grad, ops::FusedDropoutAddGradOp,
                  ops::FusedDropoutAddGradInferShapeFunctor);


REGISTER_OPERATOR(fused_rotary_position_embedding_grad, ops::FusedRotaryPositionEmbeddingGradOp,
                  ops::FusedRotaryPositionEmbeddingGradInferShapeFunctor);


REGISTER_OPERATOR(max_pool2d_v2_grad, ops::MaxPool2dV2GradOp,
                  ops::MaxPool2dV2GradInferShapeFunctor);


