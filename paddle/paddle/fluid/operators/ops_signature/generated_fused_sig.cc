// this file is generated by paddle/phi/op/yaml/generator/generate_op.py, do not edit.
#include "paddle/phi/core/compat/op_utils.h"
#include "paddle/utils/small_vector.h"

namespace phi {

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddActXpuOpArgumentMapping:

return KernelSignature("add_act_xpu", {"x", "x_max", "y", "y_max"}, {"act_type"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature AddActXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "y", "y_max"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("act_type");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("add_act_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddLayernormXpuOpArgumentMapping:

return KernelSignature("add_layernorm_xpu", {"x", "y", "scale", "bias"}, {"begin_norm_axis", "epsilon"}, {"out"});
******************************************************************
*/

KernelSignature AddLayernormXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("add_layernorm_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddcmulXpuOpArgumentMapping:

return KernelSignature("addcmul_xpu", {"x", "y", "w"}, {}, {"out"});
******************************************************************
*/

KernelSignature AddcmulXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "w"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("addcmul_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BlockMultiheadAttentionOpArgumentMapping:

return KernelSignature("block_multihead_attention", {"qkv", "key_cache", "value_cache", "seq_lens_encoder", "seq_lens_decoder", "seq_lens_this_time", "padding_offsets", "cum_offsets", "cu_seqlens_q", "cu_seqlens_k", "block_tables", "pre_key_cache", "pre_value_cache", "rope_emb", "mask", "tgt_mask", "cache_k_quant_scales", "cache_v_quant_scales", "cache_k_dequant_scales", "cache_v_dequant_scales", "qkv_out_scale", "qkv_bias", "out_shift", "out_smooth"}, {"max_seq_len", "block_size", "use_neox_style", "dynamic_cachekv_quant", "quant_round_type", "quant_max_bound", "quant_min_bound", "out_scale", "compute_dtype"}, {"fmha_out", "qkv_out", "key_cache_out", "value_cache_out"});
******************************************************************
*/

KernelSignature BlockMultiheadAttentionOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"qkv", "key_cache", "value_cache", "seq_lens_encoder", "seq_lens_decoder", "seq_lens_this_time", "padding_offsets", "cum_offsets", "cu_seqlens_q", "cu_seqlens_k", "block_tables", "pre_key_cache", "pre_value_cache", "rope_emb", "mask", "tgt_mask", "cache_k_quant_scales", "cache_v_quant_scales", "cache_k_dequant_scales", "cache_v_dequant_scales", "qkv_out_scale", "qkv_bias", "out_shift", "out_smooth"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_seq_len");
  attrs.emplace_back("block_size");
  attrs.emplace_back("use_neox_style");
  attrs.emplace_back("dynamic_cachekv_quant");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  attrs.emplace_back("out_scale");
  attrs.emplace_back("compute_dtype");
  paddle::small_vector<const char*> outputs {"fmha_out", "qkv_out", "key_cache_out", "value_cache_out"};
  return KernelSignature("block_multihead_attention", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BnActXpuOpArgumentMapping:

return KernelSignature("bn_act_xpu", {"x", "mean", "variance", "scale", "bias"}, {"momentum", "epsilon", "data_layout", "act_type"}, {"out"});
******************************************************************
*/

KernelSignature BnActXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "mean", "variance", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("momentum");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("data_layout");
  attrs.emplace_back("act_type");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("bn_act_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv1dXpuOpArgumentMapping:

return KernelSignature("conv1d_xpu", {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max"}, {"paddings", "padding_algorithm", "dilations", "strides", "groups", "act_type", "act_param"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature Conv1dXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("groups");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("conv1d_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dTransposeXpuOpArgumentMapping:

return KernelSignature("conv2d_transpose_xpu", {"x", "x_max", "filter", "filter_max", "bias"}, {"strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format", "has_bias", "with_act", "act_type"}, {"out", "out_max"});
return KernelSignature("conv2d_transpose_xpu", {"x", "x_max", "filter", "filter_max", "bias"}, {"strides", "paddings", "output_padding", "OutputSizeTensor", "padding_algorithm", "groups", "dilations", "data_format", "has_bias", "with_act", "act_type"}, {"out", "out_max"});
return KernelSignature("conv2d_transpose_xpu", {"x", "x_max", "filter", "filter_max", "bias"}, {"strides", "paddings", "output_padding", "OutputSizeTensorList", "padding_algorithm", "groups", "dilations", "data_format", "has_bias", "with_act", "act_type"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature Conv2dTransposeXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "filter", "filter_max", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back(
    ctx.HasInput("OutputSizeTensor")
    ? "OutputSizeTensor"
    : ctx.InputSize("OutputSizeTensorList") > 0
      ? "OutputSizeTensorList"
      : "output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  attrs.emplace_back("has_bias");
  attrs.emplace_back("with_act");
  attrs.emplace_back("act_type");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("conv2d_transpose_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dXpuOpArgumentMapping:

return KernelSignature("conv2d_xpu", {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max", "scale_max", "out_max_in"}, {"paddings", "dilations", "strides", "padding_algorithm", "groups", "act_type", "act_param", "out_dtype"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature Conv2dXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "filter", "filter_max", "bias", "branch", "branch_max", "scale_max", "out_max_in"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("conv2d_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossAttentionXpuOpArgumentMapping:

return KernelSignature("cross_attention_xpu", {"input_q", "input_kv", "fc_weight", "fc_weight_max", "fc_bias", "mask"}, {"head_num", "head_dim", "alpha", "out_dtype"}, {"qkv", "qkv_max"});
******************************************************************
*/

KernelSignature CrossAttentionXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input_q", "input_kv", "fc_weight", "fc_weight_max", "fc_bias", "mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("head_num");
  attrs.emplace_back("head_dim");
  attrs.emplace_back("alpha");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs {"qkv", "qkv_max"};
  return KernelSignature("cross_attention_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DequantizeXpuOpArgumentMapping:

return KernelSignature("dequantize_xpu", {"x"}, {"out_dtype", "scale"}, {"y"});
******************************************************************
*/

KernelSignature DequantizeXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("out_dtype");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs {"y"};
  return KernelSignature("dequantize_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EmbeddingWithEltwiseAddXpuOpArgumentMapping:

return KernelSignature("embedding_with_eltwise_add_xpu", {"ids", "tables", "mask"}, {"padding_idx"}, {"out", "seq_lod", "max_seq_len"});
******************************************************************
*/

KernelSignature EmbeddingWithEltwiseAddXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"ids", "tables", "mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("padding_idx");
  paddle::small_vector<const char*> outputs {"out", "seq_lod", "max_seq_len"};
  return KernelSignature("embedding_with_eltwise_add_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FastLayernormXpuOpArgumentMapping:

return KernelSignature("fast_layernorm_xpu", {"x", "scale", "bias"}, {"begin_norm_axis", "epsilon"}, {"out"});
******************************************************************
*/

KernelSignature FastLayernormXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("fast_layernorm_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FastWhereXpuOpArgumentMapping:

return KernelSignature("fast_where_xpu", {"condition", "x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature FastWhereXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"condition", "x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("fast_where_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FcOpArgumentMapping:

return KernelSignature("fc", {"Input", "W", "Bias"}, {"in_num_col_dims", "activation_type", "padding_weights"}, {"Out"});
******************************************************************
*/

KernelSignature FcOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "W", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("in_num_col_dims");
  attrs.emplace_back("activation_type");
  attrs.emplace_back("padding_weights");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FcXpuOpArgumentMapping:

return KernelSignature("fc_xpu", {"x", "x_max", "w", "w_max", "bias", "scale_max", "out_max_in"}, {"in_num_col_dims", "transpose_x", "alpha", "beta", "act_type", "act_alpha", "out_dtype"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature FcXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "w", "w_max", "bias", "scale_max", "out_max_in"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("in_num_col_dims");
  attrs.emplace_back("transpose_x");
  attrs.emplace_back("alpha");
  attrs.emplace_back("beta");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_alpha");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("fc_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedBiasActOpArgumentMapping:

return KernelSignature("fused_bias_act", {"x", "bias", "dequant_scales", "shift", "smooth"}, {"act_method", "compute_dtype", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out"});
******************************************************************
*/

KernelSignature FusedBiasActOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "bias", "dequant_scales", "shift", "smooth"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("act_method");
  attrs.emplace_back("compute_dtype");
  attrs.emplace_back("quant_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("fused_bias_act", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedBiasDropoutResidualLayerNormOpArgumentMapping:

return KernelSignature("fused_bias_dropout_residual_layer_norm", {"X", "Residual", "Bias", "LnScale", "LnBias"}, {"dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"}, {"Y", "BiasDropoutResidualOut", "DropoutMaskOut", "LnMean", "LnVariance"});
******************************************************************
*/

KernelSignature FusedBiasDropoutResidualLayerNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Residual", "Bias", "LnScale", "LnBias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_fix_seed");
  attrs.emplace_back("dropout_seed");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("ln_epsilon");
  paddle::small_vector<const char*> outputs {"Y", "BiasDropoutResidualOut", "DropoutMaskOut", "LnMean", "LnVariance"};
  return KernelSignature("fused_bias_dropout_residual_layer_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedBiasResidualLayernormOpArgumentMapping:

return KernelSignature("fused_bias_residual_layernorm", {"x", "bias", "residual", "norm_weight", "norm_bias"}, {"epsilon", "residual_alpha", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out", "residual_out", "mean", "variance"});
******************************************************************
*/

KernelSignature FusedBiasResidualLayernormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "bias", "residual", "norm_weight", "norm_bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("residual_alpha");
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("quant_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs {"out", "residual_out", "mean", "variance"};
  return KernelSignature("fused_bias_residual_layernorm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedConv2dAddActOpArgumentMapping:

return KernelSignature("fused_conv2d_add_act", {"Input", "Filter", "Bias", "ResidualData"}, {"strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format", "activation", "split_channels", "exhaustive_search", "workspace_size_MB", "fuse_alpha"}, {"Output", "Outputs"});
******************************************************************
*/

KernelSignature FusedConv2dAddActOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "Bias", "ResidualData"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  attrs.emplace_back("activation");
  attrs.emplace_back("split_channels");
  attrs.emplace_back("exhaustive_search");
  attrs.emplace_back("workspace_size_MB");
  attrs.emplace_back("fuse_alpha");
  paddle::small_vector<const char*> outputs {"Output", "Outputs"};
  return KernelSignature("fused_conv2d_add_act", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDconvDreluDbnOpArgumentMapping:

return KernelSignature("fused_dconv_drelu_dbn", {"grad_output", "weight", "grad_output_add", "residual_input", "bn1_eqscale", "bn1_eqbias", "conv_input", "bn1_mean", "bn1_inv_std", "bn1_gamma", "bn1_beta", "bn1_input", "bn2_mean", "bn2_inv_std", "bn2_gamma", "bn2_beta", "bn2_input"}, {"paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "fuse_shortcut", "fuse_dual", "fuse_add", "exhaustive_search"}, {"grad_weight", "grad_bn1_input", "grad_bn1_gamma", "grad_bn1_beta", "grad_bn2_input", "grad_bn2_gamma", "grad_bn2_beta"});
******************************************************************
*/

KernelSignature FusedDconvDreluDbnOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"grad_output", "weight", "grad_output_add", "residual_input", "bn1_eqscale", "bn1_eqbias", "conv_input", "bn1_mean", "bn1_inv_std", "bn1_gamma", "bn1_beta", "bn1_input", "bn2_mean", "bn2_inv_std", "bn2_gamma", "bn2_beta", "bn2_input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  attrs.emplace_back("fuse_shortcut");
  attrs.emplace_back("fuse_dual");
  attrs.emplace_back("fuse_add");
  attrs.emplace_back("exhaustive_search");
  paddle::small_vector<const char*> outputs {"grad_weight", "grad_bn1_input", "grad_bn1_gamma", "grad_bn1_beta", "grad_bn2_input", "grad_bn2_gamma", "grad_bn2_beta"};
  return KernelSignature("fused_dconv_drelu_dbn", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDotProductAttentionOpArgumentMapping:

return KernelSignature("fused_dot_product_attention", {"q", "k", "v", "mask"}, {"scaling_factor", "dropout_probability", "is_training", "is_causal_masking"}, {"out", "softmax_out", "rng_state"});
******************************************************************
*/

KernelSignature FusedDotProductAttentionOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scaling_factor");
  attrs.emplace_back("dropout_probability");
  attrs.emplace_back("is_training");
  attrs.emplace_back("is_causal_masking");
  paddle::small_vector<const char*> outputs {"out", "softmax_out", "rng_state"};
  return KernelSignature("fused_dot_product_attention", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDropoutAddOpArgumentMapping:

return KernelSignature("fused_dropout_add", {"x", "y", "seed_tensor"}, {"p", "is_test", "mode", "seed", "fix_seed"}, {"out", "seed_offset"});
return KernelSignature("fused_dropout_add", {"x", "y", "seed_tensor"}, {"PTensor", "is_test", "mode", "seed", "fix_seed"}, {"out", "seed_offset"});
******************************************************************
*/

KernelSignature FusedDropoutAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "seed_tensor"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("PTensor") ? "PTensor" : "p");
  attrs.emplace_back("is_test");
  attrs.emplace_back("mode");
  attrs.emplace_back("seed");
  attrs.emplace_back("fix_seed");
  paddle::small_vector<const char*> outputs {"out", "seed_offset"};
  return KernelSignature("fused_dropout_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedEmbeddingEltwiseLayernormOpArgumentMapping:

return KernelSignature("fused_embedding_eltwise_layernorm", {"Ids", "Embs", "Bias", "Scale"}, {"epsilon"}, {"Out"});
******************************************************************
*/

KernelSignature FusedEmbeddingEltwiseLayernormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Ids", "Embs", "Bias", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fused_embedding_eltwise_layernorm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedFcElementwiseLayernormOpArgumentMapping:

return KernelSignature("fused_fc_elementwise_layernorm", {"X", "W", "Y", "Bias0", "Scale", "Bias1"}, {"x_num_col_dims", "activation_type", "epsilon", "begin_norm_axis"}, {"Out", "Mean", "Variance"});
******************************************************************
*/

KernelSignature FusedFcElementwiseLayernormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "W", "Y", "Bias0", "Scale", "Bias1"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("x_num_col_dims");
  attrs.emplace_back("activation_type");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs {"Out", "Mean", "Variance"};
  return KernelSignature("fused_fc_elementwise_layernorm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedLinearParamGradAddOpArgumentMapping:

return KernelSignature("fused_linear_param_grad_add", {"x", "dout", "dweight", "dbias"}, {"multi_precision", "has_bias"}, {"dweight_out", "dbias_out"});
******************************************************************
*/

KernelSignature FusedLinearParamGradAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "dout", "dweight", "dbias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("has_bias");
  paddle::small_vector<const char*> outputs {"dweight_out", "dbias_out"};
  return KernelSignature("fused_linear_param_grad_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedMultiTransformerInt8XpuOpArgumentMapping:

return KernelSignature("fused_multi_transformer_int8_xpu", {"x", "ln_scale", "ln_bias", "qkv_in_max", "qkvw", "qkv_bias", "qkv_scales", "out_linear_in_max", "out_linear_w", "out_linear_bias", "out_linear_scales", "ffn_ln_scale", "ffn_ln_bias", "ffn1_in_max", "ffn1_weight", "ffn1_bias", "ffn1_scales", "ffn2_in_max", "ffn2_weight", "ffn2_bias", "ffn2_scales", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer"}, {"pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis"}, {"out", "cache_kv_out"});
******************************************************************
*/

KernelSignature FusedMultiTransformerInt8XpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "ln_scale", "ln_bias", "qkv_in_max", "qkvw", "qkv_bias", "qkv_scales", "out_linear_in_max", "out_linear_w", "out_linear_bias", "out_linear_scales", "ffn_ln_scale", "ffn_ln_bias", "ffn1_in_max", "ffn1_weight", "ffn1_bias", "ffn1_scales", "ffn2_in_max", "ffn2_weight", "ffn2_bias", "ffn2_scales", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pre_layer_norm");
  attrs.emplace_back("rotary_emb_dims");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("act_method");
  attrs.emplace_back("trans_qkvw");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("gather_axis");
  paddle::small_vector<const char*> outputs {"out", "cache_kv_out"};
  return KernelSignature("fused_multi_transformer_int8_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedMultiTransformerXpuOpArgumentMapping:

return KernelSignature("fused_multi_transformer_xpu", {"x", "ln_scale", "ln_bias", "qkvw", "qkvw_max", "qkv_bias", "out_linear_w", "out_linear_wmax", "out_linear_bias", "ffn_ln_scale", "ffn_ln_bias", "ffn1_weight", "ffn1_weight_max", "ffn1_bias", "ffn2_weight", "ffn2_weight_max", "ffn2_bias", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer"}, {"pre_layer_norm", "rotary_emb_dims", "epsilon", "dropout_rate", "is_test", "dropout_implementation", "act_method", "trans_qkvw", "ring_id", "gather_axis"}, {"out", "cache_kv_out"});
******************************************************************
*/

KernelSignature FusedMultiTransformerXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "ln_scale", "ln_bias", "qkvw", "qkvw_max", "qkv_bias", "out_linear_w", "out_linear_wmax", "out_linear_bias", "ffn_ln_scale", "ffn_ln_bias", "ffn1_weight", "ffn1_weight_max", "ffn1_bias", "ffn2_weight", "ffn2_weight_max", "ffn2_bias", "cache_kv", "pre_caches", "rotary_pos_emb", "time_step", "seq_lengths", "src_mask", "gather_index", "max_buffer"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pre_layer_norm");
  attrs.emplace_back("rotary_emb_dims");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("act_method");
  attrs.emplace_back("trans_qkvw");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("gather_axis");
  paddle::small_vector<const char*> outputs {"out", "cache_kv_out"};
  return KernelSignature("fused_multi_transformer_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedRotaryPositionEmbeddingOpArgumentMapping:

return KernelSignature("fused_rotary_position_embedding", {"q", "k", "v", "sin", "cos", "position_ids"}, {"use_neox_rotary_style", "time_major", "rotary_emb_base"}, {"out_q", "out_k", "out_v"});
******************************************************************
*/

KernelSignature FusedRotaryPositionEmbeddingOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "sin", "cos", "position_ids"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("use_neox_rotary_style");
  attrs.emplace_back("time_major");
  attrs.emplace_back("rotary_emb_base");
  paddle::small_vector<const char*> outputs {"out_q", "out_k", "out_v"};
  return KernelSignature("fused_rotary_position_embedding", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedScaleBiasAddReluOpArgumentMapping:

return KernelSignature("fused_scale_bias_add_relu", {"x1", "scale1", "bias1", "x2", "scale2", "bias2"}, {"fuse_dual", "exhaustive_search"}, {"out"});
******************************************************************
*/

KernelSignature FusedScaleBiasAddReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x1", "scale1", "bias1", "x2", "scale2", "bias2"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("fuse_dual");
  attrs.emplace_back("exhaustive_search");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("fused_scale_bias_add_relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedScaleBiasReluConvBnOpArgumentMapping:

return KernelSignature("fused_scale_bias_relu_conv_bn", {"x", "w", "scale", "bias", "bn_scale", "bn_bias", "input_running_mean", "input_running_var"}, {"paddings", "dilations", "strides", "padding_algorithm", "groups", "data_format", "momentum", "epsilon", "fuse_prologue", "exhaustive_search", "accumulation_count"}, {"out", "out_running_mean", "out_running_var", "saved_mean", "saved_var", "eq_scale", "eq_bias"});
******************************************************************
*/

KernelSignature FusedScaleBiasReluConvBnOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "w", "scale", "bias", "bn_scale", "bn_bias", "input_running_mean", "input_running_var"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  attrs.emplace_back("strides");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  attrs.emplace_back("momentum");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("fuse_prologue");
  attrs.emplace_back("exhaustive_search");
  attrs.emplace_back("accumulation_count");
  paddle::small_vector<const char*> outputs {"out", "out_running_mean", "out_running_var", "saved_mean", "saved_var", "eq_scale", "eq_bias"};
  return KernelSignature("fused_scale_bias_relu_conv_bn", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionGruOpArgumentMapping:

return KernelSignature("fusion_gru", {"X", "H0", "WeightX", "WeightH", "Bias"}, {"activation", "gate_activation", "is_reverse", "use_seq", "origin_mode", "force_fp32_output"}, {"ReorderedH0", "XX", "BatchedInput", "BatchedOut", "Hidden"});
******************************************************************
*/

KernelSignature FusionGruOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "H0", "WeightX", "WeightH", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("activation");
  attrs.emplace_back("gate_activation");
  attrs.emplace_back("is_reverse");
  attrs.emplace_back("use_seq");
  attrs.emplace_back("origin_mode");
  attrs.emplace_back("force_fp32_output");
  paddle::small_vector<const char*> outputs {"ReorderedH0", "XX", "BatchedInput", "BatchedOut", "Hidden"};
  return KernelSignature("fusion_gru", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionRepeatedFcReluOpArgumentMapping:

return KernelSignature("fusion_repeated_fc_relu", {"X", "W", "Bias"}, {}, {"ReluOut", "Out"});
******************************************************************
*/

KernelSignature FusionRepeatedFcReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "W", "Bias"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"ReluOut", "Out"};
  return KernelSignature("fusion_repeated_fc_relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionSeqconvEltaddReluOpArgumentMapping:

return KernelSignature("fusion_seqconv_eltadd_relu", {"X", "Filter", "Bias"}, {"contextLength", "contextStart", "contextStride"}, {"Out", "ColMat"});
******************************************************************
*/

KernelSignature FusionSeqconvEltaddReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Filter", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("contextLength");
  attrs.emplace_back("contextStart");
  attrs.emplace_back("contextStride");
  paddle::small_vector<const char*> outputs {"Out", "ColMat"};
  return KernelSignature("fusion_seqconv_eltadd_relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionSeqexpandConcatFcOpArgumentMapping:

return KernelSignature("fusion_seqexpand_concat_fc", {"X", "FCWeight", "FCBias"}, {"fc_activation"}, {"Out", "FCOut"});
******************************************************************
*/

KernelSignature FusionSeqexpandConcatFcOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "FCWeight", "FCBias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("fc_activation");
  paddle::small_vector<const char*> outputs {"Out", "FCOut"};
  return KernelSignature("fusion_seqexpand_concat_fc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionSquaredMatSubOpArgumentMapping:

return KernelSignature("fusion_squared_mat_sub", {"X", "Y"}, {"scalar"}, {"SquaredX", "SquaredY", "SquaredXY", "Out"});
******************************************************************
*/

KernelSignature FusionSquaredMatSubOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scalar");
  paddle::small_vector<const char*> outputs {"SquaredX", "SquaredY", "SquaredXY", "Out"};
  return KernelSignature("fusion_squared_mat_sub", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusionTransposeFlattenConcatOpArgumentMapping:

return KernelSignature("fusion_transpose_flatten_concat", {"X"}, {"trans_axis", "flatten_axis", "concat_axis"}, {"Out"});
******************************************************************
*/

KernelSignature FusionTransposeFlattenConcatOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("trans_axis");
  attrs.emplace_back("flatten_axis");
  attrs.emplace_back("concat_axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fusion_transpose_flatten_concat", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GenerateSequenceXpuOpArgumentMapping:

return KernelSignature("generate_sequence_xpu", {"x"}, {"dtype"}, {"out"});
******************************************************************
*/

KernelSignature GenerateSequenceXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("generate_sequence_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GroupNormSiluXpuOpArgumentMapping:

return KernelSignature("group_norm_silu_xpu", {"x", "scale", "bias"}, {"groups", "epsilon"}, {"out"});
******************************************************************
*/

KernelSignature GroupNormSiluXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("groups");
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("group_norm_silu_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LayerNormActXpuOpArgumentMapping:

return KernelSignature("layer_norm_act_xpu", {"x", "scale", "bias"}, {"begin_norm_axis", "epsilon", "act_type", "act_param"}, {"out"});
******************************************************************
*/

KernelSignature LayerNormActXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "scale", "bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("layer_norm_act_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool2dV2OpArgumentMapping:

return KernelSignature("max_pool2d_v2", {"x"}, {"kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive"}, {"out", "saved_idx"});
******************************************************************
*/

KernelSignature MaxPool2dV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("kernel_size");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("data_format");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs {"out", "saved_idx"};
  return KernelSignature("max_pool2d_v2", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiEncoderXpuOpArgumentMapping:

return KernelSignature("multi_encoder_xpu", {"x", "fc_input_max", "fc_weight", "fc_weight_max", "fc_bias", "ln_scale", "ln_bias", "smooth_scale_weight", "roformer_embedding", "mask", "seq_lod", "max_seq_len"}, {"layer_num", "norm_before", "hidden_dim", "head_num", "size_per_head", "ffn_hidden_dim_scale", "act_type", "relative_type", "slice_idx", "is_per_channel", "max_pos_len", "softmax_max_value", "quant_types"}, {"out", "x_fp16", "out_fp16"});
******************************************************************
*/

KernelSignature MultiEncoderXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "fc_input_max", "fc_weight", "fc_weight_max", "fc_bias", "ln_scale", "ln_bias", "smooth_scale_weight", "roformer_embedding", "mask", "seq_lod", "max_seq_len"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("layer_num");
  attrs.emplace_back("norm_before");
  attrs.emplace_back("hidden_dim");
  attrs.emplace_back("head_num");
  attrs.emplace_back("size_per_head");
  attrs.emplace_back("ffn_hidden_dim_scale");
  attrs.emplace_back("act_type");
  attrs.emplace_back("relative_type");
  attrs.emplace_back("slice_idx");
  attrs.emplace_back("is_per_channel");
  attrs.emplace_back("max_pos_len");
  attrs.emplace_back("softmax_max_value");
  attrs.emplace_back("quant_types");
  paddle::small_vector<const char*> outputs {"out", "x_fp16", "out_fp16"};
  return KernelSignature("multi_encoder_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiheadMatmulOpArgumentMapping:

return KernelSignature("multihead_matmul", {"Input", "W", "Bias", "BiasQK"}, {"transpose_Q", "transpose_K", "transpose_V", "alpha", "head_number"}, {"Out"});
******************************************************************
*/

KernelSignature MultiheadMatmulOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "W", "Bias", "BiasQK"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("transpose_Q");
  attrs.emplace_back("transpose_K");
  attrs.emplace_back("transpose_V");
  attrs.emplace_back("alpha");
  attrs.emplace_back("head_number");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("multihead_matmul", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QkvAttentionXpuOpArgumentMapping:

return KernelSignature("qkv_attention_xpu", {"q", "k", "v", "q_max", "k_max", "v_max", "qk_max", "qkv_max"}, {"alpha", "head_num", "head_dim", "qkv_fc_fusion", "out_dtype"}, {"qkv"});
******************************************************************
*/

KernelSignature QkvAttentionXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "q_max", "k_max", "v_max", "qk_max", "qkv_max"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  attrs.emplace_back("head_num");
  attrs.emplace_back("head_dim");
  attrs.emplace_back("qkv_fc_fusion");
  attrs.emplace_back("out_dtype");
  paddle::small_vector<const char*> outputs {"qkv"};
  return KernelSignature("qkv_attention_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QuantizeXpuOpArgumentMapping:

return KernelSignature("quantize_xpu", {"x"}, {"out_dtype", "scale"}, {"y"});
******************************************************************
*/

KernelSignature QuantizeXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("out_dtype");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs {"y"};
  return KernelSignature("quantize_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoformerRelativeEmbeddingXpuOpArgumentMapping:

return KernelSignature("roformer_relative_embedding_xpu", {"x", "sin_emb", "cos_emb"}, {"max_pos_len"}, {"out"});
******************************************************************
*/

KernelSignature RoformerRelativeEmbeddingXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "sin_emb", "cos_emb"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_pos_len");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("roformer_relative_embedding_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SelfDpAttentionOpArgumentMapping:

return KernelSignature("self_dp_attention", {"X"}, {"alpha", "head_number"}, {"Out"});
******************************************************************
*/

KernelSignature SelfDpAttentionOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  attrs.emplace_back("head_number");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("self_dp_attention", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinePosXpuOpArgumentMapping:

return KernelSignature("sine_pos_xpu", {"x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature SinePosXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("sine_pos_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SkipLayernormOpArgumentMapping:

return KernelSignature("skip_layernorm", {"X", "Y", "Scale", "Bias"}, {"epsilon", "begin_norm_axis"}, {"Out"});
******************************************************************
*/

KernelSignature SkipLayernormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("skip_layernorm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SpatialTransformerResblockXpuOpArgumentMapping:

return KernelSignature("spatial_transformer_resblock_xpu", {"x", "x_max", "conv_bias", "conv_filter", "conv_filter_max", "gn_bias", "gn_scale"}, {"dilations", "paddings", "strides", "gn_eps", "gn_groups", "groups", "conv_fix", "has_silu_fc_input", "include_silu"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature SpatialTransformerResblockXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "conv_bias", "conv_filter", "conv_filter_max", "gn_bias", "gn_scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dilations");
  attrs.emplace_back("paddings");
  attrs.emplace_back("strides");
  attrs.emplace_back("gn_eps");
  attrs.emplace_back("gn_groups");
  attrs.emplace_back("groups");
  attrs.emplace_back("conv_fix");
  attrs.emplace_back("has_silu_fc_input");
  attrs.emplace_back("include_silu");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("spatial_transformer_resblock_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqueezeExcitationBlockOpArgumentMapping:

return KernelSignature("squeeze_excitation_block", {"x", "filter", "filter_max", "bias", "branch"}, {"act_type", "act_param", "filter_dims"}, {"out"});
******************************************************************
*/

KernelSignature SqueezeExcitationBlockOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "filter", "filter_max", "bias", "branch"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("act_type");
  attrs.emplace_back("act_param");
  attrs.emplace_back("filter_dims");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("squeeze_excitation_block", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by VariableLengthMemoryEfficientAttentionOpArgumentMapping:

return KernelSignature("variable_length_memory_efficient_attention", {"query", "key", "value", "seq_lens", "kv_seq_lens", "mask"}, {"scale", "causal", "pre_cache_length"}, {"out"});
******************************************************************
*/

KernelSignature VariableLengthMemoryEfficientAttentionOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"query", "key", "value", "seq_lens", "kv_seq_lens", "mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale");
  attrs.emplace_back("causal");
  attrs.emplace_back("pre_cache_length");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("variable_length_memory_efficient_attention", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloBoxXpuOpArgumentMapping:

return KernelSignature("yolo_box_xpu", {"x", "x_max", "grid", "stride", "anchor_grid"}, {"offset"}, {"out", "out_max"});
******************************************************************
*/

KernelSignature YoloBoxXpuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "x_max", "grid", "stride", "anchor_grid"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs {"out", "out_max"};
  return KernelSignature("yolo_box_xpu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedBiasDropoutResidualLayerNormGradOpArgumentMapping:

return KernelSignature("fused_bias_dropout_residual_layer_norm_grad", {"X", "Residual", "Bias", "LnScale", "LnBias", "LnMean", "LnVariance", "BiasDropoutResidualOut", "DropoutMaskOut", "Y@GRAD"}, {"dropout_rate", "is_test", "dropout_fix_seed", "dropout_seed", "dropout_implementation", "ln_epsilon"}, {"X@GRAD", "Residual@GRAD", "Bias@GRAD", "LnScale@GRAD", "LnBias@GRAD"});
******************************************************************
*/

KernelSignature FusedBiasDropoutResidualLayerNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Residual", "Bias", "LnScale", "LnBias", "LnMean", "LnVariance", "BiasDropoutResidualOut", "DropoutMaskOut", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout_rate");
  attrs.emplace_back("is_test");
  attrs.emplace_back("dropout_fix_seed");
  attrs.emplace_back("dropout_seed");
  attrs.emplace_back("dropout_implementation");
  attrs.emplace_back("ln_epsilon");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Residual@GRAD", "Bias@GRAD", "LnScale@GRAD", "LnBias@GRAD"};
  return KernelSignature("fused_bias_dropout_residual_layer_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDotProductAttentionGradOpArgumentMapping:

return KernelSignature("fused_dot_product_attention_grad", {"q", "k", "v", "out", "softmax_out", "rng_state", "mask", "out@GRAD"}, {"scaling_factor", "dropout_probability", "is_causal_masking"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FusedDotProductAttentionGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "out", "softmax_out", "rng_state", "mask", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scaling_factor");
  attrs.emplace_back("dropout_probability");
  attrs.emplace_back("is_causal_masking");
  paddle::small_vector<const char*> outputs {"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("fused_dot_product_attention_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedDropoutAddGradOpArgumentMapping:

return KernelSignature("fused_dropout_add_grad", {"seed_offset", "out@GRAD"}, {"p", "is_test", "mode", "fix_seed"}, {"x@GRAD", "y@GRAD"});
return KernelSignature("fused_dropout_add_grad", {"seed_offset", "out@GRAD"}, {"PTensor", "is_test", "mode", "fix_seed"}, {"x@GRAD", "y@GRAD"});
******************************************************************
*/

KernelSignature FusedDropoutAddGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"seed_offset", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("PTensor") ? "PTensor" : "p");
  attrs.emplace_back("is_test");
  attrs.emplace_back("mode");
  attrs.emplace_back("fix_seed");
  paddle::small_vector<const char*> outputs {"x@GRAD", "y@GRAD"};
  return KernelSignature("fused_dropout_add_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FusedRotaryPositionEmbeddingGradOpArgumentMapping:

return KernelSignature("fused_rotary_position_embedding_grad", {"sin", "cos", "position_ids", "out_q@GRAD", "out_k@GRAD", "out_v@GRAD"}, {"use_neox_rotary_style", "time_major", "rotary_emb_base"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FusedRotaryPositionEmbeddingGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"sin", "cos", "position_ids", "out_q@GRAD", "out_k@GRAD", "out_v@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("use_neox_rotary_style");
  attrs.emplace_back("time_major");
  attrs.emplace_back("rotary_emb_base");
  paddle::small_vector<const char*> outputs {"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("fused_rotary_position_embedding_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool2dV2GradOpArgumentMapping:

return KernelSignature("max_pool2d_v2_grad", {"x", "out", "saved_idx", "out@GRAD"}, {"kernel_size", "strides", "paddings", "data_format", "global_pooling", "adaptive"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature MaxPool2dV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out", "saved_idx", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("kernel_size");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("data_format");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("max_pool2d_v2_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

}  // namespace phi

PD_REGISTER_ARG_MAPPING_FN(add_act_xpu, phi::AddActXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(add_layernorm_xpu, phi::AddLayernormXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(addcmul_xpu, phi::AddcmulXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(block_multihead_attention, phi::BlockMultiheadAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bn_act_xpu, phi::BnActXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv1d_xpu, phi::Conv1dXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_transpose_xpu, phi::Conv2dTransposeXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_xpu, phi::Conv2dXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cross_attention_xpu, phi::CrossAttentionXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dequantize_xpu, phi::DequantizeXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(embedding_with_eltwise_add_xpu, phi::EmbeddingWithEltwiseAddXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fast_layernorm_xpu, phi::FastLayernormXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fast_where_xpu, phi::FastWhereXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fc, phi::FcOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fc_xpu, phi::FcXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_bias_act, phi::FusedBiasActOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_bias_dropout_residual_layer_norm, phi::FusedBiasDropoutResidualLayerNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_bias_residual_layernorm, phi::FusedBiasResidualLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_conv2d_add_act, phi::FusedConv2dAddActOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dconv_drelu_dbn, phi::FusedDconvDreluDbnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dot_product_attention, phi::FusedDotProductAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dropout_add, phi::FusedDropoutAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_embedding_eltwise_layernorm, phi::FusedEmbeddingEltwiseLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_fc_elementwise_layernorm, phi::FusedFcElementwiseLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_linear_param_grad_add, phi::FusedLinearParamGradAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_multi_transformer_int8_xpu, phi::FusedMultiTransformerInt8XpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_multi_transformer_xpu, phi::FusedMultiTransformerXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_rotary_position_embedding, phi::FusedRotaryPositionEmbeddingOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_scale_bias_add_relu, phi::FusedScaleBiasAddReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_scale_bias_relu_conv_bn, phi::FusedScaleBiasReluConvBnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_gru, phi::FusionGruOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_repeated_fc_relu, phi::FusionRepeatedFcReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_seqconv_eltadd_relu, phi::FusionSeqconvEltaddReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_seqexpand_concat_fc, phi::FusionSeqexpandConcatFcOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_squared_mat_sub, phi::FusionSquaredMatSubOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fusion_transpose_flatten_concat, phi::FusionTransposeFlattenConcatOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(generate_sequence_xpu, phi::GenerateSequenceXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(group_norm_silu_xpu, phi::GroupNormSiluXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(layer_norm_act_xpu, phi::LayerNormActXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool2d_v2, phi::MaxPool2dV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multi_encoder_xpu, phi::MultiEncoderXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multihead_matmul, phi::MultiheadMatmulOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(qkv_attention_xpu, phi::QkvAttentionXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(quantize_xpu, phi::QuantizeXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roformer_relative_embedding_xpu, phi::RoformerRelativeEmbeddingXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(self_dp_attention, phi::SelfDpAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sine_pos_xpu, phi::SinePosXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(skip_layernorm, phi::SkipLayernormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(spatial_transformer_resblock_xpu, phi::SpatialTransformerResblockXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(squeeze_excitation_block, phi::SqueezeExcitationBlockOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(variable_length_memory_efficient_attention, phi::VariableLengthMemoryEfficientAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(yolo_box_xpu, phi::YoloBoxXpuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_bias_dropout_residual_layer_norm_grad, phi::FusedBiasDropoutResidualLayerNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dot_product_attention_grad, phi::FusedDotProductAttentionGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_dropout_add_grad, phi::FusedDropoutAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fused_rotary_position_embedding_grad, phi::FusedRotaryPositionEmbeddingGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool2d_v2_grad, phi::MaxPool2dV2GradOpArgumentMapping);
