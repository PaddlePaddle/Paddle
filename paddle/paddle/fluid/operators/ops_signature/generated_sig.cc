// this file is generated by paddle/phi/op/yaml/generator/generate_op.py, do not edit.
#include "paddle/phi/core/compat/op_utils.h"
#include "paddle/utils/small_vector.h"

namespace phi {

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AbsOpArgumentMapping:

return KernelSignature("abs", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AbsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("abs", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AccuracyOpArgumentMapping:

return KernelSignature("accuracy", {"Out", "Indices", "Label"}, {}, {"Accuracy", "Correct", "Total"});
******************************************************************
*/

KernelSignature AccuracyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Indices", "Label"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Accuracy", "Correct", "Total"};
  return KernelSignature("accuracy", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcosOpArgumentMapping:

return KernelSignature("acos", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AcosOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("acos", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcoshOpArgumentMapping:

return KernelSignature("acosh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AcoshOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("acosh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AdagradOpArgumentMapping:

return KernelSignature("adagrad", {"Param", "Grad", "Moment", "LearningRate", "MasterParam"}, {"epsilon", "multi_precision"}, {"ParamOut", "MomentOut", "MasterParamOut"});
return KernelSignature("adagrad_dense_param_sparse_grad", {"Param", "Grad", "Moment", "LearningRate", "MasterParam"}, {"epsilon", "multi_precision"}, {"ParamOut", "MomentOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature AdagradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "Moment", "LearningRate", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"ParamOut", "MomentOut", "MasterParamOut"};
 if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("Grad") &&
  ctx.IsDenseTensorInput("Moment") &&
  ctx.IsDenseTensorInput("LearningRate") &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("adagrad", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsSelectedRowsInput("Grad") &&
  ctx.IsDenseTensorInput("Moment") &&
  ctx.IsDenseTensorInput("LearningRate") &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("adagrad_dense_param_sparse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AdamaxOpArgumentMapping:

return KernelSignature("adamax", {"Param", "Grad", "LearningRate", "Moment", "InfNorm", "Beta1Pow", "MasterParam"}, {"beta1", "beta2", "epsilon", "multi_precision"}, {"ParamOut", "MomentOut", "InfNormOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature AdamaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "LearningRate", "Moment", "InfNorm", "Beta1Pow", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta1");
  attrs.emplace_back("beta2");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"ParamOut", "MomentOut", "InfNormOut", "MasterParamOut"};
  return KernelSignature("adamax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AdamwOpArgumentMapping:

return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"beta1", "beta2", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"beta1", "beta2", "EpsilonTensor", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"beta1", "Beta2Tensor", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"beta1", "Beta2Tensor", "EpsilonTensor", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"Beta1Tensor", "beta2", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"Beta1Tensor", "beta2", "EpsilonTensor", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"Beta1Tensor", "Beta2Tensor", "epsilon", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("adamw", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"Beta1Tensor", "Beta2Tensor", "EpsilonTensor", "lr_ratio", "coeff", "with_decay", "lazy_mode", "min_row_size_to_use_multithread", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature AdamwOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Beta1Tensor") ? "Beta1Tensor" : "beta1");
  attrs.emplace_back(ctx.HasInput("Beta2Tensor") ? "Beta2Tensor" : "beta2");
  attrs.emplace_back(ctx.HasInput("EpsilonTensor") ? "EpsilonTensor" : "epsilon");
  attrs.emplace_back("lr_ratio");
  attrs.emplace_back("coeff");
  attrs.emplace_back("with_decay");
  attrs.emplace_back("lazy_mode");
  attrs.emplace_back("min_row_size_to_use_multithread");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("use_global_beta_pow");
  paddle::small_vector<const char*> outputs {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"};
  return KernelSignature("adamw", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddmmOpArgumentMapping:

return KernelSignature("addmm", {"Input", "X", "Y"}, {"Beta", "Alpha"}, {"Out"});
******************************************************************
*/

KernelSignature AddmmOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Beta");
  attrs.emplace_back("Alpha");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("addmm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AffineGridOpArgumentMapping:

return KernelSignature("affine_grid", {"Theta"}, {"output_shape", "align_corners"}, {"Output"});
return KernelSignature("affine_grid", {"Theta"}, {"OutputShape", "align_corners"}, {"Output"});
******************************************************************
*/

KernelSignature AffineGridOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Theta"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("OutputShape")
    ? "OutputShape"
    : "output_shape");

  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("affine_grid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AllcloseOpArgumentMapping:

return KernelSignature("allclose", {"Input", "Other"}, {"rtol", "atol", "equal_nan"}, {"Out"});
return KernelSignature("allclose", {"Input", "Other"}, {"rtol", "Atol", "equal_nan"}, {"Out"});
return KernelSignature("allclose", {"Input", "Other"}, {"Rtol", "atol", "equal_nan"}, {"Out"});
return KernelSignature("allclose", {"Input", "Other"}, {"Rtol", "Atol", "equal_nan"}, {"Out"});
******************************************************************
*/

KernelSignature AllcloseOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Other"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Rtol") ? "Rtol" : "rtol");
  attrs.emplace_back(ctx.HasInput("Atol") ? "Atol" : "atol");
  attrs.emplace_back("equal_nan");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("allclose", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AngleOpArgumentMapping:

return KernelSignature("angle", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AngleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("angle", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ApplyPerChannelScaleOpArgumentMapping:

return KernelSignature("apply_per_channel_scale", {"x", "scales"}, {}, {"out"});
******************************************************************
*/

KernelSignature ApplyPerChannelScaleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "scales"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("apply_per_channel_scale", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgmaxOpArgumentMapping:

return KernelSignature("argmax", {"X"}, {"axis", "keepdims", "flatten", "dtype"}, {"Out"});
******************************************************************
*/

KernelSignature ArgMaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdims");
  attrs.emplace_back("flatten");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("argmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgminOpArgumentMapping:

return KernelSignature("argmin", {"X"}, {"axis", "keepdims", "flatten", "dtype"}, {"Out"});
******************************************************************
*/

KernelSignature ArgMinOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdims");
  attrs.emplace_back("flatten");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("argmin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgsortOpArgumentMapping:

return KernelSignature("argsort", {"X"}, {"axis", "descending"}, {"Out", "Indices"});
******************************************************************
*/

KernelSignature ArgsortOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("descending");
  paddle::small_vector<const char*> outputs {"Out", "Indices"};
  return KernelSignature("argsort", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsComplexOpArgumentMapping:

return KernelSignature("as_complex", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsComplexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("as_complex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsRealOpArgumentMapping:

return KernelSignature("as_real", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsRealOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("as_real", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsStridedOpArgumentMapping:

return KernelSignature("as_strided", {"input"}, {"dims", "stride", "offset"}, {"out"});
******************************************************************
*/

KernelSignature AsStridedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  attrs.emplace_back("stride");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("as_strided", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsgdOpArgumentMapping:

return KernelSignature("asgd", {"param", "grad", "learning_rate", "d", "y", "n", "master_param"}, {"multi_precision"}, {"param_out", "d_out", "y_out", "master_param_out"});
******************************************************************
*/

KernelSignature AsgdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"param", "grad", "learning_rate", "d", "y", "n", "master_param"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"param_out", "d_out", "y_out", "master_param_out"};
  return KernelSignature("asgd", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinOpArgumentMapping:

return KernelSignature("asin", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsinOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("asin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinhOpArgumentMapping:

return KernelSignature("asinh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AsinhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("asinh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanOpArgumentMapping:

return KernelSignature("atan", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AtanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("atan", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Atan2OpArgumentMapping:

return KernelSignature("atan2", {"X1", "X2"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Atan2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X1", "X2"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("atan2", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanhOpArgumentMapping:

return KernelSignature("atanh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature AtanhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("atanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AucOpArgumentMapping:

return KernelSignature("auc", {"Predict", "Label", "StatPos", "StatNeg", "InsTagWeight"}, {"curve", "num_thresholds", "slide_steps"}, {"AUC", "StatPosOut", "StatNegOut"});
******************************************************************
*/

KernelSignature AucOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Predict", "Label", "StatPos", "StatNeg", "InsTagWeight"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("curve");
  attrs.emplace_back("num_thresholds");
  attrs.emplace_back("slide_steps");
  paddle::small_vector<const char*> outputs {"AUC", "StatPosOut", "StatNegOut"};
  return KernelSignature("auc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AverageAccumulatesOpArgumentMapping:

return KernelSignature("average_accumulates", {"param", "in_sum_1", "in_sum_2", "in_sum_3", "in_num_accumulates", "in_old_num_accumulates", "in_num_updates"}, {"average_window", "max_average_window", "min_average_window"}, {"out_sum_1", "out_sum_2", "out_sum_3", "out_num_accumulates", "out_old_num_accumulates", "out_num_updates"});
******************************************************************
*/

KernelSignature AverageAccumulatesOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"param", "in_sum_1", "in_sum_2", "in_sum_3", "in_num_accumulates", "in_old_num_accumulates", "in_num_updates"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("average_window");
  attrs.emplace_back("max_average_window");
  attrs.emplace_back("min_average_window");
  paddle::small_vector<const char*> outputs {"out_sum_1", "out_sum_2", "out_sum_3", "out_num_accumulates", "out_old_num_accumulates", "out_num_updates"};
  return KernelSignature("average_accumulates", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BceLossOpArgumentMapping:

return KernelSignature("bce_loss", {"X", "Label"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BceLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Label"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bce_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BernoulliOpArgumentMapping:

return KernelSignature("bernoulli", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BernoulliOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bernoulli", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BicubicInterpOpArgumentMapping:

return KernelSignature("bicubic_interp", {"X", "OutSize", "SizeTensor", "Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature BicubicInterpV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bicubic_interp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearOpArgumentMapping:

return KernelSignature("bilinear", {"X", "Y", "Weight", "Bias"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BilinearTensorProductOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Weight", "Bias"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bilinear", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearInterpOpArgumentMapping:

return KernelSignature("bilinear_interp", {"X", "OutSize", "SizeTensor", "Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature BilinearInterpV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bilinear_interp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BincountOpArgumentMapping:

return KernelSignature("bincount", {"X", "Weights"}, {"minlength"}, {"Out"});
******************************************************************
*/

KernelSignature BincountOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Weights"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("minlength");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bincount", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BinomialOpArgumentMapping:

return KernelSignature("binomial", {"count", "prob"}, {}, {"out"});
******************************************************************
*/

KernelSignature BinomialOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"count", "prob"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("binomial", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseAndOpArgumentMapping:

return KernelSignature("bitwise_and", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseAndOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bitwise_and", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseLeftShiftOpArgumentMapping:

return KernelSignature("bitwise_left_shift", {"x", "y"}, {"is_arithmetic"}, {"out"});
******************************************************************
*/

KernelSignature BitwiseLeftShiftOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("is_arithmetic");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("bitwise_left_shift", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseNotOpArgumentMapping:

return KernelSignature("bitwise_not", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseNotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bitwise_not", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseOrOpArgumentMapping:

return KernelSignature("bitwise_or", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseOrOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bitwise_or", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseRightShiftOpArgumentMapping:

return KernelSignature("bitwise_right_shift", {"x", "y"}, {"is_arithmetic"}, {"out"});
******************************************************************
*/

KernelSignature BitwiseRightShiftOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("is_arithmetic");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("bitwise_right_shift", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BitwiseXorOpArgumentMapping:

return KernelSignature("bitwise_xor", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BitwiseXorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bitwise_xor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BmmOpArgumentMapping:

return KernelSignature("bmm", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BmmOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("bmm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BoxCoderOpArgumentMapping:

return KernelSignature("box_coder", {"PriorBox", "PriorBoxVar", "TargetBox"}, {"code_type", "box_normalized", "axis", "variance"}, {"OutputBox"});
******************************************************************
*/

KernelSignature BoxCoderOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"PriorBox", "PriorBoxVar", "TargetBox"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("code_type");
  attrs.emplace_back("box_normalized");
  attrs.emplace_back("axis");
  attrs.emplace_back("variance");
  paddle::small_vector<const char*> outputs {"OutputBox"};
  return KernelSignature("box_coder", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BroadcastTensorsOpArgumentMapping:

return KernelSignature("broadcast_tensors", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature BroadcastTensorsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("broadcast_tensors", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeilOpArgumentMapping:

return KernelSignature("ceil", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature CeilOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("ceil", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeluOpArgumentMapping:

return KernelSignature("celu", {"X"}, {"alpha"}, {"Out"});
******************************************************************
*/

KernelSignature CeluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("celu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CheckFiniteAndUnscaleOpArgumentMapping:

return KernelSignature("check_finite_and_unscale", {"X", "Scale"}, {}, {"Out", "FoundInfinite"});
******************************************************************
*/

KernelSignature CheckFiniteAndUnscaleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out", "FoundInfinite"};
  return KernelSignature("check_finite_and_unscale", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CheckNumericsOpArgumentMapping:

return KernelSignature("check_numerics", {"tensor"}, {"op_type", "var_name", "check_nan_inf_level", "stack_height_limit", "output_dir"}, {"stats", "values"});
******************************************************************
*/

KernelSignature CheckNumericsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"tensor"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("op_type");
  attrs.emplace_back("var_name");
  attrs.emplace_back("check_nan_inf_level");
  attrs.emplace_back("stack_height_limit");
  attrs.emplace_back("output_dir");
  paddle::small_vector<const char*> outputs {"stats", "values"};
  return KernelSignature("check_numerics", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskyOpArgumentMapping:

return KernelSignature("cholesky", {"X"}, {"upper"}, {"Out"});
******************************************************************
*/

KernelSignature CholeskyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cholesky", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskySolveOpArgumentMapping:

return KernelSignature("cholesky_solve", {"X", "Y"}, {"upper"}, {"Out"});
******************************************************************
*/

KernelSignature CholeskySolveOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cholesky_solve", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClassCenterSampleOpArgumentMapping:

return KernelSignature("class_center_sample", {"Label"}, {"num_classes", "num_samples", "ring_id", "rank", "nranks", "fix_seed", "seed"}, {"RemappedLabel", "SampledLocalClassCenter"});
******************************************************************
*/

KernelSignature ClassCenterSampleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Label"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("num_classes");
  attrs.emplace_back("num_samples");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("rank");
  attrs.emplace_back("nranks");
  attrs.emplace_back("fix_seed");
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs {"RemappedLabel", "SampledLocalClassCenter"};
  return KernelSignature("class_center_sample", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipOpArgumentMapping:

return KernelSignature("clip", {"X"}, {"min", "max"}, {"Out"});
******************************************************************
*/

KernelSignature ClipOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Min") ? "Min" : "min");
  attrs.emplace_back(ctx.HasInput("Max") ? "Max" : "max");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("clip", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipByNormOpArgumentMapping:

return KernelSignature("clip_by_norm", {"X"}, {"max_norm"}, {"Out"});
return KernelSignature("clip_by_norm_sr", {"X"}, {"max_norm"}, {"Out"});
******************************************************************
*/

KernelSignature ClipByNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_norm");
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("clip_by_norm", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("clip_by_norm_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CoalesceTensorOpArgumentMapping:

return KernelSignature("coalesce_tensor", {"Input"}, {"dtype", "copy_data", "set_constant", "persist_output", "constant", "use_align", "align_size", "user_defined_size_of_dtype", "concated_shapes", "concated_ranks"}, {"Output", "FusedOutput"});
******************************************************************
*/

KernelSignature CoalesceTensorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  attrs.emplace_back("copy_data");
  attrs.emplace_back("set_constant");
  attrs.emplace_back("persist_output");
  attrs.emplace_back("constant");
  attrs.emplace_back("use_align");
  attrs.emplace_back("align_size");
  attrs.emplace_back("user_defined_size_of_dtype");
  attrs.emplace_back("concated_shapes");
  attrs.emplace_back("concated_ranks");
  paddle::small_vector<const char*> outputs {"Output", "FusedOutput"};
  return KernelSignature("coalesce_tensor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ComplexOpArgumentMapping:

return KernelSignature("complex", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ComplexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("complex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ConcatOpArgumentMapping:

return KernelSignature("concat", {"X"}, {"axis"}, {"Out"});
return KernelSignature("concat", {"X"}, {"AxisTensor"}, {"Out"});
******************************************************************
*/

KernelSignature ConcatOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor") ? "AxisTensor" : "axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("concat", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ConjOpArgumentMapping:

return KernelSignature("conj", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ConjOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("conj", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dOpArgumentMapping:

return KernelSignature("conv2d", {"Input", "Filter"}, {"strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv2dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("conv2d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dOpArgumentMapping:

return KernelSignature("conv3d", {"Input", "Filter"}, {"strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("conv3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dTransposeOpArgumentMapping:

return KernelSignature("conv3d_transpose", {"Input", "Filter"}, {"strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature Conv3dTransposeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("conv3d_transpose", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CopysignOpArgumentMapping:

return KernelSignature("copysign", {"x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature CopysignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("copysign", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosOpArgumentMapping:

return KernelSignature("cos", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature CosOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cos", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CoshOpArgumentMapping:

return KernelSignature("cosh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature CoshOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cosh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CropOpArgumentMapping:

return KernelSignature("crop", {"X"}, {"shape", "offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"shape", "Offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"shape", "OffsetsTensor"}, {"Out"});
return KernelSignature("crop", {"X"}, {"Shape", "offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"Shape", "Offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"Shape", "OffsetsTensor"}, {"Out"});
return KernelSignature("crop", {"X"}, {"ShapeTensor", "offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"ShapeTensor", "Offsets"}, {"Out"});
return KernelSignature("crop", {"X"}, {"ShapeTensor", "OffsetsTensor"}, {"Out"});
******************************************************************
*/

KernelSignature CropTensorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("Shape")
    ? "Shape"
    : ctx.InputSize("ShapeTensor") > 0
      ? "ShapeTensor"
      : "shape");
  attrs.emplace_back(
    ctx.HasInput("Offsets")
    ? "Offsets"
    : ctx.InputSize("OffsetsTensor") > 0
      ? "OffsetsTensor"
      : "offsets");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("crop", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossOpArgumentMapping:

return KernelSignature("cross", {"X", "Y"}, {"dim"}, {"Out"});
******************************************************************
*/

KernelSignature CrossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cross", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossEntropyWithSoftmaxOpArgumentMapping:

return KernelSignature("cross_entropy_with_softmax", {"Logits", "Label"}, {"soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, {"Softmax", "Loss"});
******************************************************************
*/

KernelSignature SoftmaxWithCrossEntropyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Logits", "Label"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("soft_label");
  attrs.emplace_back("use_softmax");
  attrs.emplace_back("numeric_stable_mode");
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Softmax", "Loss"};
  return KernelSignature("cross_entropy_with_softmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CummaxOpArgumentMapping:

return KernelSignature("cummax", {"x"}, {"axis", "dtype"}, {"out", "indices"});
******************************************************************
*/

KernelSignature CummaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"out", "indices"};
  return KernelSignature("cummax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumminOpArgumentMapping:

return KernelSignature("cummin", {"x"}, {"axis", "dtype"}, {"out", "indices"});
******************************************************************
*/

KernelSignature CumminOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"out", "indices"};
  return KernelSignature("cummin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumprodOpArgumentMapping:

return KernelSignature("cumprod", {"X"}, {"dim"}, {"Out"});
******************************************************************
*/

KernelSignature CumprodOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cumprod", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumsumOpArgumentMapping:

return KernelSignature("cumsum", {"X"}, {"axis", "flatten", "exclusive", "reverse"}, {"Out"});
return KernelSignature("cumsum", {"X"}, {"AxisTensor", "flatten", "exclusive", "reverse"}, {"Out"});
******************************************************************
*/

KernelSignature CumsumOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("cumsum", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DataOpArgumentMapping:

return KernelSignature("data", {}, {"name", "shape", "dtype", "place"}, {"out"});
return KernelSignature("data", {}, {"name", "ShapeTensor", "dtype", "place"}, {"out"});
return KernelSignature("data", {}, {"name", "ShapeTensorList", "dtype", "place"}, {"out"});
******************************************************************
*/

KernelSignature DataOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("name");
  attrs.emplace_back(
    ctx.HasInput("ShapeTensor")
    ? "ShapeTensor"
    : ctx.InputSize("ShapeTensorList") > 0
      ? "ShapeTensorList"
      : "shape");
  attrs.emplace_back("dtype");
  
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("data", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DepthwiseConv2dOpArgumentMapping:

return KernelSignature("depthwise_conv2d", {"Input", "Filter"}, {"strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"Output"});
******************************************************************
*/

KernelSignature DepthwiseConv2dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("depthwise_conv2d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DetOpArgumentMapping:

return KernelSignature("determinant", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DeterminantOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("determinant", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagOpArgumentMapping:

return KernelSignature("diag", {"X"}, {"offset", "padding_value"}, {"Out"});
******************************************************************
*/

KernelSignature DiagV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("padding_value");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("diag", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagEmbedOpArgumentMapping:

return KernelSignature("diag_embed", {"Input"}, {"offset", "dim1", "dim2"}, {"Out"});
******************************************************************
*/

KernelSignature DiagEmbedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("dim1");
  attrs.emplace_back("dim2");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("diag_embed", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagonalOpArgumentMapping:

return KernelSignature("diagonal", {"Input"}, {"offset", "axis1", "axis2"}, {"Out"});
******************************************************************
*/

KernelSignature DiagonalOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("diagonal", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DigammaOpArgumentMapping:

return KernelSignature("digamma", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DigammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("digamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DirichletOpArgumentMapping:

return KernelSignature("dirichlet", {"Alpha"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DirichletOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Alpha"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("dirichlet", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DistOpArgumentMapping:

return KernelSignature("dist", {"X", "Y"}, {"p"}, {"Out"});
******************************************************************
*/

KernelSignature DistOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("dist", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DotOpArgumentMapping:

return KernelSignature("dot", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature DotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("dot", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EditDistanceOpArgumentMapping:

return KernelSignature("edit_distance", {"Hyps", "Refs", "HypsLength", "RefsLength"}, {"normalized"}, {"SequenceNum", "Out"});
******************************************************************
*/

KernelSignature EditDistanceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Hyps", "Refs", "HypsLength", "RefsLength"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("normalized");
  paddle::small_vector<const char*> outputs {"SequenceNum", "Out"};
  return KernelSignature("edit_distance", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigOpArgumentMapping:

return KernelSignature("eig", {"X"}, {}, {"Eigenvalues", "Eigenvectors"});
******************************************************************
*/

KernelSignature EigOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Eigenvalues", "Eigenvectors"};
  return KernelSignature("eig", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EighOpArgumentMapping:

return KernelSignature("eigh", {"X"}, {"UPLO"}, {"Eigenvalues", "Eigenvectors"});
******************************************************************
*/

KernelSignature EighOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("UPLO");
  paddle::small_vector<const char*> outputs {"Eigenvalues", "Eigenvectors"};
  return KernelSignature("eigh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigvalsOpArgumentMapping:

return KernelSignature("eigvals", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature EigvalsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("eigvals", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigvalshOpArgumentMapping:

return KernelSignature("eigvalsh", {"X"}, {"UPLO", "is_test"}, {"Eigenvalues", "Eigenvectors"});
******************************************************************
*/

KernelSignature EigvalshOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("UPLO");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs {"Eigenvalues", "Eigenvectors"};
  return KernelSignature("eigvalsh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EluOpArgumentMapping:

return KernelSignature("elu", {"X"}, {"alpha"}, {"Out"});
******************************************************************
*/

KernelSignature EluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("elu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EqualAllOpArgumentMapping:

return KernelSignature("equal_all", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature EqualAllOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("equal_all", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfOpArgumentMapping:

return KernelSignature("erf", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ErfOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("erf", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfinvOpArgumentMapping:

return KernelSignature("erfinv", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ErfinvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("erfinv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpOpArgumentMapping:

return KernelSignature("exp", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ExpOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("exp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpandAsOpArgumentMapping:

return KernelSignature("expand_as", {"X", "Y"}, {"target_shape"}, {"Out"});
******************************************************************
*/

KernelSignature ExpandAsV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("target_shape");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("expand_as", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Expm1OpArgumentMapping:

return KernelSignature("expm1", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Expm1OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("expm1", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2cOpArgumentMapping:

return KernelSignature("fft_c2c", {"X"}, {"axes", "normalization", "forward"}, {"Out"});
******************************************************************
*/

KernelSignature FftC2cOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fft_c2c", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2rOpArgumentMapping:

return KernelSignature("fft_c2r", {"X"}, {"axes", "normalization", "forward", "last_dim_size"}, {"Out"});
******************************************************************
*/

KernelSignature FftC2rOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("last_dim_size");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fft_c2r", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftR2cOpArgumentMapping:

return KernelSignature("fft_r2c", {"X"}, {"axes", "normalization", "forward", "onesided"}, {"Out"});
******************************************************************
*/

KernelSignature FftR2cOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("onesided");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fft_r2c", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillOpArgumentMapping:

return KernelSignature("fill", {"X"}, {"value"}, {"Out"});
return KernelSignature("fill", {"X"}, {"ValueTensor"}, {"Out"});
******************************************************************
*/

KernelSignature FillAnyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fill", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalOpArgumentMapping:

return KernelSignature("fill_diagonal", {"X"}, {"value", "offset", "wrap"}, {"Out"});
******************************************************************
*/

KernelSignature FillDiagonalOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("offset");
  attrs.emplace_back("wrap");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fill_diagonal", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalTensorOpArgumentMapping:

return KernelSignature("fill_diagonal_tensor", {"X", "Y"}, {"offset", "dim1", "dim2"}, {"Out"});
******************************************************************
*/

KernelSignature FillDiagonalTensorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("dim1");
  attrs.emplace_back("dim2");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("fill_diagonal_tensor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnOpArgumentMapping:

return KernelSignature("flash_attn", {"q", "k", "v", "fixed_seed_offset", "attn_mask"}, {"dropout", "causal", "return_softmax", "is_test", "rng_name"}, {"out", "softmax", "softmax_lse", "seed_offset"});
******************************************************************
*/

KernelSignature FlashAttnOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "fixed_seed_offset", "attn_mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("is_test");
  attrs.emplace_back("rng_name");
  paddle::small_vector<const char*> outputs {"out", "softmax", "softmax_lse", "seed_offset"};
  return KernelSignature("flash_attn", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnUnpaddedOpArgumentMapping:

return KernelSignature("flash_attn_unpadded", {"q", "k", "v", "cu_seqlens_q", "cu_seqlens_k", "fixed_seed_offset", "attn_mask"}, {"max_seqlen_q", "max_seqlen_k", "scale", "dropout", "causal", "return_softmax", "is_test", "rng_name"}, {"out", "softmax", "softmax_lse", "seed_offset"});
******************************************************************
*/

KernelSignature FlashAttnUnpaddedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "cu_seqlens_q", "cu_seqlens_k", "fixed_seed_offset", "attn_mask"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_seqlen_q");
  attrs.emplace_back("max_seqlen_k");
  attrs.emplace_back("scale");
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("is_test");
  attrs.emplace_back("rng_name");
  paddle::small_vector<const char*> outputs {"out", "softmax", "softmax_lse", "seed_offset"};
  return KernelSignature("flash_attn_unpadded", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnWithSparseMaskOpArgumentMapping:

return KernelSignature("flash_attn_with_sparse_mask", {"q", "k", "v", "attn_mask_start_row_indices", "fixed_seed_offset"}, {"dropout", "causal", "attn_mask_start_row", "return_softmax", "is_test", "rng_name"}, {"out", "softmax", "softmax_lse", "seed_offset"});
******************************************************************
*/

KernelSignature FlashAttnWithSparseMaskOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "attn_mask_start_row_indices", "fixed_seed_offset"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  attrs.emplace_back("attn_mask_start_row");
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("is_test");
  attrs.emplace_back("rng_name");
  paddle::small_vector<const char*> outputs {"out", "softmax", "softmax_lse", "seed_offset"};
  return KernelSignature("flash_attn_with_sparse_mask", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlipOpArgumentMapping:

return KernelSignature("flip", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature FlipOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("flip", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FloorOpArgumentMapping:

return KernelSignature("floor", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature FloorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("floor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FoldOpArgumentMapping:

return KernelSignature("fold", {"X"}, {"output_sizes", "kernel_sizes", "strides", "paddings", "dilations"}, {"Y"});
******************************************************************
*/

KernelSignature FoldOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_sizes");
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs {"Y"};
  return KernelSignature("fold", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FractionalMaxPool2dOpArgumentMapping:

return KernelSignature("fractional_max_pool2d", {"x"}, {"output_size", "kernel_size", "random_u", "return_mask"}, {"out", "mask"});
******************************************************************
*/

KernelSignature FractionalMaxPool2dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_size");
  attrs.emplace_back("kernel_size");
  attrs.emplace_back("random_u");
  attrs.emplace_back("return_mask");
  paddle::small_vector<const char*> outputs {"out", "mask"};
  return KernelSignature("fractional_max_pool2d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FractionalMaxPool3dOpArgumentMapping:

return KernelSignature("fractional_max_pool3d", {"x"}, {"output_size", "kernel_size", "random_u", "return_mask"}, {"out", "mask"});
******************************************************************
*/

KernelSignature FractionalMaxPool3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_size");
  attrs.emplace_back("kernel_size");
  attrs.emplace_back("random_u");
  attrs.emplace_back("return_mask");
  paddle::small_vector<const char*> outputs {"out", "mask"};
  return KernelSignature("fractional_max_pool3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FrameOpArgumentMapping:

return KernelSignature("frame", {"X"}, {"frame_length", "hop_length", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature FrameOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("frame_length");
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("frame", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FullIntArrayOpArgumentMapping:

return KernelSignature("full_int_array", {}, {"value", "dtype", "place"}, {"out"});
******************************************************************
*/

KernelSignature FullIntArrayOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("dtype");
  
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("full_int_array", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GammainccOpArgumentMapping:

return KernelSignature("gammaincc", {"x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature GammainccOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("gammaincc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GammalnOpArgumentMapping:

return KernelSignature("gammaln", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature GammalnOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("gammaln", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherOpArgumentMapping:

return KernelSignature("gather", {"X", "Index"}, {"axis"}, {"Out"});
return KernelSignature("gather", {"X", "Index"}, {"Axis"}, {"Out"});
******************************************************************
*/

KernelSignature GatherOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Axis") ? "Axis" : "axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("gather", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherNdOpArgumentMapping:

return KernelSignature("gather_nd", {"X", "Index"}, {}, {"Out"});
******************************************************************
*/

KernelSignature GatherNdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("gather_nd", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherTreeOpArgumentMapping:

return KernelSignature("gather_tree", {"Ids", "Parents"}, {}, {"Out"});
******************************************************************
*/

KernelSignature GatherTreeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Ids", "Parents"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("gather_tree", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GaussianInplaceOpArgumentMapping:

return KernelSignature("gaussian_inplace", {"x"}, {"mean", "std", "seed"}, {"out"});
******************************************************************
*/

KernelSignature GaussianInplaceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mean");
  attrs.emplace_back("std");
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("gaussian_inplace", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GeluOpArgumentMapping:

return KernelSignature("gelu", {"X"}, {"approximate"}, {"Out"});
******************************************************************
*/

KernelSignature GeluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("approximate");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("gelu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GenerateProposalsOpArgumentMapping:

return KernelSignature("generate_proposals", {"Scores", "BboxDeltas", "ImShape", "Anchors", "Variances"}, {"pre_nms_topN", "post_nms_topN", "nms_thresh", "min_size", "eta", "pixel_offset"}, {"RpnRois", "RpnRoiProbs", "RpnRoisNum"});
******************************************************************
*/

KernelSignature GenerateProposalsV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Scores", "BboxDeltas", "ImShape", "Anchors", "Variances"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pre_nms_topN");
  attrs.emplace_back("post_nms_topN");
  attrs.emplace_back("nms_thresh");
  attrs.emplace_back("min_size");
  attrs.emplace_back("eta");
  attrs.emplace_back("pixel_offset");
  paddle::small_vector<const char*> outputs {"RpnRois", "RpnRoiProbs", "RpnRoisNum"};
  return KernelSignature("generate_proposals", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GraphKhopSamplerOpArgumentMapping:

return KernelSignature("graph_khop_sampler", {"Row", "Col_Ptr", "X", "Eids"}, {"sample_sizes", "return_eids"}, {"Out_Src", "Out_Dst", "Sample_Index", "Reindex_X", "Out_Eids"});
******************************************************************
*/

KernelSignature GraphKhopSamplerOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Row", "Col_Ptr", "X", "Eids"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("sample_sizes");
  attrs.emplace_back("return_eids");
  paddle::small_vector<const char*> outputs {"Out_Src", "Out_Dst", "Sample_Index", "Reindex_X", "Out_Eids"};
  return KernelSignature("graph_khop_sampler", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GraphSampleNeighborsOpArgumentMapping:

return KernelSignature("graph_sample_neighbors", {"Row", "Col_Ptr", "X", "Eids", "Perm_Buffer"}, {"sample_size", "return_eids", "flag_perm_buffer"}, {"Out", "Out_Count", "Out_Eids"});
******************************************************************
*/

KernelSignature GraphSampleNeighborsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Row", "Col_Ptr", "X", "Eids", "Perm_Buffer"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("sample_size");
  attrs.emplace_back("return_eids");
  attrs.emplace_back("flag_perm_buffer");
  paddle::small_vector<const char*> outputs {"Out", "Out_Count", "Out_Eids"};
  return KernelSignature("graph_sample_neighbors", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GridSampleOpArgumentMapping:

return KernelSignature("grid_sample", {"X", "Grid"}, {"mode", "padding_mode", "align_corners"}, {"Output"});
******************************************************************
*/

KernelSignature GridSamplerOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Grid"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  attrs.emplace_back("padding_mode");
  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("grid_sample", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GroupNormOpArgumentMapping:

return KernelSignature("group_norm", {"X", "Scale", "Bias"}, {"epsilon", "groups", "data_layout"}, {"Y", "Mean", "Variance"});
******************************************************************
*/

KernelSignature GroupNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_layout");
  paddle::small_vector<const char*> outputs {"Y", "Mean", "Variance"};
  return KernelSignature("group_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GumbelSoftmaxOpArgumentMapping:

return KernelSignature("gumbel_softmax", {"X"}, {"temperature", "hard", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature GumbelSoftmaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("temperature");
  attrs.emplace_back("hard");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("gumbel_softmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardshrinkOpArgumentMapping:

return KernelSignature("hard_shrink", {"X"}, {"threshold"}, {"Out"});
******************************************************************
*/

KernelSignature HardShrinkOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("hard_shrink", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardsigmoidOpArgumentMapping:

return KernelSignature("hardsigmoid", {"X"}, {"slope", "offset"}, {"Out"});
******************************************************************
*/

KernelSignature HardSigmoidOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("slope");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("hardsigmoid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardtanhOpArgumentMapping:

return KernelSignature("hardtanh", {"X"}, {"t_min", "t_max"}, {"Out"});
******************************************************************
*/

KernelSignature BreluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("t_min");
  attrs.emplace_back("t_max");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("hardtanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HeavisideOpArgumentMapping:

return KernelSignature("heaviside", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ElementwiseHeavisideOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("heaviside", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HistogramOpArgumentMapping:

return KernelSignature("histogram", {"X"}, {"bins", "min", "max"}, {"Out"});
******************************************************************
*/

KernelSignature HistogramOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("bins");
  attrs.emplace_back("min");
  attrs.emplace_back("max");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("histogram", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HuberLossOpArgumentMapping:

return KernelSignature("huber_loss", {"X", "Y"}, {"delta"}, {"Out", "Residual"});
******************************************************************
*/

KernelSignature HuberLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("delta");
  paddle::small_vector<const char*> outputs {"Out", "Residual"};
  return KernelSignature("huber_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0OpArgumentMapping:

return KernelSignature("i0", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I0OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("i0", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0eOpArgumentMapping:

return KernelSignature("i0e", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I0eOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("i0e", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1OpArgumentMapping:

return KernelSignature("i1", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I1OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("i1", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1eOpArgumentMapping:

return KernelSignature("i1e", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature I1eOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("i1e", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IdentityLossOpArgumentMapping:

return KernelSignature("identity_loss", {"X"}, {"reduction"}, {"Out"});
******************************************************************
*/

KernelSignature IdentityLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("identity_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ImagOpArgumentMapping:

return KernelSignature("imag", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ImagOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("imag", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexAddOpArgumentMapping:

return KernelSignature("index_add", {"X", "Index", "AddValue"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature IndexAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index", "AddValue"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("index_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexPutOpArgumentMapping:

return KernelSignature("index_put", {"x", "indices", "value"}, {"accumulate"}, {"out"});
******************************************************************
*/

KernelSignature IndexPutOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "indices", "value"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("accumulate");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("index_put", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSampleOpArgumentMapping:

return KernelSignature("index_sample", {"X", "Index"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IndexSampleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("index_sample", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectOpArgumentMapping:

return KernelSignature("index_select", {"X", "Index"}, {"dim"}, {"Out"});
******************************************************************
*/

KernelSignature IndexSelectOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("index_select", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectStridedOpArgumentMapping:

return KernelSignature("index_select_strided", {"x"}, {"index", "axis"}, {"out"});
******************************************************************
*/

KernelSignature IndexSelectStridedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("index_select_strided", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InstanceNormOpArgumentMapping:

return KernelSignature("instance_norm", {"X", "Scale", "Bias"}, {"epsilon"}, {"Y", "SavedMean", "SavedVariance"});
******************************************************************
*/

KernelSignature InstanceNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"Y", "SavedMean", "SavedVariance"};
  return KernelSignature("instance_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InverseOpArgumentMapping:

return KernelSignature("inverse", {"Input"}, {}, {"Output"});
******************************************************************
*/

KernelSignature InverseOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Output"};
  return KernelSignature("inverse", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsEmptyOpArgumentMapping:

return KernelSignature("is_empty", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsEmptyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("is_empty", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IscloseOpArgumentMapping:

return KernelSignature("isclose", {"Input", "Other"}, {"rtol", "atol", "equal_nan"}, {"Out"});
******************************************************************
*/

KernelSignature IscloseOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Other"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Rtol") ? "Rtol" : "rtol");
  attrs.emplace_back(ctx.HasInput("Atol") ? "Atol" : "atol");
  attrs.emplace_back("equal_nan");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("isclose", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsfiniteOpArgumentMapping:

return KernelSignature("isfinite", {"X"}, {}, {"Out"});
return KernelSignature("isfinite_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsfiniteV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("isfinite", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("isfinite_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsinfOpArgumentMapping:

return KernelSignature("isinf", {"X"}, {}, {"Out"});
return KernelSignature("isinf_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsinfV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("isinf", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("isinf_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IsnanOpArgumentMapping:

return KernelSignature("isnan", {"X"}, {}, {"Out"});
return KernelSignature("isnan_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature IsnanV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("isnan", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("isnan_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KldivLossOpArgumentMapping:

return KernelSignature("kldiv_loss", {"X", "Target"}, {"reduction"}, {"Loss"});
******************************************************************
*/

KernelSignature KldivLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Target"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs {"Loss"};
  return KernelSignature("kldiv_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KronOpArgumentMapping:

return KernelSignature("kron", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature KronOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("kron", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KthvalueOpArgumentMapping:

return KernelSignature("kthvalue", {"X"}, {"k", "axis", "keepdim"}, {"Out", "Indices"});
******************************************************************
*/

KernelSignature KthvalueOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("k");
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs {"Out", "Indices"};
  return KernelSignature("kthvalue", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LabelSmoothOpArgumentMapping:

return KernelSignature("label_smooth", {"X", "PriorDist"}, {"epsilon"}, {"Out"});
******************************************************************
*/

KernelSignature LabelSmoothOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "PriorDist"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("label_smooth", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LambOpArgumentMapping:

return KernelSignature("lamb", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("lamb_sr", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"}, {"weight_decay", "beta1", "beta2", "epsilon", "always_adapt", "multi_precision"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature LambOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam", "SkipUpdate"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("weight_decay");
  attrs.emplace_back("beta1");
  attrs.emplace_back("beta2");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("always_adapt");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"};
 if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("Grad") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ctx.IsDenseTensorInput("Moment1") &&
  ctx.IsDenseTensorInput("Moment2") &&
  ctx.IsDenseTensorInput("Beta1Pow") &&
  ctx.IsDenseTensorInput("Beta2Pow") &&
  ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))) &&
 ((ctx.HasInput("SkipUpdate") && ctx.IsDenseTensorInput("SkipUpdate")) || (!ctx.HasInput("SkipUpdate"))))  {
    return KernelSignature("lamb", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsSelectedRowsInput("Grad") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ctx.IsDenseTensorInput("Moment1") &&
  ctx.IsDenseTensorInput("Moment2") &&
  ctx.IsDenseTensorInput("Beta1Pow") &&
  ctx.IsDenseTensorInput("Beta2Pow") &&
  ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))) &&
 ((ctx.HasInput("SkipUpdate") && ctx.IsDenseTensorInput("SkipUpdate")) || (!ctx.HasInput("SkipUpdate"))))  {
    return KernelSignature("lamb_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LayerNormOpArgumentMapping:

return KernelSignature("layer_norm", {"X", "Scale", "Bias"}, {"epsilon", "begin_norm_axis"}, {"Y", "Mean", "Variance"});
******************************************************************
*/

KernelSignature LayerNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale", "Bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs {"Y", "Mean", "Variance"};
  return KernelSignature("layer_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LeakyReluOpArgumentMapping:

return KernelSignature("leaky_relu", {"X"}, {"alpha"}, {"Out"});
******************************************************************
*/

KernelSignature LeakyReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("leaky_relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LerpOpArgumentMapping:

return KernelSignature("lerp", {"X", "Y", "Weight"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LerpOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Weight"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("lerp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LgammaOpArgumentMapping:

return KernelSignature("lgamma", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LgammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("lgamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LinearInterpOpArgumentMapping:

return KernelSignature("linear_interp", {"X", "OutSize", "SizeTensor", "Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature LinearInterpV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("linear_interp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LlmInt8LinearOpArgumentMapping:

return KernelSignature("llm_int8_linear", {"x", "weight", "bias", "weight_scale"}, {"threshold"}, {"out"});
******************************************************************
*/

KernelSignature LlmInt8LinearOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "weight", "bias", "weight_scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("llm_int8_linear", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogOpArgumentMapping:

return KernelSignature("log", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("log", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log10OpArgumentMapping:

return KernelSignature("log10", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Log10OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("log10", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log1pOpArgumentMapping:

return KernelSignature("log1p", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Log1pOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("log1p", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log2OpArgumentMapping:

return KernelSignature("log2", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Log2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("log2", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogLossOpArgumentMapping:

return KernelSignature("log_loss", {"Predicted", "Labels"}, {"epsilon"}, {"Loss"});
******************************************************************
*/

KernelSignature LogLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Predicted", "Labels"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"Loss"};
  return KernelSignature("log_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogSoftmaxOpArgumentMapping:

return KernelSignature("log_softmax", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature LogSoftmaxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("log_softmax", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogcumsumexpOpArgumentMapping:

return KernelSignature("logcumsumexp", {"X"}, {"axis", "flatten", "exclusive", "reverse"}, {"Out"});
******************************************************************
*/

KernelSignature LogcumsumexpOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logcumsumexp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalAndOpArgumentMapping:

return KernelSignature("logical_and", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalAndOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logical_and", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalNotOpArgumentMapping:

return KernelSignature("logical_not", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalNotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logical_not", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalOrOpArgumentMapping:

return KernelSignature("logical_or", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalOrOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logical_or", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogicalXorOpArgumentMapping:

return KernelSignature("logical_xor", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogicalXorOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logical_xor", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogitOpArgumentMapping:

return KernelSignature("logit", {"X"}, {"eps"}, {"Out"});
******************************************************************
*/

KernelSignature LogitOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logit", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogsigmoidOpArgumentMapping:

return KernelSignature("logsigmoid", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature LogsigmoidOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("logsigmoid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LstsqOpArgumentMapping:

return KernelSignature("lstsq", {"X", "Y"}, {"rcond", "driver"}, {"Solution", "Residuals", "Rank", "SingularValues"});
return KernelSignature("lstsq", {"X", "Y"}, {"RcondTensor", "driver"}, {"Solution", "Residuals", "Rank", "SingularValues"});
******************************************************************
*/

KernelSignature LstsqOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("rcond");
  attrs.emplace_back("driver");
  paddle::small_vector<const char*> outputs {"Solution", "Residuals", "Rank", "SingularValues"};
  return KernelSignature("lstsq", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuOpArgumentMapping:

return KernelSignature("lu", {"X"}, {"pivots"}, {"Out", "Pivots", "Infos"});
******************************************************************
*/

KernelSignature LuOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pivots");
  paddle::small_vector<const char*> outputs {"Out", "Pivots", "Infos"};
  return KernelSignature("lu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuUnpackOpArgumentMapping:

return KernelSignature("lu_unpack", {"X", "Pivots"}, {"unpack_ludata", "unpack_pivots"}, {"Pmat", "L", "U"});
******************************************************************
*/

KernelSignature LuUnpackOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Pivots"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("unpack_ludata");
  attrs.emplace_back("unpack_pivots");
  paddle::small_vector<const char*> outputs {"Pmat", "L", "U"};
  return KernelSignature("lu_unpack", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MarginCrossEntropyOpArgumentMapping:

return KernelSignature("margin_cross_entropy", {"Logits", "Label"}, {"return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, {"Softmax", "Loss"});
******************************************************************
*/

KernelSignature MarginCrossEntropyOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Logits", "Label"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("rank");
  attrs.emplace_back("nranks");
  attrs.emplace_back("margin1");
  attrs.emplace_back("margin2");
  attrs.emplace_back("margin3");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs {"Softmax", "Loss"};
  return KernelSignature("margin_cross_entropy", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaskedMultiheadAttentionOpArgumentMapping:

return KernelSignature("masked_multihead_attention", {"x", "cache_kv", "bias", "src_mask", "cum_offsets", "sequence_lengths", "rotary_tensor", "beam_cache_offset", "qkv_out_scale", "out_shift", "out_smooth"}, {"seq_len", "rotary_emb_dims", "use_neox_rotary_style", "compute_dtype", "out_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out", "cache_kv_out", "beam_cache_offset_out"});
******************************************************************
*/

KernelSignature MaskedMultiheadAttentionOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "cache_kv", "bias", "src_mask", "cum_offsets", "sequence_lengths", "rotary_tensor", "beam_cache_offset", "qkv_out_scale", "out_shift", "out_smooth"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seq_len");
  attrs.emplace_back("rotary_emb_dims");
  attrs.emplace_back("use_neox_rotary_style");
  attrs.emplace_back("compute_dtype");
  attrs.emplace_back("out_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs {"out", "cache_kv_out", "beam_cache_offset_out"};
  return KernelSignature("masked_multihead_attention", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaskedSelectOpArgumentMapping:

return KernelSignature("masked_select", {"X", "Mask"}, {}, {"Y"});
******************************************************************
*/

KernelSignature MaskedSelectOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Mask"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Y"};
  return KernelSignature("masked_select", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatrixNmsOpArgumentMapping:

return KernelSignature("matrix_nms", {"BBoxes", "Scores"}, {"score_threshold", "nms_top_k", "keep_top_k", "post_threshold", "use_gaussian", "gaussian_sigma", "background_label", "normalized"}, {"Out", "Index", "RoisNum"});
******************************************************************
*/

KernelSignature MatrixNmsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"BBoxes", "Scores"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("score_threshold");
  attrs.emplace_back("nms_top_k");
  attrs.emplace_back("keep_top_k");
  attrs.emplace_back("post_threshold");
  attrs.emplace_back("use_gaussian");
  attrs.emplace_back("gaussian_sigma");
  attrs.emplace_back("background_label");
  attrs.emplace_back("normalized");
  paddle::small_vector<const char*> outputs {"Out", "Index", "RoisNum"};
  return KernelSignature("matrix_nms", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatrixPowerOpArgumentMapping:

return KernelSignature("matrix_power", {"X"}, {"n"}, {"Out"});
******************************************************************
*/

KernelSignature MatrixPowerOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("matrix_power", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool2dWithIndexOpArgumentMapping:

return KernelSignature("max_pool2d_with_index", {"X"}, {"ksize", "strides", "paddings", "global_pooling", "adaptive"}, {"Out", "Mask"});
******************************************************************
*/

KernelSignature MaxPool2dWithIndexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs {"Out", "Mask"};
  return KernelSignature("max_pool2d_with_index", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool3dWithIndexOpArgumentMapping:

return KernelSignature("max_pool3d_with_index", {"X"}, {"ksize", "strides", "paddings", "global_pooling", "adaptive"}, {"Out", "Mask"});
******************************************************************
*/

KernelSignature MaxPool3dWithIndexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs {"Out", "Mask"};
  return KernelSignature("max_pool3d_with_index", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxoutOpArgumentMapping:

return KernelSignature("maxout", {"X"}, {"groups", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature MaxoutOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("groups");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("maxout", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeanAllOpArgumentMapping:

return KernelSignature("mean_all", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MeanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("mean_all", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MemoryEfficientAttentionOpArgumentMapping:

return KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"}, {"max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test"}, {"output", "logsumexp", "seed_and_offset"});
return KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"}, {"max_seqlen_q", "MaxSeqlenKTensor", "causal", "dropout_p", "scale", "is_test"}, {"output", "logsumexp", "seed_and_offset"});
return KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"}, {"MaxSeqlenQTensor", "max_seqlen_k", "causal", "dropout_p", "scale", "is_test"}, {"output", "logsumexp", "seed_and_offset"});
return KernelSignature("memory_efficient_attention", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"}, {"MaxSeqlenQTensor", "MaxSeqlenKTensor", "causal", "dropout_p", "scale", "is_test"}, {"output", "logsumexp", "seed_and_offset"});
******************************************************************
*/

KernelSignature MemoryEfficientAttentionOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "causal_diagonal", "seqlen_k"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("MaxSeqlenQTensor") ? "MaxSeqlenQTensor" : "max_seqlen_q");
  attrs.emplace_back(ctx.HasInput("MaxSeqlenKTensor") ? "MaxSeqlenKTensor" : "max_seqlen_k");
  attrs.emplace_back("causal");
  attrs.emplace_back("dropout_p");
  attrs.emplace_back("scale");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs {"output", "logsumexp", "seed_and_offset"};
  return KernelSignature("memory_efficient_attention", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MergeSelectedRowsOpArgumentMapping:

return KernelSignature("merge_selected_rows", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MergeSelectedRowsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("merge_selected_rows", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MergedAdamOpArgumentMapping:

return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1", "beta2", "epsilon", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1", "beta2", "EpsilonTensor", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1", "Beta2Tensor", "epsilon", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"beta1", "Beta2Tensor", "EpsilonTensor", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"Beta1Tensor", "beta2", "epsilon", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"Beta1Tensor", "beta2", "EpsilonTensor", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"Beta1Tensor", "Beta2Tensor", "epsilon", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
return KernelSignature("merged_adam", {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"}, {"Beta1Tensor", "Beta2Tensor", "EpsilonTensor", "multi_precision", "use_global_beta_pow"}, {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature MergedAdamOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "LearningRate", "Moment1", "Moment2", "Beta1Pow", "Beta2Pow", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta1");
  attrs.emplace_back("beta2");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("use_global_beta_pow");
  paddle::small_vector<const char*> outputs {"ParamOut", "Moment1Out", "Moment2Out", "Beta1PowOut", "Beta2PowOut", "MasterParamOut"};
  return KernelSignature("merged_adam", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MergedMomentumOpArgumentMapping:

return KernelSignature("merged_momentum", {"Param", "Grad", "Velocity", "LearningRate", "MasterParam"}, {"mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"ParamOut", "VelocityOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature MergedMomentumOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "Velocity", "LearningRate", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mu");
  attrs.emplace_back("use_nesterov");
  attrs.emplace_back("regularization_method");
  attrs.emplace_back("regularization_coeff");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("rescale_grad");
  paddle::small_vector<const char*> outputs {"ParamOut", "VelocityOut", "MasterParamOut"};
  return KernelSignature("merged_momentum", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeshgridOpArgumentMapping:

return KernelSignature("meshgrid", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MeshgridOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("meshgrid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ModeOpArgumentMapping:

return KernelSignature("mode", {"X"}, {"axis", "keepdim"}, {"Out", "Indices"});
******************************************************************
*/

KernelSignature ModeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs {"Out", "Indices"};
  return KernelSignature("mode", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MomentumOpArgumentMapping:

return KernelSignature("momentum", {"Param", "Grad", "Velocity", "LearningRate", "MasterParam"}, {"mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"ParamOut", "VelocityOut", "MasterParamOut"});
return KernelSignature("momentum_dense_param_sparse_grad", {"Param", "Grad", "Velocity", "LearningRate", "MasterParam"}, {"mu", "use_nesterov", "regularization_method", "regularization_coeff", "multi_precision", "rescale_grad"}, {"ParamOut", "VelocityOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature MomentumOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "Grad", "Velocity", "LearningRate", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mu");
  attrs.emplace_back("use_nesterov");
  attrs.emplace_back("regularization_method");
  attrs.emplace_back("regularization_coeff");
  attrs.emplace_back("multi_precision");
  attrs.emplace_back("rescale_grad");
  paddle::small_vector<const char*> outputs {"ParamOut", "VelocityOut", "MasterParamOut"};
 if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("Grad") &&
  ctx.IsDenseTensorInput("Velocity") &&
  ctx.IsDenseTensorInput("LearningRate") &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("momentum", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsSelectedRowsInput("Grad") &&
  ctx.IsDenseTensorInput("Velocity") &&
  ctx.IsDenseTensorInput("LearningRate") &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("momentum_dense_param_sparse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiDotOpArgumentMapping:

return KernelSignature("multi_dot", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MultiDotOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("multi_dot", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MulticlassNms3OpArgumentMapping:

return KernelSignature("multiclass_nms3", {"BBoxes", "Scores", "RoisNum"}, {"score_threshold", "nms_top_k", "keep_top_k", "nms_threshold", "normalized", "nms_eta", "background_label"}, {"Out", "Index", "NmsRoisNum"});
******************************************************************
*/

KernelSignature MulticlassNms3OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"BBoxes", "Scores", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("score_threshold");
  attrs.emplace_back("nms_top_k");
  attrs.emplace_back("keep_top_k");
  attrs.emplace_back("nms_threshold");
  attrs.emplace_back("normalized");
  attrs.emplace_back("nms_eta");
  attrs.emplace_back("background_label");
  paddle::small_vector<const char*> outputs {"Out", "Index", "NmsRoisNum"};
  return KernelSignature("multiclass_nms3", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultinomialOpArgumentMapping:

return KernelSignature("multinomial", {"X"}, {"num_samples", "replacement"}, {"Out"});
******************************************************************
*/

KernelSignature MultinomialOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("num_samples");
  attrs.emplace_back("replacement");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("multinomial", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiplexOpArgumentMapping:

return KernelSignature("multiplex", {"X", "Ids"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MultiplexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Ids"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("multiplex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MvOpArgumentMapping:

return KernelSignature("mv", {"X", "Vec"}, {}, {"Out"});
******************************************************************
*/

KernelSignature MvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Vec"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("mv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NanmedianOpArgumentMapping:

return KernelSignature("nanmedian", {"X"}, {"axis", "keepdim", "mode"}, {"Out", "MedianIndex"});
return KernelSignature("nanmedian", {"X"}, {"AxisTensorList", "keepdim", "mode"}, {"Out", "MedianIndex"});
******************************************************************
*/

KernelSignature NanmedianOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("AxisTensor")
    ? "AxisTensor"
    : ctx.InputSize("AxisTensorList") > 0
      ? "AxisTensorList"
      : "axis");
  attrs.emplace_back("keepdim");
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs {"Out", "MedianIndex"};
  return KernelSignature("nanmedian", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NearestInterpOpArgumentMapping:

return KernelSignature("nearest_interp", {"X", "OutSize", "SizeTensor", "Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature NearestInterpV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("nearest_interp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NextafterOpArgumentMapping:

return KernelSignature("nextafter", {"x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature NextafterOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("nextafter", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NllLossOpArgumentMapping:

return KernelSignature("nll_loss", {"X", "Label", "Weight"}, {"ignore_index", "reduction"}, {"Out", "Total_weight"});
******************************************************************
*/

KernelSignature NllLossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Label", "Weight"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs {"Out", "Total_weight"};
  return KernelSignature("nll_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NmsOpArgumentMapping:

return KernelSignature("nms", {"Boxes"}, {"iou_threshold"}, {"KeepBoxesIdxs"});
******************************************************************
*/

KernelSignature NmsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Boxes"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("iou_threshold");
  paddle::small_vector<const char*> outputs {"KeepBoxesIdxs"};
  return KernelSignature("nms", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NonzeroOpArgumentMapping:

return KernelSignature("nonzero", {"Condition"}, {}, {"Out"});
******************************************************************
*/

KernelSignature WhereIndexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Condition"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("nonzero", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NpuIdentityOpArgumentMapping:

return KernelSignature("npu_identity", {"x"}, {"format"}, {"out"});
******************************************************************
*/

KernelSignature NpuIdentityOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("format");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("npu_identity", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NumelOpArgumentMapping:

return KernelSignature("numel", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SizeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("numel", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by OverlapAddOpArgumentMapping:

return KernelSignature("overlap_add", {"X"}, {"hop_length", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature OverlapAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("overlap_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PNormOpArgumentMapping:

return KernelSignature("p_norm", {"X"}, {"porder", "axis", "epsilon", "keepdim", "asvector"}, {"Out"});
******************************************************************
*/

KernelSignature PNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("porder");
  attrs.emplace_back("axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("keepdim");
  attrs.emplace_back("asvector");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("p_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pad3dOpArgumentMapping:

return KernelSignature("pad3d", {"X"}, {"paddings", "mode", "value", "data_format"}, {"Out"});
return KernelSignature("pad3d", {"X"}, {"Paddings", "mode", "value", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature Pad3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("Paddings")
    ? "Paddings"
    : "paddings");

  attrs.emplace_back("mode");
  attrs.emplace_back("value");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("pad3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelShuffleOpArgumentMapping:

return KernelSignature("pixel_shuffle", {"X"}, {"upscale_factor", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature PixelShuffleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("pixel_shuffle", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelUnshuffleOpArgumentMapping:

return KernelSignature("pixel_unshuffle", {"X"}, {"downscale_factor", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature PixelUnshuffleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("downscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("pixel_unshuffle", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PoissonOpArgumentMapping:

return KernelSignature("poisson", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature PoissonOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("poisson", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PolygammaOpArgumentMapping:

return KernelSignature("polygamma", {"x"}, {"n"}, {"out"});
******************************************************************
*/

KernelSignature PolygammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("polygamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowOpArgumentMapping:

return KernelSignature("pow", {"X"}, {"factor"}, {"Out"});
return KernelSignature("pow", {"X"}, {"FactorTensor"}, {"Out"});
******************************************************************
*/

KernelSignature PowOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("pow", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PreluOpArgumentMapping:

return KernelSignature("prelu", {"X", "Alpha"}, {"data_format", "mode"}, {"Out"});
******************************************************************
*/

KernelSignature PreluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Alpha"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_format");
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("prelu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PriorBoxOpArgumentMapping:

return KernelSignature("prior_box", {"Input", "Image"}, {"min_sizes", "max_sizes", "aspect_ratios", "variances", "flip", "clip", "step_w", "step_h", "offset", "min_max_aspect_ratios_order"}, {"Boxes", "Variances"});
******************************************************************
*/

KernelSignature PriorBoxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Image"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("min_sizes");
  attrs.emplace_back("max_sizes");
  attrs.emplace_back("aspect_ratios");
  attrs.emplace_back("variances");
  attrs.emplace_back("flip");
  attrs.emplace_back("clip");
  attrs.emplace_back("step_w");
  attrs.emplace_back("step_h");
  attrs.emplace_back("offset");
  attrs.emplace_back("min_max_aspect_ratios_order");
  paddle::small_vector<const char*> outputs {"Boxes", "Variances"};
  return KernelSignature("prior_box", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PsroiPoolOpArgumentMapping:

return KernelSignature("psroi_pool", {"X", "ROIs", "RoisNum"}, {"pooled_height", "pooled_width", "output_channels", "spatial_scale"}, {"Out"});
******************************************************************
*/

KernelSignature PsroiPoolOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ROIs", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("output_channels");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("psroi_pool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PutAlongAxisOpArgumentMapping:

return KernelSignature("put_along_axis", {"Input", "Index", "Value"}, {"Axis", "Reduce", "Include_self"}, {"Result"});
******************************************************************
*/

KernelSignature PutAlongAxisOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Index", "Value"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  attrs.emplace_back("Reduce");
  attrs.emplace_back("Include_self");
  paddle::small_vector<const char*> outputs {"Result"};
  return KernelSignature("put_along_axis", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QrOpArgumentMapping:

return KernelSignature("qr", {"X"}, {"mode"}, {"Q", "R"});
******************************************************************
*/

KernelSignature QrOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs {"Q", "R"};
  return KernelSignature("qr", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RealOpArgumentMapping:

return KernelSignature("real", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RealOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("real", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReciprocalOpArgumentMapping:

return KernelSignature("reciprocal", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ReciprocalOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("reciprocal", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReindexGraphOpArgumentMapping:

return KernelSignature("graph_reindex", {"X", "Neighbors", "Count", "HashTable_Value", "HashTable_Index"}, {}, {"Reindex_Src", "Reindex_Dst", "Out_Nodes"});
******************************************************************
*/

KernelSignature GraphReindexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Neighbors", "Count", "HashTable_Value", "HashTable_Index"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Reindex_Src", "Reindex_Dst", "Out_Nodes"};
  return KernelSignature("graph_reindex", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReluOpArgumentMapping:

return KernelSignature("relu", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Relu6OpArgumentMapping:

return KernelSignature("relu6", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature Relu6OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("relu6", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RenormOpArgumentMapping:

return KernelSignature("renorm", {"X"}, {"p", "axis", "max_norm"}, {"Out"});
******************************************************************
*/

KernelSignature RenormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  attrs.emplace_back("axis");
  attrs.emplace_back("max_norm");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("renorm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RmsNormOpArgumentMapping:

return KernelSignature("rms_norm", {"x", "bias", "residual", "norm_weight", "norm_bias"}, {"epsilon", "begin_norm_axis", "quant_scale", "quant_round_type", "quant_max_bound", "quant_min_bound"}, {"out", "residual_out", "inv_var"});
******************************************************************
*/

KernelSignature RmsNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "bias", "residual", "norm_weight", "norm_bias"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("quant_scale");
  attrs.emplace_back("quant_round_type");
  attrs.emplace_back("quant_max_bound");
  attrs.emplace_back("quant_min_bound");
  paddle::small_vector<const char*> outputs {"out", "residual_out", "inv_var"};
  return KernelSignature("rms_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RmspropOpArgumentMapping:

return KernelSignature("rmsprop", {"Param", "MeanSquare", "Grad", "Moment", "LearningRate", "MeanGrad", "MasterParam"}, {"epsilon", "decay", "momentum", "centered", "multi_precision"}, {"ParamOut", "MomentOut", "MeanSquareOut", "MeanGradOut", "MasterParamOut"});
return KernelSignature("rmsprop_dense_param_sparse_grad", {"Param", "MeanSquare", "Grad", "Moment", "LearningRate", "MeanGrad", "MasterParam"}, {"epsilon", "decay", "momentum", "centered", "multi_precision"}, {"ParamOut", "MomentOut", "MeanSquareOut", "MeanGradOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature RmspropOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "MeanSquare", "Grad", "Moment", "LearningRate", "MeanGrad", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("decay");
  attrs.emplace_back("momentum");
  attrs.emplace_back("centered");
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"ParamOut", "MomentOut", "MeanSquareOut", "MeanGradOut", "MasterParamOut"};
 if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("MeanSquare") &&
  ctx.IsDenseTensorInput("Grad") &&
  ctx.IsDenseTensorInput("Moment") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ((ctx.HasInput("MeanGrad") && ctx.IsDenseTensorInput("MeanGrad")) || (!ctx.HasInput("MeanGrad"))) &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("rmsprop", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("MeanSquare") &&
  ctx.IsSelectedRowsInput("Grad") &&
  ctx.IsDenseTensorInput("Moment") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ((ctx.HasInput("MeanGrad") && ctx.IsDenseTensorInput("MeanGrad")) || (!ctx.HasInput("MeanGrad"))) &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("rmsprop_dense_param_sparse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiAlignOpArgumentMapping:

return KernelSignature("roi_align", {"X", "ROIs", "RoisNum"}, {"pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned"}, {"Out"});
******************************************************************
*/

KernelSignature RoiAlignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ROIs", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  attrs.emplace_back("sampling_ratio");
  attrs.emplace_back("aligned");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("roi_align", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiPoolOpArgumentMapping:

return KernelSignature("roi_pool", {"X", "ROIs", "RoisNum"}, {"pooled_height", "pooled_width", "spatial_scale"}, {"Out", "Argmax"});
******************************************************************
*/

KernelSignature RoiPoolOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ROIs", "RoisNum"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs {"Out", "Argmax"};
  return KernelSignature("roi_pool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RollOpArgumentMapping:

return KernelSignature("roll", {"X"}, {"shifts", "axis"}, {"Out"});
return KernelSignature("roll", {"X"}, {"ShiftsTensor", "axis"}, {"Out"});
******************************************************************
*/

KernelSignature RollOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("ShiftsTensor")
    ? "ShiftsTensor"
    : "shifts");

  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("roll", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoundOpArgumentMapping:

return KernelSignature("round", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RoundOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("round", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RpropOpArgumentMapping:

return KernelSignature("rprop", {"param", "grad", "prev", "learning_rate", "master_param", "learning_rate_range", "etas"}, {"multi_precision"}, {"param_out", "prev_out", "learning_rate_out", "master_param_out"});
******************************************************************
*/

KernelSignature RpropOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"param", "grad", "prev", "learning_rate", "master_param", "learning_rate_range", "etas"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"param_out", "prev_out", "learning_rate_out", "master_param_out"};
  return KernelSignature("rprop", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RsqrtOpArgumentMapping:

return KernelSignature("rsqrt", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature RsqrtOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("rsqrt", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScaleOpArgumentMapping:

return KernelSignature("scale", {"X"}, {"scale", "bias", "bias_after_scale"}, {"Out"});
return KernelSignature("scale", {"X"}, {"scale", "BiasTensor", "bias_after_scale"}, {"Out"});
return KernelSignature("scale", {"X"}, {"ScaleTensor", "bias", "bias_after_scale"}, {"Out"});
return KernelSignature("scale", {"X"}, {"ScaleTensor", "BiasTensor", "bias_after_scale"}, {"Out"});
return KernelSignature("scale_sr", {"X"}, {"scale", "bias", "bias_after_scale"}, {"Out"});
return KernelSignature("scale_sr", {"X"}, {"scale", "BiasTensor", "bias_after_scale"}, {"Out"});
return KernelSignature("scale_sr", {"X"}, {"ScaleTensor", "bias", "bias_after_scale"}, {"Out"});
return KernelSignature("scale_sr", {"X"}, {"ScaleTensor", "BiasTensor", "bias_after_scale"}, {"Out"});
******************************************************************
*/

KernelSignature ScaleOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("ScaleTensor") ? "ScaleTensor" : "scale");
  attrs.emplace_back(ctx.HasInput("BiasTensor") ? "BiasTensor" : "bias");
  attrs.emplace_back("bias_after_scale");
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("scale", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("scale_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterOpArgumentMapping:

return KernelSignature("scatter", {"X", "Ids", "Updates"}, {"overwrite"}, {"Out"});
******************************************************************
*/

KernelSignature ScatterOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Ids", "Updates"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("overwrite");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("scatter", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterNdAddOpArgumentMapping:

return KernelSignature("scatter_nd_add", {"X", "Index", "Updates"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ScatterNdAddOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index", "Updates"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("scatter_nd_add", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SearchsortedOpArgumentMapping:

return KernelSignature("searchsorted", {"SortedSequence", "Values"}, {"out_int32", "right"}, {"Out"});
******************************************************************
*/

KernelSignature SearchsortedOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"SortedSequence", "Values"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("out_int32");
  attrs.emplace_back("right");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("searchsorted", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SegmentPoolOpArgumentMapping:

return KernelSignature("segment_pool", {"X", "SegmentIds"}, {"pooltype"}, {"Out", "SummedIds"});
******************************************************************
*/

KernelSignature SegmentPoolOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "SegmentIds"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooltype");
  paddle::small_vector<const char*> outputs {"Out", "SummedIds"};
  return KernelSignature("segment_pool", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SeluOpArgumentMapping:

return KernelSignature("selu", {"X"}, {"scale", "alpha"}, {"Out"});
******************************************************************
*/

KernelSignature SeluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale");
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("selu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendURecvOpArgumentMapping:

return KernelSignature("send_u_recv", {"X", "Src_index", "Dst_index"}, {"reduce_op", "out_size"}, {"Out", "Dst_count"});
return KernelSignature("send_u_recv", {"X", "Src_index", "Dst_index"}, {"reduce_op", "Out_size"}, {"Out", "Dst_count"});
******************************************************************
*/

KernelSignature GraphSendRecvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Src_index", "Dst_index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduce_op");
  attrs.emplace_back(
    ctx.HasInput("Out_size")
    ? "Out_size"
    : "out_size");

  paddle::small_vector<const char*> outputs {"Out", "Dst_count"};
  return KernelSignature("send_u_recv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUeRecvOpArgumentMapping:

return KernelSignature("send_ue_recv", {"X", "Y", "Src_index", "Dst_index"}, {"message_op", "reduce_op", "out_size"}, {"Out", "Dst_count"});
return KernelSignature("send_ue_recv", {"X", "Y", "Src_index", "Dst_index"}, {"message_op", "reduce_op", "Out_size"}, {"Out", "Dst_count"});
******************************************************************
*/

KernelSignature GraphSendUeRecvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Src_index", "Dst_index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  attrs.emplace_back("reduce_op");
  attrs.emplace_back(
    ctx.HasInput("Out_size")
    ? "Out_size"
    : "out_size");

  paddle::small_vector<const char*> outputs {"Out", "Dst_count"};
  return KernelSignature("send_ue_recv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUvOpArgumentMapping:

return KernelSignature("send_uv", {"x", "y", "src_index", "dst_index"}, {"message_op"}, {"out"});
******************************************************************
*/

KernelSignature GraphSendUvOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "src_index", "dst_index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("send_uv", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SgdOpArgumentMapping:

return KernelSignature("sgd", {"Param", "LearningRate", "Grad", "MasterParam"}, {"multi_precision"}, {"ParamOut", "MasterParamOut"});
return KernelSignature("sgd_dense_param_sparse_grad", {"Param", "LearningRate", "Grad", "MasterParam"}, {"multi_precision"}, {"ParamOut", "MasterParamOut"});
return KernelSignature("sgd_sparse_param_sparse_grad", {"Param", "LearningRate", "Grad", "MasterParam"}, {"multi_precision"}, {"ParamOut", "MasterParamOut"});
******************************************************************
*/

KernelSignature SgdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Param", "LearningRate", "Grad", "MasterParam"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("multi_precision");
  paddle::small_vector<const char*> outputs {"ParamOut", "MasterParamOut"};
 if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ctx.IsDenseTensorInput("Grad") &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("sgd", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if (  ctx.IsDenseTensorInput("Param") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ctx.IsSelectedRowsInput("Grad") &&
 ((ctx.HasInput("MasterParam") && ctx.IsDenseTensorInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("sgd_dense_param_sparse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if (  ctx.IsSelectedRowsInput("Param") &&
  ctx.IsDenseTensorInput("LearningRate") &&
  ctx.IsSelectedRowsInput("Grad") &&
 ((ctx.HasInput("MasterParam") && ctx.IsSelectedRowsInput("MasterParam")) || (!ctx.HasInput("MasterParam"))))  {
    return KernelSignature("sgd_sparse_param_sparse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ShapeOpArgumentMapping:

return KernelSignature("shape", {"Input"}, {}, {"Out"});
return KernelSignature("shape_sr", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature ShapeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("Input"))  {
    return KernelSignature("shape", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("Input"))  {
    return KernelSignature("shape_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ShardIndexOpArgumentMapping:

return KernelSignature("shard_index", {"X"}, {"index_num", "nshards", "shard_id", "ignore_value"}, {"Out"});
******************************************************************
*/

KernelSignature ShardIndexOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("index_num");
  attrs.emplace_back("nshards");
  attrs.emplace_back("shard_id");
  attrs.emplace_back("ignore_value");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("shard_index", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidOpArgumentMapping:

return KernelSignature("sigmoid", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SigmoidOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("sigmoid", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidCrossEntropyWithLogitsOpArgumentMapping:

return KernelSignature("sigmoid_cross_entropy_with_logits", {"X", "Label", "pos_weight"}, {"normalize", "ignore_index"}, {"Out"});
******************************************************************
*/

KernelSignature SigmoidCrossEntropyWithLogitsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Label", "pos_weight"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("normalize");
  attrs.emplace_back("ignore_index");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("sigmoid_cross_entropy_with_logits", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SignOpArgumentMapping:

return KernelSignature("sign", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("sign", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SiluOpArgumentMapping:

return KernelSignature("silu", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SiluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("silu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinOpArgumentMapping:

return KernelSignature("sin", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SinOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("sin", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinhOpArgumentMapping:

return KernelSignature("sinh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SinhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("sinh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SlogdetOpArgumentMapping:

return KernelSignature("slogdet", {"Input"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SlogdeterminantOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("slogdet", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftplusOpArgumentMapping:

return KernelSignature("softplus", {"X"}, {"beta", "threshold"}, {"Out"});
******************************************************************
*/

KernelSignature SoftplusOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta");
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("softplus", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftshrinkOpArgumentMapping:

return KernelSignature("softshrink", {"X"}, {"lambda"}, {"Out"});
******************************************************************
*/

KernelSignature SoftshrinkOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("lambda");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("softshrink", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftsignOpArgumentMapping:

return KernelSignature("softsign", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SoftsignOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("softsign", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SolveOpArgumentMapping:

return KernelSignature("solve", {"X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SolveOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("solve", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SpectralNormOpArgumentMapping:

return KernelSignature("spectral_norm", {"Weight", "U", "V"}, {"dim", "power_iters", "eps"}, {"Out"});
******************************************************************
*/

KernelSignature SpectralNormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Weight", "U", "V"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("power_iters");
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("spectral_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqrtOpArgumentMapping:

return KernelSignature("sqrt", {"X"}, {}, {"Out"});
return KernelSignature("sqrt_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SqrtOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("sqrt", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("sqrt_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquareOpArgumentMapping:

return KernelSignature("square", {"X"}, {}, {"Out"});
return KernelSignature("square_sr", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SquareOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
 if ( ctx.IsDenseTensorInput("X"))  {
    return KernelSignature("square", std::move(inputs), std::move(attrs), std::move(outputs));
  }
 else if ( ctx.IsSelectedRowsInput("X"))  {
    return KernelSignature("square_sr", std::move(inputs), std::move(attrs), std::move(outputs));
  }
else { return KernelSignature("unregistered", {}, {}, {}); }
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquaredL2NormOpArgumentMapping:

return KernelSignature("squared_l2_norm", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature SquaredL2NormOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("squared_l2_norm", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqueezeOpArgumentMapping:

return KernelSignature("squeeze", {"X"}, {"axes"}, {"Out", "XShape"});
return KernelSignature("squeeze", {"X"}, {"AxisTensor"}, {"Out", "XShape"});
return KernelSignature("squeeze", {"X"}, {"AxisTensorList"}, {"Out", "XShape"});
******************************************************************
*/

KernelSignature Squeeze2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  paddle::small_vector<const char*> outputs {"Out", "XShape"};
  return KernelSignature("squeeze", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StackOpArgumentMapping:

return KernelSignature("stack", {"X"}, {"axis"}, {"Y"});
******************************************************************
*/

KernelSignature StackOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Y"};
  return KernelSignature("stack", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StandardGammaOpArgumentMapping:

return KernelSignature("standard_gamma", {"x"}, {}, {"out"});
******************************************************************
*/

KernelSignature StandardGammaOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("standard_gamma", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StanhOpArgumentMapping:

return KernelSignature("stanh", {"X"}, {"scale_a", "scale_b"}, {"Out"});
******************************************************************
*/

KernelSignature StanhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale_a");
  attrs.emplace_back("scale_b");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("stanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SvdOpArgumentMapping:

return KernelSignature("svd", {"X"}, {"full_matrices"}, {"U", "S", "VH"});
******************************************************************
*/

KernelSignature SvdOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("full_matrices");
  paddle::small_vector<const char*> outputs {"U", "S", "VH"};
  return KernelSignature("svd", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SwigluOpArgumentMapping:

return KernelSignature("swiglu", {"x", "y"}, {}, {"out"});
******************************************************************
*/

KernelSignature SwigluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("swiglu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TakeAlongAxisOpArgumentMapping:

return KernelSignature("take_along_axis", {"Input", "Index"}, {"Axis"}, {"Result"});
******************************************************************
*/

KernelSignature TakeAlongAxisOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Index"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  paddle::small_vector<const char*> outputs {"Result"};
  return KernelSignature("take_along_axis", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanOpArgumentMapping:

return KernelSignature("tan", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TanOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("tan", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhOpArgumentMapping:

return KernelSignature("tanh", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TanhOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("tanh", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhShrinkOpArgumentMapping:

return KernelSignature("tanh_shrink", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TanhShrinkOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("tanh_shrink", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TemporalShiftOpArgumentMapping:

return KernelSignature("temporal_shift", {"X"}, {"seg_num", "shift_ratio", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature TemporalShiftOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seg_num");
  attrs.emplace_back("shift_ratio");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("temporal_shift", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TensorUnfoldOpArgumentMapping:

return KernelSignature("tensor_unfold", {"input"}, {"axis", "size", "step"}, {"out"});
******************************************************************
*/

KernelSignature TensorUnfoldOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("size");
  attrs.emplace_back("step");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("tensor_unfold", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ThresholdedReluOpArgumentMapping:

return KernelSignature("thresholded_relu", {"X"}, {"threshold"}, {"Out"});
******************************************************************
*/

KernelSignature ThresholdedReluOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("thresholded_relu", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TopPSamplingOpArgumentMapping:

return KernelSignature("top_p_sampling", {"x", "ps", "threshold"}, {"seed"}, {"out", "ids"});
******************************************************************
*/

KernelSignature TopPSamplingOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "ps", "threshold"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs {"out", "ids"};
  return KernelSignature("top_p_sampling", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TopkOpArgumentMapping:

return KernelSignature("topk", {"X"}, {"k", "axis", "largest", "sorted"}, {"Out", "Indices"});
******************************************************************
*/

KernelSignature TopKV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("K") ? "K" : "k");
  attrs.emplace_back("axis");
  attrs.emplace_back("largest");
  attrs.emplace_back("sorted");
  paddle::small_vector<const char*> outputs {"Out", "Indices"};
  return KernelSignature("topk", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TraceOpArgumentMapping:

return KernelSignature("trace", {"Input"}, {"offset", "axis1", "axis2"}, {"Out"});
******************************************************************
*/

KernelSignature TraceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("trace", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TriangularSolveOpArgumentMapping:

return KernelSignature("triangular_solve", {"X", "Y"}, {"upper", "transpose", "unitriangular"}, {"Out"});
******************************************************************
*/

KernelSignature TriangularSolveOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  attrs.emplace_back("transpose");
  attrs.emplace_back("unitriangular");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("triangular_solve", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilinearInterpOpArgumentMapping:

return KernelSignature("trilinear_interp", {"X", "OutSize", "SizeTensor", "Scale"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"Out"});
******************************************************************
*/

KernelSignature TrilinearInterpV2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("trilinear_interp", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TruncOpArgumentMapping:

return KernelSignature("trunc", {"X"}, {}, {"Out"});
******************************************************************
*/

KernelSignature TruncOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("trunc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnbindOpArgumentMapping:

return KernelSignature("unbind", {"X"}, {"axis"}, {"Out"});
******************************************************************
*/

KernelSignature UnbindOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("unbind", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnfoldOpArgumentMapping:

return KernelSignature("unfold", {"X"}, {"kernel_sizes", "strides", "paddings", "dilations"}, {"Y"});
******************************************************************
*/

KernelSignature UnfoldOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs {"Y"};
  return KernelSignature("unfold", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UniformInplaceOpArgumentMapping:

return KernelSignature("uniform_inplace", {"X"}, {"min", "max", "seed", "diag_num", "diag_step", "diag_val"}, {"Out"});
******************************************************************
*/

KernelSignature UniformRandomInplaceOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("min");
  attrs.emplace_back("max");
  attrs.emplace_back("seed");
  attrs.emplace_back("diag_num");
  attrs.emplace_back("diag_step");
  attrs.emplace_back("diag_val");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("uniform_inplace", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UniqueConsecutiveOpArgumentMapping:

return KernelSignature("unique_consecutive", {"X"}, {"return_inverse", "return_counts", "axis", "dtype"}, {"Out", "Index", "Counts"});
******************************************************************
*/

KernelSignature UniqueConsecutiveOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("return_inverse");
  attrs.emplace_back("return_counts");
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"Out", "Index", "Counts"};
  return KernelSignature("unique_consecutive", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Unpool3dOpArgumentMapping:

return KernelSignature("unpool3d", {"X", "Indices"}, {"ksize", "strides", "paddings", "output_size", "data_format"}, {"Out"});
******************************************************************
*/

KernelSignature Unpool3dOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Indices"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_size");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("unpool3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnsqueezeOpArgumentMapping:

return KernelSignature("unsqueeze", {"X"}, {"axes"}, {"Out", "XShape"});
return KernelSignature("unsqueeze", {"X"}, {"AxesTensor"}, {"Out", "XShape"});
return KernelSignature("unsqueeze", {"X"}, {"AxesTensorList"}, {"Out", "XShape"});
******************************************************************
*/

KernelSignature Unsqueeze2OpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("AxesTensor")
    ? "AxesTensor"
    : ctx.InputSize("AxesTensorList") > 0
      ? "AxesTensorList"
      : "axes");
  paddle::small_vector<const char*> outputs {"Out", "XShape"};
  return KernelSignature("unsqueeze", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnstackOpArgumentMapping:

return KernelSignature("unstack", {"X"}, {"axis", "num"}, {"Y"});
******************************************************************
*/

KernelSignature UnstackOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("num");
  paddle::small_vector<const char*> outputs {"Y"};
  return KernelSignature("unstack", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UpdateLossScalingOpArgumentMapping:

return KernelSignature("update_loss_scaling", {"X", "FoundInfinite", "PrevLossScaling", "InGoodSteps", "InBadSteps"}, {"incr_every_n_steps", "decr_every_n_nan_or_inf", "incr_ratio", "decr_ratio", "stop_update"}, {"Out", "LossScaling", "OutGoodSteps", "OutBadSteps"});
return KernelSignature("update_loss_scaling", {"X", "FoundInfinite", "PrevLossScaling", "InGoodSteps", "InBadSteps"}, {"incr_every_n_steps", "decr_every_n_nan_or_inf", "incr_ratio", "decr_ratio", "StopUpdate"}, {"Out", "LossScaling", "OutGoodSteps", "OutBadSteps"});
******************************************************************
*/

KernelSignature UpdateLossScalingOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "FoundInfinite", "PrevLossScaling", "InGoodSteps", "InBadSteps"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("incr_every_n_steps");
  attrs.emplace_back("decr_every_n_nan_or_inf");
  attrs.emplace_back("incr_ratio");
  attrs.emplace_back("decr_ratio");
  attrs.emplace_back(ctx.HasInput("StopUpdate") ? "StopUpdate" : "stop_update");
  paddle::small_vector<const char*> outputs {"Out", "LossScaling", "OutGoodSteps", "OutBadSteps"};
  return KernelSignature("update_loss_scaling", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewDtypeOpArgumentMapping:

return KernelSignature("view_dtype", {"input"}, {"dtype"}, {"out"});
******************************************************************
*/

KernelSignature ViewDtypeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("view_dtype", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewShapeOpArgumentMapping:

return KernelSignature("view_shape", {"input"}, {"dims"}, {"out"});
******************************************************************
*/

KernelSignature ViewShapeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("view_shape", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViterbiDecodeOpArgumentMapping:

return KernelSignature("viterbi_decode", {"Input", "Transition", "Length"}, {"include_bos_eos_tag"}, {"Scores", "Path"});
******************************************************************
*/

KernelSignature ViterbiDecodeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Transition", "Length"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("include_bos_eos_tag");
  paddle::small_vector<const char*> outputs {"Scores", "Path"};
  return KernelSignature("viterbi_decode", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarpctcOpArgumentMapping:

return KernelSignature("warpctc", {"Logits", "Label", "LogitsLength", "LabelLength"}, {"blank", "norm_by_times"}, {"Loss", "WarpCTCGrad"});
******************************************************************
*/

KernelSignature WarpctcOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Logits", "Label", "LogitsLength", "LabelLength"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("norm_by_times");
  paddle::small_vector<const char*> outputs {"Loss", "WarpCTCGrad"};
  return KernelSignature("warpctc", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarprnntOpArgumentMapping:

return KernelSignature("warprnnt", {"input", "label", "input_lengths", "label_lengths"}, {"blank", "fastemit_lambda"}, {"loss", "warprnntgrad"});
******************************************************************
*/

KernelSignature WarprnntOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input", "label", "input_lengths", "label_lengths"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("fastemit_lambda");
  paddle::small_vector<const char*> outputs {"loss", "warprnntgrad"};
  return KernelSignature("warprnnt", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightDequantizeOpArgumentMapping:

return KernelSignature("weight_dequantize", {"x", "scale"}, {"algo", "out_dtype", "group_size"}, {"out"});
******************************************************************
*/

KernelSignature WeightDequantizeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("algo");
  attrs.emplace_back("out_dtype");
  attrs.emplace_back("group_size");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("weight_dequantize", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightOnlyLinearOpArgumentMapping:

return KernelSignature("weight_only_linear", {"x", "weight", "bias", "weight_scale"}, {"weight_dtype", "arch", "group_size"}, {"out"});
******************************************************************
*/

KernelSignature WeightOnlyLinearOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "weight", "bias", "weight_scale"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("weight_dtype");
  attrs.emplace_back("arch");
  attrs.emplace_back("group_size");
  paddle::small_vector<const char*> outputs {"out"};
  return KernelSignature("weight_only_linear", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightQuantizeOpArgumentMapping:

return KernelSignature("weight_quantize", {"x"}, {"algo", "arch", "group_size"}, {"out", "scale"});
******************************************************************
*/

KernelSignature WeightQuantizeOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("algo");
  attrs.emplace_back("arch");
  attrs.emplace_back("group_size");
  paddle::small_vector<const char*> outputs {"out", "scale"};
  return KernelSignature("weight_quantize", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightedSampleNeighborsOpArgumentMapping:

return KernelSignature("weighted_sample_neighbors", {"row", "colptr", "edge_weight", "input_nodes", "eids"}, {"sample_size", "return_eids"}, {"out_neighbors", "out_count", "out_eids"});
******************************************************************
*/

KernelSignature WeightedSampleNeighborsOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"row", "colptr", "edge_weight", "input_nodes", "eids"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("sample_size");
  attrs.emplace_back("return_eids");
  paddle::small_vector<const char*> outputs {"out_neighbors", "out_count", "out_eids"};
  return KernelSignature("weighted_sample_neighbors", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WhereOpArgumentMapping:

return KernelSignature("where", {"Condition", "X", "Y"}, {}, {"Out"});
******************************************************************
*/

KernelSignature WhereOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Condition", "X", "Y"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out"};
  return KernelSignature("where", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloBoxOpArgumentMapping:

return KernelSignature("yolo_box", {"X", "ImgSize"}, {"anchors", "class_num", "conf_thresh", "downsample_ratio", "clip_bbox", "scale_x_y", "iou_aware", "iou_aware_factor"}, {"Boxes", "Scores"});
******************************************************************
*/

KernelSignature YoloBoxOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ImgSize"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("anchors");
  attrs.emplace_back("class_num");
  attrs.emplace_back("conf_thresh");
  attrs.emplace_back("downsample_ratio");
  attrs.emplace_back("clip_bbox");
  attrs.emplace_back("scale_x_y");
  attrs.emplace_back("iou_aware");
  attrs.emplace_back("iou_aware_factor");
  paddle::small_vector<const char*> outputs {"Boxes", "Scores"};
  return KernelSignature("yolo_box", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloLossOpArgumentMapping:

return KernelSignature("yolo_loss", {"X", "GTBox", "GTLabel", "GTScore"}, {"anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y"}, {"Loss", "ObjectnessMask", "GTMatchMask"});
******************************************************************
*/

KernelSignature Yolov3LossOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "GTBox", "GTLabel", "GTScore"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("anchors");
  attrs.emplace_back("anchor_mask");
  attrs.emplace_back("class_num");
  attrs.emplace_back("ignore_thresh");
  attrs.emplace_back("downsample_ratio");
  attrs.emplace_back("use_label_smooth");
  attrs.emplace_back("scale_x_y");
  paddle::small_vector<const char*> outputs {"Loss", "ObjectnessMask", "GTMatchMask"};
  return KernelSignature("yolo_loss", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AbsDoubleGradOpArgumentMapping:

return KernelSignature("abs_double_grad", {"X", "grad_x@GRAD"}, {}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature AbsDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"grad_out@GRAD"};
  return KernelSignature("abs_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AbsGradOpArgumentMapping:

return KernelSignature("abs_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AbsGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("abs_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcosGradOpArgumentMapping:

return KernelSignature("acos_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AcosGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("acos_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AcoshGradOpArgumentMapping:

return KernelSignature("acosh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AcoshGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("acosh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AddmmGradOpArgumentMapping:

return KernelSignature("addmm_grad", {"Input", "X", "Y", "Out@GRAD"}, {"Alpha", "Beta"}, {"Input@GRAD", "X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature AddmmGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Alpha");
  attrs.emplace_back("Beta");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "X@GRAD", "Y@GRAD"};
  return KernelSignature("addmm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AffineGridGradOpArgumentMapping:

return KernelSignature("affine_grid_grad", {"Output@GRAD"}, {"output_shape", "align_corners"}, {"Theta@GRAD"});
return KernelSignature("affine_grid_grad", {"Output@GRAD"}, {"OutputShape", "align_corners"}, {"Theta@GRAD"});
******************************************************************
*/

KernelSignature AffineGridGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("OutputShape")
    ? "OutputShape"
    : "output_shape");

  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs {"Theta@GRAD"};
  return KernelSignature("affine_grid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AngleGradOpArgumentMapping:

return KernelSignature("angle_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AngleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("angle_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ArgsortGradOpArgumentMapping:

return KernelSignature("argsort_grad", {"Indices", "X", "Out@GRAD"}, {"axis", "descending"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ArgsortGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Indices", "X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("descending");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("argsort_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsStridedGradOpArgumentMapping:

return KernelSignature("as_strided_grad", {"input", "out@GRAD"}, {"dims", "stride", "offset"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature AsStridedGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  attrs.emplace_back("stride");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs {"input@GRAD"};
  return KernelSignature("as_strided_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinGradOpArgumentMapping:

return KernelSignature("asin_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AsinGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("asin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AsinhGradOpArgumentMapping:

return KernelSignature("asinh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AsinhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("asinh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Atan2GradOpArgumentMapping:

return KernelSignature("atan2_grad", {"X1", "X2", "Out@GRAD"}, {}, {"X1@GRAD", "X2@GRAD"});
******************************************************************
*/

KernelSignature Atan2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X1", "X2", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X1@GRAD", "X2@GRAD"};
  return KernelSignature("atan2_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanGradOpArgumentMapping:

return KernelSignature("atan_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AtanGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("atan_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by AtanhGradOpArgumentMapping:

return KernelSignature("atanh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature AtanhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("atanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BceLossGradOpArgumentMapping:

return KernelSignature("bce_loss_grad", {"X", "Label", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BceLossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Label", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("bce_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BicubicInterpGradOpArgumentMapping:

return KernelSignature("bicubic_interp_grad", {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BicubicInterpV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("bicubic_interp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearGradOpArgumentMapping:

return KernelSignature("bilinear_grad", {"X", "Y", "Weight", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD", "Weight@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature BilinearTensorProductGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Weight", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD", "Weight@GRAD", "Bias@GRAD"};
  return KernelSignature("bilinear_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BilinearInterpGradOpArgumentMapping:

return KernelSignature("bilinear_interp_grad", {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BilinearInterpV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("bilinear_interp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BmmGradOpArgumentMapping:

return KernelSignature("bmm_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature BmmGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("bmm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by BroadcastTensorsGradOpArgumentMapping:

return KernelSignature("broadcast_tensors_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BroadcastTensorsGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("broadcast_tensors_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeilGradOpArgumentMapping:

return KernelSignature("ceil_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CeilGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("ceil_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeluDoubleGradOpArgumentMapping:

return KernelSignature("celu_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {"alpha"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature CeluGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("celu_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CeluGradOpArgumentMapping:

return KernelSignature("celu_grad", {"X", "Out@GRAD"}, {"alpha"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CeluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("celu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskyGradOpArgumentMapping:

return KernelSignature("cholesky_grad", {"Out", "Out@GRAD"}, {"upper"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CholeskyGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("cholesky_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CholeskySolveGradOpArgumentMapping:

return KernelSignature("cholesky_solve_grad", {"X", "Y", "Out", "Out@GRAD"}, {"upper"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature CholeskySolveGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("cholesky_solve_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipDoubleGradOpArgumentMapping:

return KernelSignature("clip_grad", {"X", "grad_x@GRAD"}, {"min", "max"}, {"grad_out@GRAD"});
return KernelSignature("clip_grad", {"X", "grad_x@GRAD"}, {"min", "Max"}, {"grad_out@GRAD"});
return KernelSignature("clip_grad", {"X", "grad_x@GRAD"}, {"Min", "max"}, {"grad_out@GRAD"});
return KernelSignature("clip_grad", {"X", "grad_x@GRAD"}, {"Min", "Max"}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature ClipDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Min") ? "Min" : "min");
  attrs.emplace_back(ctx.HasInput("Max") ? "Max" : "max");
  paddle::small_vector<const char*> outputs {"grad_out@GRAD"};
  return KernelSignature("clip_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ClipGradOpArgumentMapping:

return KernelSignature("clip_grad", {"X", "Out@GRAD"}, {"min", "max"}, {"X@GRAD"});
return KernelSignature("clip_grad", {"X", "Out@GRAD"}, {"min", "Max"}, {"X@GRAD"});
return KernelSignature("clip_grad", {"X", "Out@GRAD"}, {"Min", "max"}, {"X@GRAD"});
return KernelSignature("clip_grad", {"X", "Out@GRAD"}, {"Min", "Max"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ClipGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Min") ? "Min" : "min");
  attrs.emplace_back(ctx.HasInput("Max") ? "Max" : "max");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("clip_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ComplexGradOpArgumentMapping:

return KernelSignature("complex_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature ComplexGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("complex_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ConcatGradOpArgumentMapping:

return KernelSignature("concat_grad", {"X", "Out@GRAD"}, {"axis"}, {"X@GRAD"});
return KernelSignature("concat_grad", {"X", "Out@GRAD"}, {"AxisTensor"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ConcatGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("AxisTensor") ? "AxisTensor" : "axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("concat_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dGradOpArgumentMapping:

return KernelSignature("conv2d_grad", {"Input", "Filter", "Output@GRAD"}, {"strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv2dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("conv2d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv2dGradGradOpArgumentMapping:

return KernelSignature("conv2d_double_grad", {"Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"}, {"strides", "paddings", "padding_algorithm", "dilations", "groups", "data_format"}, {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature Conv2dGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("dilations");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("conv2d_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dDoubleGradOpArgumentMapping:

return KernelSignature("conv3d_double_grad", {"Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"}, {"strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature Conv3dGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("conv3d_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dGradOpArgumentMapping:

return KernelSignature("conv3d_grad", {"Input", "Filter", "Output@GRAD"}, {"strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("conv3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Conv3dTransposeGradOpArgumentMapping:

return KernelSignature("conv3d_transpose_grad", {"Input", "Filter", "Output@GRAD"}, {"strides", "paddings", "output_padding", "output_size", "padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature Conv3dTransposeGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_padding");
  attrs.emplace_back("output_size");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("conv3d_transpose_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CopysignGradOpArgumentMapping:

return KernelSignature("copysign_grad", {"x", "y", "out@GRAD"}, {}, {"x@GRAD", "y@GRAD"});
******************************************************************
*/

KernelSignature CopysignGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD", "y@GRAD"};
  return KernelSignature("copysign_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosDoubleGradOpArgumentMapping:

return KernelSignature("cos_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature CosDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("cos_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosGradOpArgumentMapping:

return KernelSignature("cos_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CosGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("cos_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CosTripleGradOpArgumentMapping:

return KernelSignature("cos_triple_grad", {"X", "grad_out_forward", "grad_x_grad_forward", "grad_x@GRAD", "grad_out_grad@GRAD"}, {}, {"X@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"});
******************************************************************
*/

KernelSignature CosTripleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out_forward", "grad_x_grad_forward", "grad_x@GRAD", "grad_out_grad@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"};
  return KernelSignature("cos_triple_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CoshGradOpArgumentMapping:

return KernelSignature("cosh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CoshGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("cosh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CropGradOpArgumentMapping:

return KernelSignature("crop_grad", {"X", "Out@GRAD"}, {"offsets"}, {"X@GRAD"});
return KernelSignature("crop_grad", {"X", "Out@GRAD"}, {"Offsets"}, {"X@GRAD"});
return KernelSignature("crop_grad", {"X", "Out@GRAD"}, {"OffsetsTensor"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CropTensorGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("Offsets")
    ? "Offsets"
    : ctx.InputSize("OffsetsTensor") > 0
      ? "OffsetsTensor"
      : "offsets");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("crop_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossEntropyWithSoftmaxGradOpArgumentMapping:

return KernelSignature("cross_entropy_with_softmax_grad", {"Label", "Softmax", "Loss@GRAD"}, {"soft_label", "use_softmax", "numeric_stable_mode", "ignore_index", "axis"}, {"Logits@GRAD"});
******************************************************************
*/

KernelSignature SoftmaxWithCrossEntropyGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Label", "Softmax", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("soft_label");
  attrs.emplace_back("use_softmax");
  attrs.emplace_back("numeric_stable_mode");
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"Logits@GRAD"};
  return KernelSignature("cross_entropy_with_softmax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CrossGradOpArgumentMapping:

return KernelSignature("cross_grad", {"X", "Y", "Out@GRAD"}, {"dim"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature CrossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("cross_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CummaxGradOpArgumentMapping:

return KernelSignature("cummax_grad", {"x", "indices", "out@GRAD"}, {"axis", "dtype"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature CummaxGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "indices", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("cummax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumminGradOpArgumentMapping:

return KernelSignature("cummin_grad", {"x", "indices", "out@GRAD"}, {"axis", "dtype"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature CumminGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "indices", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("cummin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumprodGradOpArgumentMapping:

return KernelSignature("cumprod_grad", {"X", "Out", "Out@GRAD"}, {"dim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CumprodGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("cumprod_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by CumsumGradOpArgumentMapping:

return KernelSignature("cumsum_grad", {"X", "Out@GRAD"}, {"axis", "flatten", "exclusive", "reverse"}, {"X@GRAD"});
return KernelSignature("cumsum_grad", {"X", "Out@GRAD"}, {"AxisTensor", "flatten", "exclusive", "reverse"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature CumsumGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("cumsum_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DepthwiseConv2dDoubleGradOpArgumentMapping:

return KernelSignature("depthwise_conv2d_double_grad", {"Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"}, {"strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature DepthwiseConv2dGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "grad_out", "grad_input@GRAD", "grad_filter@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD", "grad_out@GRAD"};
  return KernelSignature("depthwise_conv2d_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DepthwiseConv2dGradOpArgumentMapping:

return KernelSignature("depthwise_conv2d_grad", {"Input", "Filter", "Output@GRAD"}, {"strides", "paddings", "padding_algorithm", "groups", "dilations", "data_format"}, {"Input@GRAD", "Filter@GRAD"});
******************************************************************
*/

KernelSignature DepthwiseConv2dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Filter", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("padding_algorithm");
  attrs.emplace_back("groups");
  attrs.emplace_back("dilations");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Filter@GRAD"};
  return KernelSignature("depthwise_conv2d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DetGradOpArgumentMapping:

return KernelSignature("determinant_grad", {"Input", "Out", "Out@GRAD"}, {}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature DeterminantGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Input@GRAD"};
  return KernelSignature("determinant_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagGradOpArgumentMapping:

return KernelSignature("diag_grad", {"X", "Out@GRAD"}, {"offset"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature DiagV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("diag_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DiagonalGradOpArgumentMapping:

return KernelSignature("diagonal_grad", {"Input", "Out@GRAD"}, {"offset", "axis1", "axis2"}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature DiagonalGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs {"Input@GRAD"};
  return KernelSignature("diagonal_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DigammaGradOpArgumentMapping:

return KernelSignature("digamma_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature DigammaGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("digamma_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DistGradOpArgumentMapping:

return KernelSignature("dist_grad", {"X", "Y", "Out", "Out@GRAD"}, {"p"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature DistGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("dist_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by DotGradOpArgumentMapping:

return KernelSignature("dot_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature DotGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("dot_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigGradOpArgumentMapping:

return KernelSignature("eig_grad", {"Eigenvalues", "Eigenvectors", "Eigenvalues@GRAD", "Eigenvectors@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EigGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Eigenvalues", "Eigenvectors", "Eigenvalues@GRAD", "Eigenvectors@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("eig_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EighGradOpArgumentMapping:

return KernelSignature("eigh_grad", {"Eigenvalues", "Eigenvectors", "Eigenvalues@GRAD", "Eigenvectors@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EighGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Eigenvalues", "Eigenvectors", "Eigenvalues@GRAD", "Eigenvectors@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("eigh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EigvalshGradOpArgumentMapping:

return KernelSignature("eigvalsh_grad", {"Eigenvectors", "Eigenvalues@GRAD"}, {"UPLO", "is_test"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EigvalshGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Eigenvectors", "Eigenvalues@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("UPLO");
  attrs.emplace_back("is_test");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("eigvalsh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EluDoubleGradOpArgumentMapping:

return KernelSignature("elu_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {"alpha"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature EluGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("elu_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by EluGradOpArgumentMapping:

return KernelSignature("elu_grad", {"X", "Out", "Out@GRAD"}, {"alpha"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature EluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("elu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfGradOpArgumentMapping:

return KernelSignature("erf_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ErfGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("erf_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ErfinvGradOpArgumentMapping:

return KernelSignature("erfinv_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ErfinvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("erfinv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpGradOpArgumentMapping:

return KernelSignature("exp_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ExpGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("exp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ExpandAsGradOpArgumentMapping:

return KernelSignature("expand_as_grad", {"X", "Out@GRAD"}, {"target_shape"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ExpandAsV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("target_shape");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("expand_as_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Expm1GradOpArgumentMapping:

return KernelSignature("expm1_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Expm1GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("expm1_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2cGradOpArgumentMapping:

return KernelSignature("fft_c2c_grad", {"Out@GRAD"}, {"axes", "normalization", "forward"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FftC2cGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fft_c2c_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftC2rGradOpArgumentMapping:

return KernelSignature("fft_c2r_grad", {"Out@GRAD"}, {"axes", "normalization", "forward", "last_dim_size"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FftC2rGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("last_dim_size");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fft_c2r_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FftR2cGradOpArgumentMapping:

return KernelSignature("fft_r2c_grad", {"X", "Out@GRAD"}, {"axes", "normalization", "forward", "onesided"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FftR2cGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  attrs.emplace_back("normalization");
  attrs.emplace_back("forward");
  attrs.emplace_back("onesided");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fft_r2c_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalGradOpArgumentMapping:

return KernelSignature("fill_diagonal_grad", {"Out@GRAD"}, {"value", "offset", "wrap"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FillDiagonalGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  attrs.emplace_back("offset");
  attrs.emplace_back("wrap");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fill_diagonal_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillDiagonalTensorGradOpArgumentMapping:

return KernelSignature("fill_diagonal_tensor_grad", {"Out@GRAD"}, {"offset", "dim1", "dim2"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FillDiagonalTensorGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("dim1");
  attrs.emplace_back("dim2");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fill_diagonal_tensor_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FillGradOpArgumentMapping:

return KernelSignature("fill_grad", {"Out@GRAD"}, {"value"}, {"X@GRAD"});
return KernelSignature("fill_grad", {"Out@GRAD"}, {"ValueTensor"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FillAnyGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("value");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fill_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnGradOpArgumentMapping:

return KernelSignature("flash_attn_grad", {"q", "k", "v", "out", "softmax_lse", "seed_offset", "attn_mask", "out@GRAD"}, {"dropout", "causal"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FlashAttnGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "out", "softmax_lse", "seed_offset", "attn_mask", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  paddle::small_vector<const char*> outputs {"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("flash_attn_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnUnpaddedGradOpArgumentMapping:

return KernelSignature("flash_attn_unpadded_grad", {"q", "k", "v", "cu_seqlens_q", "cu_seqlens_k", "out", "softmax_lse", "seed_offset", "attn_mask", "out@GRAD"}, {"max_seqlen_q", "max_seqlen_k", "scale", "dropout", "causal"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FlashAttnUnpaddedGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "cu_seqlens_q", "cu_seqlens_k", "out", "softmax_lse", "seed_offset", "attn_mask", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("max_seqlen_q");
  attrs.emplace_back("max_seqlen_k");
  attrs.emplace_back("scale");
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  paddle::small_vector<const char*> outputs {"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("flash_attn_unpadded_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FlashAttnWithSparseMaskGradOpArgumentMapping:

return KernelSignature("flash_attn_with_sparse_mask_grad", {"q", "k", "v", "attn_mask_start_row_indices", "out", "softmax_lse", "seed_offset", "out@GRAD"}, {"dropout", "causal", "attn_mask_start_row"}, {"q@GRAD", "k@GRAD", "v@GRAD"});
******************************************************************
*/

KernelSignature FlashAttnWithSparseMaskGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"q", "k", "v", "attn_mask_start_row_indices", "out", "softmax_lse", "seed_offset", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dropout");
  attrs.emplace_back("causal");
  attrs.emplace_back("attn_mask_start_row");
  paddle::small_vector<const char*> outputs {"q@GRAD", "k@GRAD", "v@GRAD"};
  return KernelSignature("flash_attn_with_sparse_mask_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FloorGradOpArgumentMapping:

return KernelSignature("floor_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FloorGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("floor_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FmaxGradOpArgumentMapping:

return KernelSignature("fmax_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseFmaxGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("fmax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FminGradOpArgumentMapping:

return KernelSignature("fmin_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseFminGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("fmin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FoldGradOpArgumentMapping:

return KernelSignature("fold_grad", {"X", "Y@GRAD"}, {"output_sizes", "kernel_sizes", "strides", "paddings", "dilations"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FoldGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_sizes");
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("fold_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FractionalMaxPool2dGradOpArgumentMapping:

return KernelSignature("fractional_max_pool2d_grad", {"x", "mask", "out@GRAD"}, {"output_size", "kernel_size", "random_u", "return_mask"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature FractionalMaxPool2dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "mask", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_size");
  attrs.emplace_back("kernel_size");
  attrs.emplace_back("random_u");
  attrs.emplace_back("return_mask");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("fractional_max_pool2d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FractionalMaxPool3dGradOpArgumentMapping:

return KernelSignature("fractional_max_pool3d_grad", {"x", "mask", "out@GRAD"}, {"output_size", "kernel_size", "random_u", "return_mask"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature FractionalMaxPool3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "mask", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("output_size");
  attrs.emplace_back("kernel_size");
  attrs.emplace_back("random_u");
  attrs.emplace_back("return_mask");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("fractional_max_pool3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by FrameGradOpArgumentMapping:

return KernelSignature("frame_grad", {"X", "Out@GRAD"}, {"frame_length", "hop_length", "axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature FrameGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("frame_length");
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("frame_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GammainccGradOpArgumentMapping:

return KernelSignature("gammaincc_grad", {"x", "y", "out@GRAD"}, {}, {"y@GRAD"});
******************************************************************
*/

KernelSignature GammainccGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"y@GRAD"};
  return KernelSignature("gammaincc_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GammalnGradOpArgumentMapping:

return KernelSignature("gammaln_grad", {"x", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature GammalnGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("gammaln_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherGradOpArgumentMapping:

return KernelSignature("gather_grad", {"X", "Index", "Out@GRAD"}, {"axis"}, {"X@GRAD"});
return KernelSignature("gather_grad", {"X", "Index", "Out@GRAD"}, {"Axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GatherGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("Axis") ? "Axis" : "axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("gather_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GatherNdGradOpArgumentMapping:

return KernelSignature("gather_nd_grad", {"X", "Index", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GatherNdGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("gather_nd_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GaussianInplaceGradOpArgumentMapping:

return KernelSignature("gaussian_inplace_grad", {"out@GRAD"}, {"mean", "std", "seed"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature GaussianInplaceGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mean");
  attrs.emplace_back("std");
  attrs.emplace_back("seed");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("gaussian_inplace_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GeluGradOpArgumentMapping:

return KernelSignature("gelu_grad", {"X", "Out@GRAD"}, {"approximate"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GeluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("approximate");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("gelu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GridSampleGradOpArgumentMapping:

return KernelSignature("grid_sample_grad", {"X", "Grid", "Output@GRAD"}, {"mode", "padding_mode", "align_corners"}, {"X@GRAD", "Grid@GRAD"});
******************************************************************
*/

KernelSignature GridSamplerGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Grid", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  attrs.emplace_back("padding_mode");
  attrs.emplace_back("align_corners");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Grid@GRAD"};
  return KernelSignature("grid_sample_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GroupNormGradOpArgumentMapping:

return KernelSignature("group_norm_grad", {"X", "Scale", "Bias", "Y", "Mean", "Variance", "Y@GRAD"}, {"epsilon", "groups", "data_layout"}, {"X@GRAD", "Scale@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature GroupNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale", "Bias", "Y", "Mean", "Variance", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("groups");
  attrs.emplace_back("data_layout");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Scale@GRAD", "Bias@GRAD"};
  return KernelSignature("group_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by GumbelSoftmaxGradOpArgumentMapping:

return KernelSignature("gumbel_softmax_grad", {"Out", "Out@GRAD"}, {"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GumbelSoftmaxGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("gumbel_softmax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardshrinkGradOpArgumentMapping:

return KernelSignature("hard_shrink_grad", {"X", "Out@GRAD"}, {"threshold"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature HardShrinkGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("hard_shrink_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardsigmoidGradOpArgumentMapping:

return KernelSignature("hardsigmoid_grad", {"Out", "Out@GRAD"}, {"slope", "offset"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature HardSigmoidGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("slope");
  attrs.emplace_back("offset");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("hardsigmoid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HardtanhGradOpArgumentMapping:

return KernelSignature("hardtanh_grad", {"X", "Out@GRAD"}, {"t_min", "t_max"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature BreluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("t_min");
  attrs.emplace_back("t_max");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("hardtanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HeavisideGradOpArgumentMapping:

return KernelSignature("heaviside_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature ElementwiseHeavisideGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("heaviside_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by HuberLossGradOpArgumentMapping:

return KernelSignature("huber_loss_grad", {"Residual", "Out@GRAD"}, {"delta"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature HuberLossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Residual", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("delta");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("huber_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0GradOpArgumentMapping:

return KernelSignature("i0_grad", {"x", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I0GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("i0_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I0eGradOpArgumentMapping:

return KernelSignature("i0e_grad", {"x", "out", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I0eGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("i0e_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1GradOpArgumentMapping:

return KernelSignature("i1_grad", {"x", "out", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I1GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("i1_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by I1eGradOpArgumentMapping:

return KernelSignature("i1e_grad", {"x", "out", "out@GRAD"}, {}, {"x@GRAD"});
******************************************************************
*/

KernelSignature I1eGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("i1e_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IdentityLossGradOpArgumentMapping:

return KernelSignature("identity_loss_grad", {"X", "Out@GRAD"}, {"reduction"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature IdentityLossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("identity_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ImagGradOpArgumentMapping:

return KernelSignature("imag_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ImagGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("imag_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexAddGradOpArgumentMapping:

return KernelSignature("index_add_grad", {"Index", "AddValue", "Out@GRAD"}, {"axis"}, {"X@GRAD", "AddValue@GRAD"});
******************************************************************
*/

KernelSignature IndexAddGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Index", "AddValue", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD", "AddValue@GRAD"};
  return KernelSignature("index_add_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexPutGradOpArgumentMapping:

return KernelSignature("index_put_grad", {"x", "indices", "value", "out@GRAD"}, {"accumulate"}, {"x@GRAD", "value@GRAD"});
******************************************************************
*/

KernelSignature IndexPutGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "indices", "value", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("accumulate");
  paddle::small_vector<const char*> outputs {"x@GRAD", "value@GRAD"};
  return KernelSignature("index_put_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSampleGradOpArgumentMapping:

return KernelSignature("index_sample_grad", {"X", "Index", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature IndexSampleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("index_sample_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectGradOpArgumentMapping:

return KernelSignature("index_select_grad", {"X", "Index", "Out@GRAD"}, {"dim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature IndexSelectGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Index", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("index_select_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by IndexSelectStridedGradOpArgumentMapping:

return KernelSignature("index_select_strided_grad", {"x", "out@GRAD"}, {"index", "axis"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature IndexSelectStridedGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("index");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("index_select_strided_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InstanceNormDoubleGradOpArgumentMapping:

return KernelSignature("instance_norm_double_grad", {"x", "fwd_scale", "saved_mean", "saved_variance", "grad_y", "grad_x@GRAD", "grad_scale@GRAD", "grad_bias@GRAD"}, {"epsilon"}, {"x@GRAD", "fwd_scale@GRAD", "grad_y@GRAD"});
******************************************************************
*/

KernelSignature InstanceNormDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "fwd_scale", "saved_mean", "saved_variance", "grad_y", "grad_x@GRAD", "grad_scale@GRAD", "grad_bias@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"x@GRAD", "fwd_scale@GRAD", "grad_y@GRAD"};
  return KernelSignature("instance_norm_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InstanceNormGradOpArgumentMapping:

return KernelSignature("instance_norm_grad", {"X", "Scale", "SavedMean", "SavedVariance", "Y@GRAD"}, {"epsilon"}, {"X@GRAD", "Scale@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature InstanceNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale", "SavedMean", "SavedVariance", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Scale@GRAD", "Bias@GRAD"};
  return KernelSignature("instance_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by InverseGradOpArgumentMapping:

return KernelSignature("inverse_grad", {"Output", "Output@GRAD"}, {}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature InverseGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Output", "Output@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Input@GRAD"};
  return KernelSignature("inverse_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KldivLossGradOpArgumentMapping:

return KernelSignature("kldiv_loss_grad", {"X", "Target", "Loss@GRAD"}, {"reduction"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature KldivLossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Target", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("kldiv_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KronGradOpArgumentMapping:

return KernelSignature("kron_grad", {"X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature KronGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("kron_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by KthvalueGradOpArgumentMapping:

return KernelSignature("kthvalue_grad", {"X", "Indices", "Out@GRAD"}, {"k", "axis", "keepdim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature KthvalueGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Indices", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("k");
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("kthvalue_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LabelSmoothGradOpArgumentMapping:

return KernelSignature("label_smooth_grad", {"Out@GRAD"}, {"epsilon"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LabelSmoothGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("label_smooth_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LayerNormGradOpArgumentMapping:

return KernelSignature("layer_norm_grad", {"X", "Scale", "Bias", "Mean", "Variance", "Y@GRAD"}, {"epsilon", "begin_norm_axis"}, {"X@GRAD", "Scale@GRAD", "Bias@GRAD"});
******************************************************************
*/

KernelSignature LayerNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Scale", "Bias", "Mean", "Variance", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Scale@GRAD", "Bias@GRAD"};
  return KernelSignature("layer_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LeakyReluDoubleGradOpArgumentMapping:

return KernelSignature("leaky_relu_double_grad", {"X", "grad_x@GRAD"}, {"alpha"}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature LeakyReluGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"grad_out@GRAD"};
  return KernelSignature("leaky_relu_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LeakyReluGradOpArgumentMapping:

return KernelSignature("leaky_relu_grad", {"X", "Out@GRAD"}, {"alpha"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LeakyReluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("leaky_relu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LerpGradOpArgumentMapping:

return KernelSignature("lerp_grad", {"X", "Y", "Weight", "Out", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature LerpGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Weight", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("lerp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LgammaGradOpArgumentMapping:

return KernelSignature("lgamma_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LgammaGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("lgamma_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LinearInterpGradOpArgumentMapping:

return KernelSignature("linear_interp_grad", {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LinearInterpV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("linear_interp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log10GradOpArgumentMapping:

return KernelSignature("log10_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Log10GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("log10_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log1pGradOpArgumentMapping:

return KernelSignature("log1p_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Log1pGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("log1p_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Log2GradOpArgumentMapping:

return KernelSignature("log2_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Log2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("log2_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogDoubleGradOpArgumentMapping:

return KernelSignature("log_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature LogGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("log_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogGradOpArgumentMapping:

return KernelSignature("log_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("log_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogLossGradOpArgumentMapping:

return KernelSignature("log_loss_grad", {"Predicted", "Labels", "Loss@GRAD"}, {"epsilon"}, {"Predicted@GRAD"});
******************************************************************
*/

KernelSignature LogLossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Predicted", "Labels", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  paddle::small_vector<const char*> outputs {"Predicted@GRAD"};
  return KernelSignature("log_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogSoftmaxGradOpArgumentMapping:

return KernelSignature("log_softmax_grad", {"Out", "Out@GRAD"}, {"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogSoftmaxGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("log_softmax_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogcumsumexpGradOpArgumentMapping:

return KernelSignature("logcumsumexp_grad", {"X", "Out", "Out@GRAD"}, {"axis", "flatten", "exclusive", "reverse"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogcumsumexpGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("flatten");
  attrs.emplace_back("exclusive");
  attrs.emplace_back("reverse");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("logcumsumexp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogitGradOpArgumentMapping:

return KernelSignature("logit_grad", {"X", "Out@GRAD"}, {"eps"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogitGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("logit_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LogsigmoidGradOpArgumentMapping:

return KernelSignature("logsigmoid_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LogsigmoidGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("logsigmoid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuGradOpArgumentMapping:

return KernelSignature("lu_grad", {"X", "Out", "Pivots", "Out@GRAD"}, {"pivots"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LuGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Pivots", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pivots");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("lu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by LuUnpackGradOpArgumentMapping:

return KernelSignature("lu_unpack_grad", {"X", "Pivots", "L", "U", "Pmat", "L@GRAD", "U@GRAD"}, {"unpack_ludata", "unpack_pivots"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature LuUnpackGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Pivots", "L", "U", "Pmat", "L@GRAD", "U@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("unpack_ludata");
  attrs.emplace_back("unpack_pivots");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("lu_unpack_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MarginCrossEntropyGradOpArgumentMapping:

return KernelSignature("margin_cross_entropy_grad", {"Logits", "Label", "Softmax", "Loss@GRAD"}, {"return_softmax", "ring_id", "rank", "nranks", "margin1", "margin2", "margin3", "scale"}, {"Logits@GRAD"});
******************************************************************
*/

KernelSignature MarginCrossEntropyGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Logits", "Label", "Softmax", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("return_softmax");
  attrs.emplace_back("ring_id");
  attrs.emplace_back("rank");
  attrs.emplace_back("nranks");
  attrs.emplace_back("margin1");
  attrs.emplace_back("margin2");
  attrs.emplace_back("margin3");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs {"Logits@GRAD"};
  return KernelSignature("margin_cross_entropy_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaskedSelectGradOpArgumentMapping:

return KernelSignature("masked_select_grad", {"X", "Mask", "Y@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaskedSelectGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Mask", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("masked_select_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MatrixPowerGradOpArgumentMapping:

return KernelSignature("matrix_power_grad", {"X", "Out", "Out@GRAD"}, {"n"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MatrixPowerGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("matrix_power_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool2dWithIndexGradOpArgumentMapping:

return KernelSignature("max_pool2d_with_index_grad", {"X", "Mask", "Out@GRAD"}, {"ksize", "strides", "paddings", "global_pooling", "adaptive"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaxPool2dWithIndexGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Mask", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("max_pool2d_with_index_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxPool3dWithIndexGradOpArgumentMapping:

return KernelSignature("max_pool3d_with_index_grad", {"X", "Mask", "Out@GRAD"}, {"ksize", "strides", "paddings", "global_pooling", "adaptive"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaxPool3dWithIndexGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Mask", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("global_pooling");
  attrs.emplace_back("adaptive");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("max_pool3d_with_index_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MaxoutGradOpArgumentMapping:

return KernelSignature("maxout_grad", {"X", "Out", "Out@GRAD"}, {"groups", "axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MaxoutGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("groups");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("maxout_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeanAllGradOpArgumentMapping:

return KernelSignature("mean_all_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MeanGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("mean_all_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MemoryEfficientAttentionGradOpArgumentMapping:

return KernelSignature("memory_efficient_attention_grad", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output@GRAD"}, {"max_seqlen_q", "max_seqlen_k", "causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"});
return KernelSignature("memory_efficient_attention_grad", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output@GRAD"}, {"max_seqlen_q", "MaxSeqlenKTensor", "causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"});
return KernelSignature("memory_efficient_attention_grad", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output@GRAD"}, {"MaxSeqlenQTensor", "max_seqlen_k", "causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"});
return KernelSignature("memory_efficient_attention_grad", {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output@GRAD"}, {"MaxSeqlenQTensor", "MaxSeqlenKTensor", "causal", "dropout_p", "scale"}, {"query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"});
******************************************************************
*/

KernelSignature MemoryEfficientAttentionGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"query", "key", "value", "bias", "cu_seqlens_q", "cu_seqlens_k", "output", "logsumexp", "seed_and_offset", "output@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("MaxSeqlenQTensor") ? "MaxSeqlenQTensor" : "max_seqlen_q");
  attrs.emplace_back(ctx.HasInput("MaxSeqlenKTensor") ? "MaxSeqlenKTensor" : "max_seqlen_k");
  attrs.emplace_back("causal");
  attrs.emplace_back("dropout_p");
  attrs.emplace_back("scale");
  paddle::small_vector<const char*> outputs {"query@GRAD", "key@GRAD", "value@GRAD", "bias@GRAD"};
  return KernelSignature("memory_efficient_attention_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MeshgridGradOpArgumentMapping:

return KernelSignature("meshgrid_grad", {"X", "outputs@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MeshgridGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "outputs@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("meshgrid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ModeGradOpArgumentMapping:

return KernelSignature("mode_grad", {"X", "Indices", "Out@GRAD"}, {"axis", "keepdim"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ModeGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Indices", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("keepdim");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("mode_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiDotGradOpArgumentMapping:

return KernelSignature("multi_dot_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MultiDotGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("multi_dot_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MultiplexGradOpArgumentMapping:

return KernelSignature("multiplex_grad", {"Ids", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature MultiplexGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Ids", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("multiplex_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by MvGradOpArgumentMapping:

return KernelSignature("mv_grad", {"X", "Vec", "Out@GRAD"}, {}, {"X@GRAD", "Vec@GRAD"});
******************************************************************
*/

KernelSignature MvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Vec", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Vec@GRAD"};
  return KernelSignature("mv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NanmedianGradOpArgumentMapping:

return KernelSignature("nanmedian_grad", {"X", "MedianIndex", "Out@GRAD"}, {"axis", "keepdim", "mode"}, {"X@GRAD"});
return KernelSignature("nanmedian_grad", {"X", "MedianIndex", "Out@GRAD"}, {"AxisTensorList", "keepdim", "mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NanmedianGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "MedianIndex", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("AxisTensor")
    ? "AxisTensor"
    : ctx.InputSize("AxisTensorList") > 0
      ? "AxisTensorList"
      : "axis");
  attrs.emplace_back("keepdim");
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("nanmedian_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NearestInterpGradOpArgumentMapping:

return KernelSignature("nearest_interp_grad", {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NearestInterpV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("nearest_interp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by NllLossGradOpArgumentMapping:

return KernelSignature("nll_loss_grad", {"X", "Label", "Weight", "Total_weight", "Out@GRAD"}, {"ignore_index", "reduction"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature NllLossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Label", "Weight", "Total_weight", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ignore_index");
  attrs.emplace_back("reduction");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("nll_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by OverlapAddGradOpArgumentMapping:

return KernelSignature("overlap_add_grad", {"X", "Out@GRAD"}, {"hop_length", "axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature OverlapAddGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("hop_length");
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("overlap_add_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PNormGradOpArgumentMapping:

return KernelSignature("p_norm_grad", {"X", "Out", "Out@GRAD"}, {"porder", "axis", "epsilon", "keepdim", "asvector"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("porder");
  attrs.emplace_back("axis");
  attrs.emplace_back("epsilon");
  attrs.emplace_back("keepdim");
  attrs.emplace_back("asvector");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("p_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pad3dDoubleGradOpArgumentMapping:

return KernelSignature("pad3d", {"grad_x@GRAD"}, {"paddings", "mode", "value", "data_format"}, {"grad_out@GRAD"});
return KernelSignature("pad3d", {"grad_x@GRAD"}, {"Paddings", "mode", "value", "data_format"}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature Pad3dDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("Paddings")
    ? "Paddings"
    : "paddings");

  attrs.emplace_back("mode");
  attrs.emplace_back("value");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"grad_out@GRAD"};
  return KernelSignature("pad3d", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Pad3dGradOpArgumentMapping:

return KernelSignature("pad3d_grad", {"X", "Out@GRAD"}, {"paddings", "mode", "value", "data_format"}, {"X@GRAD"});
return KernelSignature("pad3d_grad", {"X", "Out@GRAD"}, {"Paddings", "mode", "value", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Pad3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("Paddings")
    ? "Paddings"
    : "paddings");

  attrs.emplace_back("mode");
  attrs.emplace_back("value");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("pad3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelShuffleGradOpArgumentMapping:

return KernelSignature("pixel_shuffle_grad", {"Out@GRAD"}, {"upscale_factor", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PixelShuffleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("pixel_shuffle_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PixelUnshuffleGradOpArgumentMapping:

return KernelSignature("pixel_unshuffle_grad", {"Out@GRAD"}, {"downscale_factor", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PixelUnshuffleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("downscale_factor");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("pixel_unshuffle_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PoissonGradOpArgumentMapping:

return KernelSignature("poisson_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PoissonGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("poisson_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PolygammaGradOpArgumentMapping:

return KernelSignature("polygamma_grad", {"x", "out@GRAD"}, {"n"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature PolygammaGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("n");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("polygamma_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowDoubleGradOpArgumentMapping:

return KernelSignature("pow_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {"factor"}, {"X@GRAD", "grad_out@GRAD"});
return KernelSignature("pow_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {"FactorTensor"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature PowDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("pow_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowGradOpArgumentMapping:

return KernelSignature("pow_grad", {"X", "Out@GRAD"}, {"factor"}, {"X@GRAD"});
return KernelSignature("pow_grad", {"X", "Out@GRAD"}, {"FactorTensor"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PowGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("pow_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PowTripleGradOpArgumentMapping:

return KernelSignature("pow_triple_grad", {"X", "grad_out", "grad_grad_x", "grad_x@GRAD", "grad_grad_out@GRAD"}, {"factor"}, {"X@GRAD", "grad_out@GRAD", "grad_grad_x@GRAD"});
return KernelSignature("pow_triple_grad", {"X", "grad_out", "grad_grad_x", "grad_x@GRAD", "grad_grad_out@GRAD"}, {"FactorTensor"}, {"X@GRAD", "grad_out@GRAD", "grad_grad_x@GRAD"});
******************************************************************
*/

KernelSignature PowTripleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_grad_x", "grad_x@GRAD", "grad_grad_out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("FactorTensor") ? "FactorTensor" : "factor");
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD", "grad_grad_x@GRAD"};
  return KernelSignature("pow_triple_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PreluGradOpArgumentMapping:

return KernelSignature("prelu_grad", {"X", "Alpha", "Out@GRAD"}, {"data_format", "mode"}, {"X@GRAD", "Alpha@GRAD"});
******************************************************************
*/

KernelSignature PreluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Alpha", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_format");
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Alpha@GRAD"};
  return KernelSignature("prelu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PsroiPoolGradOpArgumentMapping:

return KernelSignature("psroi_pool_grad", {"X", "ROIs", "RoisNum", "Out@GRAD"}, {"pooled_height", "pooled_width", "output_channels", "spatial_scale"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature PsroiPoolGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ROIs", "RoisNum", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("output_channels");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("psroi_pool_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by PutAlongAxisGradOpArgumentMapping:

return KernelSignature("put_along_axis_grad", {"Input", "Index", "Value", "Result", "Result@GRAD"}, {"Axis", "Reduce", "Include_self"}, {"Input@GRAD", "Value@GRAD"});
******************************************************************
*/

KernelSignature PutAlongAxisGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Index", "Value", "Result", "Result@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  attrs.emplace_back("Reduce");
  attrs.emplace_back("Include_self");
  paddle::small_vector<const char*> outputs {"Input@GRAD", "Value@GRAD"};
  return KernelSignature("put_along_axis_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by QrGradOpArgumentMapping:

return KernelSignature("qr_grad", {"X", "Q", "R", "Q@GRAD", "R@GRAD"}, {"mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature QrGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Q", "R", "Q@GRAD", "R@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("qr_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RealGradOpArgumentMapping:

return KernelSignature("real_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RealGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("real_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReciprocalGradOpArgumentMapping:

return KernelSignature("reciprocal_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReciprocalGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("reciprocal_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Relu6GradOpArgumentMapping:

return KernelSignature("relu6_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Relu6GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("relu6_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReluDoubleGradOpArgumentMapping:

return KernelSignature("relu_double_grad", {"Out", "grad_x@GRAD"}, {}, {"grad_out@GRAD"});
******************************************************************
*/

KernelSignature ReluGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"grad_out@GRAD"};
  return KernelSignature("relu_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ReluGradOpArgumentMapping:

return KernelSignature("relu_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ReluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("relu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RenormGradOpArgumentMapping:

return KernelSignature("renorm_grad", {"X", "Out@GRAD"}, {"p", "axis", "max_norm"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RenormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("p");
  attrs.emplace_back("axis");
  attrs.emplace_back("max_norm");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("renorm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RmsNormGradOpArgumentMapping:

return KernelSignature("rms_norm_grad", {"x", "bias", "residual", "norm_weight", "norm_bias", "inv_var", "out@GRAD"}, {"epsilon", "begin_norm_axis", "quant_scale"}, {"x@GRAD", "norm_weight@GRAD"});
******************************************************************
*/

KernelSignature RmsNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "bias", "residual", "norm_weight", "norm_bias", "inv_var", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("epsilon");
  attrs.emplace_back("begin_norm_axis");
  attrs.emplace_back("quant_scale");
  paddle::small_vector<const char*> outputs {"x@GRAD", "norm_weight@GRAD"};
  return KernelSignature("rms_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiAlignGradOpArgumentMapping:

return KernelSignature("roi_align_grad", {"X", "ROIs", "RoisNum", "Out@GRAD"}, {"pooled_height", "pooled_width", "spatial_scale", "sampling_ratio", "aligned"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RoiAlignGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ROIs", "RoisNum", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  attrs.emplace_back("sampling_ratio");
  attrs.emplace_back("aligned");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("roi_align_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoiPoolGradOpArgumentMapping:

return KernelSignature("roi_pool_grad", {"X", "ROIs", "RoisNum", "Argmax", "Out@GRAD"}, {"pooled_height", "pooled_width", "spatial_scale"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RoiPoolGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "ROIs", "RoisNum", "Argmax", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooled_height");
  attrs.emplace_back("pooled_width");
  attrs.emplace_back("spatial_scale");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("roi_pool_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RollGradOpArgumentMapping:

return KernelSignature("roll_grad", {"X", "Out@GRAD"}, {"shifts", "axis"}, {"X@GRAD"});
return KernelSignature("roll_grad", {"X", "Out@GRAD"}, {"ShiftsTensor", "axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RollGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(
    ctx.HasInput("ShiftsTensor")
    ? "ShiftsTensor"
    : "shifts");

  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("roll_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RoundGradOpArgumentMapping:

return KernelSignature("round_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RoundGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("round_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RsqrtDoubleGradOpArgumentMapping:

return KernelSignature("rsqrt_double_grad", {"Out", "grad_x", "grad_x@GRAD"}, {}, {"Out@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature RsqrtGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "grad_x", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out@GRAD", "grad_out@GRAD"};
  return KernelSignature("rsqrt_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by RsqrtGradOpArgumentMapping:

return KernelSignature("rsqrt_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature RsqrtGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("rsqrt_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterGradOpArgumentMapping:

return KernelSignature("scatter_grad", {"Ids", "Updates", "Out@GRAD"}, {"overwrite"}, {"X@GRAD", "Updates@GRAD"});
******************************************************************
*/

KernelSignature ScatterGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Ids", "Updates", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("overwrite");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Updates@GRAD"};
  return KernelSignature("scatter_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ScatterNdAddGradOpArgumentMapping:

return KernelSignature("scatter_nd_add_grad", {"Index", "Updates", "Out@GRAD"}, {}, {"X@GRAD", "Updates@GRAD"});
******************************************************************
*/

KernelSignature ScatterNdAddGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Index", "Updates", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Updates@GRAD"};
  return KernelSignature("scatter_nd_add_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SegmentPoolGradOpArgumentMapping:

return KernelSignature("segment_pool_grad", {"X", "SegmentIds", "Out", "SummedIds", "Out@GRAD"}, {"pooltype"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SegmentPoolGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "SegmentIds", "Out", "SummedIds", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("pooltype");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("segment_pool_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SeluGradOpArgumentMapping:

return KernelSignature("selu_grad", {"Out", "Out@GRAD"}, {"scale", "alpha"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SeluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale");
  attrs.emplace_back("alpha");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("selu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendURecvGradOpArgumentMapping:

return KernelSignature("send_u_recv_grad", {"X", "Src_index", "Dst_index", "Out", "Dst_count", "Out@GRAD"}, {"reduce_op"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature GraphSendRecvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Src_index", "Dst_index", "Out", "Dst_count", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("reduce_op");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("send_u_recv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUeRecvGradOpArgumentMapping:

return KernelSignature("send_ue_recv_grad", {"X", "Y", "Src_index", "Dst_index", "Out", "Dst_count", "Out@GRAD"}, {"message_op", "reduce_op"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature GraphSendUeRecvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Src_index", "Dst_index", "Out", "Dst_count", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  attrs.emplace_back("reduce_op");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("send_ue_recv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SendUvGradOpArgumentMapping:

return KernelSignature("send_uv_grad", {"x", "y", "src_index", "dst_index", "out@GRAD"}, {"message_op"}, {"x@GRAD", "y@GRAD"});
******************************************************************
*/

KernelSignature GraphSendUvGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "src_index", "dst_index", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("message_op");
  paddle::small_vector<const char*> outputs {"x@GRAD", "y@GRAD"};
  return KernelSignature("send_uv_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidCrossEntropyWithLogitsGradOpArgumentMapping:

return KernelSignature("sigmoid_cross_entropy_with_logits_grad", {"X", "Label", "pos_weight", "Out@GRAD"}, {"normalize", "ignore_index"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SigmoidCrossEntropyWithLogitsGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Label", "pos_weight", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("normalize");
  attrs.emplace_back("ignore_index");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("sigmoid_cross_entropy_with_logits_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidDoubleGradOpArgumentMapping:

return KernelSignature("sigmoid_double_grad", {"Out", "fwd_grad_out", "grad_x@GRAD"}, {}, {"Out@GRAD", "fwd_grad_out@GRAD"});
******************************************************************
*/

KernelSignature SigmoidGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "fwd_grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out@GRAD", "fwd_grad_out@GRAD"};
  return KernelSignature("sigmoid_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidGradOpArgumentMapping:

return KernelSignature("sigmoid_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SigmoidGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("sigmoid_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SigmoidTripleGradOpArgumentMapping:

return KernelSignature("sigmoid_triple_grad", {"Out", "fwd_grad_out", "grad_grad_x", "grad_out@GRAD", "grad_grad_out@GRAD"}, {}, {"Out@GRAD", "fwd_grad_out@GRAD", "grad_grad_x@GRAD"});
******************************************************************
*/

KernelSignature SigmoidTripleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "fwd_grad_out", "grad_grad_x", "grad_out@GRAD", "grad_grad_out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out@GRAD", "fwd_grad_out@GRAD", "grad_grad_x@GRAD"};
  return KernelSignature("sigmoid_triple_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SiluGradOpArgumentMapping:

return KernelSignature("silu_grad", {"X", "Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SiluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("silu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinDoubleGradOpArgumentMapping:

return KernelSignature("sin_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SinDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("sin_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinGradOpArgumentMapping:

return KernelSignature("sin_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SinGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("sin_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinTripleGradOpArgumentMapping:

return KernelSignature("sin_triple_grad", {"X", "grad_out_forward", "grad_x_grad_forward", "grad_x@GRAD", "grad_out_grad@GRAD"}, {}, {"X@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"});
******************************************************************
*/

KernelSignature SinTripleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out_forward", "grad_x_grad_forward", "grad_x@GRAD", "grad_out_grad@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"};
  return KernelSignature("sin_triple_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SinhGradOpArgumentMapping:

return KernelSignature("sinh_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SinhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("sinh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SlogdetGradOpArgumentMapping:

return KernelSignature("slogdet_grad", {"Input", "Out", "Out@GRAD"}, {}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature SlogdeterminantGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Input@GRAD"};
  return KernelSignature("slogdet_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftplusDoubleGradOpArgumentMapping:

return KernelSignature("softplus_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {"beta", "threshold"}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SoftplusDoubleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta");
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("softplus_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftplusGradOpArgumentMapping:

return KernelSignature("softplus_grad", {"X", "Out@GRAD"}, {"beta", "threshold"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SoftplusGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("beta");
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("softplus_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftshrinkGradOpArgumentMapping:

return KernelSignature("softshrink_grad", {"X", "Out@GRAD"}, {"lambda"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SoftshrinkGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("lambda");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("softshrink_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SoftsignGradOpArgumentMapping:

return KernelSignature("softsign_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SoftsignGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("softsign_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SolveGradOpArgumentMapping:

return KernelSignature("solve_grad", {"X", "Y", "Out", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature SolveGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("solve_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SpectralNormGradOpArgumentMapping:

return KernelSignature("spectral_norm_grad", {"Weight", "U", "V", "Out@GRAD"}, {"dim", "power_iters", "eps"}, {"Weight@GRAD"});
******************************************************************
*/

KernelSignature SpectralNormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Weight", "U", "V", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dim");
  attrs.emplace_back("power_iters");
  attrs.emplace_back("eps");
  paddle::small_vector<const char*> outputs {"Weight@GRAD"};
  return KernelSignature("spectral_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqrtDoubleGradOpArgumentMapping:

return KernelSignature("sqrt_double_grad", {"Out", "grad_x", "grad_x@GRAD"}, {}, {"Out@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SqrtGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "grad_x", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out@GRAD", "grad_out@GRAD"};
  return KernelSignature("sqrt_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqrtGradOpArgumentMapping:

return KernelSignature("sqrt_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SqrtGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("sqrt_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquareDoubleGradOpArgumentMapping:

return KernelSignature("square_double_grad", {"X", "grad_out", "grad_x@GRAD"}, {}, {"X@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature SquareGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "grad_out@GRAD"};
  return KernelSignature("square_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquareGradOpArgumentMapping:

return KernelSignature("square_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SquareGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("square_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SquaredL2NormGradOpArgumentMapping:

return KernelSignature("squared_l2_norm_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SquaredL2NormGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("squared_l2_norm_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SqueezeGradOpArgumentMapping:

return KernelSignature("squeeze_grad", {"XShape", "Out@GRAD"}, {"axes"}, {"X@GRAD"});
return KernelSignature("squeeze_grad", {"XShape", "Out@GRAD"}, {"AxisTensor"}, {"X@GRAD"});
return KernelSignature("squeeze_grad", {"XShape", "Out@GRAD"}, {"AxisTensorList"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Squeeze2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"XShape", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axes");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("squeeze_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StackGradOpArgumentMapping:

return KernelSignature("stack_grad", {"Y@GRAD"}, {"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature StackGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("stack_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by StanhGradOpArgumentMapping:

return KernelSignature("stanh_grad", {"X", "Out@GRAD"}, {"scale_a", "scale_b"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature StanhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("scale_a");
  attrs.emplace_back("scale_b");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("stanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SvdGradOpArgumentMapping:

return KernelSignature("svd_grad", {"X", "U", "VH", "S", "U@GRAD", "VH@GRAD", "S@GRAD"}, {"full_matrices"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature SvdGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "U", "VH", "S", "U@GRAD", "VH@GRAD", "S@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("full_matrices");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("svd_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by SwigluGradOpArgumentMapping:

return KernelSignature("swiglu_grad", {"x", "y", "out@GRAD"}, {}, {"x@GRAD", "y@GRAD"});
******************************************************************
*/

KernelSignature SwigluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "y", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"x@GRAD", "y@GRAD"};
  return KernelSignature("swiglu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TakeAlongAxisGradOpArgumentMapping:

return KernelSignature("take_along_axis_grad", {"Input", "Index", "Result@GRAD"}, {"Axis"}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature TakeAlongAxisGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Index", "Result@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("Axis");
  paddle::small_vector<const char*> outputs {"Input@GRAD"};
  return KernelSignature("take_along_axis_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanGradOpArgumentMapping:

return KernelSignature("tan_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TanGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("tan_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhDoubleGradOpArgumentMapping:

return KernelSignature("tanh_double_grad", {"Out", "grad_out", "grad_x@GRAD"}, {}, {"Out@GRAD", "grad_out@GRAD"});
******************************************************************
*/

KernelSignature TanhGradGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "grad_out", "grad_x@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out@GRAD", "grad_out@GRAD"};
  return KernelSignature("tanh_double_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhGradOpArgumentMapping:

return KernelSignature("tanh_grad", {"Out", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TanhGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("tanh_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhShrinkGradOpArgumentMapping:

return KernelSignature("tanh_shrink_grad", {"X", "Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TanhShrinkGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("tanh_shrink_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TanhTripleGradOpArgumentMapping:

return KernelSignature("tanh_triple_grad", {"Out", "grad_out_forward", "grad_x_grad_forward", "grad_out_new@GRAD", "grad_out_grad@GRAD"}, {}, {"Out@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"});
******************************************************************
*/

KernelSignature TanhTripleGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out", "grad_out_forward", "grad_x_grad_forward", "grad_out_new@GRAD", "grad_out_grad@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"Out@GRAD", "grad_out_forward@GRAD", "grad_x_grad_forward@GRAD"};
  return KernelSignature("tanh_triple_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TemporalShiftGradOpArgumentMapping:

return KernelSignature("temporal_shift_grad", {"Out@GRAD"}, {"seg_num", "shift_ratio", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TemporalShiftGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("seg_num");
  attrs.emplace_back("shift_ratio");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("temporal_shift_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TensorUnfoldGradOpArgumentMapping:

return KernelSignature("tensor_unfold_grad", {"input", "out@GRAD"}, {"axis", "size", "step"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature TensorUnfoldGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  attrs.emplace_back("size");
  attrs.emplace_back("step");
  paddle::small_vector<const char*> outputs {"input@GRAD"};
  return KernelSignature("tensor_unfold_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ThresholdedReluGradOpArgumentMapping:

return KernelSignature("thresholded_relu_grad", {"X", "Out@GRAD"}, {"threshold"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature ThresholdedReluGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("threshold");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("thresholded_relu_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TopkGradOpArgumentMapping:

return KernelSignature("topk_grad", {"X", "Indices", "Out@GRAD"}, {"k", "axis", "largest", "sorted"}, {"X@GRAD"});
return KernelSignature("topk_grad", {"X", "Indices", "Out@GRAD"}, {"K", "axis", "largest", "sorted"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TopKV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Indices", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back(ctx.HasInput("K") ? "K" : "k");
  attrs.emplace_back("axis");
  attrs.emplace_back("largest");
  attrs.emplace_back("sorted");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("topk_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TraceGradOpArgumentMapping:

return KernelSignature("trace_grad", {"Input", "Out@GRAD"}, {"offset", "axis1", "axis2"}, {"Input@GRAD"});
******************************************************************
*/

KernelSignature TraceGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Input", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("offset");
  attrs.emplace_back("axis1");
  attrs.emplace_back("axis2");
  paddle::small_vector<const char*> outputs {"Input@GRAD"};
  return KernelSignature("trace_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TriangularSolveGradOpArgumentMapping:

return KernelSignature("triangular_solve_grad", {"X", "Y", "Out", "Out@GRAD"}, {"upper", "transpose", "unitriangular"}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature TriangularSolveGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("upper");
  attrs.emplace_back("transpose");
  attrs.emplace_back("unitriangular");
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("triangular_solve_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TrilinearInterpGradOpArgumentMapping:

return KernelSignature("trilinear_interp_grad", {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"}, {"data_layout", "out_d", "out_h", "out_w", "scale", "interp_method", "align_corners", "align_mode"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TrilinearInterpV2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "OutSize", "SizeTensor", "Scale", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("data_layout");
  attrs.emplace_back("out_d");
  attrs.emplace_back("out_h");
  attrs.emplace_back("out_w");
  attrs.emplace_back("scale");
  attrs.emplace_back("interp_method");
  attrs.emplace_back("align_corners");
  attrs.emplace_back("align_mode");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("trilinear_interp_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by TruncGradOpArgumentMapping:

return KernelSignature("trunc_grad", {"Out@GRAD"}, {}, {"X@GRAD"});
******************************************************************
*/

KernelSignature TruncGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("trunc_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnfoldGradOpArgumentMapping:

return KernelSignature("unfold_grad", {"X", "Y@GRAD"}, {"kernel_sizes", "strides", "paddings", "dilations"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UnfoldGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("kernel_sizes");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("dilations");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("unfold_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UniformInplaceGradOpArgumentMapping:

return KernelSignature("uniform_inplace_grad", {"Out@GRAD"}, {"min", "max", "seed", "diag_num", "diag_step", "diag_val"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UniformRandomInplaceGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("min");
  attrs.emplace_back("max");
  attrs.emplace_back("seed");
  attrs.emplace_back("diag_num");
  attrs.emplace_back("diag_step");
  attrs.emplace_back("diag_val");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("uniform_inplace_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnsqueezeGradOpArgumentMapping:

return KernelSignature("unsqueeze_grad", {"XShape", "Out@GRAD"}, {"axes"}, {"X@GRAD"});
return KernelSignature("unsqueeze_grad", {"XShape", "Out@GRAD"}, {"AxesTensor"}, {"X@GRAD"});
return KernelSignature("unsqueeze_grad", {"XShape", "Out@GRAD"}, {"AxesTensorList"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Unsqueeze2GradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"XShape", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("unsqueeze_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by UnstackGradOpArgumentMapping:

return KernelSignature("unstack_grad", {"Y@GRAD"}, {"axis"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature UnstackGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Y@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("axis");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("unstack_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewDtypeGradOpArgumentMapping:

return KernelSignature("view_dtype_grad", {"input", "out@GRAD"}, {"dtype"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature ViewDtypeGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dtype");
  paddle::small_vector<const char*> outputs {"input@GRAD"};
  return KernelSignature("view_dtype_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by ViewShapeGradOpArgumentMapping:

return KernelSignature("view_shape_grad", {"input", "out@GRAD"}, {"dims"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature ViewShapeGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("dims");
  paddle::small_vector<const char*> outputs {"input@GRAD"};
  return KernelSignature("view_shape_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarpctcGradOpArgumentMapping:

return KernelSignature("warpctc_grad", {"Logits", "LogitsLength", "WarpCTCGrad", "Loss@GRAD"}, {"blank", "norm_by_times"}, {"Logits@GRAD"});
******************************************************************
*/

KernelSignature WarpctcGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Logits", "LogitsLength", "WarpCTCGrad", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("norm_by_times");
  paddle::small_vector<const char*> outputs {"Logits@GRAD"};
  return KernelSignature("warpctc_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WarprnntGradOpArgumentMapping:

return KernelSignature("warprnnt_grad", {"input", "input_lengths", "warprnntgrad", "loss@GRAD"}, {"blank", "fastemit_lambda"}, {"input@GRAD"});
******************************************************************
*/

KernelSignature WarprnntGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"input", "input_lengths", "warprnntgrad", "loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("blank");
  attrs.emplace_back("fastemit_lambda");
  paddle::small_vector<const char*> outputs {"input@GRAD"};
  return KernelSignature("warprnnt_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WeightOnlyLinearGradOpArgumentMapping:

return KernelSignature("weight_only_linear_grad", {"x", "weight", "bias", "weight_scale", "out@GRAD"}, {"weight_dtype", "arch", "group_size"}, {"x@GRAD"});
******************************************************************
*/

KernelSignature WeightOnlyLinearGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"x", "weight", "bias", "weight_scale", "out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("weight_dtype");
  attrs.emplace_back("arch");
  attrs.emplace_back("group_size");
  paddle::small_vector<const char*> outputs {"x@GRAD"};
  return KernelSignature("weight_only_linear_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by WhereGradOpArgumentMapping:

return KernelSignature("where_grad", {"Condition", "X", "Y", "Out@GRAD"}, {}, {"X@GRAD", "Y@GRAD"});
******************************************************************
*/

KernelSignature WhereGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"Condition", "X", "Y", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  paddle::small_vector<const char*> outputs {"X@GRAD", "Y@GRAD"};
  return KernelSignature("where_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by YoloLossGradOpArgumentMapping:

return KernelSignature("yolo_loss_grad", {"X", "GTBox", "GTLabel", "GTScore", "ObjectnessMask", "GTMatchMask", "Loss@GRAD"}, {"anchors", "anchor_mask", "class_num", "ignore_thresh", "downsample_ratio", "use_label_smooth", "scale_x_y"}, {"X@GRAD", "GTBox@GRAD", "GTLabel@GRAD", "GTScore@GRAD"});
******************************************************************
*/

KernelSignature Yolov3LossGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "GTBox", "GTLabel", "GTScore", "ObjectnessMask", "GTMatchMask", "Loss@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("anchors");
  attrs.emplace_back("anchor_mask");
  attrs.emplace_back("class_num");
  attrs.emplace_back("ignore_thresh");
  attrs.emplace_back("downsample_ratio");
  attrs.emplace_back("use_label_smooth");
  attrs.emplace_back("scale_x_y");
  paddle::small_vector<const char*> outputs {"X@GRAD", "GTBox@GRAD", "GTLabel@GRAD", "GTScore@GRAD"};
  return KernelSignature("yolo_loss_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

/*
******************************************************************
NOTE: The following codes are for 'get_compat_kernel_signature.py'
All possible KernelSignatures returned by Unpool3dGradOpArgumentMapping:

return KernelSignature("unpool3d_grad", {"X", "Indices", "Out", "Out@GRAD"}, {"ksize", "strides", "paddings", "output_size", "data_format"}, {"X@GRAD"});
******************************************************************
*/

KernelSignature Unpool3dGradOpArgumentMapping(const ArgumentMappingContext& ctx) {
  paddle::small_vector<const char*> inputs {"X", "Indices", "Out", "Out@GRAD"};
  paddle::small_vector<const char*> attrs;
  attrs.emplace_back("ksize");
  attrs.emplace_back("strides");
  attrs.emplace_back("paddings");
  attrs.emplace_back("output_size");
  attrs.emplace_back("data_format");
  paddle::small_vector<const char*> outputs {"X@GRAD"};
  return KernelSignature("unpool3d_grad", std::move(inputs), std::move(attrs), std::move(outputs));
}

}  // namespace phi

PD_REGISTER_ARG_MAPPING_FN(abs, phi::AbsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(accuracy, phi::AccuracyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acos, phi::AcosOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acosh, phi::AcoshOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(adagrad, phi::AdagradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(adamax, phi::AdamaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(adamw, phi::AdamwOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(addmm, phi::AddmmOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(affine_grid, phi::AffineGridOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(allclose, phi::AllcloseOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(angle, phi::AngleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(apply_per_channel_scale, phi::ApplyPerChannelScaleOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(arg_max, argmax);
PD_REGISTER_ARG_MAPPING_FN(arg_max, phi::ArgMaxOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(arg_min, argmin);
PD_REGISTER_ARG_MAPPING_FN(arg_min, phi::ArgMinOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(argsort, phi::ArgsortOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_complex, phi::AsComplexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_real, phi::AsRealOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_strided, phi::AsStridedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asgd, phi::AsgdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asin, phi::AsinOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asinh, phi::AsinhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan, phi::AtanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan2, phi::Atan2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atanh, phi::AtanhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(auc, phi::AucOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(average_accumulates, phi::AverageAccumulatesOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bce_loss, phi::BceLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bernoulli, phi::BernoulliOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bicubic_interp_v2, bicubic_interp);
PD_REGISTER_ARG_MAPPING_FN(bicubic_interp_v2, phi::BicubicInterpV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_tensor_product, bilinear);
PD_REGISTER_ARG_MAPPING_FN(bilinear_tensor_product, phi::BilinearTensorProductOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_interp_v2, bilinear_interp);
PD_REGISTER_ARG_MAPPING_FN(bilinear_interp_v2, phi::BilinearInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bincount, phi::BincountOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(binomial, phi::BinomialOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_and, phi::BitwiseAndOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_left_shift, phi::BitwiseLeftShiftOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_not, phi::BitwiseNotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_or, phi::BitwiseOrOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_right_shift, phi::BitwiseRightShiftOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bitwise_xor, phi::BitwiseXorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bmm, phi::BmmOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(box_coder, phi::BoxCoderOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(broadcast_tensors, phi::BroadcastTensorsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(ceil, phi::CeilOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(celu, phi::CeluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(check_finite_and_unscale, phi::CheckFiniteAndUnscaleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(check_numerics, phi::CheckNumericsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky, phi::CholeskyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky_solve, phi::CholeskySolveOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(class_center_sample, phi::ClassCenterSampleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip, phi::ClipOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip_by_norm, phi::ClipByNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(coalesce_tensor, phi::CoalesceTensorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(complex, phi::ComplexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(concat, phi::ConcatOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conj, phi::ConjOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d, phi::Conv2dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d, phi::Conv3dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d_transpose, phi::Conv3dTransposeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(copysign, phi::CopysignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos, phi::CosOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cosh, phi::CoshOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(crop_tensor, crop);
PD_REGISTER_ARG_MAPPING_FN(crop_tensor, phi::CropTensorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cross, phi::CrossOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(softmax_with_cross_entropy, cross_entropy_with_softmax);
PD_REGISTER_ARG_MAPPING_FN(softmax_with_cross_entropy, phi::SoftmaxWithCrossEntropyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummax, phi::CummaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummin, phi::CumminOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumprod, phi::CumprodOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumsum, phi::CumsumOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(data, phi::DataOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d, phi::DepthwiseConv2dOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(determinant, det);
PD_REGISTER_ARG_MAPPING_FN(determinant, phi::DeterminantOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(diag_v2, diag);
PD_REGISTER_ARG_MAPPING_FN(diag_v2, phi::DiagV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(diag_embed, phi::DiagEmbedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(diagonal, phi::DiagonalOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(digamma, phi::DigammaOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dirichlet, phi::DirichletOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dist, phi::DistOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dot, phi::DotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(edit_distance, phi::EditDistanceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eig, phi::EigOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigh, phi::EighOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigvals, phi::EigvalsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigvalsh, phi::EigvalshOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(elu, phi::EluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(equal_all, phi::EqualAllOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erf, phi::ErfOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erfinv, phi::ErfinvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(exp, phi::ExpOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(expand_as_v2, expand_as);
PD_REGISTER_ARG_MAPPING_FN(expand_as_v2, phi::ExpandAsV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(expm1, phi::Expm1OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2c, phi::FftC2cOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2r, phi::FftC2rOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_r2c, phi::FftR2cOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(fill_any, fill);
PD_REGISTER_ARG_MAPPING_FN(fill_any, phi::FillAnyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal, phi::FillDiagonalOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal_tensor, phi::FillDiagonalTensorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn, phi::FlashAttnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_unpadded, phi::FlashAttnUnpaddedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_with_sparse_mask, phi::FlashAttnWithSparseMaskOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flip, phi::FlipOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(floor, phi::FloorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fold, phi::FoldOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fractional_max_pool2d, phi::FractionalMaxPool2dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fractional_max_pool3d, phi::FractionalMaxPool3dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(frame, phi::FrameOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(full_int_array, phi::FullIntArrayOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gammaincc, phi::GammainccOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gammaln, phi::GammalnOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather, phi::GatherOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_nd, phi::GatherNdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_tree, phi::GatherTreeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gaussian_inplace, phi::GaussianInplaceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gelu, phi::GeluOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(generate_proposals_v2, generate_proposals);
PD_REGISTER_ARG_MAPPING_FN(generate_proposals_v2, phi::GenerateProposalsV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(graph_khop_sampler, phi::GraphKhopSamplerOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(graph_sample_neighbors, phi::GraphSampleNeighborsOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(grid_sampler, grid_sample);
PD_REGISTER_ARG_MAPPING_FN(grid_sampler, phi::GridSamplerOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(group_norm, phi::GroupNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gumbel_softmax, phi::GumbelSoftmaxOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_shrink, hardshrink);
PD_REGISTER_ARG_MAPPING_FN(hard_shrink, phi::HardShrinkOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_sigmoid, hardsigmoid);
PD_REGISTER_ARG_MAPPING_FN(hard_sigmoid, phi::HardSigmoidOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(brelu, hardtanh);
PD_REGISTER_ARG_MAPPING_FN(brelu, phi::BreluOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_heaviside, heaviside);
PD_REGISTER_ARG_MAPPING_FN(elementwise_heaviside, phi::ElementwiseHeavisideOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(histogram, phi::HistogramOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(huber_loss, phi::HuberLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0, phi::I0OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0e, phi::I0eOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1, phi::I1OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1e, phi::I1eOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(identity_loss, phi::IdentityLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(imag, phi::ImagOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_add, phi::IndexAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_put, phi::IndexPutOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_sample, phi::IndexSampleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select, phi::IndexSelectOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select_strided, phi::IndexSelectStridedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(instance_norm, phi::InstanceNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(inverse, phi::InverseOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(is_empty, phi::IsEmptyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(isclose, phi::IscloseOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(isfinite_v2, isfinite);
PD_REGISTER_ARG_MAPPING_FN(isfinite_v2, phi::IsfiniteV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(isinf_v2, isinf);
PD_REGISTER_ARG_MAPPING_FN(isinf_v2, phi::IsinfV2OpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(isnan_v2, isnan);
PD_REGISTER_ARG_MAPPING_FN(isnan_v2, phi::IsnanV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kldiv_loss, phi::KldivLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kron, phi::KronOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kthvalue, phi::KthvalueOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(label_smooth, phi::LabelSmoothOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lamb, phi::LambOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(layer_norm, phi::LayerNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(leaky_relu, phi::LeakyReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lerp, phi::LerpOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lgamma, phi::LgammaOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(linear_interp_v2, linear_interp);
PD_REGISTER_ARG_MAPPING_FN(linear_interp_v2, phi::LinearInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(llm_int8_linear, phi::LlmInt8LinearOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log, phi::LogOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log10, phi::Log10OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log1p, phi::Log1pOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log2, phi::Log2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_loss, phi::LogLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_softmax, phi::LogSoftmaxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logcumsumexp, phi::LogcumsumexpOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_and, phi::LogicalAndOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_not, phi::LogicalNotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_or, phi::LogicalOrOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logical_xor, phi::LogicalXorOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logit, phi::LogitOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logsigmoid, phi::LogsigmoidOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lstsq, phi::LstsqOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu, phi::LuOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu_unpack, phi::LuUnpackOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(margin_cross_entropy, phi::MarginCrossEntropyOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(masked_multihead_attention, phi::MaskedMultiheadAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(masked_select, phi::MaskedSelectOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(matrix_nms, phi::MatrixNmsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(matrix_power, phi::MatrixPowerOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool2d_with_index, phi::MaxPool2dWithIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool3d_with_index, phi::MaxPool3dWithIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(maxout, phi::MaxoutOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(mean, mean_all);
PD_REGISTER_ARG_MAPPING_FN(mean, phi::MeanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(memory_efficient_attention, phi::MemoryEfficientAttentionOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(merge_selected_rows, phi::MergeSelectedRowsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(merged_adam, phi::MergedAdamOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(merged_momentum, phi::MergedMomentumOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(meshgrid, phi::MeshgridOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mode, phi::ModeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(momentum, phi::MomentumOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multi_dot, phi::MultiDotOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multiclass_nms3, phi::MulticlassNms3OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multinomial, phi::MultinomialOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multiplex, phi::MultiplexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mv, phi::MvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nanmedian, phi::NanmedianOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(nearest_interp_v2, nearest_interp);
PD_REGISTER_ARG_MAPPING_FN(nearest_interp_v2, phi::NearestInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nextafter, phi::NextafterOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nll_loss, phi::NllLossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nms, phi::NmsOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(where_index, nonzero);
PD_REGISTER_ARG_MAPPING_FN(where_index, phi::WhereIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(npu_identity, phi::NpuIdentityOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(size, numel);
PD_REGISTER_ARG_MAPPING_FN(size, phi::SizeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(overlap_add, phi::OverlapAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(p_norm, phi::PNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pad3d, phi::Pad3dOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_shuffle, phi::PixelShuffleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_unshuffle, phi::PixelUnshuffleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(poisson, phi::PoissonOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(polygamma, phi::PolygammaOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow, phi::PowOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(prelu, phi::PreluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(prior_box, phi::PriorBoxOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(psroi_pool, phi::PsroiPoolOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(put_along_axis, phi::PutAlongAxisOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(qr, phi::QrOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(real, phi::RealOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(reciprocal, phi::ReciprocalOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_reindex, reindex_graph);
PD_REGISTER_ARG_MAPPING_FN(graph_reindex, phi::GraphReindexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu, phi::ReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu6, phi::Relu6OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(renorm, phi::RenormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rms_norm, phi::RmsNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rmsprop, phi::RmspropOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_align, phi::RoiAlignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_pool, phi::RoiPoolOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roll, phi::RollOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(round, phi::RoundOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rprop, phi::RpropOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rsqrt, phi::RsqrtOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scale, phi::ScaleOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter, phi::ScatterOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter_nd_add, phi::ScatterNdAddOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(searchsorted, phi::SearchsortedOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(segment_pool, phi::SegmentPoolOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(selu, phi::SeluOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_recv, send_u_recv);
PD_REGISTER_ARG_MAPPING_FN(graph_send_recv, phi::GraphSendRecvOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_ue_recv, send_ue_recv);
PD_REGISTER_ARG_MAPPING_FN(graph_send_ue_recv, phi::GraphSendUeRecvOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_uv, send_uv);
PD_REGISTER_ARG_MAPPING_FN(graph_send_uv, phi::GraphSendUvOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sgd, phi::SgdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(shape, phi::ShapeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(shard_index, phi::ShardIndexOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid, phi::SigmoidOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_cross_entropy_with_logits, phi::SigmoidCrossEntropyWithLogitsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sign, phi::SignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(silu, phi::SiluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin, phi::SinOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sinh, phi::SinhOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(slogdeterminant, slogdet);
PD_REGISTER_ARG_MAPPING_FN(slogdeterminant, phi::SlogdeterminantOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softplus, phi::SoftplusOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softshrink, phi::SoftshrinkOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softsign, phi::SoftsignOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(solve, phi::SolveOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(spectral_norm, phi::SpectralNormOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sqrt, phi::SqrtOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(square, phi::SquareOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(squared_l2_norm, phi::SquaredL2NormOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(squeeze2, squeeze);
PD_REGISTER_ARG_MAPPING_FN(squeeze2, phi::Squeeze2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stack, phi::StackOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(standard_gamma, phi::StandardGammaOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stanh, phi::StanhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(svd, phi::SvdOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(swiglu, phi::SwigluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(take_along_axis, phi::TakeAlongAxisOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tan, phi::TanOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh, phi::TanhOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_shrink, phi::TanhShrinkOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(temporal_shift, phi::TemporalShiftOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tensor_unfold, phi::TensorUnfoldOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(thresholded_relu, phi::ThresholdedReluOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(top_p_sampling, phi::TopPSamplingOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(top_k_v2, topk);
PD_REGISTER_ARG_MAPPING_FN(top_k_v2, phi::TopKV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trace, phi::TraceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(triangular_solve, phi::TriangularSolveOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(trilinear_interp_v2, trilinear_interp);
PD_REGISTER_ARG_MAPPING_FN(trilinear_interp_v2, phi::TrilinearInterpV2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trunc, phi::TruncOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unbind, phi::UnbindOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unfold, phi::UnfoldOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(uniform_random_inplace, uniform_inplace);
PD_REGISTER_ARG_MAPPING_FN(uniform_random_inplace, phi::UniformRandomInplaceOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unique_consecutive, phi::UniqueConsecutiveOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unpool3d, phi::Unpool3dOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(unsqueeze2, unsqueeze);
PD_REGISTER_ARG_MAPPING_FN(unsqueeze2, phi::Unsqueeze2OpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unstack, phi::UnstackOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(update_loss_scaling, phi::UpdateLossScalingOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_dtype, phi::ViewDtypeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_shape, phi::ViewShapeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(viterbi_decode, phi::ViterbiDecodeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warpctc, phi::WarpctcOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warprnnt, phi::WarprnntOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_dequantize, phi::WeightDequantizeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_only_linear, phi::WeightOnlyLinearOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_quantize, phi::WeightQuantizeOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weighted_sample_neighbors, phi::WeightedSampleNeighborsOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(where, phi::WhereOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(yolo_box, phi::YoloBoxOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(yolov3_loss, yolo_loss);
PD_REGISTER_ARG_MAPPING_FN(yolov3_loss, phi::Yolov3LossOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(abs_double_grad, phi::AbsDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(abs_grad, phi::AbsGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acos_grad, phi::AcosGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(acosh_grad, phi::AcoshGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(addmm_grad, phi::AddmmGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(affine_grid_grad, phi::AffineGridGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(angle_grad, phi::AngleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(argsort_grad, phi::ArgsortGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(as_strided_grad, phi::AsStridedGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asin_grad, phi::AsinGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(asinh_grad, phi::AsinhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan2_grad, phi::Atan2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atan_grad, phi::AtanGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(atanh_grad, phi::AtanhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bce_loss_grad, phi::BceLossGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bicubic_interp_v2_grad, bicubic_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(bicubic_interp_v2_grad, phi::BicubicInterpV2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_tensor_product_grad, bilinear_grad);
PD_REGISTER_ARG_MAPPING_FN(bilinear_tensor_product_grad, phi::BilinearTensorProductGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(bilinear_interp_v2_grad, bilinear_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(bilinear_interp_v2_grad, phi::BilinearInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(bmm_grad, phi::BmmGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(broadcast_tensors_grad, phi::BroadcastTensorsGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(ceil_grad, phi::CeilGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(celu_grad_grad, celu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(celu_grad_grad, phi::CeluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(celu_grad, phi::CeluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky_grad, phi::CholeskyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cholesky_solve_grad, phi::CholeskySolveGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip_double_grad, phi::ClipDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(clip_grad, phi::ClipGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(complex_grad, phi::ComplexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(concat_grad, phi::ConcatGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_grad, phi::Conv2dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv2d_grad_grad, phi::Conv2dGradGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(conv3d_grad_grad, conv3d_double_grad);
PD_REGISTER_ARG_MAPPING_FN(conv3d_grad_grad, phi::Conv3dGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d_grad, phi::Conv3dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(conv3d_transpose_grad, phi::Conv3dTransposeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(copysign_grad, phi::CopysignGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos_double_grad, phi::CosDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos_grad, phi::CosGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cos_triple_grad, phi::CosTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cosh_grad, phi::CoshGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(crop_tensor_grad, crop_grad);
PD_REGISTER_ARG_MAPPING_FN(crop_tensor_grad, phi::CropTensorGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(softmax_with_cross_entropy_grad, cross_entropy_with_softmax_grad);
PD_REGISTER_ARG_MAPPING_FN(softmax_with_cross_entropy_grad, phi::SoftmaxWithCrossEntropyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cross_grad, phi::CrossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummax_grad, phi::CummaxGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cummin_grad, phi::CumminGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumprod_grad, phi::CumprodGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(cumsum_grad, phi::CumsumGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(depthwise_conv2d_grad_grad, depthwise_conv2d_double_grad);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d_grad_grad, phi::DepthwiseConv2dGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(depthwise_conv2d_grad, phi::DepthwiseConv2dGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(determinant_grad, det_grad);
PD_REGISTER_ARG_MAPPING_FN(determinant_grad, phi::DeterminantGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(diag_v2_grad, diag_grad);
PD_REGISTER_ARG_MAPPING_FN(diag_v2_grad, phi::DiagV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(diagonal_grad, phi::DiagonalGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(digamma_grad, phi::DigammaGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dist_grad, phi::DistGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(dot_grad, phi::DotGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eig_grad, phi::EigGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigh_grad, phi::EighGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(eigvalsh_grad, phi::EigvalshGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elu_grad_grad, elu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(elu_grad_grad, phi::EluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(elu_grad, phi::EluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erf_grad, phi::ErfGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(erfinv_grad, phi::ErfinvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(exp_grad, phi::ExpGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(expand_as_v2_grad, expand_as_grad);
PD_REGISTER_ARG_MAPPING_FN(expand_as_v2_grad, phi::ExpandAsV2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(expand_v2_double_grad, expand_double_grad);
PD_REGISTER_ARG_MAPPING_FN(expm1_grad, phi::Expm1GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2c_grad, phi::FftC2cGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_c2r_grad, phi::FftC2rGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fft_r2c_grad, phi::FftR2cGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal_grad, phi::FillDiagonalGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fill_diagonal_tensor_grad, phi::FillDiagonalTensorGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(fill_any_grad, fill_grad);
PD_REGISTER_ARG_MAPPING_FN(fill_any_grad, phi::FillAnyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_grad, phi::FlashAttnGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_unpadded_grad, phi::FlashAttnUnpaddedGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(flash_attn_with_sparse_mask_grad, phi::FlashAttnWithSparseMaskGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(floor_grad, phi::FloorGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_fmax_grad, fmax_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_fmax_grad, phi::ElementwiseFmaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_fmin_grad, fmin_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_fmin_grad, phi::ElementwiseFminGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fold_grad, phi::FoldGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fractional_max_pool2d_grad, phi::FractionalMaxPool2dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(fractional_max_pool3d_grad, phi::FractionalMaxPool3dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(frame_grad, phi::FrameGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gammaincc_grad, phi::GammainccGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gammaln_grad, phi::GammalnGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_grad, phi::GatherGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gather_nd_grad, phi::GatherNdGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gaussian_inplace_grad, phi::GaussianInplaceGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gelu_grad, phi::GeluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(grid_sampler_grad, grid_sample_grad);
PD_REGISTER_ARG_MAPPING_FN(grid_sampler_grad, phi::GridSamplerGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(group_norm_grad, phi::GroupNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(gumbel_softmax_grad, phi::GumbelSoftmaxGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_shrink_grad, hardshrink_grad);
PD_REGISTER_ARG_MAPPING_FN(hard_shrink_grad, phi::HardShrinkGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(hard_sigmoid_grad, hardsigmoid_grad);
PD_REGISTER_ARG_MAPPING_FN(hard_sigmoid_grad, phi::HardSigmoidGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(brelu_grad, hardtanh_grad);
PD_REGISTER_ARG_MAPPING_FN(brelu_grad, phi::BreluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(elementwise_heaviside_grad, heaviside_grad);
PD_REGISTER_ARG_MAPPING_FN(elementwise_heaviside_grad, phi::ElementwiseHeavisideGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(huber_loss_grad, phi::HuberLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0_grad, phi::I0GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i0e_grad, phi::I0eGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1_grad, phi::I1GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(i1e_grad, phi::I1eGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(identity_loss_grad, phi::IdentityLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(imag_grad, phi::ImagGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_add_grad, phi::IndexAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_put_grad, phi::IndexPutGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_sample_grad, phi::IndexSampleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select_grad, phi::IndexSelectGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(index_select_strided_grad, phi::IndexSelectStridedGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(instance_norm_double_grad, phi::InstanceNormDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(instance_norm_grad, phi::InstanceNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(inverse_grad, phi::InverseGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kldiv_loss_grad, phi::KldivLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kron_grad, phi::KronGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(kthvalue_grad, phi::KthvalueGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(label_smooth_grad, phi::LabelSmoothGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(layer_norm_grad, phi::LayerNormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(leaky_relu_grad_grad, leaky_relu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(leaky_relu_grad_grad, phi::LeakyReluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(leaky_relu_grad, phi::LeakyReluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lerp_grad, phi::LerpGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lgamma_grad, phi::LgammaGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(linear_interp_v2_grad, linear_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(linear_interp_v2_grad, phi::LinearInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log10_grad, phi::Log10GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log1p_grad, phi::Log1pGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log2_grad, phi::Log2GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(log_grad_grad, log_double_grad);
PD_REGISTER_ARG_MAPPING_FN(log_grad_grad, phi::LogGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_grad, phi::LogGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_loss_grad, phi::LogLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(log_softmax_grad, phi::LogSoftmaxGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logcumsumexp_grad, phi::LogcumsumexpGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logit_grad, phi::LogitGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(logsigmoid_grad, phi::LogsigmoidGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu_grad, phi::LuGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(lu_unpack_grad, phi::LuUnpackGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(margin_cross_entropy_grad, phi::MarginCrossEntropyGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(masked_select_grad, phi::MaskedSelectGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(matrix_power_grad, phi::MatrixPowerGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool2d_with_index_grad, phi::MaxPool2dWithIndexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(max_pool3d_with_index_grad, phi::MaxPool3dWithIndexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(maxout_grad, phi::MaxoutGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(mean_grad, mean_all_grad);
PD_REGISTER_ARG_MAPPING_FN(mean_grad, phi::MeanGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(memory_efficient_attention_grad, phi::MemoryEfficientAttentionGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(meshgrid_grad, phi::MeshgridGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mode_grad, phi::ModeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multi_dot_grad, phi::MultiDotGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(multiplex_grad, phi::MultiplexGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(mv_grad, phi::MvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nanmedian_grad, phi::NanmedianGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(nearest_interp_v2_grad, nearest_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(nearest_interp_v2_grad, phi::NearestInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(nll_loss_grad, phi::NllLossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(overlap_add_grad, phi::OverlapAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(p_norm_grad, phi::PNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pad3d_double_grad, phi::Pad3dDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pad3d_grad, phi::Pad3dGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_shuffle_grad, phi::PixelShuffleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pixel_unshuffle_grad, phi::PixelUnshuffleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(poisson_grad, phi::PoissonGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(polygamma_grad, phi::PolygammaGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow_double_grad, phi::PowDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow_grad, phi::PowGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(pow_triple_grad, phi::PowTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(prelu_grad, phi::PreluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(psroi_pool_grad, phi::PsroiPoolGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(put_along_axis_grad, phi::PutAlongAxisGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(qr_grad, phi::QrGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(real_grad, phi::RealGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(reciprocal_grad, phi::ReciprocalGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu6_grad, phi::Relu6GradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(relu_grad_grad, relu_double_grad);
PD_REGISTER_ARG_MAPPING_FN(relu_grad_grad, phi::ReluGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(relu_grad, phi::ReluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(renorm_grad, phi::RenormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rms_norm_grad, phi::RmsNormGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_align_grad, phi::RoiAlignGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roi_pool_grad, phi::RoiPoolGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(roll_grad, phi::RollGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(round_grad, phi::RoundGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(rsqrt_grad_grad, rsqrt_double_grad);
PD_REGISTER_ARG_MAPPING_FN(rsqrt_grad_grad, phi::RsqrtGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(rsqrt_grad, phi::RsqrtGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter_grad, phi::ScatterGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(scatter_nd_add_grad, phi::ScatterNdAddGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(segment_pool_grad, phi::SegmentPoolGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(selu_grad, phi::SeluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_recv_grad, send_u_recv_grad);
PD_REGISTER_ARG_MAPPING_FN(graph_send_recv_grad, phi::GraphSendRecvGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_ue_recv_grad, send_ue_recv_grad);
PD_REGISTER_ARG_MAPPING_FN(graph_send_ue_recv_grad, phi::GraphSendUeRecvGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(graph_send_uv_grad, send_uv_grad);
PD_REGISTER_ARG_MAPPING_FN(graph_send_uv_grad, phi::GraphSendUvGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_cross_entropy_with_logits_grad, phi::SigmoidCrossEntropyWithLogitsGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(sigmoid_grad_grad, sigmoid_double_grad);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_grad_grad, phi::SigmoidGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_grad, phi::SigmoidGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sigmoid_triple_grad, phi::SigmoidTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(silu_grad, phi::SiluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin_double_grad, phi::SinDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin_grad, phi::SinGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sin_triple_grad, phi::SinTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sinh_grad, phi::SinhGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(slogdeterminant_grad, slogdet_grad);
PD_REGISTER_ARG_MAPPING_FN(slogdeterminant_grad, phi::SlogdeterminantGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softplus_double_grad, phi::SoftplusDoubleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softplus_grad, phi::SoftplusGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softshrink_grad, phi::SoftshrinkGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(softsign_grad, phi::SoftsignGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(solve_grad, phi::SolveGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(spectral_norm_grad, phi::SpectralNormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(sqrt_grad_grad, sqrt_double_grad);
PD_REGISTER_ARG_MAPPING_FN(sqrt_grad_grad, phi::SqrtGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(sqrt_grad, phi::SqrtGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(square_grad_grad, square_double_grad);
PD_REGISTER_ARG_MAPPING_FN(square_grad_grad, phi::SquareGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(square_grad, phi::SquareGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(squared_l2_norm_grad, phi::SquaredL2NormGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(squeeze2_double_grad, squeeze_double_grad);
PD_REGISTER_BASE_KERNEL_NAME(squeeze2_grad, squeeze_grad);
PD_REGISTER_ARG_MAPPING_FN(squeeze2_grad, phi::Squeeze2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stack_grad, phi::StackGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(stanh_grad, phi::StanhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(svd_grad, phi::SvdGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(swiglu_grad, phi::SwigluGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(take_along_axis_grad, phi::TakeAlongAxisGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tan_grad, phi::TanGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(tanh_grad_grad, tanh_double_grad);
PD_REGISTER_ARG_MAPPING_FN(tanh_grad_grad, phi::TanhGradGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_grad, phi::TanhGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_shrink_grad, phi::TanhShrinkGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tanh_triple_grad, phi::TanhTripleGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(temporal_shift_grad, phi::TemporalShiftGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(tensor_unfold_grad, phi::TensorUnfoldGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(thresholded_relu_grad, phi::ThresholdedReluGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(top_k_v2_grad, topk_grad);
PD_REGISTER_ARG_MAPPING_FN(top_k_v2_grad, phi::TopKV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trace_grad, phi::TraceGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(triangular_solve_grad, phi::TriangularSolveGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(trilinear_interp_v2_grad, trilinear_interp_grad);
PD_REGISTER_ARG_MAPPING_FN(trilinear_interp_v2_grad, phi::TrilinearInterpV2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(trunc_grad, phi::TruncGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unfold_grad, phi::UnfoldGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(uniform_random_inplace_grad, uniform_inplace_grad);
PD_REGISTER_ARG_MAPPING_FN(uniform_random_inplace_grad, phi::UniformRandomInplaceGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(unsqueeze2_double_grad, unsqueeze_double_grad);
PD_REGISTER_BASE_KERNEL_NAME(unsqueeze2_grad, unsqueeze_grad);
PD_REGISTER_ARG_MAPPING_FN(unsqueeze2_grad, phi::Unsqueeze2GradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unstack_grad, phi::UnstackGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_dtype_grad, phi::ViewDtypeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(view_shape_grad, phi::ViewShapeGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warpctc_grad, phi::WarpctcGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(warprnnt_grad, phi::WarprnntGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(weight_only_linear_grad, phi::WeightOnlyLinearGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(where_grad, phi::WhereGradOpArgumentMapping);
PD_REGISTER_BASE_KERNEL_NAME(yolov3_loss_grad, yolo_loss_grad);
PD_REGISTER_ARG_MAPPING_FN(yolov3_loss_grad, phi::Yolov3LossGradOpArgumentMapping);
PD_REGISTER_ARG_MAPPING_FN(unpool3d_grad, phi::Unpool3dGradOpArgumentMapping);
