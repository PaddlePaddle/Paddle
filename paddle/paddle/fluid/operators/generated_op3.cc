// this file is generated by paddle/phi/api/yaml/generator/generate_op.py, do not edit.
#include <string>
#include "paddle/fluid/framework/convert_utils.h"
#include "paddle/fluid/framework/infershape_utils.h"
#include "paddle/fluid/framework/op_registry.h"
#include "paddle/fluid/framework/op_version_registry.h"
#include "paddle/fluid/prim/api/composite_backward/composite_backward_api.h"
#include "paddle/fluid/prim/utils/static/composite_grad_desc_maker.h"
#include "paddle/fluid/prim/utils/static/desc_tensor.h"
#include "paddle/fluid/operators/generator/get_expected_kernel_func.h"
#include "paddle/phi/core/infermeta_utils.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/binary.h"
#include "paddle/phi/infermeta/fusion.h"
#include "paddle/phi/infermeta/multiary.h"
#include "paddle/phi/infermeta/nullary.h"
#include "paddle/phi/infermeta/ternary.h"
#include "paddle/phi/infermeta/unary.h"

namespace paddle {
namespace operators {

using paddle::framework::GradVarName;


class KthvalueOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of kthvalue op.");
    AddOutput("Out", "(Tensor), output 0 of kthvalue op.");
    AddOutput("Indices", "(Tensor), output 1 of kthvalue op.");
    AddAttr<int>("k", "(int), attribute 0 for kthvalue op.")
        .SetDefault(1);
    AddAttr<int>("axis", "(int), attribute 1 for kthvalue op.")
        .SetDefault(-1);
    AddAttr<bool>("keepdim", "(bool), attribute 2 for kthvalue op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of kthvalue op.
)DOC");
  }
};


class KthvalueOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(kthvalue, KthvalueInferShapeFunctor,
                            PD_INFER_META(phi::KthvalueInferMeta));



class LabelSmoothOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of label_smooth op.");
    AddInput("PriorDist", "(Tensor), input 1 of label_smooth op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of label_smooth op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for label_smooth op.")
        .SetDefault(0.0f);
    AddComment(R"DOC(
TODO: Documentation of label_smooth op.
)DOC");
  }
};


class LabelSmoothOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(label_smooth, LabelSmoothInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class LambOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Param", "(Tensor), input 0 of lamb op.");
    AddInput("Grad", "(Tensor), input 1 of lamb op.");
    AddInput("LearningRate", "(Tensor), input 2 of lamb op.");
    AddInput("Moment1", "(Tensor), input 3 of lamb op.");
    AddInput("Moment2", "(Tensor), input 4 of lamb op.");
    AddInput("Beta1Pow", "(Tensor), input 5 of lamb op.");
    AddInput("Beta2Pow", "(Tensor), input 6 of lamb op.");
    AddInput("MasterParam", "(Tensor), input 7 of lamb op.")
        .AsDispensable();
    AddInput("SkipUpdate", "(Tensor), input 8 of lamb op.")
        .AsDispensable();
    AddOutput("ParamOut", "(Tensor), output 0 of lamb op.");
    AddOutput("Moment1Out", "(Tensor), output 1 of lamb op.");
    AddOutput("Moment2Out", "(Tensor), output 2 of lamb op.");
    AddOutput("Beta1PowOut", "(Tensor), output 3 of lamb op.")
        .AsDispensable();
    AddOutput("Beta2PowOut", "(Tensor), output 4 of lamb op.")
        .AsDispensable();
    AddOutput("MasterParamOut", "(Tensor), output 5 of lamb op.")
        .AsDispensable();
    AddAttr<float>("weight_decay", "(float), attribute 0 for lamb op.")
    ;
    AddAttr<float>("beta1", "(float), attribute 1 for lamb op.")
        .SetDefault(0.9);
    AddAttr<float>("beta2", "(float), attribute 2 for lamb op.")
        .SetDefault(0.999);
    AddAttr<float>("epsilon", "(float), attribute 3 for lamb op.")
        .SetDefault(1.0e-6f);
    AddAttr<bool>("always_adapt", "(bool), attribute 4 for lamb op.")
        .SetDefault(false);
    AddAttr<bool>("multi_precision", "(bool), attribute 5 for lamb op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of lamb op.
)DOC");
  }
};


class LambOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Param");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(lamb, LambInferShapeFunctor,
                            PD_INFER_META(phi::LambInferMeta));



class LayerNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of layer_norm op.");
    AddInput("Scale", "(Tensor), input 1 of layer_norm op.")
        .AsDispensable();
    AddInput("Bias", "(Tensor), input 2 of layer_norm op.")
        .AsDispensable();
    AddOutput("Y", "(Tensor), output 0 of layer_norm op.");
    AddOutput("Mean", "(Tensor), output 1 of layer_norm op.")
        .AsIntermediate();
    AddOutput("Variance", "(Tensor), output 2 of layer_norm op.")
        .AsIntermediate();
    AddAttr<float>("epsilon", "(float), attribute 0 for layer_norm op.")
        .SetDefault(1e-5);
    AddAttr<int>("begin_norm_axis", "(int), attribute 1 for layer_norm op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of layer_norm op.
)DOC");
  }
};


class LayerNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetLayerNormExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(layer_norm, LayerNormInferShapeFunctor,
                            PD_INFER_META(phi::LayerNormInferMeta));



class LeakyReluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of leaky_relu op.");
    AddOutput("Out", "(Tensor), output 0 of leaky_relu op.");
    AddAttr<float>("alpha", "(float), attribute 0 for leaky_relu op.")
        .SetDefault(0.02f);
    AddComment(R"DOC(
TODO: Documentation of leaky_relu op.
)DOC");
  }
};


class LeakyReluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(leaky_relu, LeakyReluInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LeakyReluInplaceInferer,
                           {"X", "Out"});



class LerpOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of lerp op.");
    AddInput("Y", "(Tensor), input 1 of lerp op.");
    AddInput("Weight", "(Tensor), input 2 of lerp op.");
    AddOutput("Out", "(Tensor), output 0 of lerp op.");
    AddComment(R"DOC(
TODO: Documentation of lerp op.
)DOC");
  }
};


class LerpOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(lerp, LerpInferShapeFunctor,
                            PD_INFER_META(phi::LerpInferMeta));
DECLARE_INPLACE_OP_INFERER(LerpInplaceInferer,
                           {"X", "Out"});



class LgammaOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of lgamma op.");
    AddOutput("Out", "(Tensor), output 0 of lgamma op.");
    AddComment(R"DOC(
TODO: Documentation of lgamma op.
)DOC");
  }
};


class LgammaOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(lgamma, LgammaInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LgammaInplaceInferer,
                           {"X", "Out"});



class LinearInterpV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of linear_interp_v2 op.");
    AddInput("OutSize", "(Tensor), input 1 of linear_interp_v2 op.")
        .AsDispensable();
    AddInput("SizeTensor", "(Tensor[]), input 2 of linear_interp_v2 op.")
        .AsDuplicable()
        .AsDispensable();
    AddInput("Scale", "(Tensor), input 3 of linear_interp_v2 op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of linear_interp_v2 op.");
    AddAttr<std::string>("data_layout", "(std::string), attribute 0 for linear_interp_v2 op.")
        .SetDefault("NCHW");
    AddAttr<int>("out_d", "(int), attribute 1 for linear_interp_v2 op.")
        .SetDefault(0);
    AddAttr<int>("out_h", "(int), attribute 2 for linear_interp_v2 op.")
        .SetDefault(0);
    AddAttr<int>("out_w", "(int), attribute 3 for linear_interp_v2 op.")
        .SetDefault(0);
    AddAttr<std::vector<float>>("scale", "(std::vector<float>), attribute 4 for linear_interp_v2 op.")
        .SetDefault({});
    AddAttr<std::string>("interp_method", "(std::string), attribute 5 for linear_interp_v2 op.")
        .SetDefault("bilinear");
    AddAttr<bool>("align_corners", "(bool), attribute 6 for linear_interp_v2 op.")
        .SetDefault(true);
    AddAttr<int>("align_mode", "(int), attribute 7 for linear_interp_v2 op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of linear_interp_v2 op.
)DOC");
  }
};


class LinearInterpV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "OutSize" || var_name == "SizeTensor" || var_name == "Scale") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(linear_interp_v2, LinearInterpV2InferShapeFunctor,
                            PD_INFER_META(phi::InterpolateInferMeta));



class LlmInt8LinearOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of llm_int8_linear op.");
    AddInput("weight", "(Tensor), input 1 of llm_int8_linear op.");
    AddInput("bias", "(Tensor), input 2 of llm_int8_linear op.")
        .AsDispensable();
    AddInput("weight_scale", "(Tensor), input 3 of llm_int8_linear op.");
    AddOutput("out", "(Tensor), output 0 of llm_int8_linear op.");
    AddAttr<float>("threshold", "(float), attribute 0 for llm_int8_linear op.")
        .SetDefault(6.0);
    AddComment(R"DOC(
TODO: Documentation of llm_int8_linear op.
)DOC");
  }
};


class LlmInt8LinearOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(llm_int8_linear, LlmInt8LinearInferShapeFunctor,
                            PD_INFER_META(phi::LLMInt8LinearInferMeta));



class LogOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of log op.");
    AddOutput("Out", "(Tensor), output 0 of log op.");
    AddComment(R"DOC(
TODO: Documentation of log op.
)DOC");
  }
};


class LogOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log, LogInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LogInplaceInferer,
                           {"X", "Out"});



class Log10OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of log10 op.");
    AddOutput("Out", "(Tensor), output 0 of log10 op.");
    AddComment(R"DOC(
TODO: Documentation of log10 op.
)DOC");
  }
};


class Log10Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log10, Log10InferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Log10InplaceInferer,
                           {"X", "Out"});



class Log1pOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of log1p op.");
    AddOutput("Out", "(Tensor), output 0 of log1p op.");
    AddComment(R"DOC(
TODO: Documentation of log1p op.
)DOC");
  }
};


class Log1pOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log1p, Log1pInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Log1pInplaceInferer,
                           {"X", "Out"});



class Log2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of log2 op.");
    AddOutput("Out", "(Tensor), output 0 of log2 op.");
    AddComment(R"DOC(
TODO: Documentation of log2 op.
)DOC");
  }
};


class Log2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log2, Log2InferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Log2InplaceInferer,
                           {"X", "Out"});



class LogLossOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Predicted", "(Tensor), input 0 of log_loss op.");
    AddInput("Labels", "(Tensor), input 1 of log_loss op.");
    AddOutput("Loss", "(Tensor), output 0 of log_loss op.");
    AddAttr<float>("epsilon", "(float), attribute 0 for log_loss op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of log_loss op.
)DOC");
  }
};


class LogLossOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log_loss, LogLossInferShapeFunctor,
                            PD_INFER_META(phi::LogLossInferMeta));



class LogSoftmaxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of log_softmax op.");
    AddOutput("Out", "(Tensor), output 0 of log_softmax op.");
    AddAttr<int>("axis", "(int), attribute 0 for log_softmax op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of log_softmax op.
)DOC");
  }
};


class LogSoftmaxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(log_softmax, LogSoftmaxInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMetaCheckAxis));



class LogcumsumexpOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logcumsumexp op.");
    AddOutput("Out", "(Tensor), output 0 of logcumsumexp op.");
    AddAttr<int>("axis", "(int), attribute 0 for logcumsumexp op.")
        .SetDefault(-1);
    AddAttr<bool>("flatten", "(bool), attribute 1 for logcumsumexp op.")
        .SetDefault(false);
    AddAttr<bool>("exclusive", "(bool), attribute 2 for logcumsumexp op.")
        .SetDefault(false);
    AddAttr<bool>("reverse", "(bool), attribute 3 for logcumsumexp op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of logcumsumexp op.
)DOC");
  }
};


class LogcumsumexpOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(logcumsumexp, LogcumsumexpInferShapeFunctor,
                            PD_INFER_META(phi::CumInferMeta));



class LogicalAndOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logical_and op.");
    AddInput("Y", "(Tensor), input 1 of logical_and op.");
    AddOutput("Out", "(Tensor), output 0 of logical_and op.");
    AddComment(R"DOC(
TODO: Documentation of logical_and op.
)DOC");
  }
};


class LogicalAndOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(logical_and, LogicalAndInferShapeFunctor,
                            PD_INFER_META(phi::LogicalBinaryInferMeta));
DECLARE_INPLACE_OP_INFERER(LogicalAndInplaceInferer,
                           {"X", "Out"});



class LogicalNotOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logical_not op.");
    AddOutput("Out", "(Tensor), output 0 of logical_not op.");
    AddComment(R"DOC(
TODO: Documentation of logical_not op.
)DOC");
  }
};


class LogicalNotOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(logical_not, LogicalNotInferShapeFunctor,
                            PD_INFER_META(phi::LogicalNotInferMeta));
DECLARE_INPLACE_OP_INFERER(LogicalNotInplaceInferer,
                           {"X", "Out"});



class LogicalOrOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logical_or op.");
    AddInput("Y", "(Tensor), input 1 of logical_or op.");
    AddOutput("Out", "(Tensor), output 0 of logical_or op.");
    AddComment(R"DOC(
TODO: Documentation of logical_or op.
)DOC");
  }
};


class LogicalOrOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(logical_or, LogicalOrInferShapeFunctor,
                            PD_INFER_META(phi::LogicalBinaryInferMeta));
DECLARE_INPLACE_OP_INFERER(LogicalOrInplaceInferer,
                           {"X", "Out"});



class LogicalXorOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logical_xor op.");
    AddInput("Y", "(Tensor), input 1 of logical_xor op.");
    AddOutput("Out", "(Tensor), output 0 of logical_xor op.");
    AddComment(R"DOC(
TODO: Documentation of logical_xor op.
)DOC");
  }
};


class LogicalXorOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
      kt.set_backend(
          phi::TransToPhiBackend(ctx.Input<phi::DenseTensor>("X")->place()));
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(logical_xor, LogicalXorInferShapeFunctor,
                            PD_INFER_META(phi::LogicalBinaryInferMeta));
DECLARE_INPLACE_OP_INFERER(LogicalXorInplaceInferer,
                           {"X", "Out"});



class LogitOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logit op.");
    AddOutput("Out", "(Tensor), output 0 of logit op.");
    AddAttr<float>("eps", "(float), attribute 0 for logit op.")
        .SetDefault(1e-6f);
    AddComment(R"DOC(
TODO: Documentation of logit op.
)DOC");
  }
};


class LogitOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(logit, LogitInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LogitInplaceInferer,
                           {"X", "Out"});



class LogsigmoidOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of logsigmoid op.");
    AddOutput("Out", "(Tensor), output 0 of logsigmoid op.");
    AddComment(R"DOC(
TODO: Documentation of logsigmoid op.
)DOC");
  }
};


class LogsigmoidOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(logsigmoid, LogsigmoidInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class LstsqOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of lstsq op.");
    AddInput("Y", "(Tensor), input 1 of lstsq op.");
    AddOutput("Solution", "(Tensor), output 0 of lstsq op.");
    AddOutput("Residuals", "(Tensor), output 1 of lstsq op.")
        .AsDispensable();
    AddOutput("Rank", "(Tensor), output 2 of lstsq op.");
    AddOutput("SingularValues", "(Tensor), output 3 of lstsq op.");
    AddAttr<float>("rcond", "(float), attribute 0 for lstsq op.")
        .SetDefault(0.0f)
        .SupportTensor();
    AddAttr<std::string>("driver", "(std::string), attribute 1 for lstsq op.")
        .SetDefault("gels");
    AddComment(R"DOC(
TODO: Documentation of lstsq op.
)DOC");
  }
};


class LstsqOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(lstsq, LstsqInferShapeFunctor,
                            PD_INFER_META(phi::LstsqInferMeta));



class LuOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of lu op.");
    AddOutput("Out", "(Tensor), output 0 of lu op.");
    AddOutput("Pivots", "(Tensor), output 1 of lu op.");
    AddOutput("Infos", "(Tensor), output 2 of lu op.");
    AddAttr<bool>("pivots", "(bool), attribute 0 for lu op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of lu op.
)DOC");
  }
};


class LuOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(lu, LuInferShapeFunctor,
                            PD_INFER_META(phi::LUInferMeta));
DECLARE_INPLACE_OP_INFERER(LuInplaceInferer,
                           {"X", "Out"});



class LuUnpackOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of lu_unpack op.");
    AddInput("Pivots", "(Tensor), input 1 of lu_unpack op.");
    AddOutput("Pmat", "(Tensor), output 0 of lu_unpack op.");
    AddOutput("L", "(Tensor), output 1 of lu_unpack op.");
    AddOutput("U", "(Tensor), output 2 of lu_unpack op.");
    AddAttr<bool>("unpack_ludata", "(bool), attribute 0 for lu_unpack op.")
        .SetDefault(true);
    AddAttr<bool>("unpack_pivots", "(bool), attribute 1 for lu_unpack op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of lu_unpack op.
)DOC");
  }
};


class LuUnpackOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(lu_unpack, LuUnpackInferShapeFunctor,
                            PD_INFER_META(phi::LUUnpackInferMeta));



class MarginCrossEntropyOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Logits", "(Tensor), input 0 of margin_cross_entropy op.");
    AddInput("Label", "(Tensor), input 1 of margin_cross_entropy op.");
    AddOutput("Softmax", "(Tensor), output 0 of margin_cross_entropy op.");
    AddOutput("Loss", "(Tensor), output 1 of margin_cross_entropy op.");
    AddAttr<bool>("return_softmax", "(bool), attribute 0 for margin_cross_entropy op.")
        .SetDefault(false);
    AddAttr<int>("ring_id", "(int), attribute 1 for margin_cross_entropy op.")
        .SetDefault(0);
    AddAttr<int>("rank", "(int), attribute 2 for margin_cross_entropy op.")
        .SetDefault(0);
    AddAttr<int>("nranks", "(int), attribute 3 for margin_cross_entropy op.")
        .SetDefault(1);
    AddAttr<float>("margin1", "(float), attribute 4 for margin_cross_entropy op.")
        .SetDefault(1.0f);
    AddAttr<float>("margin2", "(float), attribute 5 for margin_cross_entropy op.")
        .SetDefault(0.5f);
    AddAttr<float>("margin3", "(float), attribute 6 for margin_cross_entropy op.")
        .SetDefault(0.0f);
    AddAttr<float>("scale", "(float), attribute 7 for margin_cross_entropy op.")
        .SetDefault(64.0f);
    AddComment(R"DOC(
TODO: Documentation of margin_cross_entropy op.
)DOC");
  }
};


class MarginCrossEntropyOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Logits");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(margin_cross_entropy, MarginCrossEntropyInferShapeFunctor,
                            PD_INFER_META(phi::MarginCrossEntropyInferMeta));



class MaskedMultiheadAttentionOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of masked_multihead_attention op.");
    AddInput("cache_kv", "(Tensor), input 1 of masked_multihead_attention op.");
    AddInput("bias", "(Tensor), input 2 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("src_mask", "(Tensor), input 3 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("cum_offsets", "(Tensor), input 4 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("sequence_lengths", "(Tensor), input 5 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("rotary_tensor", "(Tensor), input 6 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("beam_cache_offset", "(Tensor), input 7 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("qkv_out_scale", "(Tensor), input 8 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("out_shift", "(Tensor), input 9 of masked_multihead_attention op.")
        .AsDispensable();
    AddInput("out_smooth", "(Tensor), input 10 of masked_multihead_attention op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of masked_multihead_attention op.");
    AddOutput("cache_kv_out", "(Tensor), output 1 of masked_multihead_attention op.");
    AddOutput("beam_cache_offset_out", "(Tensor), output 2 of masked_multihead_attention op.");
    AddAttr<int>("seq_len", "(int), attribute 0 for masked_multihead_attention op.")
    ;
    AddAttr<int>("rotary_emb_dims", "(int), attribute 1 for masked_multihead_attention op.")
    ;
    AddAttr<bool>("use_neox_rotary_style", "(bool), attribute 2 for masked_multihead_attention op.")
        .SetDefault(false);
    AddAttr<std::string>("compute_dtype", "(std::string), attribute 3 for masked_multihead_attention op.")
        .SetDefault("default");
    AddAttr<float>("out_scale", "(float), attribute 4 for masked_multihead_attention op.")
        .SetDefault(-1);
    AddAttr<int>("quant_round_type", "(int), attribute 5 for masked_multihead_attention op.")
        .SetDefault(1);
    AddAttr<float>("quant_max_bound", "(float), attribute 6 for masked_multihead_attention op.")
        .SetDefault(127.0);
    AddAttr<float>("quant_min_bound", "(float), attribute 7 for masked_multihead_attention op.")
        .SetDefault(-127.0);
    AddComment(R"DOC(
TODO: Documentation of masked_multihead_attention op.
)DOC");
  }
};


class MaskedMultiheadAttentionOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(masked_multihead_attention, MaskedMultiheadAttentionInferShapeFunctor,
                            PD_INFER_META(phi::MaskedMultiheadAttentionInferMeta));



class MaskedSelectOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of masked_select op.");
    AddInput("Mask", "(Tensor), input 1 of masked_select op.");
    AddOutput("Y", "(Tensor), output 0 of masked_select op.");
    AddComment(R"DOC(
TODO: Documentation of masked_select op.
)DOC");
  }
};


class MaskedSelectOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(masked_select, MaskedSelectInferShapeFunctor,
                            PD_INFER_META(phi::MaskedSelectInferMeta));



class MatrixNmsOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("BBoxes", "(Tensor), input 0 of matrix_nms op.");
    AddInput("Scores", "(Tensor), input 1 of matrix_nms op.");
    AddOutput("Out", "(Tensor), output 0 of matrix_nms op.");
    AddOutput("Index", "(Tensor), output 1 of matrix_nms op.");
    AddOutput("RoisNum", "(Tensor), output 2 of matrix_nms op.")
        .AsDispensable();
    AddAttr<float>("score_threshold", "(float), attribute 0 for matrix_nms op.")
    ;
    AddAttr<int>("nms_top_k", "(int), attribute 1 for matrix_nms op.")
    ;
    AddAttr<int>("keep_top_k", "(int), attribute 2 for matrix_nms op.")
    ;
    AddAttr<float>("post_threshold", "(float), attribute 3 for matrix_nms op.")
        .SetDefault(0.);
    AddAttr<bool>("use_gaussian", "(bool), attribute 4 for matrix_nms op.")
        .SetDefault(false);
    AddAttr<float>("gaussian_sigma", "(float), attribute 5 for matrix_nms op.")
        .SetDefault(2.);
    AddAttr<int>("background_label", "(int), attribute 6 for matrix_nms op.")
        .SetDefault(0);
    AddAttr<bool>("normalized", "(bool), attribute 7 for matrix_nms op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of matrix_nms op.
)DOC");
  }
};


class MatrixNmsOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
    return GetMatrixNmsExpectedKernelType(ctx, this);
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(matrix_nms, MatrixNmsInferShapeFunctor,
                            PD_INFER_META(phi::MatrixNMSInferMeta));



class MatrixPowerOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of matrix_power op.");
    AddOutput("Out", "(Tensor), output 0 of matrix_power op.");
    AddAttr<int>("n", "(int), attribute 0 for matrix_power op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of matrix_power op.
)DOC");
  }
};


class MatrixPowerOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(matrix_power, MatrixPowerInferShapeFunctor,
                            PD_INFER_META(phi::MatrixPowerInferMeta));



class MaxPool2dWithIndexOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of max_pool2d_with_index op.");
    AddOutput("Out", "(Tensor), output 0 of max_pool2d_with_index op.");
    AddOutput("Mask", "(Tensor), output 1 of max_pool2d_with_index op.");
    AddAttr<std::vector<int>>("ksize", "(std::vector<int>), attribute 0 for max_pool2d_with_index op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 1 for max_pool2d_with_index op.")
        .SetDefault({1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 2 for max_pool2d_with_index op.")
        .SetDefault({0, 0});
    AddAttr<bool>("global_pooling", "(bool), attribute 3 for max_pool2d_with_index op.")
        .SetDefault(false);
    AddAttr<bool>("adaptive", "(bool), attribute 4 for max_pool2d_with_index op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of max_pool2d_with_index op.
)DOC");
  }
};


class MaxPool2dWithIndexOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(max_pool2d_with_index, MaxPool2dWithIndexInferShapeFunctor,
                            PD_INFER_META(phi::MaxPoolWithIndexInferMeta));



class MaxPool3dWithIndexOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of max_pool3d_with_index op.");
    AddOutput("Out", "(Tensor), output 0 of max_pool3d_with_index op.");
    AddOutput("Mask", "(Tensor), output 1 of max_pool3d_with_index op.");
    AddAttr<std::vector<int>>("ksize", "(std::vector<int>), attribute 0 for max_pool3d_with_index op.")
    ;
    AddAttr<std::vector<int>>("strides", "(std::vector<int>), attribute 1 for max_pool3d_with_index op.")
        .SetDefault({1, 1, 1});
    AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 2 for max_pool3d_with_index op.")
        .SetDefault({0, 0, 0});
    AddAttr<bool>("global_pooling", "(bool), attribute 3 for max_pool3d_with_index op.")
        .SetDefault(false);
    AddAttr<bool>("adaptive", "(bool), attribute 4 for max_pool3d_with_index op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of max_pool3d_with_index op.
)DOC");
  }
};


class MaxPool3dWithIndexOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(max_pool3d_with_index, MaxPool3dWithIndexInferShapeFunctor,
                            PD_INFER_META(phi::MaxPoolWithIndexInferMeta));



class MaxoutOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of maxout op.");
    AddOutput("Out", "(Tensor), output 0 of maxout op.");
    AddAttr<int>("groups", "(int), attribute 0 for maxout op.")
    ;
    AddAttr<int>("axis", "(int), attribute 1 for maxout op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of maxout op.
)DOC");
  }
};


class MaxoutOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(maxout, MaxoutInferShapeFunctor,
                            PD_INFER_META(phi::MaxOutInferMeta));



class MeanOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of mean op.");
    AddOutput("Out", "(Tensor), output 0 of mean op.");
    AddComment(R"DOC(
TODO: Documentation of mean op.
)DOC");
  }
};


class MeanOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(mean, MeanInferShapeFunctor,
                            PD_INFER_META(phi::MeanAllInferMeta));



class MemoryEfficientAttentionOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("query", "(Tensor), input 0 of memory_efficient_attention op.");
    AddInput("key", "(Tensor), input 1 of memory_efficient_attention op.");
    AddInput("value", "(Tensor), input 2 of memory_efficient_attention op.");
    AddInput("bias", "(Tensor), input 3 of memory_efficient_attention op.")
        .AsDispensable();
    AddInput("cu_seqlens_q", "(Tensor), input 4 of memory_efficient_attention op.")
        .AsDispensable();
    AddInput("cu_seqlens_k", "(Tensor), input 5 of memory_efficient_attention op.")
        .AsDispensable();
    AddInput("causal_diagonal", "(Tensor), input 6 of memory_efficient_attention op.")
        .AsDispensable();
    AddInput("seqlen_k", "(Tensor), input 7 of memory_efficient_attention op.")
        .AsDispensable();
    AddOutput("output", "(Tensor), output 0 of memory_efficient_attention op.");
    AddOutput("logsumexp", "(Tensor), output 1 of memory_efficient_attention op.");
    AddOutput("seed_and_offset", "(Tensor), output 2 of memory_efficient_attention op.");
    AddInput("MaxSeqlenQTensor", "attribute 0 for memory_efficient_attention op from 0D Tensor.")
        .AsDispensable();
    AddAttr<float>("max_seqlen_q", "(float), attribute 0 for memory_efficient_attention op.")
    ;
    AddInput("MaxSeqlenKTensor", "attribute 1 for memory_efficient_attention op from 0D Tensor.")
        .AsDispensable();
    AddAttr<float>("max_seqlen_k", "(float), attribute 1 for memory_efficient_attention op.")
    ;
    AddAttr<bool>("causal", "(bool), attribute 2 for memory_efficient_attention op.")
    ;
    AddAttr<double>("dropout_p", "(double), attribute 3 for memory_efficient_attention op.")
    ;
    AddAttr<float>("scale", "(float), attribute 4 for memory_efficient_attention op.")
    ;
    AddAttr<bool>("is_test", "(bool), attribute 5 for memory_efficient_attention op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of memory_efficient_attention op.
)DOC");
  }
};


class MemoryEfficientAttentionOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "query");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(memory_efficient_attention, MemoryEfficientAttentionInferShapeFunctor,
                            PD_INFER_META(phi::MemoryEfficientAttentionInferMeta));



class MergeSelectedRowsOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of merge_selected_rows op.");
    AddOutput("Out", "(Tensor), output 0 of merge_selected_rows op.");
    AddComment(R"DOC(
TODO: Documentation of merge_selected_rows op.
)DOC");
  }
};


class MergeSelectedRowsOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


class MergeSelectedRowsInferVarType : public framework::PassInDtypeAndVarTypeToOutput {
 protected:
  std::unordered_map<std::string, std::string>& GetInputOutputWithSameType() const override {
      static std::unordered_map<std::string, std::string> m{{"X", /*->*/ "Out"}};
      return m;
  }
};


DECLARE_INFER_SHAPE_FUNCTOR(merge_selected_rows, MergeSelectedRowsInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class MergedAdamOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Param", "(Tensor[]), input 0 of merged_adam op.")
        .AsDuplicable();
    AddInput("Grad", "(Tensor[]), input 1 of merged_adam op.")
        .AsDuplicable();
    AddInput("LearningRate", "(Tensor[]), input 2 of merged_adam op.")
        .AsDuplicable();
    AddInput("Moment1", "(Tensor[]), input 3 of merged_adam op.")
        .AsDuplicable();
    AddInput("Moment2", "(Tensor[]), input 4 of merged_adam op.")
        .AsDuplicable();
    AddInput("Beta1Pow", "(Tensor[]), input 5 of merged_adam op.")
        .AsDuplicable();
    AddInput("Beta2Pow", "(Tensor[]), input 6 of merged_adam op.")
        .AsDuplicable();
    AddInput("MasterParam", "(Tensor[]), input 7 of merged_adam op.")
        .AsDuplicable()
        .AsDispensable();
    AddOutput("ParamOut", "(Tensor[]), output 0 of merged_adam op.")
        .AsDuplicable();
    AddOutput("Moment1Out", "(Tensor[]), output 1 of merged_adam op.")
        .AsDuplicable();
    AddOutput("Moment2Out", "(Tensor[]), output 2 of merged_adam op.")
        .AsDuplicable();
    AddOutput("Beta1PowOut", "(Tensor[]), output 3 of merged_adam op.")
        .AsDuplicable();
    AddOutput("Beta2PowOut", "(Tensor[]), output 4 of merged_adam op.")
        .AsDuplicable();
    AddOutput("MasterParamOut", "(Tensor[]), output 5 of merged_adam op.")
        .AsDuplicable()
        .AsDispensable();
    AddAttr<float>("beta1", "(float), attribute 0 for merged_adam op.")
        .SetDefault(0.9f)
        .SupportTensor();
    AddAttr<float>("beta2", "(float), attribute 1 for merged_adam op.")
        .SetDefault(0.999f)
        .SupportTensor();
    AddAttr<float>("epsilon", "(float), attribute 2 for merged_adam op.")
        .SetDefault(1.0e-8f)
        .SupportTensor();
    AddAttr<bool>("multi_precision", "(bool), attribute 3 for merged_adam op.")
        .SetDefault(false);
    AddAttr<bool>("use_global_beta_pow", "(bool), attribute 4 for merged_adam op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of merged_adam op.
)DOC");
  }
};


class MergedAdamOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Param");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(merged_adam, MergedAdamInferShapeFunctor,
                            PD_INFER_META(phi::MergedAdamInferMeta));



class MergedMomentumOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Param", "(Tensor[]), input 0 of merged_momentum op.")
        .AsDuplicable();
    AddInput("Grad", "(Tensor[]), input 1 of merged_momentum op.")
        .AsDuplicable();
    AddInput("Velocity", "(Tensor[]), input 2 of merged_momentum op.")
        .AsDuplicable();
    AddInput("LearningRate", "(Tensor[]), input 3 of merged_momentum op.")
        .AsDuplicable();
    AddInput("MasterParam", "(Tensor[]), input 4 of merged_momentum op.")
        .AsDuplicable()
        .AsDispensable();
    AddOutput("ParamOut", "(Tensor[]), output 0 of merged_momentum op.")
        .AsDuplicable();
    AddOutput("VelocityOut", "(Tensor[]), output 1 of merged_momentum op.")
        .AsDuplicable();
    AddOutput("MasterParamOut", "(Tensor[]), output 2 of merged_momentum op.")
        .AsDuplicable()
        .AsDispensable();
    AddAttr<float>("mu", "(float), attribute 0 for merged_momentum op.")
    ;
    AddAttr<bool>("use_nesterov", "(bool), attribute 1 for merged_momentum op.")
        .SetDefault(false);
    AddAttr<std::vector<std::string>>("regularization_method", "(std::vector<std::string>), attribute 2 for merged_momentum op.")
        .SetDefault({});
    AddAttr<std::vector<float>>("regularization_coeff", "(std::vector<float>), attribute 3 for merged_momentum op.")
        .SetDefault({});
    AddAttr<bool>("multi_precision", "(bool), attribute 4 for merged_momentum op.")
        .SetDefault(false);
    AddAttr<float>("rescale_grad", "(float), attribute 5 for merged_momentum op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of merged_momentum op.
)DOC");
  }
};


class MergedMomentumOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Param");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(merged_momentum, MergedMomentumInferShapeFunctor,
                            PD_INFER_META(phi::MergedMomentumInferMeta));



class MeshgridOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor[]), input 0 of meshgrid op.")
        .AsDuplicable();
    AddOutput("Out", "(Tensor[]), output 0 of meshgrid op.")
        .AsDuplicable();
    AddComment(R"DOC(
TODO: Documentation of meshgrid op.
)DOC");
  }
};


class MeshgridOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(meshgrid, MeshgridInferShapeFunctor,
                            PD_INFER_META(phi::MeshgridInferMeta));



class ModeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of mode op.");
    AddOutput("Out", "(Tensor), output 0 of mode op.");
    AddOutput("Indices", "(Tensor), output 1 of mode op.");
    AddAttr<int>("axis", "(int), attribute 0 for mode op.")
        .SetDefault(-1);
    AddAttr<bool>("keepdim", "(bool), attribute 1 for mode op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of mode op.
)DOC");
  }
};


class ModeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(mode, ModeInferShapeFunctor,
                            PD_INFER_META(phi::ModeInferMeta));



class MomentumOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Param", "(Tensor), input 0 of momentum op.");
    AddInput("Grad", "(Tensor), input 1 of momentum op.");
    AddInput("Velocity", "(Tensor), input 2 of momentum op.");
    AddInput("LearningRate", "(Tensor), input 3 of momentum op.");
    AddInput("MasterParam", "(Tensor), input 4 of momentum op.")
        .AsDispensable();
    AddOutput("ParamOut", "(Tensor), output 0 of momentum op.");
    AddOutput("VelocityOut", "(Tensor), output 1 of momentum op.");
    AddOutput("MasterParamOut", "(Tensor), output 2 of momentum op.")
        .AsDispensable();
    AddAttr<float>("mu", "(float), attribute 0 for momentum op.")
    ;
    AddAttr<bool>("use_nesterov", "(bool), attribute 1 for momentum op.")
        .SetDefault(false);
    AddAttr<std::string>("regularization_method", "(std::string), attribute 2 for momentum op.")
        .SetDefault("");
    AddAttr<float>("regularization_coeff", "(float), attribute 3 for momentum op.")
        .SetDefault(0.0f);
    AddAttr<bool>("multi_precision", "(bool), attribute 4 for momentum op.")
        .SetDefault(false);
    AddAttr<float>("rescale_grad", "(float), attribute 5 for momentum op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of momentum op.
)DOC");
  }
};


class MomentumOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Param");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(momentum, MomentumInferShapeFunctor,
                            PD_INFER_META(phi::MomentumInferMeta));



class MultiDotOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor[]), input 0 of multi_dot op.")
        .AsDuplicable();
    AddOutput("Out", "(Tensor), output 0 of multi_dot op.");
    AddComment(R"DOC(
TODO: Documentation of multi_dot op.
)DOC");
  }
};


class MultiDotOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(multi_dot, MultiDotInferShapeFunctor,
                            PD_INFER_META(phi::MultiDotInferMeta));



class MulticlassNms3OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("BBoxes", "(Tensor), input 0 of multiclass_nms3 op.");
    AddInput("Scores", "(Tensor), input 1 of multiclass_nms3 op.");
    AddInput("RoisNum", "(Tensor), input 2 of multiclass_nms3 op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of multiclass_nms3 op.");
    AddOutput("Index", "(Tensor), output 1 of multiclass_nms3 op.");
    AddOutput("NmsRoisNum", "(Tensor), output 2 of multiclass_nms3 op.")
        .AsDispensable();
    AddAttr<float>("score_threshold", "(float), attribute 0 for multiclass_nms3 op.")
    ;
    AddAttr<int>("nms_top_k", "(int), attribute 1 for multiclass_nms3 op.")
    ;
    AddAttr<int>("keep_top_k", "(int), attribute 2 for multiclass_nms3 op.")
    ;
    AddAttr<float>("nms_threshold", "(float), attribute 3 for multiclass_nms3 op.")
        .SetDefault(0.3);
    AddAttr<bool>("normalized", "(bool), attribute 4 for multiclass_nms3 op.")
        .SetDefault(true);
    AddAttr<float>("nms_eta", "(float), attribute 5 for multiclass_nms3 op.")
        .SetDefault(1.0);
    AddAttr<int>("background_label", "(int), attribute 6 for multiclass_nms3 op.")
        .SetDefault(0);
    AddComment(R"DOC(
TODO: Documentation of multiclass_nms3 op.
)DOC");
  }
};


class MulticlassNms3Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Scores");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(multiclass_nms3, MulticlassNms3InferShapeFunctor,
                            PD_INFER_META(phi::MultiClassNMSInferMeta));



class MultinomialOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of multinomial op.");
    AddOutput("Out", "(Tensor), output 0 of multinomial op.");
    AddAttr<int>("num_samples", "(int), attribute 0 for multinomial op.")
        .SetDefault(1)
        .SupportTensor();
    AddAttr<bool>("replacement", "(bool), attribute 1 for multinomial op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of multinomial op.
)DOC");
  }
};


class MultinomialOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(multinomial, MultinomialInferShapeFunctor,
                            PD_INFER_META(phi::MultinomialInferMeta));



class MultiplexOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor[]), input 0 of multiplex op.")
        .AsDuplicable();
    AddInput("Ids", "(Tensor), input 1 of multiplex op.");
    AddOutput("Out", "(Tensor), output 0 of multiplex op.");
    AddComment(R"DOC(
TODO: Documentation of multiplex op.
)DOC");
  }
};


class MultiplexOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "Ids") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(multiplex, MultiplexInferShapeFunctor,
                            PD_INFER_META(phi::MultiplexInferMeta));



class MvOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of mv op.");
    AddInput("Vec", "(Tensor), input 1 of mv op.");
    AddOutput("Out", "(Tensor), output 0 of mv op.");
    AddComment(R"DOC(
TODO: Documentation of mv op.
)DOC");
  }
};


class MvOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(mv, MvInferShapeFunctor,
                            PD_INFER_META(phi::MvInferMeta));



class NanmedianOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of nanmedian op.");
    AddOutput("Out", "(Tensor), output 0 of nanmedian op.");
    AddOutput("MedianIndex", "(Tensor), output 1 of nanmedian op.")
        .AsExtra();
      AddAttr<std::vector<int>>("axis", "(std::vector<int>), attribute 0 for nanmedian op.")
        .SetDefault({});
    AddAttr<bool>("keepdim", "(bool), attribute 1 for nanmedian op.")
        .SetDefault(true);
    AddAttr<std::string>("mode", "(std::string), attribute 2 for nanmedian op.")
        .SetDefault("avg");
    AddComment(R"DOC(
TODO: Documentation of nanmedian op.
)DOC");
  }
};


class NanmedianOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(nanmedian, NanmedianInferShapeFunctor,
                            PD_INFER_META(phi::NanmedianInferMeta));



class NearestInterpV2OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of nearest_interp_v2 op.");
    AddInput("OutSize", "(Tensor), input 1 of nearest_interp_v2 op.")
        .AsDispensable();
    AddInput("SizeTensor", "(Tensor[]), input 2 of nearest_interp_v2 op.")
        .AsDuplicable()
        .AsDispensable();
    AddInput("Scale", "(Tensor), input 3 of nearest_interp_v2 op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of nearest_interp_v2 op.");
    AddAttr<std::string>("data_layout", "(std::string), attribute 0 for nearest_interp_v2 op.")
        .SetDefault("NCHW");
    AddAttr<int>("out_d", "(int), attribute 1 for nearest_interp_v2 op.")
        .SetDefault(0);
    AddAttr<int>("out_h", "(int), attribute 2 for nearest_interp_v2 op.")
        .SetDefault(0);
    AddAttr<int>("out_w", "(int), attribute 3 for nearest_interp_v2 op.")
        .SetDefault(0);
    AddAttr<std::vector<float>>("scale", "(std::vector<float>), attribute 4 for nearest_interp_v2 op.")
        .SetDefault({});
    AddAttr<std::string>("interp_method", "(std::string), attribute 5 for nearest_interp_v2 op.")
        .SetDefault("bilinear");
    AddAttr<bool>("align_corners", "(bool), attribute 6 for nearest_interp_v2 op.")
        .SetDefault(true);
    AddAttr<int>("align_mode", "(int), attribute 7 for nearest_interp_v2 op.")
        .SetDefault(1);
    AddComment(R"DOC(
TODO: Documentation of nearest_interp_v2 op.
)DOC");
  }
};


class NearestInterpV2Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "OutSize" || var_name == "SizeTensor" || var_name == "Scale") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(nearest_interp_v2, NearestInterpV2InferShapeFunctor,
                            PD_INFER_META(phi::InterpolateInferMeta));



class NextafterOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of nextafter op.");
    AddInput("y", "(Tensor), input 1 of nextafter op.");
    AddOutput("out", "(Tensor), output 0 of nextafter op.");
    AddComment(R"DOC(
TODO: Documentation of nextafter op.
)DOC");
  }
};


class NextafterOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(nextafter, NextafterInferShapeFunctor,
                            PD_INFER_META(phi::ElementwiseInferMeta));



class NllLossOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of nll_loss op.");
    AddInput("Label", "(Tensor), input 1 of nll_loss op.");
    AddInput("Weight", "(Tensor), input 2 of nll_loss op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of nll_loss op.");
    AddOutput("Total_weight", "(Tensor), output 1 of nll_loss op.");
    AddAttr<int64_t>("ignore_index", "(int64_t), attribute 0 for nll_loss op.")
        .SetDefault(-100);
    AddAttr<std::string>("reduction", "(std::string), attribute 1 for nll_loss op.")
        .SetDefault("mean");
    AddComment(R"DOC(
TODO: Documentation of nll_loss op.
)DOC");
  }
};


class NllLossOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(nll_loss, NllLossInferShapeFunctor,
                            PD_INFER_META(phi::NllLossRawInferMeta));



class NmsOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Boxes", "(Tensor), input 0 of nms op.");
    AddOutput("KeepBoxesIdxs", "(Tensor), output 0 of nms op.");
    AddAttr<float>("iou_threshold", "(float), attribute 0 for nms op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of nms op.
)DOC");
  }
};


class NmsOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Boxes");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(nms, NmsInferShapeFunctor,
                            PD_INFER_META(phi::NMSInferMeta));



class WhereIndexOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Condition", "(Tensor), input 0 of where_index op.");
    AddOutput("Out", "(Tensor), output 0 of where_index op.");
    AddComment(R"DOC(
TODO: Documentation of where_index op.
)DOC");
  }
};


class WhereIndexOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Condition");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(where_index, WhereIndexInferShapeFunctor,
                            PD_INFER_META(phi::NonZeroInferMeta));



class NpuIdentityOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of npu_identity op.");
    AddOutput("out", "(Tensor), output 0 of npu_identity op.");
    AddAttr<int>("format", "(int), attribute 0 for npu_identity op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of npu_identity op.
)DOC");
  }
};


class NpuIdentityOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(npu_identity, NpuIdentityInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class SizeOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of size op.");
    AddOutput("Out", "(Tensor), output 0 of size op.");
    AddComment(R"DOC(
TODO: Documentation of size op.
)DOC");
  }
};


class SizeOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "Input") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(size, SizeInferShapeFunctor,
                            PD_INFER_META(phi::NumelInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(SizeNoNeedBufferVarInferer,
                                    "Input");


class OverlapAddOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of overlap_add op.");
    AddOutput("Out", "(Tensor), output 0 of overlap_add op.");
    AddAttr<int>("hop_length", "(int), attribute 0 for overlap_add op.")
    ;
    AddAttr<int>("axis", "(int), attribute 1 for overlap_add op.")
        .SetDefault(-1);
    AddComment(R"DOC(
TODO: Documentation of overlap_add op.
)DOC");
  }
};


class OverlapAddOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(overlap_add, OverlapAddInferShapeFunctor,
                            PD_INFER_META(phi::OverlapAddInferMeta));



class PNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of p_norm op.");
    AddOutput("Out", "(Tensor), output 0 of p_norm op.");
    AddAttr<float>("porder", "(float), attribute 0 for p_norm op.")
        .SetDefault(2);
    AddAttr<int>("axis", "(int), attribute 1 for p_norm op.")
        .SetDefault(-1);
    AddAttr<float>("epsilon", "(float), attribute 2 for p_norm op.")
        .SetDefault(1.0e-12f);
    AddAttr<bool>("keepdim", "(bool), attribute 3 for p_norm op.")
        .SetDefault(false);
    AddAttr<bool>("asvector", "(bool), attribute 4 for p_norm op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of p_norm op.
)DOC");
  }
};


class PNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(p_norm, PNormInferShapeFunctor,
                            PD_INFER_META(phi::PNormInferMeta));



class Pad3dOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of pad3d op.");
    AddOutput("Out", "(Tensor), output 0 of pad3d op.");
    AddInput("Paddings", "attribute 0 for pad3d op from 1D integer Tensor.")
        .AsDispensable();
      AddAttr<std::vector<int>>("paddings", "(std::vector<int>), attribute 0 for pad3d op.")
    ;
    AddAttr<std::string>("mode", "(std::string), attribute 1 for pad3d op.")
        .SetDefault("constant");
    AddAttr<float>("value", "(float), attribute 2 for pad3d op.")
        .SetDefault(0.0);
    AddAttr<std::string>("data_format", "(std::string), attribute 3 for pad3d op.")
        .SetDefault("NCDHW");
    AddComment(R"DOC(
TODO: Documentation of pad3d op.
)DOC");
  }
};


class Pad3dOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pad3d, Pad3dInferShapeFunctor,
                            PD_INFER_META(phi::Pad3dInferMeta));



class PixelShuffleOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of pixel_shuffle op.");
    AddOutput("Out", "(Tensor), output 0 of pixel_shuffle op.");
    AddAttr<int>("upscale_factor", "(int), attribute 0 for pixel_shuffle op.")
        .SetDefault(1);
    AddAttr<std::string>("data_format", "(std::string), attribute 1 for pixel_shuffle op.")
        .SetDefault("NCHW");
    AddComment(R"DOC(
TODO: Documentation of pixel_shuffle op.
)DOC");
  }
};


class PixelShuffleOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pixel_shuffle, PixelShuffleInferShapeFunctor,
                            PD_INFER_META(phi::PixelShuffleInferMeta));



class PixelUnshuffleOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of pixel_unshuffle op.");
    AddOutput("Out", "(Tensor), output 0 of pixel_unshuffle op.");
    AddAttr<int>("downscale_factor", "(int), attribute 0 for pixel_unshuffle op.")
        .SetDefault(1);
    AddAttr<std::string>("data_format", "(std::string), attribute 1 for pixel_unshuffle op.")
        .SetDefault("NCHW");
    AddComment(R"DOC(
TODO: Documentation of pixel_unshuffle op.
)DOC");
  }
};


class PixelUnshuffleOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pixel_unshuffle, PixelUnshuffleInferShapeFunctor,
                            PD_INFER_META(phi::PixelUnshuffleInferMeta));



class PoissonOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of poisson op.");
    AddOutput("Out", "(Tensor), output 0 of poisson op.");
    AddComment(R"DOC(
TODO: Documentation of poisson op.
)DOC");
  }
};


class PoissonOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(poisson, PoissonInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class PolygammaOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of polygamma op.");
    AddOutput("out", "(Tensor), output 0 of polygamma op.");
    AddAttr<int>("n", "(int), attribute 0 for polygamma op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of polygamma op.
)DOC");
  }
};


class PolygammaOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(polygamma, PolygammaInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(PolygammaInplaceInferer,
                           {"x", "out"});



class PowOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of pow op.");
    AddOutput("Out", "(Tensor), output 0 of pow op.");
    AddInput("FactorTensor", "attribute 0 for pow op from 0D Tensor.")
        .AsDispensable();
    AddAttr<float>("factor", "(float), attribute 0 for pow op.")
        .SetDefault(1.0f);
    AddComment(R"DOC(
TODO: Documentation of pow op.
)DOC");
  }
};


class PowOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pow, PowInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(PowInplaceInferer,
                           {"X", "Out"});



class PreluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of prelu op.");
    AddInput("Alpha", "(Tensor), input 1 of prelu op.");
    AddOutput("Out", "(Tensor), output 0 of prelu op.");
    AddAttr<std::string>("data_format", "(std::string), attribute 0 for prelu op.")
        .SetDefault("NCHW");
    AddAttr<std::string>("mode", "(std::string), attribute 1 for prelu op.")
        .SetDefault("all");
    AddComment(R"DOC(
TODO: Documentation of prelu op.
)DOC");
  }
};


class PreluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(prelu, PreluInferShapeFunctor,
                            PD_INFER_META(phi::PReluInferMeta));



class PriorBoxOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of prior_box op.");
    AddInput("Image", "(Tensor), input 1 of prior_box op.");
    AddOutput("Boxes", "(Tensor), output 0 of prior_box op.");
    AddOutput("Variances", "(Tensor), output 1 of prior_box op.");
    AddAttr<std::vector<float>>("min_sizes", "(std::vector<float>), attribute 0 for prior_box op.")
    ;
    AddAttr<std::vector<float>>("max_sizes", "(std::vector<float>), attribute 1 for prior_box op.")
        .SetDefault({});
    AddAttr<std::vector<float>>("aspect_ratios", "(std::vector<float>), attribute 2 for prior_box op.")
        .SetDefault({});
    AddAttr<std::vector<float>>("variances", "(std::vector<float>), attribute 3 for prior_box op.")
        .SetDefault({});
    AddAttr<bool>("flip", "(bool), attribute 4 for prior_box op.")
        .SetDefault(true);
    AddAttr<bool>("clip", "(bool), attribute 5 for prior_box op.")
        .SetDefault(true);
    AddAttr<float>("step_w", "(float), attribute 6 for prior_box op.")
        .SetDefault(0.0);
    AddAttr<float>("step_h", "(float), attribute 7 for prior_box op.")
        .SetDefault(0.0);
    AddAttr<float>("offset", "(float), attribute 8 for prior_box op.")
        .SetDefault(0.5);
    AddAttr<bool>("min_max_aspect_ratios_order", "(bool), attribute 9 for prior_box op.")
        .SetDefault(false);
    AddComment(R"DOC(
TODO: Documentation of prior_box op.
)DOC");
  }
};


class PriorBoxOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(prior_box, PriorBoxInferShapeFunctor,
                            PD_INFER_META(phi::PriorBoxInferMeta));



class PsroiPoolOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of psroi_pool op.");
    AddInput("ROIs", "(Tensor), input 1 of psroi_pool op.");
    AddInput("RoisNum", "(Tensor), input 2 of psroi_pool op.")
        .AsDispensable();
    AddOutput("Out", "(Tensor), output 0 of psroi_pool op.");
    AddAttr<int>("pooled_height", "(int), attribute 0 for psroi_pool op.")
        .SetDefault(1);
    AddAttr<int>("pooled_width", "(int), attribute 1 for psroi_pool op.")
        .SetDefault(1);
    AddAttr<int>("output_channels", "(int), attribute 2 for psroi_pool op.")
        .SetDefault(1);
    AddAttr<float>("spatial_scale", "(float), attribute 3 for psroi_pool op.")
        .SetDefault(1.0);
    AddComment(R"DOC(
TODO: Documentation of psroi_pool op.
)DOC");
  }
};


class PsroiPoolOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(psroi_pool, PsroiPoolInferShapeFunctor,
                            PD_INFER_META(phi::PsroiPoolInferMeta));



class PutAlongAxisOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("Input", "(Tensor), input 0 of put_along_axis op.");
    AddInput("Index", "(Tensor), input 1 of put_along_axis op.");
    AddInput("Value", "(Tensor), input 2 of put_along_axis op.");
    AddOutput("Result", "(Tensor), output 0 of put_along_axis op.");
    AddAttr<int>("Axis", "(int), attribute 0 for put_along_axis op.")
    ;
    AddAttr<std::string>("Reduce", "(std::string), attribute 1 for put_along_axis op.")
        .SetDefault("assign");
    AddAttr<bool>("Include_self", "(bool), attribute 2 for put_along_axis op.")
        .SetDefault(true);
    AddComment(R"DOC(
TODO: Documentation of put_along_axis op.
)DOC");
  }
};


class PutAlongAxisOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Input");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(put_along_axis, PutAlongAxisInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(PutAlongAxisInplaceInferer,
                           {"Input", "Result"});



class QrOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of qr op.");
    AddOutput("Q", "(Tensor), output 0 of qr op.");
    AddOutput("R", "(Tensor), output 1 of qr op.");
    AddAttr<std::string>("mode", "(std::string), attribute 0 for qr op.")
        .SetDefault("reduced");
    AddComment(R"DOC(
TODO: Documentation of qr op.
)DOC");
  }
};


class QrOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(qr, QrInferShapeFunctor,
                            PD_INFER_META(phi::QrInferMeta));



class RealOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of real op.");
    AddOutput("Out", "(Tensor), output 0 of real op.");
    AddComment(R"DOC(
TODO: Documentation of real op.
)DOC");
  }
};


class RealOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(real, RealInferShapeFunctor,
                            PD_INFER_META(phi::RealAndImagInferMeta));



class ReciprocalOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reciprocal op.");
    AddOutput("Out", "(Tensor), output 0 of reciprocal op.");
    AddComment(R"DOC(
TODO: Documentation of reciprocal op.
)DOC");
  }
};


class ReciprocalOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reciprocal, ReciprocalInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ReciprocalInplaceInferer,
                           {"X", "Out"});



class GraphReindexOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of graph_reindex op.");
    AddInput("Neighbors", "(Tensor), input 1 of graph_reindex op.");
    AddInput("Count", "(Tensor), input 2 of graph_reindex op.");
    AddInput("HashTable_Value", "(Tensor), input 3 of graph_reindex op.")
        .AsDispensable();
    AddInput("HashTable_Index", "(Tensor), input 4 of graph_reindex op.")
        .AsDispensable();
    AddOutput("Reindex_Src", "(Tensor), output 0 of graph_reindex op.");
    AddOutput("Reindex_Dst", "(Tensor), output 1 of graph_reindex op.");
    AddOutput("Out_Nodes", "(Tensor), output 2 of graph_reindex op.");
    AddComment(R"DOC(
TODO: Documentation of graph_reindex op.
)DOC");
  }
};


class GraphReindexOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(graph_reindex, GraphReindexInferShapeFunctor,
                            PD_INFER_META(phi::GraphReindexInferMeta));



class ReluOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of relu op.");
    AddOutput("Out", "(Tensor), output 0 of relu op.");
    AddComment(R"DOC(
TODO: Documentation of relu op.
)DOC");
  }
};


class ReluOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(relu, ReluInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ReluInplaceInferer,
                           {"X", "Out"});



class Relu6OpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of relu6 op.");
    AddOutput("Out", "(Tensor), output 0 of relu6 op.");
    AddComment(R"DOC(
TODO: Documentation of relu6 op.
)DOC");
  }
};


class Relu6Op : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(relu6, Relu6InferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



class RenormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of renorm op.");
    AddOutput("Out", "(Tensor), output 0 of renorm op.");
    AddAttr<float>("p", "(float), attribute 0 for renorm op.")
    ;
    AddAttr<int>("axis", "(int), attribute 1 for renorm op.")
    ;
    AddAttr<float>("max_norm", "(float), attribute 2 for renorm op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of renorm op.
)DOC");
  }
};


class RenormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(renorm, RenormInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(RenormInplaceInferer,
                           {"X", "Out"});



class ReverseOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("X", "(Tensor), input 0 of reverse op.");
    AddOutput("Out", "(Tensor), output 0 of reverse op.");
    AddAttr<std::vector<int>>("axis", "(std::vector<int>), attribute 0 for reverse op.")

        .SupportTensor();
    AddComment(R"DOC(
TODO: Documentation of reverse op.
)DOC");
  }
};


class ReverseOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(reverse, ReverseInferShapeFunctor,
                            PD_INFER_META(phi::ReverseInferMeta));



class RmsNormOpMaker : public framework::OpProtoAndCheckerMaker {
 public:
  void Make() override {
    AddInput("x", "(Tensor), input 0 of rms_norm op.");
    AddInput("bias", "(Tensor), input 1 of rms_norm op.")
        .AsDispensable();
    AddInput("residual", "(Tensor), input 2 of rms_norm op.")
        .AsDispensable();
    AddInput("norm_weight", "(Tensor), input 3 of rms_norm op.");
    AddInput("norm_bias", "(Tensor), input 4 of rms_norm op.")
        .AsDispensable();
    AddOutput("out", "(Tensor), output 0 of rms_norm op.");
    AddOutput("residual_out", "(Tensor), output 1 of rms_norm op.")
        .AsDispensable();
    AddOutput("inv_var", "(Tensor), output 2 of rms_norm op.")
        .AsIntermediate();
    AddAttr<float>("epsilon", "(float), attribute 0 for rms_norm op.")
    ;
    AddAttr<int>("begin_norm_axis", "(int), attribute 1 for rms_norm op.")
    ;
    AddAttr<float>("quant_scale", "(float), attribute 2 for rms_norm op.")
    ;
    AddAttr<int>("quant_round_type", "(int), attribute 3 for rms_norm op.")
    ;
    AddAttr<float>("quant_max_bound", "(float), attribute 4 for rms_norm op.")
    ;
    AddAttr<float>("quant_min_bound", "(float), attribute 5 for rms_norm op.")
    ;
    AddComment(R"DOC(
TODO: Documentation of rms_norm op.
)DOC");
  }
};


class RmsNormOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(rms_norm, RmsNormInferShapeFunctor,
                            PD_INFER_META(phi::RmsNormInferMeta));




template <typename T>
class KthvalueGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("kthvalue_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Indices", this->Output("Indices"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class KthvalueGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(kthvalue_grad, KthvalueGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LabelSmoothGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("label_smooth_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LabelSmoothGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(label_smooth_grad, LabelSmoothGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LayerNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("layer_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    if (this->HasInput("Scale")) {
      grad_op->SetInput("Scale", this->Input("Scale"));
    }
    if (this->HasInput("Bias")) {
      grad_op->SetInput("Bias", this->Input("Bias"));
    }
    grad_op->SetInput("Mean", this->Output("Mean"));
    grad_op->SetInput("Variance", this->Output("Variance"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Scale"), this->InputGrad("Scale"));
    grad_op->SetOutput(GradVarName("Bias"), this->InputGrad("Bias"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LayerNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(layer_norm_grad, LayerNormGradInferShapeFunctor,
                            PD_INFER_META(phi::LayerNormGradInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(LayerNormGradNoNeedBufferVarInferer,
                                    "Bias");

class LayerNormCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto scale = this->GetOptionalSingleForwardInput("Scale");
    auto bias = this->GetOptionalSingleForwardInput("Bias");
    auto mean = this->GetSingleForwardOutput("Mean");
    auto variance = this->GetSingleForwardOutput("Variance");
    auto out_grad = this->GetSingleOutputGrad("Y");


    //get attr
    const float epsilon = this->Attr<float>("epsilon");
    const int begin_norm_axis = this->Attr<int>("begin_norm_axis");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto scale_grad_t = this->GetSingleInputGrad("Scale");
    auto bias_grad_t = this->GetSingleInputGrad("Bias");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto scale_grad = this->GetOutputPtr(&scale_grad_t);
    auto bias_grad = this->GetOutputPtr(&bias_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto scale_grad_name = this->GetOutputName(scale_grad_t);
    auto bias_grad_name = this->GetOutputName(bias_grad_t);

    //call composite backward func
    VLOG(6) << "Runing layer_norm_grad composite func";
    prim::layer_norm_grad<prim::DescTensor>(x, scale, bias, mean, variance, out_grad, epsilon, begin_norm_axis, x_grad, scale_grad, bias_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(scale_grad_t, scale_grad_name);
    this->RecoverOutputName(bias_grad_t, bias_grad_name);

  }
};

template <typename T>
class LeakyReluGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("leaky_relu_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LeakyReluGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(leaky_relu_grad, LeakyReluGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LeakyReluGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


class LeakyReluCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr
    const float negative_slope = this->Attr<float>("alpha");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing leaky_relu_grad composite func";
    prim::leaky_relu_grad<prim::DescTensor>(x, out_grad, negative_slope, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class LeakyReluGradGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("leaky_relu_grad_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LeakyReluGradGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(leaky_relu_grad_grad, LeakyReluGradGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LeakyReluGradGradInplaceInferer,
                           {GradVarName("grad_x"), GradVarName("grad_out")});



template <typename T>
class LerpGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("lerp_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Y", this->Input("Y"));
    grad_op->SetInput("Weight", this->Input("Weight"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Y"), this->InputGrad("Y"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LerpGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(lerp_grad, LerpGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class LgammaGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("lgamma_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LgammaGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(lgamma_grad, LgammaGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LinearInterpV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("linear_interp_v2_grad");

    grad_op->SetInput("X", this->Input("X"));
    if (this->HasInput("OutSize")) {
      grad_op->SetInput("OutSize", this->Input("OutSize"));
    }
    if (this->HasInput("SizeTensor")) {
      grad_op->SetInput("SizeTensor", this->Input("SizeTensor"));
    }
    if (this->HasInput("Scale")) {
      grad_op->SetInput("Scale", this->Input("Scale"));
    }
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LinearInterpV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "OutSize" || var_name == "SizeTensor" || var_name == "Scale") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(linear_interp_v2_grad, LinearInterpV2GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(LinearInterpV2GradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class LogGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log_grad, LogGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LogGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


class LogCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing log_grad composite func";
    prim::log_grad<prim::DescTensor>(x, out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class LogGradGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log_grad_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("grad_out", this->Input(GradVarName("Out")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogGradGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log_grad_grad, LogGradGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));
DECLARE_INPLACE_OP_INFERER(LogGradGradInplaceInferer,
                           {GradVarName("grad_x"), GradVarName("grad_out")});


class LogGradCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto grad_out = this->GetSingleForwardInput(GradVarName("Out"));
    auto grad_x_grad = this->GetSingleOutputGrad(GradVarName("X"));


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto grad_out_grad_t = this->GetSingleInputGrad(GradVarName("Out"));

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto grad_out_grad = this->GetOutputPtr(&grad_out_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto grad_out_grad_name = this->GetOutputName(grad_out_grad_t);

    //call composite backward func
    VLOG(6) << "Runing log_double_grad composite func";
    prim::log_double_grad<prim::DescTensor>(x, grad_out, grad_x_grad, x_grad, grad_out_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(grad_out_grad_t, grad_out_grad_name);

  }
};

template <typename T>
class Log10GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log10_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class Log10GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log10_grad, Log10GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Log10GradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class Log1pGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log1p_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class Log1pGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log1p_grad, Log1pGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Log1pGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class Log2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log2_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class Log2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log2_grad, Log2GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Log2GradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class LogLossGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log_loss_grad");

    grad_op->SetInput("Predicted", this->Input("Predicted"));
    grad_op->SetInput("Labels", this->Input("Labels"));
    grad_op->SetInput(GradVarName("Loss"), this->OutputGrad("Loss"));

    grad_op->SetOutput(GradVarName("Predicted"), this->InputGrad("Predicted"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogLossGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(log_loss_grad, LogLossGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LogSoftmaxGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("log_softmax_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogSoftmaxGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(log_softmax_grad, LogSoftmaxGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LogcumsumexpGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("logcumsumexp_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogcumsumexpGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(logcumsumexp_grad, LogcumsumexpGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LogitGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("logit_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogitGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(logit_grad, LogitGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class LogsigmoidGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("logsigmoid_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LogsigmoidGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(logsigmoid_grad, LogsigmoidGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(LogsigmoidGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class LuGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("lu_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput("Pivots", this->Output("Pivots"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LuGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(lu_grad, LuGradInferShapeFunctor,
                            PD_INFER_META(phi::LUGradInferMeta));
DECLARE_INPLACE_OP_INFERER(LuGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class LuUnpackGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("lu_unpack_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Pivots", this->Input("Pivots"));
    grad_op->SetInput("L", this->Output("L"));
    grad_op->SetInput("U", this->Output("U"));
    grad_op->SetInput("Pmat", this->Output("Pmat"));
    grad_op->SetInput(GradVarName("L"), this->OutputGrad("L"));
    grad_op->SetInput(GradVarName("U"), this->OutputGrad("U"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class LuUnpackGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(lu_unpack_grad, LuUnpackGradInferShapeFunctor,
                            PD_INFER_META(phi::LUUnpackGradInferMeta));



template <typename T>
class MarginCrossEntropyGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("margin_cross_entropy_grad");

    grad_op->SetInput("Logits", this->Input("Logits"));
    grad_op->SetInput("Label", this->Input("Label"));
    grad_op->SetInput("Softmax", this->Output("Softmax"));
    grad_op->SetInput(GradVarName("Loss"), this->OutputGrad("Loss"));

    grad_op->SetOutput(GradVarName("Logits"), this->InputGrad("Logits"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MarginCrossEntropyGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "Softmax");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(margin_cross_entropy_grad, MarginCrossEntropyGradInferShapeFunctor,
                            PD_INFER_META(phi::MarginCrossEntropyGradInferMeta));
DECLARE_INPLACE_OP_INFERER(MarginCrossEntropyGradInplaceInferer,
                           {"Softmax", GradVarName("Logits")});



template <typename T>
class MaskedSelectGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("masked_select_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Mask", this->Input("Mask"));
    grad_op->SetInput(GradVarName("Y"), this->OutputGrad("Y"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MaskedSelectGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(masked_select_grad, MaskedSelectGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(MaskedSelectGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class MatrixPowerGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("matrix_power_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MatrixPowerGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(matrix_power_grad, MatrixPowerGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class MaxPool2dWithIndexGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("max_pool2d_with_index_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Mask", this->Output("Mask"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MaxPool2dWithIndexGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(max_pool2d_with_index_grad, MaxPool2dWithIndexGradInferShapeFunctor,
                            PD_INFER_META(phi::MaxPoolWithIndexGradInferMeta));



template <typename T>
class MaxPool3dWithIndexGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("max_pool3d_with_index_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Mask", this->Output("Mask"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MaxPool3dWithIndexGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(max_pool3d_with_index_grad, MaxPool3dWithIndexGradInferShapeFunctor,
                            PD_INFER_META(phi::MaxPoolWithIndexGradInferMeta));



template <typename T>
class MaxoutGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("maxout_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MaxoutGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(maxout_grad, MaxoutGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralUnaryGradInferMeta));



template <typename T>
class MeanGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("mean_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MeanGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(mean_grad, MeanGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedExceptLayoutInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(MeanGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class MemoryEfficientAttentionGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("memory_efficient_attention_grad");

    grad_op->SetInput("query", this->Input("query"));
    grad_op->SetInput("key", this->Input("key"));
    grad_op->SetInput("value", this->Input("value"));
    if (this->HasInput("bias")) {
      grad_op->SetInput("bias", this->Input("bias"));
    }
    if (this->HasInput("cu_seqlens_q")) {
      grad_op->SetInput("cu_seqlens_q", this->Input("cu_seqlens_q"));
    }
    if (this->HasInput("cu_seqlens_k")) {
      grad_op->SetInput("cu_seqlens_k", this->Input("cu_seqlens_k"));
    }
    grad_op->SetInput("output", this->Output("output"));
    grad_op->SetInput("logsumexp", this->Output("logsumexp"));
    grad_op->SetInput("seed_and_offset", this->Output("seed_and_offset"));
    grad_op->SetInput(GradVarName("output"), this->OutputGrad("output"));

    grad_op->SetOutput(GradVarName("query"), this->InputGrad("query"));
    grad_op->SetOutput(GradVarName("key"), this->InputGrad("key"));
    grad_op->SetOutput(GradVarName("value"), this->InputGrad("value"));
    grad_op->SetOutput(GradVarName("bias"), this->InputGrad("bias"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("MaxSeqlenQTensor")) {
      grad_op->SetInput("MaxSeqlenQTensor", this->Input("MaxSeqlenQTensor"));
    }
    if (this->HasInput("MaxSeqlenKTensor")) {
      grad_op->SetInput("MaxSeqlenKTensor", this->Input("MaxSeqlenKTensor"));
    }
  }
};


class MemoryEfficientAttentionGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("output"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(memory_efficient_attention_grad, MemoryEfficientAttentionGradInferShapeFunctor,
                            PD_INFER_META(phi::MemoryEfficientAttentionGradInferMeta));



template <typename T>
class MeshgridGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("meshgrid_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("outputs"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X", false));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MeshgridGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("outputs"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(meshgrid_grad, MeshgridGradInferShapeFunctor,
                            PD_INFER_META(phi::MeshgridGradInferMeta));



template <typename T>
class ModeGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("mode_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Indices", this->Output("Indices"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ModeGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(mode_grad, ModeGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class MultiDotGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("multi_dot_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X", false));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MultiDotGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(multi_dot_grad, MultiDotGradInferShapeFunctor,
                            PD_INFER_META(phi::MultiDotGradInferMeta));



template <typename T>
class MultiplexGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("multiplex_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Ids", this->Input("Ids"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X", false));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MultiplexGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "Ids") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(multiplex_grad, MultiplexGradInferShapeFunctor,
                            PD_INFER_META(phi::MultiplexGradInferMeta));



template <typename T>
class MvGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("mv_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Vec", this->Input("Vec"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Vec"), this->InputGrad("Vec"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class MvGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(mv_grad, MvGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class NanmedianGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("nanmedian_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("MedianIndex", this->Output("MedianIndex"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class NanmedianGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(nanmedian_grad, NanmedianGradInferShapeFunctor,
                            PD_INFER_META(phi::NanmedianGradInferMeta));



template <typename T>
class NearestInterpV2GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("nearest_interp_v2_grad");

    grad_op->SetInput("X", this->Input("X"));
    if (this->HasInput("OutSize")) {
      grad_op->SetInput("OutSize", this->Input("OutSize"));
    }
    if (this->HasInput("SizeTensor")) {
      grad_op->SetInput("SizeTensor", this->Input("SizeTensor"));
    }
    if (this->HasInput("Scale")) {
      grad_op->SetInput("Scale", this->Input("Scale"));
    }
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class NearestInterpV2GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

  phi::KernelKey GetKernelTypeForVar(
      const std::string& var_name,
      const phi::DenseTensor& tensor,
      const phi::KernelKey& expected_kernel_type) const override {
        if (var_name == "OutSize" || var_name == "SizeTensor" || var_name == "Scale") {
          return phi::KernelKey(phi::Backend::ALL_BACKEND,
                              expected_kernel_type.layout(),
                              expected_kernel_type.dtype());
        }
        else {
            return phi::KernelKey(
              tensor.place(), tensor.layout(), expected_kernel_type.dtype());
        }
      }

};


DECLARE_INFER_SHAPE_FUNCTOR(nearest_interp_v2_grad, NearestInterpV2GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(NearestInterpV2GradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class NllLossGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("nll_loss_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Label", this->Input("Label"));
    if (this->HasInput("Weight")) {
      grad_op->SetInput("Weight", this->Input("Weight"));
    }
    grad_op->SetInput("Total_weight", this->Output("Total_weight"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class NllLossGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(nll_loss_grad, NllLossGradInferShapeFunctor,
                            PD_INFER_META(phi::NllLossGradInferMeta));



template <typename T>
class OverlapAddGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("overlap_add_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class OverlapAddGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(overlap_add_grad, OverlapAddGradInferShapeFunctor,
                            PD_INFER_META(phi::OverlapAddGradInferMeta));



template <typename T>
class PNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("p_norm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(p_norm_grad, PNormGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralUnaryGradInferMeta));



template <typename T>
class Pad3dGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pad3d_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("Paddings")) {
      grad_op->SetInput("Paddings", this->Input("Paddings"));
    }
  }
};


class Pad3dGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pad3d_grad, Pad3dGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));

DECLARE_NO_NEED_BUFFER_VARS_INFERER(Pad3dGradNoNeedBufferVarInferer,
                                    "X");


template <typename T>
class Pad3dDoubleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pad3d_double_grad");

    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("Paddings")) {
      grad_op->SetInput("Paddings", this->Input("Paddings"));
    }
  }
};


class Pad3dDoubleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pad3d_double_grad, Pad3dDoubleGradInferShapeFunctor,
                            PD_INFER_META(phi::Pad3dInferMeta));



template <typename T>
class PixelShuffleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pixel_shuffle_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PixelShuffleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pixel_shuffle_grad, PixelShuffleGradInferShapeFunctor,
                            PD_INFER_META(phi::PixelShuffleGradInferMeta));



template <typename T>
class PixelUnshuffleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pixel_unshuffle_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PixelUnshuffleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(pixel_unshuffle_grad, PixelUnshuffleGradInferShapeFunctor,
                            PD_INFER_META(phi::PixelUnshuffleGradInferMeta));



template <typename T>
class PoissonGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("poisson_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PoissonGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(poisson_grad, PoissonGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class PolygammaGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("polygamma_grad");

    grad_op->SetInput("x", this->Input("x"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PolygammaGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(polygamma_grad, PolygammaGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class PowGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pow_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("FactorTensor")) {
      grad_op->SetInput("FactorTensor", this->Input("FactorTensor"));
    }
  }
};


class PowGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pow_grad, PowGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(PowGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


class PowCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto out_grad = this->GetSingleOutputGrad("Out");

    auto tensor_y = this->GetOptionalSingleForwardInput("FactorTensor");
    if (tensor_y) {
      PADDLE_THROW(phi::errors::Unimplemented(
          "We don't support dynamic tensor attribute FactorTensor for pow_grad composite"
          "for now. "));
    }
    //get attr
    const float y = this->Attr<float>("factor");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing pow_grad composite func";
    prim::pow_grad<prim::DescTensor>(x, out_grad, y, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class PowDoubleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pow_double_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("grad_out", this->Input(GradVarName("Out")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("FactorTensor")) {
      grad_op->SetInput("FactorTensor", this->Input("FactorTensor"));
    }
  }
};


class PowDoubleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pow_double_grad, PowDoubleGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));
DECLARE_INPLACE_OP_INFERER(PowDoubleGradInplaceInferer,
                           {GradVarName("grad_x"), GradVarName("X")});


class PowDoubleCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto x = this->GetSingleForwardInput("X");
    auto grad_out = this->GetSingleForwardInput(GradVarName("Out"));
    auto grad_x_grad = this->GetSingleOutputGrad(GradVarName("X"));

    auto tensor_y = this->GetOptionalSingleForwardInput("FactorTensor");
    if (tensor_y) {
      PADDLE_THROW(phi::errors::Unimplemented(
          "We don't support dynamic tensor attribute FactorTensor for pow_double_grad composite"
          "for now. "));
    }
    //get attr
    const float y = this->Attr<float>("factor");

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");
    auto grad_out_grad_t = this->GetSingleInputGrad(GradVarName("Out"));

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);
    auto grad_out_grad = this->GetOutputPtr(&grad_out_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);
    auto grad_out_grad_name = this->GetOutputName(grad_out_grad_t);

    //call composite backward func
    VLOG(6) << "Runing pow_double_grad composite func";
    prim::pow_double_grad<prim::DescTensor>(x, grad_out, grad_x_grad, y, x_grad, grad_out_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);
    this->RecoverOutputName(grad_out_grad_t, grad_out_grad_name);

  }
};

template <typename T>
class PowTripleGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("pow_triple_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("grad_out", this->Input("grad_out"));
    grad_op->SetInput("grad_grad_x", this->Input(GradVarName("grad_x")));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));
    if (this->HasOutput(GradVarName("grad_out"))) {
      grad_op->SetInput(GradVarName("grad_grad_out"), this->OutputGrad(GradVarName("grad_out")));
    }

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad("grad_out"));
    grad_op->SetOutput(GradVarName("grad_grad_x"), this->InputGrad(GradVarName("grad_x")));

    grad_op->SetAttrMap(this->Attrs());
    if (this->HasInput("FactorTensor")) {
      grad_op->SetInput("FactorTensor", this->Input("FactorTensor"));
    }
  }
};


class PowTripleGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(pow_triple_grad, PowTripleGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralTernaryGradInferMeta));



template <typename T>
class PreluGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("prelu_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Alpha", this->Input("Alpha"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));
    grad_op->SetOutput(GradVarName("Alpha"), this->InputGrad("Alpha"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PreluGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(prelu_grad, PreluGradInferShapeFunctor,
                            PD_INFER_META(phi::PreluGradInferMeta));



template <typename T>
class PsroiPoolGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("psroi_pool_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("ROIs", this->Input("ROIs"));
    if (this->HasInput("RoisNum")) {
      grad_op->SetInput("RoisNum", this->Input("RoisNum"));
    }
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PsroiPoolGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "X");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(psroi_pool_grad, PsroiPoolGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralUnaryGradInferMeta));



template <typename T>
class PutAlongAxisGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("put_along_axis_grad");

    grad_op->SetInput("Input", this->Input("Input"));
    grad_op->SetInput("Index", this->Input("Index"));
    grad_op->SetInput("Value", this->Input("Value"));
    grad_op->SetInput("Result", this->Output("Result"));
    grad_op->SetInput(GradVarName("Result"), this->OutputGrad("Result"));

    grad_op->SetOutput(GradVarName("Input"), this->InputGrad("Input"));
    grad_op->SetOutput(GradVarName("Value"), this->InputGrad("Value"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class PutAlongAxisGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(put_along_axis_grad, PutAlongAxisGradInferShapeFunctor,
                            PD_INFER_META(phi::GeneralBinaryGradInferMeta));



template <typename T>
class QrGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("qr_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput("Q", this->Output("Q"));
    grad_op->SetInput("R", this->Output("R"));
    grad_op->SetInput(GradVarName("Q"), this->OutputGrad("Q"));
    grad_op->SetInput(GradVarName("R"), this->OutputGrad("R"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class QrGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(qr_grad, QrGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));



template <typename T>
class RealGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("real_grad");

    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class RealGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, GradVarName("Out"));
    data_type = framework::ToComplexType(data_type);
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(real_grad, RealGradInferShapeFunctor,
                            PD_INFER_META(phi::RealAndImagGradInferMeta));



template <typename T>
class ReciprocalGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reciprocal_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ReciprocalGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(reciprocal_grad, ReciprocalGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ReciprocalGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class ReluGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("relu_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ReluGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(relu_grad, ReluGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ReluGradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});


class ReluCompositeGradOpMaker : public prim::CompositeGradOpMakerBase {
 public:
  using prim::CompositeGradOpMakerBase::CompositeGradOpMakerBase;
  void Apply() override {
    //get inputs
    auto out = this->GetSingleForwardOutput("Out");
    auto out_grad = this->GetSingleOutputGrad("Out");


    //get attr

    //get output
    auto x_grad_t = this->GetSingleInputGrad("X");

    //get output ptr
    auto x_grad = this->GetOutputPtr(&x_grad_t);

    //get output orginal name
    auto x_grad_name = this->GetOutputName(x_grad_t);

    //call composite backward func
    VLOG(6) << "Runing relu_grad composite func";
    prim::relu_grad<prim::DescTensor>(out, out_grad, x_grad);
    //recover output name
    this->RecoverOutputName(x_grad_t, x_grad_name);

  }
};

template <typename T>
class ReluGradGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("relu_grad_grad");

    grad_op->SetInput("Out", this->Input("Out"));
    grad_op->SetInput(GradVarName("grad_x"), this->OutputGrad(GradVarName("X")));

    grad_op->SetOutput(GradVarName("grad_out"), this->InputGrad(GradVarName("Out")));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class ReluGradGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(relu_grad_grad, ReluGradGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(ReluGradGradInplaceInferer,
                           {GradVarName("grad_x"), GradVarName("grad_out")});



template <typename T>
class Relu6GradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("relu6_grad");

    grad_op->SetInput("Out", this->Output("Out"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class Relu6GradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(relu6_grad, Relu6GradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));
DECLARE_INPLACE_OP_INFERER(Relu6GradInplaceInferer,
                           {GradVarName("Out"), GradVarName("X")});



template <typename T>
class RenormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("renorm_grad");

    grad_op->SetInput("X", this->Input("X"));
    grad_op->SetInput(GradVarName("Out"), this->OutputGrad("Out"));

    grad_op->SetOutput(GradVarName("X"), this->InputGrad("X"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class RenormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
};


DECLARE_INFER_SHAPE_FUNCTOR(renorm_grad, RenormGradInferShapeFunctor,
                            PD_INFER_META(phi::UnchangedInferMeta));


template <typename T>
class ReverseGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("reverse");

    grad_op->SetInput("X", this->OutputGrad("Out"));

    grad_op->SetOutput("Out", this->InputGrad("X"));

    if (this->HasInput("AxisTensor")) {
      grad_op->SetInput("AxisTensor", this->Input("AxisTensor"));
    }
    if (this->HasInput("AxisTensorList")) {
      grad_op->SetInput("AxisTensorList", this->Input("AxisTensorList"));
    }

    grad_op->SetAttr("axis", this->GetAttr("axis"));
  }
};


template <typename T>
class RmsNormGradOpMaker : public framework::SingleGradOpMaker<T> {
 public:
  using framework::SingleGradOpMaker<T>::SingleGradOpMaker;

 protected:
  void Apply(GradOpPtr<T> grad_op) const override {
    grad_op->SetType("rms_norm_grad");

    grad_op->SetInput("x", this->Input("x"));
    if (this->HasInput("bias")) {
      grad_op->SetInput("bias", this->Input("bias"));
    }
    if (this->HasInput("residual")) {
      grad_op->SetInput("residual", this->Input("residual"));
    }
    grad_op->SetInput("norm_weight", this->Input("norm_weight"));
    if (this->HasInput("norm_bias")) {
      grad_op->SetInput("norm_bias", this->Input("norm_bias"));
    }
    grad_op->SetInput("inv_var", this->Output("inv_var"));
    grad_op->SetInput(GradVarName("out"), this->OutputGrad("out"));

    grad_op->SetOutput(GradVarName("x"), this->InputGrad("x"));
    grad_op->SetOutput(GradVarName("norm_weight"), this->InputGrad("norm_weight"));

    grad_op->SetAttrMap(this->Attrs());
  }
};


class RmsNormGradOp : public framework::OperatorWithKernel {
 public:
  using framework::OperatorWithKernel::OperatorWithKernel;
 protected:
  phi::KernelKey GetExpectedKernelType(
      const framework::ExecutionContext& ctx) const override {
      phi::KernelKey kt;
    auto data_type = framework::OperatorWithKernel::IndicateVarDataType(ctx, "x");
    kt = phi::KernelKey(data_type, ctx.GetPlace());
    return kt;
  }

};


DECLARE_INFER_SHAPE_FUNCTOR(rms_norm_grad, RmsNormGradInferShapeFunctor,
                            PD_INFER_META(phi::RmsNormGradInferMeta));


}  // namespace operators
}  // namespace paddle

namespace ops = paddle::operators;
REGISTER_OPERATOR(kthvalue, ops::KthvalueOp,
                  ops::KthvalueOpMaker,
                  ops::KthvalueGradOpMaker<paddle::framework::OpDesc>,
                  ops::KthvalueGradOpMaker<paddle::imperative::OpBase>,
                  ops::KthvalueInferShapeFunctor);


REGISTER_OPERATOR(label_smooth, ops::LabelSmoothOp,
                  ops::LabelSmoothOpMaker,
                  ops::LabelSmoothGradOpMaker<paddle::framework::OpDesc>,
                  ops::LabelSmoothGradOpMaker<paddle::imperative::OpBase>,
                  ops::LabelSmoothInferShapeFunctor);


REGISTER_OPERATOR(lamb, ops::LambOp,
                  ops::LambOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LambInferShapeFunctor);

REGISTER_OP_VERSION(lamb)
  .AddCheckpoint(
    R"ROC(Upgrade lamb, add two new outputs [Beta1PowOut] and [Beta2PowOut].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewOutput("Beta1PowOut", "The Output beta1 power accumulator. 'Beta1PowOut' is dispensable.")
        .NewOutput("Beta2PowOut", "The Output beta2 power accumulator. 'Beta2PowOut' is dispensable."))
;

REGISTER_OPERATOR(layer_norm, ops::LayerNormOp,
                  ops::LayerNormOpMaker,
                  ops::LayerNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::LayerNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::LayerNormCompositeGradOpMaker,
                  ops::LayerNormInferShapeFunctor);


REGISTER_OPERATOR(leaky_relu, ops::LeakyReluOp,
                  ops::LeakyReluOpMaker,
                  ops::LeakyReluGradOpMaker<paddle::framework::OpDesc>,
                  ops::LeakyReluGradOpMaker<paddle::imperative::OpBase>,
                  ops::LeakyReluInplaceInferer,
                  ops::LeakyReluCompositeGradOpMaker,
                  ops::LeakyReluInferShapeFunctor);


REGISTER_OPERATOR(lerp, ops::LerpOp,
                  ops::LerpOpMaker,
                  ops::LerpGradOpMaker<paddle::framework::OpDesc>,
                  ops::LerpGradOpMaker<paddle::imperative::OpBase>,
                  ops::LerpInplaceInferer,
                  ops::LerpInferShapeFunctor);


REGISTER_OPERATOR(lgamma, ops::LgammaOp,
                  ops::LgammaOpMaker,
                  ops::LgammaGradOpMaker<paddle::framework::OpDesc>,
                  ops::LgammaGradOpMaker<paddle::imperative::OpBase>,
                  ops::LgammaInplaceInferer,
                  ops::LgammaInferShapeFunctor);


REGISTER_OPERATOR(linear_interp_v2, ops::LinearInterpV2Op,
                  ops::LinearInterpV2OpMaker,
                  ops::LinearInterpV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::LinearInterpV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::LinearInterpV2InferShapeFunctor);


REGISTER_OPERATOR(llm_int8_linear, ops::LlmInt8LinearOp,
                  ops::LlmInt8LinearOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LlmInt8LinearInferShapeFunctor);


REGISTER_OPERATOR(log, ops::LogOp,
                  ops::LogOpMaker,
                  ops::LogGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogInplaceInferer,
                  ops::LogCompositeGradOpMaker,
                  ops::LogInferShapeFunctor);


REGISTER_OPERATOR(log10, ops::Log10Op,
                  ops::Log10OpMaker,
                  ops::Log10GradOpMaker<paddle::framework::OpDesc>,
                  ops::Log10GradOpMaker<paddle::imperative::OpBase>,
                  ops::Log10InplaceInferer,
                  ops::Log10InferShapeFunctor);


REGISTER_OPERATOR(log1p, ops::Log1pOp,
                  ops::Log1pOpMaker,
                  ops::Log1pGradOpMaker<paddle::framework::OpDesc>,
                  ops::Log1pGradOpMaker<paddle::imperative::OpBase>,
                  ops::Log1pInplaceInferer,
                  ops::Log1pInferShapeFunctor);


REGISTER_OPERATOR(log2, ops::Log2Op,
                  ops::Log2OpMaker,
                  ops::Log2GradOpMaker<paddle::framework::OpDesc>,
                  ops::Log2GradOpMaker<paddle::imperative::OpBase>,
                  ops::Log2InplaceInferer,
                  ops::Log2InferShapeFunctor);


REGISTER_OPERATOR(log_loss, ops::LogLossOp,
                  ops::LogLossOpMaker,
                  ops::LogLossGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogLossGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogLossInferShapeFunctor);


REGISTER_OPERATOR(log_softmax, ops::LogSoftmaxOp,
                  ops::LogSoftmaxOpMaker,
                  ops::LogSoftmaxGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogSoftmaxGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogSoftmaxInferShapeFunctor);


REGISTER_OPERATOR(logcumsumexp, ops::LogcumsumexpOp,
                  ops::LogcumsumexpOpMaker,
                  ops::LogcumsumexpGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogcumsumexpGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogcumsumexpInferShapeFunctor);


REGISTER_OPERATOR(logical_and, ops::LogicalAndOp,
                  ops::LogicalAndOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogicalAndInplaceInferer,
                  ops::LogicalAndInferShapeFunctor);


REGISTER_OPERATOR(logical_not, ops::LogicalNotOp,
                  ops::LogicalNotOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogicalNotInplaceInferer,
                  ops::LogicalNotInferShapeFunctor);


REGISTER_OPERATOR(logical_or, ops::LogicalOrOp,
                  ops::LogicalOrOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogicalOrInplaceInferer,
                  ops::LogicalOrInferShapeFunctor);


REGISTER_OPERATOR(logical_xor, ops::LogicalXorOp,
                  ops::LogicalXorOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogicalXorInplaceInferer,
                  ops::LogicalXorInferShapeFunctor);


REGISTER_OPERATOR(logit, ops::LogitOp,
                  ops::LogitOpMaker,
                  ops::LogitGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogitGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogitInplaceInferer,
                  ops::LogitInferShapeFunctor);


REGISTER_OPERATOR(logsigmoid, ops::LogsigmoidOp,
                  ops::LogsigmoidOpMaker,
                  ops::LogsigmoidGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogsigmoidGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogsigmoidInferShapeFunctor);


REGISTER_OPERATOR(lstsq, ops::LstsqOp,
                  ops::LstsqOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::LstsqInferShapeFunctor);

REGISTER_OP_VERSION(lstsq)
  .AddCheckpoint(
    R"ROC(Upgrade lstsq, add 1 outputs [Residuals].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewOutput("Residuals", "Output tensor of lstsq operator, meaning the squared residuals of the calculated solutions."))
;

REGISTER_OPERATOR(lu, ops::LuOp,
                  ops::LuOpMaker,
                  ops::LuGradOpMaker<paddle::framework::OpDesc>,
                  ops::LuGradOpMaker<paddle::imperative::OpBase>,
                  ops::LuInplaceInferer,
                  ops::LuInferShapeFunctor);


REGISTER_OPERATOR(lu_unpack, ops::LuUnpackOp,
                  ops::LuUnpackOpMaker,
                  ops::LuUnpackGradOpMaker<paddle::framework::OpDesc>,
                  ops::LuUnpackGradOpMaker<paddle::imperative::OpBase>,
                  ops::LuUnpackInferShapeFunctor);


REGISTER_OPERATOR(margin_cross_entropy, ops::MarginCrossEntropyOp,
                  ops::MarginCrossEntropyOpMaker,
                  ops::MarginCrossEntropyGradOpMaker<paddle::framework::OpDesc>,
                  ops::MarginCrossEntropyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MarginCrossEntropyInferShapeFunctor);


REGISTER_OPERATOR(masked_multihead_attention, ops::MaskedMultiheadAttentionOp,
                  ops::MaskedMultiheadAttentionOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MaskedMultiheadAttentionInferShapeFunctor);


REGISTER_OPERATOR(masked_select, ops::MaskedSelectOp,
                  ops::MaskedSelectOpMaker,
                  ops::MaskedSelectGradOpMaker<paddle::framework::OpDesc>,
                  ops::MaskedSelectGradOpMaker<paddle::imperative::OpBase>,
                  ops::MaskedSelectInferShapeFunctor);


REGISTER_OPERATOR(matrix_nms, ops::MatrixNmsOp,
                  ops::MatrixNmsOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MatrixNmsInferShapeFunctor);

REGISTER_OP_VERSION(matrix_nms)
  .AddCheckpoint(
    R"ROC(Upgrade matrix_nms, add a new output [RoisNum].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewOutput("RoisNum", "The number of RoIs in each image."))
;

REGISTER_OPERATOR(matrix_power, ops::MatrixPowerOp,
                  ops::MatrixPowerOpMaker,
                  ops::MatrixPowerGradOpMaker<paddle::framework::OpDesc>,
                  ops::MatrixPowerGradOpMaker<paddle::imperative::OpBase>,
                  ops::MatrixPowerInferShapeFunctor);


REGISTER_OPERATOR(max_pool2d_with_index, ops::MaxPool2dWithIndexOp,
                  ops::MaxPool2dWithIndexOpMaker,
                  ops::MaxPool2dWithIndexGradOpMaker<paddle::framework::OpDesc>,
                  ops::MaxPool2dWithIndexGradOpMaker<paddle::imperative::OpBase>,
                  ops::MaxPool2dWithIndexInferShapeFunctor);


REGISTER_OPERATOR(max_pool3d_with_index, ops::MaxPool3dWithIndexOp,
                  ops::MaxPool3dWithIndexOpMaker,
                  ops::MaxPool3dWithIndexGradOpMaker<paddle::framework::OpDesc>,
                  ops::MaxPool3dWithIndexGradOpMaker<paddle::imperative::OpBase>,
                  ops::MaxPool3dWithIndexInferShapeFunctor);


REGISTER_OPERATOR(maxout, ops::MaxoutOp,
                  ops::MaxoutOpMaker,
                  ops::MaxoutGradOpMaker<paddle::framework::OpDesc>,
                  ops::MaxoutGradOpMaker<paddle::imperative::OpBase>,
                  ops::MaxoutInferShapeFunctor);


REGISTER_OPERATOR(mean, ops::MeanOp,
                  ops::MeanOpMaker,
                  ops::MeanGradOpMaker<paddle::framework::OpDesc>,
                  ops::MeanGradOpMaker<paddle::imperative::OpBase>,
                  ops::MeanInferShapeFunctor);


REGISTER_OPERATOR(memory_efficient_attention, ops::MemoryEfficientAttentionOp,
                  ops::MemoryEfficientAttentionOpMaker,
                  ops::MemoryEfficientAttentionGradOpMaker<paddle::framework::OpDesc>,
                  ops::MemoryEfficientAttentionGradOpMaker<paddle::imperative::OpBase>,
                  ops::MemoryEfficientAttentionInferShapeFunctor);


REGISTER_OPERATOR(merge_selected_rows, ops::MergeSelectedRowsOp,
                  ops::MergeSelectedRowsOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MergeSelectedRowsInferVarType,
                  ops::MergeSelectedRowsInferShapeFunctor);


REGISTER_OPERATOR(merged_adam, ops::MergedAdamOp,
                  ops::MergedAdamOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MergedAdamInferShapeFunctor);


REGISTER_OPERATOR(merged_momentum, ops::MergedMomentumOp,
                  ops::MergedMomentumOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MergedMomentumInferShapeFunctor);


REGISTER_OPERATOR(meshgrid, ops::MeshgridOp,
                  ops::MeshgridOpMaker,
                  ops::MeshgridGradOpMaker<paddle::framework::OpDesc>,
                  ops::MeshgridGradOpMaker<paddle::imperative::OpBase>,
                  ops::MeshgridInferShapeFunctor);


REGISTER_OPERATOR(mode, ops::ModeOp,
                  ops::ModeOpMaker,
                  ops::ModeGradOpMaker<paddle::framework::OpDesc>,
                  ops::ModeGradOpMaker<paddle::imperative::OpBase>,
                  ops::ModeInferShapeFunctor);


REGISTER_OPERATOR(momentum, ops::MomentumOp,
                  ops::MomentumOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MomentumInferShapeFunctor);

REGISTER_OP_VERSION(momentum)
  .AddCheckpoint(
    R"ROC(Upgrade momentum add 4 attributes [regularization_method, regularization_coeff, multi_precision, rescale_grad].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewInput("MasterParam", "FP32 master weight for AMP.")
        .NewOutput("MasterParamOut", "The updated FP32 master weight for AMP. It shared memory with Input(MasterParam).")
        .NewAttr("regularization_method", "(string) regularization_method, right now only support l2decay or none", std::string(""))
        .NewAttr("regularization_coeff", "(float) regularization_coeff", 0.0)
        .NewAttr("multi_precision", "(bool) Whether to use multi-precision during weight updating.", false)
        .NewAttr("rescale_grad", "(float) Multiply the gradient with `rescale_grad` before updating. Often choose to be `1.0/batch_size`.", 1.0))
;

REGISTER_OPERATOR(multi_dot, ops::MultiDotOp,
                  ops::MultiDotOpMaker,
                  ops::MultiDotGradOpMaker<paddle::framework::OpDesc>,
                  ops::MultiDotGradOpMaker<paddle::imperative::OpBase>,
                  ops::MultiDotInferShapeFunctor);


REGISTER_OPERATOR(multiclass_nms3, ops::MulticlassNms3Op,
                  ops::MulticlassNms3OpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MulticlassNms3InferShapeFunctor);


REGISTER_OPERATOR(multinomial, ops::MultinomialOp,
                  ops::MultinomialOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::MultinomialInferShapeFunctor);


REGISTER_OPERATOR(multiplex, ops::MultiplexOp,
                  ops::MultiplexOpMaker,
                  ops::MultiplexGradOpMaker<paddle::framework::OpDesc>,
                  ops::MultiplexGradOpMaker<paddle::imperative::OpBase>,
                  ops::MultiplexInferShapeFunctor);


REGISTER_OPERATOR(mv, ops::MvOp,
                  ops::MvOpMaker,
                  ops::MvGradOpMaker<paddle::framework::OpDesc>,
                  ops::MvGradOpMaker<paddle::imperative::OpBase>,
                  ops::MvInferShapeFunctor);


REGISTER_OPERATOR(nanmedian, ops::NanmedianOp,
                  ops::NanmedianOpMaker,
                  ops::NanmedianGradOpMaker<paddle::framework::OpDesc>,
                  ops::NanmedianGradOpMaker<paddle::imperative::OpBase>,
                  ops::NanmedianInferShapeFunctor);


REGISTER_OPERATOR(nearest_interp_v2, ops::NearestInterpV2Op,
                  ops::NearestInterpV2OpMaker,
                  ops::NearestInterpV2GradOpMaker<paddle::framework::OpDesc>,
                  ops::NearestInterpV2GradOpMaker<paddle::imperative::OpBase>,
                  ops::NearestInterpV2InferShapeFunctor);


REGISTER_OPERATOR(nextafter, ops::NextafterOp,
                  ops::NextafterOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::NextafterInferShapeFunctor);


REGISTER_OPERATOR(nll_loss, ops::NllLossOp,
                  ops::NllLossOpMaker,
                  ops::NllLossGradOpMaker<paddle::framework::OpDesc>,
                  ops::NllLossGradOpMaker<paddle::imperative::OpBase>,
                  ops::NllLossInferShapeFunctor);


REGISTER_OPERATOR(nms, ops::NmsOp,
                  ops::NmsOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::NmsInferShapeFunctor);


REGISTER_OPERATOR(where_index, ops::WhereIndexOp,
                  ops::WhereIndexOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::WhereIndexInferShapeFunctor);


REGISTER_OPERATOR(npu_identity, ops::NpuIdentityOp,
                  ops::NpuIdentityOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::NpuIdentityInferShapeFunctor);


REGISTER_OPERATOR(size, ops::SizeOp,
                  ops::SizeOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::SizeNoNeedBufferVarInferer,
                  ops::SizeInferShapeFunctor);


REGISTER_OPERATOR(overlap_add, ops::OverlapAddOp,
                  ops::OverlapAddOpMaker,
                  ops::OverlapAddGradOpMaker<paddle::framework::OpDesc>,
                  ops::OverlapAddGradOpMaker<paddle::imperative::OpBase>,
                  ops::OverlapAddInferShapeFunctor);


REGISTER_OPERATOR(p_norm, ops::PNormOp,
                  ops::PNormOpMaker,
                  ops::PNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::PNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::PNormInferShapeFunctor);

REGISTER_OP_VERSION(p_norm)
  .AddCheckpoint(
    R"ROC(Upgrade p_norm, add 1 attribute [asvector].)ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("asvector", "Compute as vector when axis is None and input is matrix.", false))
;

REGISTER_OPERATOR(pad3d, ops::Pad3dOp,
                  ops::Pad3dOpMaker,
                  ops::Pad3dGradOpMaker<paddle::framework::OpDesc>,
                  ops::Pad3dGradOpMaker<paddle::imperative::OpBase>,
                  ops::Pad3dInferShapeFunctor);


REGISTER_OPERATOR(pixel_shuffle, ops::PixelShuffleOp,
                  ops::PixelShuffleOpMaker,
                  ops::PixelShuffleGradOpMaker<paddle::framework::OpDesc>,
                  ops::PixelShuffleGradOpMaker<paddle::imperative::OpBase>,
                  ops::PixelShuffleInferShapeFunctor);

REGISTER_OP_VERSION(pixel_shuffle)
  .AddCheckpoint(
    R"ROC(Compatible upgrade of pixel_shuffle, add a new attribute [data_format])ROC",
      paddle::framework::compatible::OpVersionDesc()
        .NewAttr("data_format", "Specify the data format of the input data", true))
;

REGISTER_OPERATOR(pixel_unshuffle, ops::PixelUnshuffleOp,
                  ops::PixelUnshuffleOpMaker,
                  ops::PixelUnshuffleGradOpMaker<paddle::framework::OpDesc>,
                  ops::PixelUnshuffleGradOpMaker<paddle::imperative::OpBase>,
                  ops::PixelUnshuffleInferShapeFunctor);


REGISTER_OPERATOR(poisson, ops::PoissonOp,
                  ops::PoissonOpMaker,
                  ops::PoissonGradOpMaker<paddle::framework::OpDesc>,
                  ops::PoissonGradOpMaker<paddle::imperative::OpBase>,
                  ops::PoissonInferShapeFunctor);


REGISTER_OPERATOR(polygamma, ops::PolygammaOp,
                  ops::PolygammaOpMaker,
                  ops::PolygammaGradOpMaker<paddle::framework::OpDesc>,
                  ops::PolygammaGradOpMaker<paddle::imperative::OpBase>,
                  ops::PolygammaInplaceInferer,
                  ops::PolygammaInferShapeFunctor);


REGISTER_OPERATOR(pow, ops::PowOp,
                  ops::PowOpMaker,
                  ops::PowGradOpMaker<paddle::framework::OpDesc>,
                  ops::PowGradOpMaker<paddle::imperative::OpBase>,
                  ops::PowInplaceInferer,
                  ops::PowCompositeGradOpMaker,
                  ops::PowInferShapeFunctor);


REGISTER_OPERATOR(prelu, ops::PreluOp,
                  ops::PreluOpMaker,
                  ops::PreluGradOpMaker<paddle::framework::OpDesc>,
                  ops::PreluGradOpMaker<paddle::imperative::OpBase>,
                  ops::PreluInferShapeFunctor);


REGISTER_OPERATOR(prior_box, ops::PriorBoxOp,
                  ops::PriorBoxOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::PriorBoxInferShapeFunctor);


REGISTER_OPERATOR(psroi_pool, ops::PsroiPoolOp,
                  ops::PsroiPoolOpMaker,
                  ops::PsroiPoolGradOpMaker<paddle::framework::OpDesc>,
                  ops::PsroiPoolGradOpMaker<paddle::imperative::OpBase>,
                  ops::PsroiPoolInferShapeFunctor);


REGISTER_OPERATOR(put_along_axis, ops::PutAlongAxisOp,
                  ops::PutAlongAxisOpMaker,
                  ops::PutAlongAxisGradOpMaker<paddle::framework::OpDesc>,
                  ops::PutAlongAxisGradOpMaker<paddle::imperative::OpBase>,
                  ops::PutAlongAxisInplaceInferer,
                  ops::PutAlongAxisInferShapeFunctor);


REGISTER_OPERATOR(qr, ops::QrOp,
                  ops::QrOpMaker,
                  ops::QrGradOpMaker<paddle::framework::OpDesc>,
                  ops::QrGradOpMaker<paddle::imperative::OpBase>,
                  ops::QrInferShapeFunctor);


REGISTER_OPERATOR(real, ops::RealOp,
                  ops::RealOpMaker,
                  ops::RealGradOpMaker<paddle::framework::OpDesc>,
                  ops::RealGradOpMaker<paddle::imperative::OpBase>,
                  ops::RealInferShapeFunctor);


REGISTER_OPERATOR(reciprocal, ops::ReciprocalOp,
                  ops::ReciprocalOpMaker,
                  ops::ReciprocalGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReciprocalGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReciprocalInplaceInferer,
                  ops::ReciprocalInferShapeFunctor);


REGISTER_OPERATOR(graph_reindex, ops::GraphReindexOp,
                  ops::GraphReindexOpMaker,
                  paddle::framework::EmptyGradOpMaker<paddle::framework::OpDesc>,
                  paddle::framework::EmptyGradOpMaker<paddle::imperative::OpBase>,
                  ops::GraphReindexInferShapeFunctor);


REGISTER_OPERATOR(relu, ops::ReluOp,
                  ops::ReluOpMaker,
                  ops::ReluGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReluGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReluInplaceInferer,
                  ops::ReluCompositeGradOpMaker,
                  ops::ReluInferShapeFunctor);


REGISTER_OPERATOR(relu6, ops::Relu6Op,
                  ops::Relu6OpMaker,
                  ops::Relu6GradOpMaker<paddle::framework::OpDesc>,
                  ops::Relu6GradOpMaker<paddle::imperative::OpBase>,
                  ops::Relu6InferShapeFunctor);


REGISTER_OPERATOR(renorm, ops::RenormOp,
                  ops::RenormOpMaker,
                  ops::RenormGradOpMaker<paddle::framework::OpDesc>,
                  ops::RenormGradOpMaker<paddle::imperative::OpBase>,
                  ops::RenormInplaceInferer,
                  ops::RenormInferShapeFunctor);


REGISTER_OPERATOR(reverse, ops::ReverseOp,
                  ops::ReverseOpMaker,
                  ops::ReverseGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReverseGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReverseInferShapeFunctor);


REGISTER_OPERATOR(rms_norm, ops::RmsNormOp,
                  ops::RmsNormOpMaker,
                  ops::RmsNormGradOpMaker<paddle::framework::OpDesc>,
                  ops::RmsNormGradOpMaker<paddle::imperative::OpBase>,
                  ops::RmsNormInferShapeFunctor);


REGISTER_OPERATOR(kthvalue_grad, ops::KthvalueGradOp,
                  ops::KthvalueGradInferShapeFunctor);


REGISTER_OPERATOR(label_smooth_grad, ops::LabelSmoothGradOp,
                  ops::LabelSmoothGradInferShapeFunctor);


REGISTER_OPERATOR(layer_norm_grad, ops::LayerNormGradOp,
                  ops::LayerNormGradNoNeedBufferVarInferer,
                  ops::LayerNormGradInferShapeFunctor);


REGISTER_OPERATOR(leaky_relu_grad, ops::LeakyReluGradOp,
                  ops::LeakyReluGradGradOpMaker<paddle::framework::OpDesc>,
                  ops::LeakyReluGradGradOpMaker<paddle::imperative::OpBase>,
                  ops::LeakyReluGradInplaceInferer,
                  ops::LeakyReluGradInferShapeFunctor);


REGISTER_OPERATOR(leaky_relu_grad_grad, ops::LeakyReluGradGradOp,
                  ops::LeakyReluGradGradInplaceInferer,
                  ops::LeakyReluGradGradInferShapeFunctor);


REGISTER_OPERATOR(lerp_grad, ops::LerpGradOp,
                  ops::LerpGradInferShapeFunctor);


REGISTER_OPERATOR(lgamma_grad, ops::LgammaGradOp,
                  ops::LgammaGradInferShapeFunctor);


REGISTER_OPERATOR(linear_interp_v2_grad, ops::LinearInterpV2GradOp,
                  ops::LinearInterpV2GradNoNeedBufferVarInferer,
                  ops::LinearInterpV2GradInferShapeFunctor);


REGISTER_OPERATOR(log_grad, ops::LogGradOp,
                  ops::LogGradGradOpMaker<paddle::framework::OpDesc>,
                  ops::LogGradGradOpMaker<paddle::imperative::OpBase>,
                  ops::LogGradInplaceInferer,
                  ops::LogGradCompositeGradOpMaker,
                  ops::LogGradInferShapeFunctor);


REGISTER_OPERATOR(log_grad_grad, ops::LogGradGradOp,
                  ops::LogGradGradInplaceInferer,
                  ops::LogGradGradInferShapeFunctor);


REGISTER_OPERATOR(log10_grad, ops::Log10GradOp,
                  ops::Log10GradInplaceInferer,
                  ops::Log10GradInferShapeFunctor);


REGISTER_OPERATOR(log1p_grad, ops::Log1pGradOp,
                  ops::Log1pGradInplaceInferer,
                  ops::Log1pGradInferShapeFunctor);


REGISTER_OPERATOR(log2_grad, ops::Log2GradOp,
                  ops::Log2GradInplaceInferer,
                  ops::Log2GradInferShapeFunctor);


REGISTER_OPERATOR(log_loss_grad, ops::LogLossGradOp,
                  ops::LogLossGradInferShapeFunctor);


REGISTER_OPERATOR(log_softmax_grad, ops::LogSoftmaxGradOp,
                  ops::LogSoftmaxGradInferShapeFunctor);


REGISTER_OPERATOR(logcumsumexp_grad, ops::LogcumsumexpGradOp,
                  ops::LogcumsumexpGradInferShapeFunctor);


REGISTER_OPERATOR(logit_grad, ops::LogitGradOp,
                  ops::LogitGradInferShapeFunctor);


REGISTER_OPERATOR(logsigmoid_grad, ops::LogsigmoidGradOp,
                  ops::LogsigmoidGradInplaceInferer,
                  ops::LogsigmoidGradInferShapeFunctor);


REGISTER_OPERATOR(lu_grad, ops::LuGradOp,
                  ops::LuGradInplaceInferer,
                  ops::LuGradInferShapeFunctor);


REGISTER_OPERATOR(lu_unpack_grad, ops::LuUnpackGradOp,
                  ops::LuUnpackGradInferShapeFunctor);


REGISTER_OPERATOR(margin_cross_entropy_grad, ops::MarginCrossEntropyGradOp,
                  ops::MarginCrossEntropyGradInplaceInferer,
                  ops::MarginCrossEntropyGradInferShapeFunctor);


REGISTER_OPERATOR(masked_select_grad, ops::MaskedSelectGradOp,
                  ops::MaskedSelectGradNoNeedBufferVarInferer,
                  ops::MaskedSelectGradInferShapeFunctor);


REGISTER_OPERATOR(matrix_power_grad, ops::MatrixPowerGradOp,
                  ops::MatrixPowerGradInferShapeFunctor);


REGISTER_OPERATOR(max_pool2d_with_index_grad, ops::MaxPool2dWithIndexGradOp,
                  ops::MaxPool2dWithIndexGradInferShapeFunctor);


REGISTER_OPERATOR(max_pool3d_with_index_grad, ops::MaxPool3dWithIndexGradOp,
                  ops::MaxPool3dWithIndexGradInferShapeFunctor);


REGISTER_OPERATOR(maxout_grad, ops::MaxoutGradOp,
                  ops::MaxoutGradInferShapeFunctor);


REGISTER_OPERATOR(mean_grad, ops::MeanGradOp,
                  ops::MeanGradNoNeedBufferVarInferer,
                  ops::MeanGradInferShapeFunctor);


REGISTER_OPERATOR(memory_efficient_attention_grad, ops::MemoryEfficientAttentionGradOp,
                  ops::MemoryEfficientAttentionGradInferShapeFunctor);


REGISTER_OPERATOR(meshgrid_grad, ops::MeshgridGradOp,
                  ops::MeshgridGradInferShapeFunctor);


REGISTER_OPERATOR(mode_grad, ops::ModeGradOp,
                  ops::ModeGradInferShapeFunctor);


REGISTER_OPERATOR(multi_dot_grad, ops::MultiDotGradOp,
                  ops::MultiDotGradInferShapeFunctor);


REGISTER_OPERATOR(multiplex_grad, ops::MultiplexGradOp,
                  ops::MultiplexGradInferShapeFunctor);


REGISTER_OPERATOR(mv_grad, ops::MvGradOp,
                  ops::MvGradInferShapeFunctor);


REGISTER_OPERATOR(nanmedian_grad, ops::NanmedianGradOp,
                  ops::NanmedianGradInferShapeFunctor);


REGISTER_OPERATOR(nearest_interp_v2_grad, ops::NearestInterpV2GradOp,
                  ops::NearestInterpV2GradNoNeedBufferVarInferer,
                  ops::NearestInterpV2GradInferShapeFunctor);


REGISTER_OPERATOR(nll_loss_grad, ops::NllLossGradOp,
                  ops::NllLossGradInferShapeFunctor);


REGISTER_OPERATOR(overlap_add_grad, ops::OverlapAddGradOp,
                  ops::OverlapAddGradInferShapeFunctor);


REGISTER_OPERATOR(p_norm_grad, ops::PNormGradOp,
                  ops::PNormGradInferShapeFunctor);


REGISTER_OPERATOR(pad3d_grad, ops::Pad3dGradOp,
                  ops::Pad3dDoubleGradOpMaker<paddle::framework::OpDesc>,
                  ops::Pad3dDoubleGradOpMaker<paddle::imperative::OpBase>,
                  ops::Pad3dGradNoNeedBufferVarInferer,
                  ops::Pad3dGradInferShapeFunctor);


REGISTER_OPERATOR(pad3d_double_grad, ops::Pad3dDoubleGradOp,
                  ops::Pad3dDoubleGradInferShapeFunctor);


REGISTER_OPERATOR(pixel_shuffle_grad, ops::PixelShuffleGradOp,
                  ops::PixelShuffleGradInferShapeFunctor);


REGISTER_OPERATOR(pixel_unshuffle_grad, ops::PixelUnshuffleGradOp,
                  ops::PixelUnshuffleGradInferShapeFunctor);


REGISTER_OPERATOR(poisson_grad, ops::PoissonGradOp,
                  ops::PoissonGradInferShapeFunctor);


REGISTER_OPERATOR(polygamma_grad, ops::PolygammaGradOp,
                  ops::PolygammaGradInferShapeFunctor);


REGISTER_OPERATOR(pow_grad, ops::PowGradOp,
                  ops::PowDoubleGradOpMaker<paddle::framework::OpDesc>,
                  ops::PowDoubleGradOpMaker<paddle::imperative::OpBase>,
                  ops::PowGradInplaceInferer,
                  ops::PowDoubleCompositeGradOpMaker,
                  ops::PowGradInferShapeFunctor);


REGISTER_OPERATOR(pow_double_grad, ops::PowDoubleGradOp,
                  ops::PowTripleGradOpMaker<paddle::framework::OpDesc>,
                  ops::PowTripleGradOpMaker<paddle::imperative::OpBase>,
                  ops::PowDoubleGradInplaceInferer,
                  ops::PowDoubleGradInferShapeFunctor);


REGISTER_OPERATOR(pow_triple_grad, ops::PowTripleGradOp,
                  ops::PowTripleGradInferShapeFunctor);


REGISTER_OPERATOR(prelu_grad, ops::PreluGradOp,
                  ops::PreluGradInferShapeFunctor);


REGISTER_OPERATOR(psroi_pool_grad, ops::PsroiPoolGradOp,
                  ops::PsroiPoolGradInferShapeFunctor);


REGISTER_OPERATOR(put_along_axis_grad, ops::PutAlongAxisGradOp,
                  ops::PutAlongAxisGradInferShapeFunctor);


REGISTER_OPERATOR(qr_grad, ops::QrGradOp,
                  ops::QrGradInferShapeFunctor);


REGISTER_OPERATOR(real_grad, ops::RealGradOp,
                  ops::RealGradInferShapeFunctor);


REGISTER_OPERATOR(reciprocal_grad, ops::ReciprocalGradOp,
                  ops::ReciprocalGradInplaceInferer,
                  ops::ReciprocalGradInferShapeFunctor);


REGISTER_OPERATOR(relu_grad, ops::ReluGradOp,
                  ops::ReluGradGradOpMaker<paddle::framework::OpDesc>,
                  ops::ReluGradGradOpMaker<paddle::imperative::OpBase>,
                  ops::ReluGradInplaceInferer,
                  ops::ReluGradInferShapeFunctor);


REGISTER_OPERATOR(relu_grad_grad, ops::ReluGradGradOp,
                  ops::ReluGradGradInplaceInferer,
                  ops::ReluGradGradInferShapeFunctor);


REGISTER_OPERATOR(relu6_grad, ops::Relu6GradOp,
                  ops::Relu6GradInplaceInferer,
                  ops::Relu6GradInferShapeFunctor);


REGISTER_OPERATOR(renorm_grad, ops::RenormGradOp,
                  ops::RenormGradInferShapeFunctor);


REGISTER_OPERATOR(rms_norm_grad, ops::RmsNormGradOp,
                  ops::RmsNormGradInferShapeFunctor);


