// Generated by paddle/phi/api/yaml/generator/tensor_operants_gen.py

#include "paddle/phi/api/include/operants_manager.h"

#include "glog/logging.h"
#include "paddle/phi/core/enforce.h"
#include "paddle/common/errors.h"
#include "paddle/common/flags.h"


COMMON_DECLARE_string(tensor_operants_mode);

namespace paddle {

OperantsManager& OperantsManager::Instance() {
  static OperantsManager g_op_manager;
  return g_op_manager;
}

Tensor OperantsManager::abs(const Tensor& x) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::abs_ad_func";
    return this->eager_operants->abs(x);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::abs<DescTensor>";
    return this->static_operants->abs(x);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::abs";
    return this->phi_operants->abs(x);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::bitwise_and(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::bitwise_and_ad_func";
    return this->eager_operants->bitwise_and(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::bitwise_and<DescTensor>";
    return this->static_operants->bitwise_and(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::bitwise_and";
    return this->phi_operants->bitwise_and(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::bitwise_not(const Tensor& x) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::bitwise_not_ad_func";
    return this->eager_operants->bitwise_not(x);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::bitwise_not<DescTensor>";
    return this->static_operants->bitwise_not(x);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::bitwise_not";
    return this->phi_operants->bitwise_not(x);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::bitwise_or(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::bitwise_or_ad_func";
    return this->eager_operants->bitwise_or(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::bitwise_or<DescTensor>";
    return this->static_operants->bitwise_or(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::bitwise_or";
    return this->phi_operants->bitwise_or(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::bitwise_xor(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::bitwise_xor_ad_func";
    return this->eager_operants->bitwise_xor(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::bitwise_xor<DescTensor>";
    return this->static_operants->bitwise_xor(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::bitwise_xor";
    return this->phi_operants->bitwise_xor(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::exp(const Tensor& x) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::exp_ad_func";
    return this->eager_operants->exp(x);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::exp<DescTensor>";
    return this->static_operants->exp(x);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::exp";
    return this->phi_operants->exp(x);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::expand(const Tensor& x, const IntArray& shape) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::expand_ad_func";
    return this->eager_operants->expand(x, shape);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::expand<DescTensor>";
    return this->static_operants->expand(x, shape);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::expand";
    return this->phi_operants->expand(x, shape);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::floor(const Tensor& x) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::floor_ad_func";
    return this->eager_operants->floor(x);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::floor<DescTensor>";
    return this->static_operants->floor(x);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::floor";
    return this->phi_operants->floor(x);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::gather_nd(const Tensor& x, const Tensor& index) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::gather_nd_ad_func";
    return this->eager_operants->gather_nd(x, index);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::gather_nd<DescTensor>";
    return this->static_operants->gather_nd(x, index);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::gather_nd";
    return this->phi_operants->gather_nd(x, index);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::log(const Tensor& x) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::log_ad_func";
    return this->eager_operants->log(x);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::log<DescTensor>";
    return this->static_operants->log(x);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::log";
    return this->phi_operants->log(x);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::roll(const Tensor& x, const IntArray& shifts, const std::vector<int64_t>& axis) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::roll_ad_func";
    return this->eager_operants->roll(x, shifts, axis);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::roll<DescTensor>";
    return this->static_operants->roll(x, shifts, axis);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::roll";
    return this->phi_operants->roll(x, shifts, axis);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::scale(const Tensor& x, const Scalar& scale, const Scalar& bias, bool bias_after_scale) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::scale_ad_func";
    return this->eager_operants->scale(x, scale, bias, bias_after_scale);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::scale<DescTensor>";
    return this->static_operants->scale(x, scale, bias, bias_after_scale);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::scale";
    return this->phi_operants->scale(x, scale, bias, bias_after_scale);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::scatter(const Tensor& x, const Tensor& index, const Tensor& updates, bool overwrite) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::scatter_ad_func";
    return this->eager_operants->scatter(x, index, updates, overwrite);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::scatter<DescTensor>";
    return this->static_operants->scatter(x, index, updates, overwrite);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::scatter";
    return this->phi_operants->scatter(x, index, updates, overwrite);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::scatter_nd_add(const Tensor& x, const Tensor& index, const Tensor& updates) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::scatter_nd_add_ad_func";
    return this->eager_operants->scatter_nd_add(x, index, updates);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::scatter_nd_add<DescTensor>";
    return this->static_operants->scatter_nd_add(x, index, updates);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::scatter_nd_add";
    return this->phi_operants->scatter_nd_add(x, index, updates);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::add(const Tensor& x, const Scalar& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::add_ad_func";
    return this->eager_operants->add(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::add<DescTensor>";
    return this->static_operants->add(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::add";
    return this->phi_operants->add(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::add(const Scalar& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::add_ad_func";
    return this->eager_operants->add(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::add<DescTensor>";
    return this->static_operants->add(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::add";
    return this->phi_operants->add(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::add(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::add_ad_func";
    return this->eager_operants->add(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::add<DescTensor>";
    return this->static_operants->add(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::add";
    return this->phi_operants->add(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::assign(const Tensor& x) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::assign_ad_func";
    return this->eager_operants->assign(x);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::assign<DescTensor>";
    return this->static_operants->assign(x);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::assign";
    return this->phi_operants->assign(x);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::divide(const Tensor& x, const Scalar& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::divide_ad_func";
    return this->eager_operants->divide(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::divide<DescTensor>";
    return this->static_operants->divide(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::divide";
    return this->phi_operants->divide(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::divide(const Scalar& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::divide_ad_func";
    return this->eager_operants->divide(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::divide<DescTensor>";
    return this->static_operants->divide(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::divide";
    return this->phi_operants->divide(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::divide(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::divide_ad_func";
    return this->eager_operants->divide(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::divide<DescTensor>";
    return this->static_operants->divide(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::divide";
    return this->phi_operants->divide(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::pow(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::pow_ad_func";
    return this->eager_operants->pow(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::pow<DescTensor>";
    return this->static_operants->pow(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::pow";
    return this->phi_operants->pow(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::pow(const Tensor& x, const Scalar& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::pow_ad_func";
    return this->eager_operants->pow(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::pow<DescTensor>";
    return this->static_operants->pow(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::pow";
    return this->phi_operants->pow(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::elementwise_pow(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::elementwise_pow_ad_func";
    return this->eager_operants->elementwise_pow(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::elementwise_pow<DescTensor>";
    return this->static_operants->elementwise_pow(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::elementwise_pow";
    return this->phi_operants->elementwise_pow(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::equal(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::equal_ad_func";
    return this->eager_operants->equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::equal<DescTensor>";
    return this->static_operants->equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::equal";
    return this->phi_operants->equal(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::greater_equal(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::greater_equal_ad_func";
    return this->eager_operants->greater_equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::greater_equal<DescTensor>";
    return this->static_operants->greater_equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::greater_equal";
    return this->phi_operants->greater_equal(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::greater_than(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::greater_than_ad_func";
    return this->eager_operants->greater_than(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::greater_than<DescTensor>";
    return this->static_operants->greater_than(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::greater_than";
    return this->phi_operants->greater_than(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::less_equal(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::less_equal_ad_func";
    return this->eager_operants->less_equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::less_equal<DescTensor>";
    return this->static_operants->less_equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::less_equal";
    return this->phi_operants->less_equal(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::less_than(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::less_than_ad_func";
    return this->eager_operants->less_than(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::less_than<DescTensor>";
    return this->static_operants->less_than(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::less_than";
    return this->phi_operants->less_than(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::matmul(const Tensor& x, const Tensor& y, bool transpose_x, bool transpose_y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::matmul_ad_func";
    return this->eager_operants->matmul(x, y, transpose_x, transpose_y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::matmul<DescTensor>";
    return this->static_operants->matmul(x, y, transpose_x, transpose_y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::matmul";
    return this->phi_operants->matmul(x, y, transpose_x, transpose_y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::max(const Tensor& x, const IntArray& axis, bool keepdim) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::max_ad_func";
    return this->eager_operants->max(x, axis, keepdim);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::max<DescTensor>";
    return this->static_operants->max(x, axis, keepdim);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::max";
    return this->phi_operants->max(x, axis, keepdim);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::maximum(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::maximum_ad_func";
    return this->eager_operants->maximum(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::maximum<DescTensor>";
    return this->static_operants->maximum(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::maximum";
    return this->phi_operants->maximum(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::minimum(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::minimum_ad_func";
    return this->eager_operants->minimum(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::minimum<DescTensor>";
    return this->static_operants->minimum(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::minimum";
    return this->phi_operants->minimum(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::multiply(const Tensor& x, const Scalar& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::multiply_ad_func";
    return this->eager_operants->multiply(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>";
    return this->static_operants->multiply(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::multiply";
    return this->phi_operants->multiply(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::multiply(const Scalar& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::multiply_ad_func";
    return this->eager_operants->multiply(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>";
    return this->static_operants->multiply(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::multiply";
    return this->phi_operants->multiply(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::multiply(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::multiply_ad_func";
    return this->eager_operants->multiply(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::multiply<DescTensor>";
    return this->static_operants->multiply(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::multiply";
    return this->phi_operants->multiply(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::not_equal(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::not_equal_ad_func";
    return this->eager_operants->not_equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::not_equal<DescTensor>";
    return this->static_operants->not_equal(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::not_equal";
    return this->phi_operants->not_equal(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::subtract(const Tensor& x, const Scalar& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::subtract_ad_func";
    return this->eager_operants->subtract(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::subtract<DescTensor>";
    return this->static_operants->subtract(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::subtract";
    return this->phi_operants->subtract(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::subtract(const Scalar& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::subtract_ad_func";
    return this->eager_operants->subtract(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::subtract<DescTensor>";
    return this->static_operants->subtract(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::subtract";
    return this->phi_operants->subtract(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::subtract(const Tensor& x, const Tensor& y) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::subtract_ad_func";
    return this->eager_operants->subtract(x, y);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::subtract<DescTensor>";
    return this->static_operants->subtract(x, y);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::subtract";
    return this->phi_operants->subtract(x, y);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::sum(const Tensor& x, const IntArray& axis, DataType dtype, bool keepdim) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::sum_ad_func";
    return this->eager_operants->sum(x, axis, dtype, keepdim);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::sum<DescTensor>";
    return this->static_operants->sum(x, axis, dtype, keepdim);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::sum";
    return this->phi_operants->sum(x, axis, dtype, keepdim);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

Tensor OperantsManager::tile(const Tensor& x, const IntArray& repeat_times) {
  if (FLAGS_tensor_operants_mode == "eager") {
    PADDLE_ENFORCE_NE(
        this->eager_operants.get(),
        nullptr,
        phi::errors::Unavailable("The eager_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing eager mode API ::tile_ad_func";
    return this->eager_operants->tile(x, repeat_times);
  } else if (FLAGS_tensor_operants_mode == "static") {
    PADDLE_ENFORCE_NE(
        this->static_operants.get(),
        nullptr,
        phi::errors::Unavailable("The static_operants pointer of "
                                 "OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing static mode API paddle::prim::tile<DescTensor>";
    return this->static_operants->tile(x, repeat_times);
  } else if (FLAGS_tensor_operants_mode == "phi") {
    PADDLE_ENFORCE_NE(
        this->phi_operants.get(),
        nullptr,
        phi::errors::Unavailable(
            "The phi_operants pointer of OperantsManager is not initialized"));
    VLOG(4) << "OperantsManager reusing phi mode API paddle::experimental::tile";
    return this->phi_operants->tile(x, repeat_times);
  } else {
    PADDLE_THROW(phi::errors::Unimplemented(
        "FLAGS_tensor_operants_mode is not nitialized, please set "
        "FLAGS_tensor_operants_mode first, which currently supports eager, "
        "phi, and static mode"));
  }
}

}  // namespace paddle

