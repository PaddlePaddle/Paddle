
#include "paddle/phi/api/backward/backward_api.h"
#include <memory>

#include "glog/logging.h"
#include "paddle/common/flags.h"

#include "paddle/phi/api/lib/api_custom_impl.h"
#include "paddle/phi/api/lib/api_gen_utils.h"
#include "paddle/phi/api/lib/data_transform.h"
#include "paddle/phi/api/lib/kernel_dispatch.h"
#include "paddle/phi/common/type_traits.h"
#include "paddle/phi/core/kernel_registry.h"
#include "paddle/phi/api/include/api.h"
#include "paddle/phi/infermeta/backward.h"
#include "paddle/phi/infermeta/unary.h"
#include "paddle/phi/infermeta/fusion.h"

#include "paddle/phi/api/profiler/event_tracing.h"
#include "paddle/phi/api/profiler/supplement_tracing.h"

#ifdef PADDLE_WITH_DISTRIBUTE
#include "paddle/phi/infermeta/spmd_rules/rules.h"
#include "paddle/phi/core/distributed/auto_parallel/reshard/reshard_utils.h"
#endif

PD_DECLARE_bool(conv2d_disable_cudnn);
COMMON_DECLARE_int32(low_precision_op_list);

namespace paddle {
namespace experimental {


PADDLE_API void abs_double_grad(const Tensor& x, const Tensor& grad_x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(grad_x_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "abs_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "abs_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("abs_double_grad", kernel_data_type);
  }
  VLOG(6) << "abs_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {false, true}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("abs_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("abs_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("abs_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_x_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void abs_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("abs_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "abs_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "abs_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "abs_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("abs_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("abs_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "abs_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "abs_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("abs_grad", kernel_data_type);
  }
  VLOG(6) << "abs_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("abs_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("abs_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("abs_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void acos_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("acos_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "acos_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "acos_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "acos_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("acos_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("acos_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "acos_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acos_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("acos_grad", kernel_data_type);
  }
  VLOG(6) << "acos_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("acos_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("acos_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("acos_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void acosh_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("acosh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "acosh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "acosh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "acosh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("acosh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("acosh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "acosh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "acosh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("acosh_grad", kernel_data_type);
  }
  VLOG(6) << "acosh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("acosh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("acosh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("acosh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void addmm_grad(const Tensor& input, const Tensor& x, const Tensor& y, const Tensor& out_grad, float alpha, float beta, Tensor* input_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("addmm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input.impl()), MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "addmm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "addmm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "addmm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[1], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[2], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["alpha"] = alpha;
         attrs["beta"] = beta;
         phi::RecordOpInfoSupplement("addmm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("addmm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_x, *input_y, *input_out_grad, alpha, beta, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, input_grad, "input_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "addmm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "addmm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("addmm_grad", kernel_data_type);
  }
  VLOG(6) << "addmm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     attrs["beta"] = beta;
     phi::RecordOpInfoSupplement("addmm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(x_grad);
  auto kernel_out_2 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("addmm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("addmm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_x, *input_y, *input_out_grad, alpha, beta, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void affine_grid_grad(const Tensor& input, const Tensor& output_grad, const IntArray& output_shape, bool align_corners, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_output_grad);
    DebugInfoForInferSpmd("affine_grid_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::AffineGridGradInferMeta(MakeMetaTensor(*output_grad.impl()), output_shape, align_corners, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "affine_grid_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "affine_grid_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "affine_grid_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[0], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"output_grad", {
         (*input_output_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["output_shape"] = output_shape.GetData();
         attrs["align_corners"] = align_corners;
         phi::RecordOpInfoSupplement("affine_grid_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::AffineGridGradInferMeta(MakeMetaTensor(*input_output_grad), output_shape, align_corners, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("affine_grid_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_output_grad, phi::IntArray(output_shape), align_corners, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "affine_grid_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "affine_grid_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("affine_grid_grad", kernel_data_type);
  }
  VLOG(6) << "affine_grid_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"output_grad", {
     (*input_output_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_shape"] = output_shape.GetData();
     attrs["align_corners"] = align_corners;
     phi::RecordOpInfoSupplement("affine_grid_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("affine_grid_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::AffineGridGradInferMeta(MakeMetaTensor(*input_output_grad), output_shape, align_corners, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("affine_grid_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_output_grad, phi::IntArray(output_shape), align_corners, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void angle_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("angle_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "angle_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "angle_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "angle_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("angle_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("angle_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "angle_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "angle_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("angle_grad", kernel_data_type);
  }
  VLOG(6) << "angle_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("angle_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("angle_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("angle_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void argsort_grad(const Tensor& indices, const Tensor& x, const Tensor& out_grad, int axis, bool descending, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(indices, x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(indices, x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_indices, meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("argsort_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "argsort_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "argsort_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "argsort_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[0], "indices");
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[1], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"indices", {
         (*input_indices).dims()}},
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["descending"] = descending;
         phi::RecordOpInfoSupplement("argsort_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("argsort_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_indices, *input_x, *input_out_grad, axis, descending, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "argsort_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "argsort_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("argsort_grad", kernel_data_type);
  }
  VLOG(6) << "argsort_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"indices", {
     (*input_indices).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["descending"] = descending;
     phi::RecordOpInfoSupplement("argsort_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("argsort_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("argsort_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_indices, *input_x, *input_out_grad, axis, descending, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void as_strided_grad(const Tensor& input, const Tensor& out_grad, const std::vector<int64_t>& dims, const std::vector<int64_t>& stride, int64_t offset, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("as_strided_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::StridedUnChangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "as_strided_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "as_strided_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "as_strided_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dims"] = dims;
         attrs["stride"] = stride;
         attrs["offset"] = offset;
         phi::RecordOpInfoSupplement("as_strided_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("as_strided_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::vector<int64_t>&, int64_t, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, dims, stride, offset, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "as_strided_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "as_strided_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("as_strided_grad", kernel_data_type);
  }
  VLOG(6) << "as_strided_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dims"] = dims;
     attrs["stride"] = stride;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("as_strided_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("as_strided_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::vector<int64_t>&, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("as_strided_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, dims, stride, offset, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void asin_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("asin_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "asin_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "asin_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "asin_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("asin_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("asin_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "asin_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asin_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asin_grad", kernel_data_type);
  }
  VLOG(6) << "asin_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("asin_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asin_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asin_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void asinh_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("asinh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "asinh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "asinh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "asinh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("asinh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("asinh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "asinh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "asinh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("asinh_grad", kernel_data_type);
  }
  VLOG(6) << "asinh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("asinh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("asinh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("asinh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void atan2_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("atan2_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "atan2_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "atan2_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "atan2_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("atan2_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("atan2_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "atan2_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan2_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atan2_grad", kernel_data_type);
  }
  VLOG(6) << "atan2_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atan2_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atan2_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atan2_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void atan_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("atan_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "atan_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "atan_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "atan_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("atan_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("atan_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "atan_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atan_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atan_grad", kernel_data_type);
  }
  VLOG(6) << "atan_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atan_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atan_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atan_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void atanh_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("atanh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "atanh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "atanh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "atanh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("atanh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("atanh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "atanh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "atanh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("atanh_grad", kernel_data_type);
  }
  VLOG(6) << "atanh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("atanh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("atanh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("atanh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void bce_loss_grad(const Tensor& input, const Tensor& label, const Tensor& out_grad, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, label, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, label, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_label, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("bce_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "bce_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "bce_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "bce_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[1], "label");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("bce_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("bce_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_label, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "bce_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bce_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bce_loss_grad", kernel_data_type);
  }
  VLOG(6) << "bce_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bce_loss_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bce_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bce_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void bicubic_interp_grad(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const Tensor& output_grad, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_size, size_tensor, scale_tensor, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(output_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_size = out_size ? MakeDistMetaTensor(*(*out_size).impl()) : phi::distributed::DistMetaTensor();
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_size_tensor;
    if (size_tensor) {
        for(auto& e : *size_tensor) {
            meta_dist_input_size_tensor.push_back(MakeDistMetaTensor(*e.impl()));
        }
    }
    auto meta_dist_input_scale_tensor = scale_tensor ? MakeDistMetaTensor(*(*scale_tensor).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_size, meta_dist_input_size_tensor, meta_dist_input_scale_tensor, meta_dist_input_output_grad);
    DebugInfoForInferSpmd("bicubic_interp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "bicubic_interp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "bicubic_interp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "bicubic_interp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_size = ReshardApiInputToKernelInput(dev_ctx, out_size, spmd_info.first[1], "out_size");
      auto dist_input_size_tensor = ReshardApiInputToKernelInput(dev_ctx, size_tensor, spmd_info.first[2], "size_tensor");
      auto dist_input_scale_tensor = ReshardApiInputToKernelInput(dev_ctx, scale_tensor, spmd_info.first[3], "scale_tensor");
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[4], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_size = PrepareDataForDistTensor(dist_input_out_size, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out_size = dist_input_out_size ? paddle::make_optional<phi::DenseTensor>((*dist_input_out_size)->value()) : paddle::none;

      auto dist_input_size_tensor_vec = PrepareDataForDistTensor(dist_input_size_tensor, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {true}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_size_tensor_vec;
      if (size_tensor) {
        for (auto tmp : *dist_input_size_tensor_vec) {
          dense_input_size_tensor_vec.emplace_back(&tmp->value());
      }
    }
    paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor(dense_input_size_tensor_vec);
    std::vector<phi::MetaTensor> dense_input_size_tensor_meta_vec = MakeMetaTensor(dense_input_size_tensor_vec);
    std::vector<const phi::MetaTensor*> dense_input_size_tensor_meta_ptr_vec_tmp(dense_input_size_tensor_meta_vec.size());
    for (size_t i = 0; i < dense_input_size_tensor_meta_ptr_vec_tmp.size(); ++i) {
      dense_input_size_tensor_meta_ptr_vec_tmp[i] = &dense_input_size_tensor_meta_vec[i];
    }
    paddle::optional<std::vector<const phi::MetaTensor*>> dense_input_size_tensor_meta_ptr_vec =
            size_tensor ? paddle::make_optional<std::vector<const phi::MetaTensor*>>(dense_input_size_tensor_meta_ptr_vec_tmp) : paddle::none;

      dist_input_scale_tensor = PrepareDataForDistTensor(dist_input_scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale_tensor = dist_input_scale_tensor ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale_tensor)->value()) : paddle::none;

      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_size_record_shapes;
         if(input_out_size){
           out_size_record_shapes.push_back((*input_out_size).dims());
         }
         std::vector<phi::DDim> scale_tensor_record_shapes;
         if(input_scale_tensor){
           scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_size", out_size_record_shapes},
         {"scale_tensor", scale_tensor_record_shapes},
         {"output_grad", {
         (*input_output_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         if (input_size_tensor){
           ddims_vec.reserve(input_size_tensor->size());
           for (size_t i = 0; i < input_size_tensor->size(); ++i) {
             ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
           }
         }
         input_shapes.emplace_back("size_tensor", ddims_vec);
         phi::AttributeMap attrs;
         attrs["data_format"] = data_format;
         attrs["out_d"] = out_d;
         attrs["out_h"] = out_h;
         attrs["out_w"] = out_w;
         attrs["scale"] = scale;
         attrs["interp_method"] = interp_method;
         attrs["align_corners"] = align_corners;
         attrs["align_mode"] = align_mode;
         phi::RecordOpInfoSupplement("bicubic_interp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("bicubic_interp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "bicubic_interp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bicubic_interp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bicubic_interp_grad", kernel_data_type);
  }
  VLOG(6) << "bicubic_interp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor", scale_tensor_record_shapes},
     {"output_grad", {
     (*input_output_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("bicubic_interp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bicubic_interp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bicubic_interp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void bilinear_grad(const Tensor& x, const Tensor& y, const Tensor& weight, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad, Tensor* weight_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, weight, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_weight = MakeDistMetaTensor(*weight.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_weight, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("bilinear_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(weight_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_3 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_3 = shared_dist_out_3.get();
    phi::DenseTensor* dense_out_3 = dist_out_3 ? dist_out_3->unsafe_mutable_value() : nullptr;
    if (dense_out_3 && !rank_is_in_current_mesh && !dist_out_3->defined()) {
      *dense_out_3 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_out_3(dist_out_3);
    phi::BilinearGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), MakeMetaTensor(*weight.impl()), MakeMetaTensor(*out_grad.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr, dist_out_3 ? &meta_dist_out_3 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_3, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "bilinear_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "bilinear_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "bilinear_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_weight = ReshardApiInputToKernelInput(dev_ctx, weight, spmd_info.first[2], "weight");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_weight = PrepareDataForDistTensor(dist_input_weight, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_weight = &dist_input_weight->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"weight", {
         (*input_weight).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("bilinear_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::MetaTensor meta_dense_out_3(dense_out_3);
      phi::BilinearGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_weight), MakeMetaTensor(*input_out_grad), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr, dense_out_3 ? &meta_dense_out_3 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("bilinear_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, *input_out_grad, dense_out_0, dense_out_1, dense_out_2, dense_out_3);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
        TransDataBackend(dense_out_3, kernel_backend, dense_out_3);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, weight_grad, "weight_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_3, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "bilinear_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bilinear_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bilinear_grad", kernel_data_type);
  }
  VLOG(6) << "bilinear_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bilinear_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);
  auto kernel_out_2 = SetKernelOutput(weight_grad);
  auto kernel_out_3 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bilinear_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::BilinearGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_weight), MakeMetaTensor(*input_out_grad), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bilinear_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, *input_out_grad, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  
}

PADDLE_API void bilinear_interp_grad(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const Tensor& output_grad, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_size, size_tensor, scale_tensor, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(output_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_size = out_size ? MakeDistMetaTensor(*(*out_size).impl()) : phi::distributed::DistMetaTensor();
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_size_tensor;
    if (size_tensor) {
        for(auto& e : *size_tensor) {
            meta_dist_input_size_tensor.push_back(MakeDistMetaTensor(*e.impl()));
        }
    }
    auto meta_dist_input_scale_tensor = scale_tensor ? MakeDistMetaTensor(*(*scale_tensor).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_size, meta_dist_input_size_tensor, meta_dist_input_scale_tensor, meta_dist_input_output_grad);
    DebugInfoForInferSpmd("bilinear_interp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "bilinear_interp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "bilinear_interp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "bilinear_interp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_size = ReshardApiInputToKernelInput(dev_ctx, out_size, spmd_info.first[1], "out_size");
      auto dist_input_size_tensor = ReshardApiInputToKernelInput(dev_ctx, size_tensor, spmd_info.first[2], "size_tensor");
      auto dist_input_scale_tensor = ReshardApiInputToKernelInput(dev_ctx, scale_tensor, spmd_info.first[3], "scale_tensor");
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[4], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_size = PrepareDataForDistTensor(dist_input_out_size, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out_size = dist_input_out_size ? paddle::make_optional<phi::DenseTensor>((*dist_input_out_size)->value()) : paddle::none;

      auto dist_input_size_tensor_vec = PrepareDataForDistTensor(dist_input_size_tensor, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {true}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_size_tensor_vec;
      if (size_tensor) {
        for (auto tmp : *dist_input_size_tensor_vec) {
          dense_input_size_tensor_vec.emplace_back(&tmp->value());
      }
    }
    paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor(dense_input_size_tensor_vec);
    std::vector<phi::MetaTensor> dense_input_size_tensor_meta_vec = MakeMetaTensor(dense_input_size_tensor_vec);
    std::vector<const phi::MetaTensor*> dense_input_size_tensor_meta_ptr_vec_tmp(dense_input_size_tensor_meta_vec.size());
    for (size_t i = 0; i < dense_input_size_tensor_meta_ptr_vec_tmp.size(); ++i) {
      dense_input_size_tensor_meta_ptr_vec_tmp[i] = &dense_input_size_tensor_meta_vec[i];
    }
    paddle::optional<std::vector<const phi::MetaTensor*>> dense_input_size_tensor_meta_ptr_vec =
            size_tensor ? paddle::make_optional<std::vector<const phi::MetaTensor*>>(dense_input_size_tensor_meta_ptr_vec_tmp) : paddle::none;

      dist_input_scale_tensor = PrepareDataForDistTensor(dist_input_scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale_tensor = dist_input_scale_tensor ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale_tensor)->value()) : paddle::none;

      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_size_record_shapes;
         if(input_out_size){
           out_size_record_shapes.push_back((*input_out_size).dims());
         }
         std::vector<phi::DDim> scale_tensor_record_shapes;
         if(input_scale_tensor){
           scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_size", out_size_record_shapes},
         {"scale_tensor", scale_tensor_record_shapes},
         {"output_grad", {
         (*input_output_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         if (input_size_tensor){
           ddims_vec.reserve(input_size_tensor->size());
           for (size_t i = 0; i < input_size_tensor->size(); ++i) {
             ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
           }
         }
         input_shapes.emplace_back("size_tensor", ddims_vec);
         phi::AttributeMap attrs;
         attrs["data_format"] = data_format;
         attrs["out_d"] = out_d;
         attrs["out_h"] = out_h;
         attrs["out_w"] = out_w;
         attrs["scale"] = scale;
         attrs["interp_method"] = interp_method;
         attrs["align_corners"] = align_corners;
         attrs["align_mode"] = align_mode;
         phi::RecordOpInfoSupplement("bilinear_interp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("bilinear_interp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "bilinear_interp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bilinear_interp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bilinear_interp_grad", kernel_data_type);
  }
  VLOG(6) << "bilinear_interp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor", scale_tensor_record_shapes},
     {"output_grad", {
     (*input_output_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("bilinear_interp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bilinear_interp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bilinear_interp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void bmm_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("bmm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::BmmGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), MakeMetaTensor(*out_grad.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "bmm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "bmm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "bmm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("bmm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::BmmGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_out_grad), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("bmm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "bmm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "bmm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("bmm_grad", kernel_data_type);
  }
  VLOG(6) << "bmm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("bmm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("bmm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::BmmGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_out_grad), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("bmm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void broadcast_tensors_grad(const std::vector<Tensor>& input, const std::vector<Tensor>& out_grad, std::vector<Tensor*> input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(input[0].impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_input;
    for(auto& e : input) {
        meta_dist_input_input.push_back(MakeDistMetaTensor(*e.impl()));
    }
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_out_grad;
    for(auto& e : out_grad) {
        meta_dist_input_out_grad.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("broadcast_tensors_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    std::vector<phi::MetaTensor> input_meta_vec;
    for (auto tmp : input) {
      input_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> input_meta_ptr_vec(input_meta_vec.size());
    for (size_t i=0; i < input_meta_ptr_vec.size(); ++i) {
      input_meta_ptr_vec[i] = &input_meta_vec[i];
    }

    phi::UnchangedMultiInferMeta(input_meta_ptr_vec, dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    for (size_t i = 0; i < dist_out.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out[i], current_process_mesh);
    }


    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "broadcast_tensors_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "broadcast_tensors_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "broadcast_tensors_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      auto dist_input_input_vec = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_input_vec;
      for (auto tmp : dist_input_input_vec) {
        dense_input_input_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_input_meta_vec = MakeMetaTensor(dense_input_input_vec);
      std::vector<const phi::MetaTensor*> dense_input_input_meta_ptr_vec(dense_input_input_meta_vec.size());
      for (size_t i = 0; i < dense_input_input_meta_ptr_vec.size(); ++i) {
        dense_input_input_meta_ptr_vec[i] = &dense_input_input_meta_vec[i];
      }

      auto dist_input_out_grad_vec = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_out_grad_vec;
      for (auto tmp : dist_input_out_grad_vec) {
        dense_input_out_grad_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_out_grad_meta_vec = MakeMetaTensor(dense_input_out_grad_vec);
      std::vector<const phi::MetaTensor*> dense_input_out_grad_meta_ptr_vec(dense_input_out_grad_meta_vec.size());
      for (size_t i = 0; i < dense_input_out_grad_meta_ptr_vec.size(); ++i) {
        dense_input_out_grad_meta_ptr_vec[i] = &dense_input_out_grad_meta_vec[i];
      }

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_input_vec.size());
         for (size_t i = 0; i < dense_input_input_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_input_vec[i]).dims());
         }
         input_shapes.emplace_back("input", ddims_vec);
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_out_grad_vec.size());
         for (size_t i = 0; i < dense_input_out_grad_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_out_grad_vec[i]).dims());
         }
         input_shapes.emplace_back("out_grad", ddims_vec);
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("broadcast_tensors_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::UnchangedMultiInferMeta(dense_input_input_meta_ptr_vec, dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("broadcast_tensors_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, dense_input_input_vec, dense_input_out_grad_vec, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "broadcast_tensors_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "broadcast_tensors_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("broadcast_tensors_grad", kernel_data_type);
  }
  VLOG(6) << "broadcast_tensors_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input_vec = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_input(input_input_vec->size());
  for (size_t i = 0; i < input_input.size(); ++i) {
    input_input[i] = &input_input_vec->at(i);
  }
  auto input_out_grad_vec = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_out_grad(input_out_grad_vec->size());
  for (size_t i = 0; i < input_out_grad.size(); ++i) {
    input_out_grad[i] = &input_out_grad_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_input.size());
     for (size_t i = 0; i < input_input.size(); ++i) {
       ddims_vec.emplace_back((*input_input[i]).dims());
     }
     input_shapes.emplace_back("input", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_out_grad.size());
     for (size_t i = 0; i < input_out_grad.size(); ++i) {
       ddims_vec.emplace_back((*input_out_grad[i]).dims());
     }
     input_shapes.emplace_back("out_grad", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("broadcast_tensors_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("broadcast_tensors_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto input_meta_vec = MakeMetaTensor(input_input);
  std::vector<const phi::MetaTensor*> input_metas(input_meta_vec.size());
  for (size_t i = 0; i < input_meta_vec.size(); ++i) {
    input_metas[i] = &input_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnchangedMultiInferMeta(input_metas, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("broadcast_tensors_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_input, input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void ceil_grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("ceil_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "ceil_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "ceil_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "ceil_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("ceil_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("ceil_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "ceil_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "ceil_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("ceil_grad", kernel_data_type);
  }
  VLOG(6) << "ceil_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("ceil_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("ceil_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("ceil_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void celu_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, float alpha, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "celu_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "celu_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("celu_double_grad", kernel_data_type);
  }
  VLOG(6) << "celu_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("celu_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("celu_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("celu_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, alpha, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void celu_grad(const Tensor& x, const Tensor& out_grad, float alpha, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("celu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "celu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "celu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "celu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["alpha"] = alpha;
         phi::RecordOpInfoSupplement("celu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("celu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, alpha, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "celu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "celu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("celu_grad", kernel_data_type);
  }
  VLOG(6) << "celu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("celu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("celu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("celu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cholesky_grad(const Tensor& out, const Tensor& out_grad, bool upper, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cholesky_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cholesky_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cholesky_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cholesky_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["upper"] = upper;
         phi::RecordOpInfoSupplement("cholesky_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cholesky_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, upper, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cholesky_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cholesky_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cholesky_grad", kernel_data_type);
  }
  VLOG(6) << "cholesky_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper"] = upper;
     phi::RecordOpInfoSupplement("cholesky_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cholesky_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cholesky_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, upper, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cholesky_solve_grad(const Tensor& x, const Tensor& y, const Tensor& out, const Tensor& out_grad, bool upper, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cholesky_solve_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cholesky_solve_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cholesky_solve_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cholesky_solve_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["upper"] = upper;
         phi::RecordOpInfoSupplement("cholesky_solve_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cholesky_solve_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, upper, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cholesky_solve_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cholesky_solve_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cholesky_solve_grad", kernel_data_type);
  }
  VLOG(6) << "cholesky_solve_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper"] = upper;
     phi::RecordOpInfoSupplement("cholesky_solve_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cholesky_solve_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cholesky_solve_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, upper, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void clip_double_grad(const Tensor& x, const Tensor& grad_x_grad, const Scalar& min, const Scalar& max, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "clip_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("clip_double_grad", kernel_data_type);
  }
  VLOG(6) << "clip_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (min.dtype()) {
      case DataType::FLOAT32:
          attrs["min"] = static_cast<float>(min.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["min"] = static_cast<double>(min.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["min"] = static_cast<float>(min.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["min"] = static_cast<float>(min.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["min"] = static_cast<int32_t>(min.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["min"] = static_cast<int64_t>(min.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["min"] = static_cast<int16_t>(min.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["min"] = static_cast<int8_t>(min.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["min"] = static_cast<uint16_t>(min.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["min"] = static_cast<uint8_t>(min.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["min"] = static_cast<bool>(min.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["min"] = static_cast<float>(min.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["min"] = static_cast<double>(min.to<complex128>());
          break;
      default:
          attrs["min"] = "";
          break;
    }
    switch (max.dtype()) {
      case DataType::FLOAT32:
          attrs["max"] = static_cast<float>(max.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max"] = static_cast<double>(max.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max"] = static_cast<float>(max.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max"] = static_cast<float>(max.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max"] = static_cast<int32_t>(max.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max"] = static_cast<int64_t>(max.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max"] = static_cast<int16_t>(max.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max"] = static_cast<int8_t>(max.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max"] = static_cast<uint16_t>(max.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max"] = static_cast<uint8_t>(max.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max"] = static_cast<bool>(max.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max"] = static_cast<float>(max.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max"] = static_cast<double>(max.to<complex128>());
          break;
      default:
          attrs["max"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("clip_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("clip_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("clip_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_x_grad, phi::Scalar(min), phi::Scalar(max), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void clip_grad(const Tensor& x, const Tensor& out_grad, const Scalar& min, const Scalar& max, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("clip_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "clip_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "clip_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "clip_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (min.dtype()) {
          case DataType::FLOAT32:
              attrs["min"] = static_cast<float>(min.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["min"] = static_cast<double>(min.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["min"] = static_cast<float>(min.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["min"] = static_cast<float>(min.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["min"] = static_cast<int32_t>(min.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["min"] = static_cast<int64_t>(min.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["min"] = static_cast<int16_t>(min.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["min"] = static_cast<int8_t>(min.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["min"] = static_cast<uint16_t>(min.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["min"] = static_cast<uint8_t>(min.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["min"] = static_cast<bool>(min.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["min"] = static_cast<float>(min.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["min"] = static_cast<double>(min.to<complex128>());
              break;
          default:
              attrs["min"] = "";
              break;
        }
        switch (max.dtype()) {
          case DataType::FLOAT32:
              attrs["max"] = static_cast<float>(max.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["max"] = static_cast<double>(max.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["max"] = static_cast<float>(max.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["max"] = static_cast<float>(max.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["max"] = static_cast<int32_t>(max.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["max"] = static_cast<int64_t>(max.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["max"] = static_cast<int16_t>(max.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["max"] = static_cast<int8_t>(max.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["max"] = static_cast<uint16_t>(max.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["max"] = static_cast<uint8_t>(max.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["max"] = static_cast<bool>(max.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["max"] = static_cast<float>(max.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["max"] = static_cast<double>(max.to<complex128>());
              break;
          default:
              attrs["max"] = "";
              break;
        }
         phi::RecordOpInfoSupplement("clip_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("clip_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::Scalar(min), phi::Scalar(max), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "clip_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "clip_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("clip_grad", kernel_data_type);
  }
  VLOG(6) << "clip_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (min.dtype()) {
      case DataType::FLOAT32:
          attrs["min"] = static_cast<float>(min.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["min"] = static_cast<double>(min.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["min"] = static_cast<float>(min.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["min"] = static_cast<float>(min.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["min"] = static_cast<int32_t>(min.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["min"] = static_cast<int64_t>(min.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["min"] = static_cast<int16_t>(min.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["min"] = static_cast<int8_t>(min.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["min"] = static_cast<uint16_t>(min.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["min"] = static_cast<uint8_t>(min.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["min"] = static_cast<bool>(min.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["min"] = static_cast<float>(min.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["min"] = static_cast<double>(min.to<complex128>());
          break;
      default:
          attrs["min"] = "";
          break;
    }
    switch (max.dtype()) {
      case DataType::FLOAT32:
          attrs["max"] = static_cast<float>(max.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max"] = static_cast<double>(max.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max"] = static_cast<float>(max.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max"] = static_cast<float>(max.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max"] = static_cast<int32_t>(max.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max"] = static_cast<int64_t>(max.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max"] = static_cast<int16_t>(max.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max"] = static_cast<int8_t>(max.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max"] = static_cast<uint16_t>(max.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max"] = static_cast<uint8_t>(max.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max"] = static_cast<bool>(max.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max"] = static_cast<float>(max.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max"] = static_cast<double>(max.to<complex128>());
          break;
      default:
          attrs["max"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("clip_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("clip_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("clip_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::Scalar(min), phi::Scalar(max), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void complex_grad(const Tensor& real, const Tensor& imag, const Tensor& out_grad, Tensor* real_grad, Tensor* imag_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(real, imag, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(real);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(real, imag, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_real = MakeDistMetaTensor(*real.impl());
    auto meta_dist_input_imag = MakeDistMetaTensor(*imag.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_real, meta_dist_input_imag, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("complex_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(real_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(imag_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::ComplexGradInferMeta(MakeMetaTensor(*real.impl()), MakeMetaTensor(*imag.impl()), MakeMetaTensor(*out_grad.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "complex_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "complex_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "complex_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_real = ReshardApiInputToKernelInput(dev_ctx, real, spmd_info.first[0], "real");
      auto dist_input_imag = ReshardApiInputToKernelInput(dev_ctx, imag, spmd_info.first[1], "imag");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_real = PrepareDataForDistTensor(dist_input_real, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_real = &dist_input_real->value();

      dist_input_imag = PrepareDataForDistTensor(dist_input_imag, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_imag = &dist_input_imag->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"real", {
         (*input_real).dims()}},
         {"imag", {
         (*input_imag).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("complex_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::ComplexGradInferMeta(MakeMetaTensor(*input_real), MakeMetaTensor(*input_imag), MakeMetaTensor(*input_out_grad), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("complex_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_real, *input_imag, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, real_grad, "real_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, imag_grad, "imag_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "complex_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "complex_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("complex_grad", kernel_data_type);
  }
  VLOG(6) << "complex_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_real = PrepareData(real, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_imag = PrepareData(imag, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"real", {
     (*input_real).dims()}},
     {"imag", {
     (*input_imag).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("complex_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(real_grad);
  auto kernel_out_1 = SetKernelOutput(imag_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("complex_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ComplexGradInferMeta(MakeMetaTensor(*input_real), MakeMetaTensor(*input_imag), MakeMetaTensor(*input_out_grad), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("complex_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_real, *input_imag, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void concat_grad(const std::vector<Tensor>& x, const Tensor& out_grad, const Scalar& axis, std::vector<Tensor*> x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_x;
    for(auto& e : x) {
        meta_dist_input_x.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ConcatGradInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("concat_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    std::vector<phi::MetaTensor> x_meta_vec;
    for (auto tmp : x) {
      x_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> x_meta_ptr_vec(x_meta_vec.size());
    for (size_t i=0; i < x_meta_ptr_vec.size(); ++i) {
      x_meta_ptr_vec[i] = &x_meta_vec[i];
    }

    phi::UnchangedMultiInferMeta(x_meta_ptr_vec, dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    // API `concat_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "concat_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "concat_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "concat_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      auto dist_input_x_vec = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_x_vec;
      for (auto tmp : dist_input_x_vec) {
        dense_input_x_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_x_meta_vec = MakeMetaTensor(dense_input_x_vec);
      std::vector<const phi::MetaTensor*> dense_input_x_meta_ptr_vec(dense_input_x_meta_vec.size());
      for (size_t i = 0; i < dense_input_x_meta_ptr_vec.size(); ++i) {
        dense_input_x_meta_ptr_vec[i] = &dense_input_x_meta_vec[i];
      }

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_x_vec.size());
         for (size_t i = 0; i < dense_input_x_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_x_vec[i]).dims());
         }
         input_shapes.emplace_back("x", ddims_vec);
         phi::AttributeMap attrs;
        switch (axis.dtype()) {
          case DataType::FLOAT32:
              attrs["axis"] = static_cast<float>(axis.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["axis"] = static_cast<double>(axis.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["axis"] = static_cast<float>(axis.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["axis"] = static_cast<bool>(axis.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["axis"] = static_cast<float>(axis.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["axis"] = static_cast<double>(axis.to<complex128>());
              break;
          default:
              attrs["axis"] = "";
              break;
        }
         phi::RecordOpInfoSupplement("concat_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::UnchangedMultiInferMeta(dense_input_x_meta_ptr_vec, dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("concat_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const phi::Scalar&, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, dense_input_x_vec, *input_out_grad, phi::Scalar(axis), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "concat_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "concat_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("concat_grad", kernel_data_type);
  }
  VLOG(6) << "concat_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("concat_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("concat_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnchangedMultiInferMeta(x_metas, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const phi::Scalar&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("concat_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, *input_out_grad, phi::Scalar(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void conv2d_grad(const Tensor& input, const Tensor& filter, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format, Tensor* input_grad, Tensor* filter_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, filter, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_filter, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("conv2d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input.impl()), MakeMetaTensor(*filter.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "conv2d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "conv2d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "conv2d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["dilations"] = dilations;
         attrs["groups"] = groups;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("conv2d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("conv2d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, const std::vector<int>&, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_out_grad, strides, paddings, padding_algorithm, dilations, groups, data_format, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, input_grad, "input_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "conv2d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d_grad", kernel_data_type);
  }
  VLOG(6) << "conv2d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["dilations"] = dilations;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, const std::vector<int>&, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_out_grad, strides, paddings, padding_algorithm, dilations, groups, data_format, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void conv2d_grad_grad(const Tensor& input, const Tensor& filter, const Tensor& grad_out, const paddle::optional<Tensor>& grad_input_grad, const paddle::optional<Tensor>& grad_filter_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, const std::vector<int>& dilations, int groups, const std::string& data_format, Tensor* input_grad, Tensor* filter_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, filter, grad_out, grad_input_grad, grad_filter_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter, grad_out, grad_input_grad, grad_filter_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_grad_out = MakeDistMetaTensor(*grad_out.impl());
    auto meta_dist_input_grad_input_grad = grad_input_grad ? MakeDistMetaTensor(*(*grad_input_grad).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_grad_filter_grad = grad_filter_grad ? MakeDistMetaTensor(*(*grad_filter_grad).impl()) : phi::distributed::DistMetaTensor();
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_filter, meta_dist_input_grad_out, meta_dist_input_grad_input_grad, meta_dist_input_grad_filter_grad);
    DebugInfoForInferSpmd("conv2d_grad_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(grad_out_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input.impl()), MakeMetaTensor(*filter.impl()), MakeMetaTensor(*grad_out.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "conv2d_grad_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "conv2d_double_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "conv2d_double_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_grad_out = ReshardApiInputToKernelInput(dev_ctx, grad_out, spmd_info.first[2], "grad_out");
      auto dist_input_grad_input_grad = ReshardApiInputToKernelInput(dev_ctx, grad_input_grad, spmd_info.first[3], "grad_input_grad");
      auto dist_input_grad_filter_grad = ReshardApiInputToKernelInput(dev_ctx, grad_filter_grad, spmd_info.first[4], "grad_filter_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_grad_out = PrepareDataForDistTensor(dist_input_grad_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_grad_out = &dist_input_grad_out->value();

      dist_input_grad_input_grad = PrepareDataForDistTensor(dist_input_grad_input_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_grad_input_grad = dist_input_grad_input_grad ? paddle::make_optional<phi::DenseTensor>((*dist_input_grad_input_grad)->value()) : paddle::none;

      dist_input_grad_filter_grad = PrepareDataForDistTensor(dist_input_grad_filter_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_grad_filter_grad = dist_input_grad_filter_grad ? paddle::make_optional<phi::DenseTensor>((*dist_input_grad_filter_grad)->value()) : paddle::none;

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> grad_input_grad_record_shapes;
         if(input_grad_input_grad){
           grad_input_grad_record_shapes.push_back((*input_grad_input_grad).dims());
         }
         std::vector<phi::DDim> grad_filter_grad_record_shapes;
         if(input_grad_filter_grad){
           grad_filter_grad_record_shapes.push_back((*input_grad_filter_grad).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"grad_out", {
         (*input_grad_out).dims()}},
         {"grad_input_grad", grad_input_grad_record_shapes},
         {"grad_filter_grad",
         grad_filter_grad_record_shapes}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["dilations"] = dilations;
         attrs["groups"] = groups;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("conv2d_grad_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_grad_out), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("conv2d_grad_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, const std::string&, const std::vector<int>&, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_grad_out, input_grad_input_grad, input_grad_filter_grad, strides, paddings, padding_algorithm, dilations, groups, data_format, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, input_grad, "input_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, grad_out_grad, "grad_out_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "conv2d_grad_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d_grad_grad", kernel_data_type);
  }
  VLOG(6) << "conv2d_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_input_grad = PrepareData(grad_input_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_filter_grad = PrepareData(grad_filter_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_input_grad_record_shapes;
     if(input_grad_input_grad){
       grad_input_grad_record_shapes.push_back((*input_grad_input_grad).dims());
     }
     std::vector<phi::DDim> grad_filter_grad_record_shapes;
     if(input_grad_filter_grad){
       grad_filter_grad_record_shapes.push_back((*input_grad_filter_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_input_grad", grad_input_grad_record_shapes},
     {"grad_filter_grad",
     grad_filter_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["dilations"] = dilations;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d_grad_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d_grad_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, const std::string&, const std::vector<int>&, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d_grad_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_grad_out, input_grad_input_grad, input_grad_filter_grad, strides, paddings, padding_algorithm, dilations, groups, data_format, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void conv3d_double_grad(const Tensor& input, const Tensor& filter, const Tensor& grad_out, const paddle::optional<Tensor>& grad_input_grad, const paddle::optional<Tensor>& grad_filter_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* input_grad, Tensor* filter_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, filter, grad_out, grad_input_grad, grad_filter_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter, grad_out, grad_input_grad, grad_filter_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "conv3d_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv3d_double_grad", kernel_data_type);
  }
  VLOG(6) << "conv3d_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_input_grad = PrepareData(grad_input_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_filter_grad = PrepareData(grad_filter_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_input_grad_record_shapes;
     if(input_grad_input_grad){
       grad_input_grad_record_shapes.push_back((*input_grad_input_grad).dims());
     }
     std::vector<phi::DDim> grad_filter_grad_record_shapes;
     if(input_grad_filter_grad){
       grad_filter_grad_record_shapes.push_back((*input_grad_filter_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_input_grad", grad_input_grad_record_shapes},
     {"grad_filter_grad",
     grad_filter_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv3d_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv3d_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv3d_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_grad_out, input_grad_input_grad, input_grad_filter_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void conv3d_grad(const Tensor& input, const Tensor& filter, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* input_grad, Tensor* filter_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, filter, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_filter, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("conv3d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input.impl()), MakeMetaTensor(*filter.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "conv3d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "conv3d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "conv3d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["groups"] = groups;
         attrs["dilations"] = dilations;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("conv3d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("conv3d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_out_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, input_grad, "input_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "conv3d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv3d_grad", kernel_data_type);
  }
  VLOG(6) << "conv3d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv3d_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv3d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv3d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_out_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void conv3d_transpose_grad(const Tensor& x, const Tensor& filter, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const std::vector<int>& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* x_grad, Tensor* filter_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, filter, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_filter, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("conv3d_transpose_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::ConvTransposeGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*filter.impl()), MakeMetaTensor(*out_grad.impl()), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "conv3d_transpose_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "conv3d_transpose_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "conv3d_transpose_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["output_padding"] = output_padding;
         attrs["output_size"] = output_size;
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["groups"] = groups;
         attrs["dilations"] = dilations;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("conv3d_transpose_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::ConvTransposeGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_out_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("conv3d_transpose_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "conv3d_transpose_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv3d_transpose_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv3d_transpose_grad", kernel_data_type);
  }
  VLOG(6) << "conv3d_transpose_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv3d_transpose_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv3d_transpose_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ConvTransposeGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_out_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv3d_transpose_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_out_grad, strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void copysign_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("copysign_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "copysign_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "copysign_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "copysign_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("copysign_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("copysign_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "copysign_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "copysign_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("copysign_grad", kernel_data_type);
  }
  VLOG(6) << "copysign_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("copysign_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("copysign_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("copysign_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void cos_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "cos_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cos_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cos_double_grad", kernel_data_type);
  }
  VLOG(6) << "cos_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cos_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cos_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cos_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void cos_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cos_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `cos_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cos_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cos_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cos_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("cos_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cos_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cos_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cos_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cos_grad", kernel_data_type);
  }
  VLOG(6) << "cos_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cos_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cos_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cos_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cos_triple_grad(const Tensor& x, const paddle::optional<Tensor>& grad_out_forward, const paddle::optional<Tensor>& grad_x_grad_forward, const Tensor& grad_x_grad, const paddle::optional<Tensor>& grad_out_grad_grad, Tensor* x_grad, Tensor* grad_out_forward_grad, Tensor* grad_x_grad_forward_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "cos_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cos_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cos_triple_grad", kernel_data_type);
  }
  VLOG(6) << "cos_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_forward = PrepareData(grad_out_forward, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad_forward = PrepareData(grad_x_grad_forward, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_grad_grad = PrepareData(grad_out_grad_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_out_forward_record_shapes;
     if(input_grad_out_forward){
       grad_out_forward_record_shapes.push_back((*input_grad_out_forward).dims());
     }
     std::vector<phi::DDim> grad_x_grad_forward_record_shapes;
     if(input_grad_x_grad_forward){
       grad_x_grad_forward_record_shapes.push_back((*input_grad_x_grad_forward).dims());
     }
     std::vector<phi::DDim> grad_out_grad_grad_record_shapes;
     if(input_grad_out_grad_grad){
       grad_out_grad_grad_record_shapes.push_back((*input_grad_out_grad_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out_forward", grad_out_forward_record_shapes},
     {"grad_x_grad_forward", grad_x_grad_forward_record_shapes},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}},
     {"grad_out_grad_grad",
     grad_out_grad_grad_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cos_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_forward_grad);
  auto kernel_out_2 = SetKernelOutput(grad_x_grad_forward_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cos_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), MakeMetaTensor(input_grad_x_grad_forward), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cos_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_grad_out_forward, input_grad_x_grad_forward, *input_grad_x_grad, input_grad_out_grad_grad, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void cosh_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cosh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cosh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cosh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cosh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("cosh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cosh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cosh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cosh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cosh_grad", kernel_data_type);
  }
  VLOG(6) << "cosh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("cosh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cosh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cosh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void crop_grad(const Tensor& x, const Tensor& out_grad, const IntArray& offsets, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("crop_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::CropGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*out_grad.impl()), offsets, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "crop_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "crop_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "crop_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["offsets"] = offsets.GetData();
         phi::RecordOpInfoSupplement("crop_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::CropGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), offsets, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("crop_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(offsets), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "crop_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "crop_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("crop_grad", kernel_data_type);
  }
  VLOG(6) << "crop_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["offsets"] = offsets.GetData();
     phi::RecordOpInfoSupplement("crop_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("crop_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CropGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), offsets, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("crop_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(offsets), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cross_entropy_with_softmax_grad(const Tensor& label, const Tensor& softmax, const Tensor& loss_grad, bool soft_label, bool use_softmax, bool numeric_stable_mode, int ignore_index, int axis, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(label, softmax, loss_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(loss_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(loss_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(label, softmax, loss_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_softmax = MakeDistMetaTensor(*softmax.impl());
    auto meta_dist_input_loss_grad = MakeDistMetaTensor(*loss_grad.impl());
    auto spmd_info = phi::distributed::CrossEntropyWithSoftmaxGradInferSpmd(meta_dist_input_label, meta_dist_input_softmax, meta_dist_input_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis);
    DebugInfoForInferSpmd("cross_entropy_with_softmax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::CrossEntropyWithSoftmaxGradInferMeta(MakeMetaTensor(*label.impl()), MakeMetaTensor(*softmax.impl()), MakeMetaTensor(*loss_grad.impl()), soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `cross_entropy_with_softmax_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cross_entropy_with_softmax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cross_entropy_with_softmax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cross_entropy_with_softmax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[0], "label");
      auto dist_input_softmax = ReshardApiInputToKernelInput(dev_ctx, softmax, spmd_info.first[1], "softmax");
      auto dist_input_loss_grad = ReshardApiInputToKernelInput(dev_ctx, loss_grad, spmd_info.first[2], "loss_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_softmax = PrepareDataForDistTensor(dist_input_softmax, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_softmax = &dist_input_softmax->value();

      dist_input_loss_grad = PrepareDataForDistTensor(dist_input_loss_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_loss_grad = &dist_input_loss_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"label", {
         (*input_label).dims()}},
         {"softmax", {
         (*input_softmax).dims()}},
         {"loss_grad", {
         (*input_loss_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["soft_label"] = soft_label;
         attrs["use_softmax"] = use_softmax;
         attrs["numeric_stable_mode"] = numeric_stable_mode;
         attrs["ignore_index"] = ignore_index;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("cross_entropy_with_softmax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::CrossEntropyWithSoftmaxGradInferMeta(MakeMetaTensor(*input_label), MakeMetaTensor(*input_softmax), MakeMetaTensor(*input_loss_grad), soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cross_entropy_with_softmax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_label, *input_softmax, *input_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cross_entropy_with_softmax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross_entropy_with_softmax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cross_entropy_with_softmax_grad", kernel_data_type);
  }
  VLOG(6) << "cross_entropy_with_softmax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_softmax = PrepareData(softmax, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_loss_grad = PrepareData(loss_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"label", {
     (*input_label).dims()}},
     {"softmax", {
     (*input_softmax).dims()}},
     {"loss_grad", {
     (*input_loss_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["soft_label"] = soft_label;
     attrs["use_softmax"] = use_softmax;
     attrs["numeric_stable_mode"] = numeric_stable_mode;
     attrs["ignore_index"] = ignore_index;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cross_entropy_with_softmax_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cross_entropy_with_softmax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::CrossEntropyWithSoftmaxGradInferMeta(MakeMetaTensor(*input_label), MakeMetaTensor(*input_softmax), MakeMetaTensor(*input_loss_grad), soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cross_entropy_with_softmax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_label, *input_softmax, *input_loss_grad, soft_label, use_softmax, numeric_stable_mode, ignore_index, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cross_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, int axis, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cross_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cross_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cross_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cross_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("cross_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cross_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cross_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cross_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cross_grad", kernel_data_type);
  }
  VLOG(6) << "cross_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cross_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cross_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cross_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void cummax_grad(const Tensor& x, const Tensor& indices, const Tensor& out_grad, int axis, DataType dtype, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cummax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cummax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cummax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cummax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("cummax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cummax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, axis, dtype, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cummax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cummax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cummax_grad", kernel_data_type);
  }
  VLOG(6) << "cummax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cummax_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cummax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cummax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, axis, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cummin_grad(const Tensor& x, const Tensor& indices, const Tensor& out_grad, int axis, DataType dtype, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cummin_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cummin_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cummin_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cummin_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("cummin_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cummin_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, axis, dtype, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cummin_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cummin_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cummin_grad", kernel_data_type);
  }
  VLOG(6) << "cummin_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("cummin_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cummin_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cummin_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, axis, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cumprod_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, int dim, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cumprod_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cumprod_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cumprod_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cumprod_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dim"] = dim;
         phi::RecordOpInfoSupplement("cumprod_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cumprod_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dim, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cumprod_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumprod_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cumprod_grad", kernel_data_type);
  }
  VLOG(6) << "cumprod_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dim"] = dim;
     phi::RecordOpInfoSupplement("cumprod_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cumprod_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cumprod_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void cumsum_grad(const Tensor& x, const Tensor& out_grad, const Scalar& axis, bool flatten, bool exclusive, bool reverse, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("cumsum_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "cumsum_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "cumsum_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "cumsum_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (axis.dtype()) {
          case DataType::FLOAT32:
              attrs["axis"] = static_cast<float>(axis.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["axis"] = static_cast<double>(axis.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["axis"] = static_cast<float>(axis.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["axis"] = static_cast<bool>(axis.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["axis"] = static_cast<float>(axis.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["axis"] = static_cast<double>(axis.to<complex128>());
              break;
          default:
              attrs["axis"] = "";
              break;
        }
         attrs["flatten"] = flatten;
         attrs["exclusive"] = exclusive;
         attrs["reverse"] = reverse;
         phi::RecordOpInfoSupplement("cumsum_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("cumsum_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, bool, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::Scalar(axis), flatten, exclusive, reverse, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "cumsum_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "cumsum_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("cumsum_grad", kernel_data_type);
  }
  VLOG(6) << "cumsum_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     attrs["flatten"] = flatten;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("cumsum_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("cumsum_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, bool, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("cumsum_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::Scalar(axis), flatten, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void depthwise_conv2d_double_grad(const Tensor& input, const Tensor& filter, const Tensor& grad_out, const paddle::optional<Tensor>& grad_input_grad, const paddle::optional<Tensor>& grad_filter_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* input_grad, Tensor* filter_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, filter, grad_out, grad_input_grad, grad_filter_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter, grad_out, grad_input_grad, grad_filter_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "depthwise_conv2d_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("depthwise_conv2d_double_grad", kernel_data_type);
  }
  VLOG(6) << "depthwise_conv2d_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_input_grad = PrepareData(grad_input_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_filter_grad = PrepareData(grad_filter_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_input_grad_record_shapes;
     if(input_grad_input_grad){
       grad_input_grad_record_shapes.push_back((*input_grad_input_grad).dims());
     }
     std::vector<phi::DDim> grad_filter_grad_record_shapes;
     if(input_grad_filter_grad){
       grad_filter_grad_record_shapes.push_back((*input_grad_filter_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_input_grad", grad_input_grad_record_shapes},
     {"grad_filter_grad",
     grad_filter_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("depthwise_conv2d_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("depthwise_conv2d_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("depthwise_conv2d_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_grad_out, input_grad_input_grad, input_grad_filter_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void depthwise_conv2d_grad(const Tensor& input, const Tensor& filter, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* input_grad, Tensor* filter_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, filter, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, filter, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_filter, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("depthwise_conv2d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input.impl()), MakeMetaTensor(*filter.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "depthwise_conv2d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "depthwise_conv2d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "depthwise_conv2d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["groups"] = groups;
         attrs["dilations"] = dilations;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("depthwise_conv2d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("depthwise_conv2d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_out_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, input_grad, "input_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "depthwise_conv2d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("depthwise_conv2d_grad", kernel_data_type);
  }
  VLOG(6) << "depthwise_conv2d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("depthwise_conv2d_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("depthwise_conv2d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_filter), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("depthwise_conv2d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_filter, *input_out_grad, strides, paddings, padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void det_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("det_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "det_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "determinant_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "determinant_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("det_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("det_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "det_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "determinant_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("det_grad", kernel_data_type);
  }
  VLOG(6) << "determinant_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("det_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("det_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("det_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void diag_grad(const Tensor& x, const Tensor& out_grad, int offset, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("diag_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "diag_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "diag_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "diag_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["offset"] = offset;
         phi::RecordOpInfoSupplement("diag_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("diag_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, offset, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "diag_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diag_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("diag_grad", kernel_data_type);
  }
  VLOG(6) << "diag_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("diag_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("diag_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("diag_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, offset, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void diagonal_grad(const Tensor& x, const Tensor& out_grad, int offset, int axis1, int axis2, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("diagonal_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "diagonal_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "diagonal_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "diagonal_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["offset"] = offset;
         attrs["axis1"] = axis1;
         attrs["axis2"] = axis2;
         phi::RecordOpInfoSupplement("diagonal_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("diagonal_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, offset, axis1, axis2, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "diagonal_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "diagonal_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("diagonal_grad", kernel_data_type);
  }
  VLOG(6) << "diagonal_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["axis1"] = axis1;
     attrs["axis2"] = axis2;
     phi::RecordOpInfoSupplement("diagonal_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("diagonal_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("diagonal_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, offset, axis1, axis2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void digamma_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("digamma_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "digamma_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "digamma_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "digamma_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("digamma_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("digamma_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "digamma_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "digamma_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("digamma_grad", kernel_data_type);
  }
  VLOG(6) << "digamma_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("digamma_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("digamma_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("digamma_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void dist_grad(const Tensor& x, const Tensor& y, const Tensor& out, const Tensor& out_grad, float p, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("dist_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "dist_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "dist_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "dist_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["p"] = p;
         phi::RecordOpInfoSupplement("dist_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("dist_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, p, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "dist_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dist_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dist_grad", kernel_data_type);
  }
  VLOG(6) << "dist_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["p"] = p;
     phi::RecordOpInfoSupplement("dist_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dist_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dist_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, p, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void dot_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("dot_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "dot_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "dot_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "dot_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("dot_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("dot_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "dot_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dot_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dot_grad", kernel_data_type);
  }
  VLOG(6) << "dot_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("dot_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dot_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dot_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void eig_grad(const Tensor& out_w, const Tensor& out_v, const Tensor& out_w_grad, const Tensor& out_v_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_w, out_v, out_w_grad, out_v_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_v_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_v);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_w, out_v, out_w_grad, out_v_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_w = MakeDistMetaTensor(*out_w.impl());
    auto meta_dist_input_out_v = MakeDistMetaTensor(*out_v.impl());
    auto meta_dist_input_out_w_grad = MakeDistMetaTensor(*out_w_grad.impl());
    auto meta_dist_input_out_v_grad = MakeDistMetaTensor(*out_v_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_w, meta_dist_input_out_v, meta_dist_input_out_w_grad, meta_dist_input_out_v_grad);
    DebugInfoForInferSpmd("eig_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::EigGradInferMeta(MakeMetaTensor(*out_w.impl()), MakeMetaTensor(*out_v.impl()), MakeMetaTensor(*out_w_grad.impl()), MakeMetaTensor(*out_v_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "eig_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "eig_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "eig_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_w = ReshardApiInputToKernelInput(dev_ctx, out_w, spmd_info.first[0], "out_w");
      auto dist_input_out_v = ReshardApiInputToKernelInput(dev_ctx, out_v, spmd_info.first[1], "out_v");
      auto dist_input_out_w_grad = ReshardApiInputToKernelInput(dev_ctx, out_w_grad, spmd_info.first[2], "out_w_grad");
      auto dist_input_out_v_grad = ReshardApiInputToKernelInput(dev_ctx, out_v_grad, spmd_info.first[3], "out_v_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_w = PrepareDataForDistTensor(dist_input_out_w, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_w = &dist_input_out_w->value();

      dist_input_out_v = PrepareDataForDistTensor(dist_input_out_v, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_v = &dist_input_out_v->value();

      dist_input_out_w_grad = PrepareDataForDistTensor(dist_input_out_w_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_w_grad = &dist_input_out_w_grad->value();

      dist_input_out_v_grad = PrepareDataForDistTensor(dist_input_out_v_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_v_grad = &dist_input_out_v_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_w", {
         (*input_out_w).dims()}},
         {"out_v", {
         (*input_out_v).dims()}},
         {"out_w_grad", {
         (*input_out_w_grad).dims()}},
         {"out_v_grad", {
         (*input_out_v_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("eig_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::EigGradInferMeta(MakeMetaTensor(*input_out_w), MakeMetaTensor(*input_out_v), MakeMetaTensor(*input_out_w_grad), MakeMetaTensor(*input_out_v_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("eig_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_w, *input_out_v, *input_out_w_grad, *input_out_v_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "eig_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eig_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eig_grad", kernel_data_type);
  }
  VLOG(6) << "eig_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_w = PrepareData(out_w, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_v = PrepareData(out_v, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_w_grad = PrepareData(out_w_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_v_grad = PrepareData(out_v_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_w", {
     (*input_out_w).dims()}},
     {"out_v", {
     (*input_out_v).dims()}},
     {"out_w_grad", {
     (*input_out_w_grad).dims()}},
     {"out_v_grad", {
     (*input_out_v_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("eig_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eig_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::EigGradInferMeta(MakeMetaTensor(*input_out_w), MakeMetaTensor(*input_out_v), MakeMetaTensor(*input_out_w_grad), MakeMetaTensor(*input_out_v_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eig_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_w, *input_out_v, *input_out_w_grad, *input_out_v_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void eigh_grad(const Tensor& out_w, const Tensor& out_v, const Tensor& out_w_grad, const Tensor& out_v_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_w, out_v, out_w_grad, out_v_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_v_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_v);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_w, out_v, out_w_grad, out_v_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_w = MakeDistMetaTensor(*out_w.impl());
    auto meta_dist_input_out_v = MakeDistMetaTensor(*out_v.impl());
    auto meta_dist_input_out_w_grad = MakeDistMetaTensor(*out_w_grad.impl());
    auto meta_dist_input_out_v_grad = MakeDistMetaTensor(*out_v_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_w, meta_dist_input_out_v, meta_dist_input_out_w_grad, meta_dist_input_out_v_grad);
    DebugInfoForInferSpmd("eigh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_v.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "eigh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "eigh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "eigh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_w = ReshardApiInputToKernelInput(dev_ctx, out_w, spmd_info.first[0], "out_w");
      auto dist_input_out_v = ReshardApiInputToKernelInput(dev_ctx, out_v, spmd_info.first[1], "out_v");
      auto dist_input_out_w_grad = ReshardApiInputToKernelInput(dev_ctx, out_w_grad, spmd_info.first[2], "out_w_grad");
      auto dist_input_out_v_grad = ReshardApiInputToKernelInput(dev_ctx, out_v_grad, spmd_info.first[3], "out_v_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_w = PrepareDataForDistTensor(dist_input_out_w, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_w = &dist_input_out_w->value();

      dist_input_out_v = PrepareDataForDistTensor(dist_input_out_v, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_v = &dist_input_out_v->value();

      dist_input_out_w_grad = PrepareDataForDistTensor(dist_input_out_w_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_w_grad = &dist_input_out_w_grad->value();

      dist_input_out_v_grad = PrepareDataForDistTensor(dist_input_out_v_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_v_grad = &dist_input_out_v_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_w", {
         (*input_out_w).dims()}},
         {"out_v", {
         (*input_out_v).dims()}},
         {"out_w_grad", {
         (*input_out_w_grad).dims()}},
         {"out_v_grad", {
         (*input_out_v_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("eigh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_v), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("eigh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_w, *input_out_v, *input_out_w_grad, *input_out_v_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "eigh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eigh_grad", kernel_data_type);
  }
  VLOG(6) << "eigh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_w = PrepareData(out_w, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_v = PrepareData(out_v, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_w_grad = PrepareData(out_w_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_v_grad = PrepareData(out_v_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_w", {
     (*input_out_w).dims()}},
     {"out_v", {
     (*input_out_v).dims()}},
     {"out_w_grad", {
     (*input_out_w_grad).dims()}},
     {"out_v_grad", {
     (*input_out_v_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("eigh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eigh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_v), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eigh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_w, *input_out_v, *input_out_w_grad, *input_out_v_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void eigvalsh_grad(const Tensor& eigenvectors, const Tensor& eigenvalues_grad, const std::string& uplo, bool is_test, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(eigenvectors, eigenvalues_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(eigenvalues_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(eigenvectors);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(eigenvectors, eigenvalues_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_eigenvectors = MakeDistMetaTensor(*eigenvectors.impl());
    auto meta_dist_input_eigenvalues_grad = MakeDistMetaTensor(*eigenvalues_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_eigenvectors, meta_dist_input_eigenvalues_grad);
    DebugInfoForInferSpmd("eigvalsh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::EigvalshGradInferMeta(MakeMetaTensor(*eigenvectors.impl()), MakeMetaTensor(*eigenvalues_grad.impl()), uplo, is_test, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "eigvalsh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "eigvalsh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "eigvalsh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_eigenvectors = ReshardApiInputToKernelInput(dev_ctx, eigenvectors, spmd_info.first[0], "eigenvectors");
      auto dist_input_eigenvalues_grad = ReshardApiInputToKernelInput(dev_ctx, eigenvalues_grad, spmd_info.first[1], "eigenvalues_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_eigenvectors = PrepareDataForDistTensor(dist_input_eigenvectors, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_eigenvectors = &dist_input_eigenvectors->value();

      dist_input_eigenvalues_grad = PrepareDataForDistTensor(dist_input_eigenvalues_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_eigenvalues_grad = &dist_input_eigenvalues_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"eigenvectors", {
         (*input_eigenvectors).dims()}},
         {"eigenvalues_grad", {
         (*input_eigenvalues_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["uplo"] = uplo;
         attrs["is_test"] = is_test;
         phi::RecordOpInfoSupplement("eigvalsh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::EigvalshGradInferMeta(MakeMetaTensor(*input_eigenvectors), MakeMetaTensor(*input_eigenvalues_grad), uplo, is_test, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("eigvalsh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_eigenvectors, *input_eigenvalues_grad, uplo, is_test, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "eigvalsh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "eigvalsh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("eigvalsh_grad", kernel_data_type);
  }
  VLOG(6) << "eigvalsh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_eigenvectors = PrepareData(eigenvectors, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_eigenvalues_grad = PrepareData(eigenvalues_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"eigenvectors", {
     (*input_eigenvectors).dims()}},
     {"eigenvalues_grad", {
     (*input_eigenvalues_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["uplo"] = uplo;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("eigvalsh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("eigvalsh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::EigvalshGradInferMeta(MakeMetaTensor(*input_eigenvectors), MakeMetaTensor(*input_eigenvalues_grad), uplo, is_test, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("eigvalsh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_eigenvectors, *input_eigenvalues_grad, uplo, is_test, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void elu_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, float alpha, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "elu_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elu_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("elu_double_grad", kernel_data_type);
  }
  VLOG(6) << "elu_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("elu_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("elu_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("elu_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, alpha, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void elu_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, float alpha, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("elu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "elu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "elu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "elu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["alpha"] = alpha;
         phi::RecordOpInfoSupplement("elu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("elu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, alpha, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "elu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("elu_grad", kernel_data_type);
  }
  VLOG(6) << "elu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("elu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("elu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("elu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void erf_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("erf_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "erf_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "erf_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "erf_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("erf_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("erf_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "erf_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erf_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("erf_grad", kernel_data_type);
  }
  VLOG(6) << "erf_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("erf_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("erf_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("erf_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void erfinv_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("erfinv_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "erfinv_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "erfinv_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "erfinv_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("erfinv_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("erfinv_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "erfinv_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "erfinv_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("erfinv_grad", kernel_data_type);
  }
  VLOG(6) << "erfinv_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("erfinv_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("erfinv_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("erfinv_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void exp_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("exp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `exp_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "exp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "exp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "exp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("exp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("exp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "exp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "exp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("exp_grad", kernel_data_type);
  }
  VLOG(6) << "exp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("exp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("exp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("exp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void expand_as_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int>& target_shape, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("expand_as_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "expand_as_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "expand_as_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "expand_as_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["target_shape"] = target_shape;
         phi::RecordOpInfoSupplement("expand_as_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("expand_as_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, target_shape, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "expand_as_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expand_as_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expand_as_grad", kernel_data_type);
  }
  VLOG(6) << "expand_as_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["target_shape"] = target_shape;
     phi::RecordOpInfoSupplement("expand_as_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expand_as_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expand_as_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, target_shape, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void expand_grad(const Tensor& x, const Tensor& out_grad, const IntArray& shape, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("expand_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "expand_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "expand_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "expand_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["shape"] = shape.GetData();
         phi::RecordOpInfoSupplement("expand_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("expand_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(shape), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "expand_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expand_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expand_grad", kernel_data_type);
  }
  VLOG(6) << "expand_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["shape"] = shape.GetData();
     phi::RecordOpInfoSupplement("expand_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expand_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expand_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(shape), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void expm1_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("expm1_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "expm1_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "expm1_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "expm1_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("expm1_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("expm1_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "expm1_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "expm1_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("expm1_grad", kernel_data_type);
  }
  VLOG(6) << "expm1_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("expm1_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("expm1_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("expm1_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fft_c2c_grad(const Tensor& out_grad, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fft_c2c_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fft_c2c_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fft_c2c_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fft_c2c_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axes"] = axes;
         attrs["normalization"] = normalization;
         attrs["forward"] = forward;
         phi::RecordOpInfoSupplement("fft_c2c_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fft_c2c_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, axes, normalization, forward, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fft_c2c_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_c2c_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fft_c2c_grad", kernel_data_type);
  }
  VLOG(6) << "fft_c2c_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["normalization"] = normalization;
     attrs["forward"] = forward;
     phi::RecordOpInfoSupplement("fft_c2c_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fft_c2c_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fft_c2c_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, axes, normalization, forward, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fft_c2r_grad(const Tensor& out_grad, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, int64_t last_dim_size, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fft_c2r_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::FFTC2RGradInferMeta(MakeMetaTensor(*out_grad.impl()), axes, normalization, forward, last_dim_size, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fft_c2r_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fft_c2r_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fft_c2r_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axes"] = axes;
         attrs["normalization"] = normalization;
         attrs["forward"] = forward;
         attrs["last_dim_size"] = last_dim_size;
         phi::RecordOpInfoSupplement("fft_c2r_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::FFTC2RGradInferMeta(MakeMetaTensor(*input_out_grad), axes, normalization, forward, last_dim_size, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fft_c2r_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, int64_t, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, axes, normalization, forward, last_dim_size, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fft_c2r_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_c2r_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fft_c2r_grad", kernel_data_type);
  }
  VLOG(6) << "fft_c2r_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["normalization"] = normalization;
     attrs["forward"] = forward;
     attrs["last_dim_size"] = last_dim_size;
     phi::RecordOpInfoSupplement("fft_c2r_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fft_c2r_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FFTC2RGradInferMeta(MakeMetaTensor(*input_out_grad), axes, normalization, forward, last_dim_size, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fft_c2r_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, axes, normalization, forward, last_dim_size, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fft_r2c_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int64_t>& axes, const std::string& normalization, bool forward, bool onesided, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fft_r2c_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fft_r2c_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fft_r2c_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fft_r2c_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axes"] = axes;
         attrs["normalization"] = normalization;
         attrs["forward"] = forward;
         attrs["onesided"] = onesided;
         phi::RecordOpInfoSupplement("fft_r2c_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fft_r2c_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, axes, normalization, forward, onesided, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fft_r2c_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fft_r2c_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fft_r2c_grad", kernel_data_type);
  }
  VLOG(6) << "fft_r2c_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["normalization"] = normalization;
     attrs["forward"] = forward;
     attrs["onesided"] = onesided;
     phi::RecordOpInfoSupplement("fft_r2c_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fft_r2c_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, const std::string&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fft_r2c_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, axes, normalization, forward, onesided, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fill_diagonal_grad(const Tensor& out_grad, float value, int offset, bool wrap, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fill_diagonal_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::FillDiagonalGradInferMeta(MakeMetaTensor(*out_grad.impl()), value, offset, wrap, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fill_diagonal_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fill_diagonal_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fill_diagonal_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["value"] = value;
         attrs["offset"] = offset;
         attrs["wrap"] = wrap;
         phi::RecordOpInfoSupplement("fill_diagonal_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::FillDiagonalGradInferMeta(MakeMetaTensor(*input_out_grad), value, offset, wrap, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fill_diagonal_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, value, offset, wrap, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fill_diagonal_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_diagonal_grad", kernel_data_type);
  }
  VLOG(6) << "fill_diagonal_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["value"] = value;
     attrs["offset"] = offset;
     attrs["wrap"] = wrap;
     phi::RecordOpInfoSupplement("fill_diagonal_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_diagonal_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FillDiagonalGradInferMeta(MakeMetaTensor(*input_out_grad), value, offset, wrap, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_diagonal_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, value, offset, wrap, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fill_diagonal_tensor_grad(const Tensor& out_grad, int64_t offset, int dim1, int dim2, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fill_diagonal_tensor_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::FillDiagonalTensorGradInferMeta(MakeMetaTensor(*out_grad.impl()), offset, dim1, dim2, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fill_diagonal_tensor_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fill_diagonal_tensor_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fill_diagonal_tensor_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["offset"] = offset;
         attrs["dim1"] = dim1;
         attrs["dim2"] = dim2;
         phi::RecordOpInfoSupplement("fill_diagonal_tensor_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::FillDiagonalTensorGradInferMeta(MakeMetaTensor(*input_out_grad), offset, dim1, dim2, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fill_diagonal_tensor_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int64_t, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, offset, dim1, dim2, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fill_diagonal_tensor_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_diagonal_tensor_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_diagonal_tensor_grad", kernel_data_type);
  }
  VLOG(6) << "fill_diagonal_tensor_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["dim1"] = dim1;
     attrs["dim2"] = dim2;
     phi::RecordOpInfoSupplement("fill_diagonal_tensor_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_diagonal_tensor_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::FillDiagonalTensorGradInferMeta(MakeMetaTensor(*input_out_grad), offset, dim1, dim2, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int64_t, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_diagonal_tensor_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, offset, dim1, dim2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fill_grad(const Tensor& out_grad, const Scalar& value, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fill_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fill_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fill_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fill_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (value.dtype()) {
          case DataType::FLOAT32:
              attrs["value"] = static_cast<float>(value.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["value"] = static_cast<double>(value.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["value"] = static_cast<float>(value.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["value"] = static_cast<float>(value.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["value"] = static_cast<bool>(value.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["value"] = static_cast<float>(value.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["value"] = static_cast<double>(value.to<complex128>());
              break;
          default:
              attrs["value"] = "";
              break;
        }
         phi::RecordOpInfoSupplement("fill_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fill_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, phi::Scalar(value), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fill_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fill_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fill_grad", kernel_data_type);
  }
  VLOG(6) << "fill_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (value.dtype()) {
      case DataType::FLOAT32:
          attrs["value"] = static_cast<float>(value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["value"] = static_cast<double>(value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["value"] = static_cast<float>(value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["value"] = static_cast<float>(value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["value"] = static_cast<int32_t>(value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["value"] = static_cast<int64_t>(value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["value"] = static_cast<int16_t>(value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["value"] = static_cast<int8_t>(value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["value"] = static_cast<uint16_t>(value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["value"] = static_cast<uint8_t>(value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["value"] = static_cast<bool>(value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["value"] = static_cast<float>(value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["value"] = static_cast<double>(value.to<complex128>());
          break;
      default:
          attrs["value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("fill_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fill_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fill_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, phi::Scalar(value), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void flash_attn_grad(const Tensor& q, const Tensor& k, const Tensor& v, const Tensor& out, const Tensor& softmax_lse, const Tensor& seed_offset, const paddle::optional<Tensor>& attn_mask, const Tensor& out_grad, float dropout, bool causal, Tensor* q_grad, Tensor* k_grad, Tensor* v_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(q, k, v, out, softmax_lse, seed_offset, attn_mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(q);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, out, softmax_lse, seed_offset, attn_mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_q = MakeDistMetaTensor(*q.impl());
    auto meta_dist_input_k = MakeDistMetaTensor(*k.impl());
    auto meta_dist_input_v = MakeDistMetaTensor(*v.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_softmax_lse = MakeDistMetaTensor(*softmax_lse.impl());
    auto meta_dist_input_seed_offset = MakeDistMetaTensor(*seed_offset.impl());
    auto meta_dist_input_attn_mask = attn_mask ? MakeDistMetaTensor(*(*attn_mask).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::FlashAttGradInferSpmd(meta_dist_input_q, meta_dist_input_k, meta_dist_input_v, meta_dist_input_out, meta_dist_input_softmax_lse, meta_dist_input_seed_offset, meta_dist_input_attn_mask, meta_dist_input_out_grad, dropout, causal);
    DebugInfoForInferSpmd("flash_attn_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(q_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(k_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(v_grad, !rank_is_in_current_mesh, spmd_info.second[2]);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::FlashAttnGradInferMeta(MakeMetaTensor(*q.impl()), MakeMetaTensor(*k.impl()), MakeMetaTensor(*v.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `flash_attn_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "flash_attn_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "flash_attn_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "flash_attn_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_q = ReshardApiInputToKernelInput(dev_ctx, q, spmd_info.first[0], "q");
      auto dist_input_k = ReshardApiInputToKernelInput(dev_ctx, k, spmd_info.first[1], "k");
      auto dist_input_v = ReshardApiInputToKernelInput(dev_ctx, v, spmd_info.first[2], "v");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[3], "out");
      auto dist_input_softmax_lse = ReshardApiInputToKernelInput(dev_ctx, softmax_lse, spmd_info.first[4], "softmax_lse");
      auto dist_input_seed_offset = ReshardApiInputToKernelInput(dev_ctx, seed_offset, spmd_info.first[5], "seed_offset");
      auto dist_input_attn_mask = ReshardApiInputToKernelInput(dev_ctx, attn_mask, spmd_info.first[6], "attn_mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[7], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_q = PrepareDataForDistTensor(dist_input_q, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_q = &dist_input_q->value();

      dist_input_k = PrepareDataForDistTensor(dist_input_k, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_k = &dist_input_k->value();

      dist_input_v = PrepareDataForDistTensor(dist_input_v, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_v = &dist_input_v->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_softmax_lse = PrepareDataForDistTensor(dist_input_softmax_lse, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_softmax_lse = &dist_input_softmax_lse->value();

      dist_input_seed_offset = PrepareDataForDistTensor(dist_input_seed_offset, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_seed_offset = &dist_input_seed_offset->value();

      dist_input_attn_mask = PrepareDataForDistTensor(dist_input_attn_mask, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_attn_mask = dist_input_attn_mask ? paddle::make_optional<phi::DenseTensor>((*dist_input_attn_mask)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> attn_mask_record_shapes;
         if(input_attn_mask){
           attn_mask_record_shapes.push_back((*input_attn_mask).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"q", {
         (*input_q).dims()}},
         {"k", {
         (*input_k).dims()}},
         {"v", {
         (*input_v).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"softmax_lse", {
         (*input_softmax_lse).dims()}},
         {"seed_offset", {
         (*input_seed_offset).dims()}},
         {"attn_mask", attn_mask_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dropout"] = dropout;
         attrs["causal"] = causal;
         phi::RecordOpInfoSupplement("flash_attn_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::FlashAttnGradInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("flash_attn_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_out, *input_softmax_lse, *input_seed_offset, input_attn_mask, *input_out_grad, dropout, causal, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, q_grad, "q_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, k_grad, "k_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, v_grad, "v_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "flash_attn_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_grad", kernel_data_type);
  }
  VLOG(6) << "flash_attn_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_softmax_lse = PrepareData(softmax_lse, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seed_offset = PrepareData(seed_offset, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"softmax_lse", {
     (*input_softmax_lse).dims()}},
     {"seed_offset", {
     (*input_seed_offset).dims()}},
     {"attn_mask", attn_mask_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     phi::RecordOpInfoSupplement("flash_attn_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(q_grad);
  auto kernel_out_1 = SetKernelOutput(k_grad);
  auto kernel_out_2 = SetKernelOutput(v_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::FlashAttnGradInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_out, *input_softmax_lse, *input_seed_offset, input_attn_mask, *input_out_grad, dropout, causal, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void flash_attn_unpadded_grad(const Tensor& q, const Tensor& k, const Tensor& v, const Tensor& cu_seqlens_q, const Tensor& cu_seqlens_k, const Tensor& out, const Tensor& softmax_lse, const Tensor& seed_offset, const paddle::optional<Tensor>& attn_mask, const Tensor& out_grad, int64_t max_seqlen_q, int64_t max_seqlen_k, float scale, float dropout, bool causal, Tensor* q_grad, Tensor* k_grad, Tensor* v_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(q, k, v, cu_seqlens_q, cu_seqlens_k, out, softmax_lse, seed_offset, attn_mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(q);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, cu_seqlens_q, cu_seqlens_k, out, softmax_lse, seed_offset, attn_mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_q = MakeDistMetaTensor(*q.impl());
    auto meta_dist_input_k = MakeDistMetaTensor(*k.impl());
    auto meta_dist_input_v = MakeDistMetaTensor(*v.impl());
    auto meta_dist_input_cu_seqlens_q = MakeDistMetaTensor(*cu_seqlens_q.impl());
    auto meta_dist_input_cu_seqlens_k = MakeDistMetaTensor(*cu_seqlens_k.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_softmax_lse = MakeDistMetaTensor(*softmax_lse.impl());
    auto meta_dist_input_seed_offset = MakeDistMetaTensor(*seed_offset.impl());
    auto meta_dist_input_attn_mask = attn_mask ? MakeDistMetaTensor(*(*attn_mask).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_q, meta_dist_input_k, meta_dist_input_v, meta_dist_input_cu_seqlens_q, meta_dist_input_cu_seqlens_k, meta_dist_input_out, meta_dist_input_softmax_lse, meta_dist_input_seed_offset, meta_dist_input_attn_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("flash_attn_unpadded_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(q_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(k_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(v_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::FlashAttnGradInferMeta(MakeMetaTensor(*q.impl()), MakeMetaTensor(*k.impl()), MakeMetaTensor(*v.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "flash_attn_unpadded_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "flash_attn_unpadded_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "flash_attn_unpadded_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_q = ReshardApiInputToKernelInput(dev_ctx, q, spmd_info.first[0], "q");
      auto dist_input_k = ReshardApiInputToKernelInput(dev_ctx, k, spmd_info.first[1], "k");
      auto dist_input_v = ReshardApiInputToKernelInput(dev_ctx, v, spmd_info.first[2], "v");
      auto dist_input_cu_seqlens_q = ReshardApiInputToKernelInput(dev_ctx, cu_seqlens_q, spmd_info.first[3], "cu_seqlens_q");
      auto dist_input_cu_seqlens_k = ReshardApiInputToKernelInput(dev_ctx, cu_seqlens_k, spmd_info.first[4], "cu_seqlens_k");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[5], "out");
      auto dist_input_softmax_lse = ReshardApiInputToKernelInput(dev_ctx, softmax_lse, spmd_info.first[6], "softmax_lse");
      auto dist_input_seed_offset = ReshardApiInputToKernelInput(dev_ctx, seed_offset, spmd_info.first[7], "seed_offset");
      auto dist_input_attn_mask = ReshardApiInputToKernelInput(dev_ctx, attn_mask, spmd_info.first[8], "attn_mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[9], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_q = PrepareDataForDistTensor(dist_input_q, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_q = &dist_input_q->value();

      dist_input_k = PrepareDataForDistTensor(dist_input_k, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_k = &dist_input_k->value();

      dist_input_v = PrepareDataForDistTensor(dist_input_v, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_v = &dist_input_v->value();

      dist_input_cu_seqlens_q = PrepareDataForDistTensor(dist_input_cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_cu_seqlens_q = &dist_input_cu_seqlens_q->value();

      dist_input_cu_seqlens_k = PrepareDataForDistTensor(dist_input_cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_cu_seqlens_k = &dist_input_cu_seqlens_k->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_softmax_lse = PrepareDataForDistTensor(dist_input_softmax_lse, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_softmax_lse = &dist_input_softmax_lse->value();

      dist_input_seed_offset = PrepareDataForDistTensor(dist_input_seed_offset, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_seed_offset = &dist_input_seed_offset->value();

      dist_input_attn_mask = PrepareDataForDistTensor(dist_input_attn_mask, GetKernelInputArgDef(kernel.InputAt(8), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_attn_mask = dist_input_attn_mask ? paddle::make_optional<phi::DenseTensor>((*dist_input_attn_mask)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(9), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> attn_mask_record_shapes;
         if(input_attn_mask){
           attn_mask_record_shapes.push_back((*input_attn_mask).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"q", {
         (*input_q).dims()}},
         {"k", {
         (*input_k).dims()}},
         {"v", {
         (*input_v).dims()}},
         {"cu_seqlens_q", {
         (*input_cu_seqlens_q).dims()}},
         {"cu_seqlens_k", {
         (*input_cu_seqlens_k).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"softmax_lse", {
         (*input_softmax_lse).dims()}},
         {"seed_offset", {
         (*input_seed_offset).dims()}},
         {"attn_mask", attn_mask_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["max_seqlen_q"] = max_seqlen_q;
         attrs["max_seqlen_k"] = max_seqlen_k;
         attrs["scale"] = scale;
         attrs["dropout"] = dropout;
         attrs["causal"] = causal;
         phi::RecordOpInfoSupplement("flash_attn_unpadded_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::FlashAttnGradInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("flash_attn_unpadded_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int64_t, int64_t, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_cu_seqlens_q, *input_cu_seqlens_k, *input_out, *input_softmax_lse, *input_seed_offset, input_attn_mask, *input_out_grad, max_seqlen_q, max_seqlen_k, scale, dropout, causal, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, q_grad, "q_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, k_grad, "k_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, v_grad, "v_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "flash_attn_unpadded_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_unpadded_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_unpadded_grad", kernel_data_type);
  }
  VLOG(6) << "flash_attn_unpadded_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_q = PrepareData(cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_k = PrepareData(cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_softmax_lse = PrepareData(softmax_lse, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seed_offset = PrepareData(seed_offset, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask = PrepareData(attn_mask, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(9), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> attn_mask_record_shapes;
     if(input_attn_mask){
       attn_mask_record_shapes.push_back((*input_attn_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"cu_seqlens_q", {
     (*input_cu_seqlens_q).dims()}},
     {"cu_seqlens_k", {
     (*input_cu_seqlens_k).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"softmax_lse", {
     (*input_softmax_lse).dims()}},
     {"seed_offset", {
     (*input_seed_offset).dims()}},
     {"attn_mask", attn_mask_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["max_seqlen_q"] = max_seqlen_q;
     attrs["max_seqlen_k"] = max_seqlen_k;
     attrs["scale"] = scale;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     phi::RecordOpInfoSupplement("flash_attn_unpadded_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(q_grad);
  auto kernel_out_1 = SetKernelOutput(k_grad);
  auto kernel_out_2 = SetKernelOutput(v_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_unpadded_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::FlashAttnGradInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int64_t, int64_t, float, float, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_unpadded_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_cu_seqlens_q, *input_cu_seqlens_k, *input_out, *input_softmax_lse, *input_seed_offset, input_attn_mask, *input_out_grad, max_seqlen_q, max_seqlen_k, scale, dropout, causal, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void flash_attn_with_sparse_mask_grad(const Tensor& q, const Tensor& k, const Tensor& v, const Tensor& attn_mask_start_row_indices, const Tensor& out, const Tensor& softmax_lse, const Tensor& seed_offset, const Tensor& out_grad, float dropout, bool causal, int attn_mask_start_row, Tensor* q_grad, Tensor* k_grad, Tensor* v_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(q, k, v, attn_mask_start_row_indices, out, softmax_lse, seed_offset, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(q);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(q, k, v, attn_mask_start_row_indices, out, softmax_lse, seed_offset, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_q = MakeDistMetaTensor(*q.impl());
    auto meta_dist_input_k = MakeDistMetaTensor(*k.impl());
    auto meta_dist_input_v = MakeDistMetaTensor(*v.impl());
    auto meta_dist_input_attn_mask_start_row_indices = MakeDistMetaTensor(*attn_mask_start_row_indices.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_softmax_lse = MakeDistMetaTensor(*softmax_lse.impl());
    auto meta_dist_input_seed_offset = MakeDistMetaTensor(*seed_offset.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_q, meta_dist_input_k, meta_dist_input_v, meta_dist_input_attn_mask_start_row_indices, meta_dist_input_out, meta_dist_input_softmax_lse, meta_dist_input_seed_offset, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("flash_attn_with_sparse_mask_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(q_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(k_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(v_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::FlashAttnGradInferMeta(MakeMetaTensor(*q.impl()), MakeMetaTensor(*k.impl()), MakeMetaTensor(*v.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "flash_attn_with_sparse_mask_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "flash_attn_with_sparse_mask_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "flash_attn_with_sparse_mask_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_q = ReshardApiInputToKernelInput(dev_ctx, q, spmd_info.first[0], "q");
      auto dist_input_k = ReshardApiInputToKernelInput(dev_ctx, k, spmd_info.first[1], "k");
      auto dist_input_v = ReshardApiInputToKernelInput(dev_ctx, v, spmd_info.first[2], "v");
      auto dist_input_attn_mask_start_row_indices = ReshardApiInputToKernelInput(dev_ctx, attn_mask_start_row_indices, spmd_info.first[3], "attn_mask_start_row_indices");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[4], "out");
      auto dist_input_softmax_lse = ReshardApiInputToKernelInput(dev_ctx, softmax_lse, spmd_info.first[5], "softmax_lse");
      auto dist_input_seed_offset = ReshardApiInputToKernelInput(dev_ctx, seed_offset, spmd_info.first[6], "seed_offset");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[7], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_q = PrepareDataForDistTensor(dist_input_q, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_q = &dist_input_q->value();

      dist_input_k = PrepareDataForDistTensor(dist_input_k, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_k = &dist_input_k->value();

      dist_input_v = PrepareDataForDistTensor(dist_input_v, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_v = &dist_input_v->value();

      dist_input_attn_mask_start_row_indices = PrepareDataForDistTensor(dist_input_attn_mask_start_row_indices, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_attn_mask_start_row_indices = &dist_input_attn_mask_start_row_indices->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_softmax_lse = PrepareDataForDistTensor(dist_input_softmax_lse, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_softmax_lse = &dist_input_softmax_lse->value();

      dist_input_seed_offset = PrepareDataForDistTensor(dist_input_seed_offset, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_seed_offset = &dist_input_seed_offset->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"q", {
         (*input_q).dims()}},
         {"k", {
         (*input_k).dims()}},
         {"v", {
         (*input_v).dims()}},
         {"attn_mask_start_row_indices", {
         (*input_attn_mask_start_row_indices).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"softmax_lse", {
         (*input_softmax_lse).dims()}},
         {"seed_offset", {
         (*input_seed_offset).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dropout"] = dropout;
         attrs["causal"] = causal;
         attrs["attn_mask_start_row"] = attn_mask_start_row;
         phi::RecordOpInfoSupplement("flash_attn_with_sparse_mask_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::FlashAttnGradInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("flash_attn_with_sparse_mask_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_attn_mask_start_row_indices, *input_out, *input_softmax_lse, *input_seed_offset, *input_out_grad, dropout, causal, attn_mask_start_row, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, q_grad, "q_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, k_grad, "k_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, v_grad, "v_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "flash_attn_with_sparse_mask_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flash_attn_with_sparse_mask_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flash_attn_with_sparse_mask_grad", kernel_data_type);
  }
  VLOG(6) << "flash_attn_with_sparse_mask_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_k = PrepareData(k, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_attn_mask_start_row_indices = PrepareData(attn_mask_start_row_indices, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_softmax_lse = PrepareData(softmax_lse, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seed_offset = PrepareData(seed_offset, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"q", {
     (*input_q).dims()}},
     {"k", {
     (*input_k).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"attn_mask_start_row_indices", {
     (*input_attn_mask_start_row_indices).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"softmax_lse", {
     (*input_softmax_lse).dims()}},
     {"seed_offset", {
     (*input_seed_offset).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dropout"] = dropout;
     attrs["causal"] = causal;
     attrs["attn_mask_start_row"] = attn_mask_start_row;
     phi::RecordOpInfoSupplement("flash_attn_with_sparse_mask_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(q_grad);
  auto kernel_out_1 = SetKernelOutput(k_grad);
  auto kernel_out_2 = SetKernelOutput(v_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flash_attn_with_sparse_mask_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::FlashAttnGradInferMeta(MakeMetaTensor(*input_q), MakeMetaTensor(*input_k), MakeMetaTensor(*input_v), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, bool, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flash_attn_with_sparse_mask_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_q, *input_k, *input_v, *input_attn_mask_start_row_indices, *input_out, *input_softmax_lse, *input_seed_offset, *input_out_grad, dropout, causal, attn_mask_start_row, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void flatten_grad(const Tensor& xshape, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(xshape, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(xshape, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_xshape = MakeDistMetaTensor(*xshape.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_xshape, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("flatten_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::KernelWithXShapeInferMeta(MakeMetaTensor(*xshape.impl()), MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "flatten_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "flatten_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "flatten_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_xshape = ReshardApiInputToKernelInput(dev_ctx, xshape, spmd_info.first[0], "xshape");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_xshape = PrepareDataForDistTensor(dist_input_xshape, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_xshape = &dist_input_xshape->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"xshape", {
         (*input_xshape).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("flatten_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("flatten_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_xshape, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "flatten_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "flatten_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("flatten_grad", kernel_data_type);
  }
  VLOG(6) << "flatten_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_xshape = PrepareData(xshape, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"xshape", {
     (*input_xshape).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("flatten_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("flatten_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("flatten_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_xshape, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void floor_grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("floor_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "floor_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "floor_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "floor_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("floor_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("floor_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "floor_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "floor_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("floor_grad", kernel_data_type);
  }
  VLOG(6) << "floor_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("floor_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("floor_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("floor_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fmax_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fmax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fmax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fmax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fmax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("fmax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fmax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fmax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fmax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fmax_grad", kernel_data_type);
  }
  VLOG(6) << "fmax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fmax_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fmax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fmax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void fmin_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fmin_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fmin_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fmin_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fmin_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("fmin_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fmin_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fmin_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fmin_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fmin_grad", kernel_data_type);
  }
  VLOG(6) << "fmin_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fmin_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fmin_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fmin_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void fold_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int>& output_sizes, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fold_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fold_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fold_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fold_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["output_sizes"] = output_sizes;
         attrs["kernel_sizes"] = kernel_sizes;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["dilations"] = dilations;
         phi::RecordOpInfoSupplement("fold_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fold_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, output_sizes, kernel_sizes, strides, paddings, dilations, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fold_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fold_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fold_grad", kernel_data_type);
  }
  VLOG(6) << "fold_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_sizes"] = output_sizes;
     attrs["kernel_sizes"] = kernel_sizes;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["dilations"] = dilations;
     phi::RecordOpInfoSupplement("fold_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fold_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fold_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, output_sizes, kernel_sizes, strides, paddings, dilations, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fractional_max_pool2d_grad(const Tensor& x, const Tensor& mask, const Tensor& out_grad, const std::vector<int>& output_size, const std::vector<int>& kernel_size, float random_u, bool return_mask, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_mask = MakeDistMetaTensor(*mask.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fractional_max_pool2d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fractional_max_pool2d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fractional_max_pool2d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fractional_max_pool2d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[1], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mask = &dist_input_mask->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"mask", {
         (*input_mask).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["output_size"] = output_size;
         attrs["kernel_size"] = kernel_size;
         attrs["random_u"] = random_u;
         attrs["return_mask"] = return_mask;
         phi::RecordOpInfoSupplement("fractional_max_pool2d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fractional_max_pool2d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, float, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, output_size, kernel_size, random_u, return_mask, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fractional_max_pool2d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fractional_max_pool2d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fractional_max_pool2d_grad", kernel_data_type);
  }
  VLOG(6) << "fractional_max_pool2d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_size"] = output_size;
     attrs["kernel_size"] = kernel_size;
     attrs["random_u"] = random_u;
     attrs["return_mask"] = return_mask;
     phi::RecordOpInfoSupplement("fractional_max_pool2d_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fractional_max_pool2d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, float, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fractional_max_pool2d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, output_size, kernel_size, random_u, return_mask, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fractional_max_pool3d_grad(const Tensor& x, const Tensor& mask, const Tensor& out_grad, const std::vector<int>& output_size, const std::vector<int>& kernel_size, float random_u, bool return_mask, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_mask = MakeDistMetaTensor(*mask.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fractional_max_pool3d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fractional_max_pool3d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fractional_max_pool3d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fractional_max_pool3d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[1], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mask = &dist_input_mask->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"mask", {
         (*input_mask).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["output_size"] = output_size;
         attrs["kernel_size"] = kernel_size;
         attrs["random_u"] = random_u;
         attrs["return_mask"] = return_mask;
         phi::RecordOpInfoSupplement("fractional_max_pool3d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fractional_max_pool3d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, float, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, output_size, kernel_size, random_u, return_mask, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fractional_max_pool3d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fractional_max_pool3d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fractional_max_pool3d_grad", kernel_data_type);
  }
  VLOG(6) << "fractional_max_pool3d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["output_size"] = output_size;
     attrs["kernel_size"] = kernel_size;
     attrs["random_u"] = random_u;
     attrs["return_mask"] = return_mask;
     phi::RecordOpInfoSupplement("fractional_max_pool3d_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fractional_max_pool3d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, float, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fractional_max_pool3d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, output_size, kernel_size, random_u, return_mask, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void frame_grad(const Tensor& x, const Tensor& out_grad, int frame_length, int hop_length, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("frame_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "frame_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "frame_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "frame_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["frame_length"] = frame_length;
         attrs["hop_length"] = hop_length;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("frame_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("frame_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, frame_length, hop_length, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "frame_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "frame_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("frame_grad", kernel_data_type);
  }
  VLOG(6) << "frame_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["frame_length"] = frame_length;
     attrs["hop_length"] = hop_length;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("frame_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("frame_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("frame_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, frame_length, hop_length, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void gammaincc_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gammaincc_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*y.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gammaincc_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gammaincc_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gammaincc_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("gammaincc_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_y), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gammaincc_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gammaincc_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gammaincc_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gammaincc_grad", kernel_data_type);
  }
  VLOG(6) << "gammaincc_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gammaincc_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gammaincc_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_y), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gammaincc_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void gammaln_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gammaln_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gammaln_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gammaln_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gammaln_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("gammaln_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gammaln_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gammaln_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gammaln_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gammaln_grad", kernel_data_type);
  }
  VLOG(6) << "gammaln_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gammaln_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gammaln_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gammaln_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void gather_grad(const Tensor& x, const Tensor& index, const Tensor& out_grad, const Scalar& axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, index, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_index, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gather_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gather_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gather_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gather_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[1], "index");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"index", {
         (*input_index).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (axis.dtype()) {
          case DataType::FLOAT32:
              attrs["axis"] = static_cast<float>(axis.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["axis"] = static_cast<double>(axis.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["axis"] = static_cast<float>(axis.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["axis"] = static_cast<bool>(axis.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["axis"] = static_cast<float>(axis.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["axis"] = static_cast<double>(axis.to<complex128>());
              break;
          default:
              attrs["axis"] = "";
              break;
        }
         phi::RecordOpInfoSupplement("gather_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gather_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, phi::Scalar(axis), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gather_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gather_grad", kernel_data_type);
  }
  VLOG(6) << "gather_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (axis.dtype()) {
      case DataType::FLOAT32:
          attrs["axis"] = static_cast<float>(axis.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["axis"] = static_cast<double>(axis.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["axis"] = static_cast<float>(axis.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["axis"] = static_cast<int32_t>(axis.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["axis"] = static_cast<int64_t>(axis.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["axis"] = static_cast<int16_t>(axis.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["axis"] = static_cast<int8_t>(axis.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["axis"] = static_cast<uint16_t>(axis.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["axis"] = static_cast<uint8_t>(axis.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["axis"] = static_cast<bool>(axis.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["axis"] = static_cast<float>(axis.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["axis"] = static_cast<double>(axis.to<complex128>());
          break;
      default:
          attrs["axis"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("gather_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gather_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gather_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, phi::Scalar(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void gather_nd_grad(const Tensor& x, const Tensor& index, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, index, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_index, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gather_nd_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GatherNdGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*index.impl()), MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gather_nd_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gather_nd_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gather_nd_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[1], "index");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"index", {
         (*input_index).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("gather_nd_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GatherNdGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gather_nd_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gather_nd_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gather_nd_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gather_nd_grad", kernel_data_type);
  }
  VLOG(6) << "gather_nd_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("gather_nd_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gather_nd_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GatherNdGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_index), MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gather_nd_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void gaussian_inplace_grad(const Tensor& out_grad, float mean, float std, int seed, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gaussian_inplace_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gaussian_inplace_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gaussian_inplace_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gaussian_inplace_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["mean"] = mean;
         attrs["std"] = std;
         attrs["seed"] = seed;
         phi::RecordOpInfoSupplement("gaussian_inplace_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gaussian_inplace_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, mean, std, seed, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gaussian_inplace_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gaussian_inplace_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gaussian_inplace_grad", kernel_data_type);
  }
  VLOG(6) << "gaussian_inplace_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["mean"] = mean;
     attrs["std"] = std;
     attrs["seed"] = seed;
     phi::RecordOpInfoSupplement("gaussian_inplace_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gaussian_inplace_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gaussian_inplace_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, mean, std, seed, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void gelu_grad(const Tensor& x, const Tensor& out_grad, bool approximate, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gelu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gelu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gelu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gelu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["approximate"] = approximate;
         phi::RecordOpInfoSupplement("gelu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gelu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, approximate, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gelu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gelu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gelu_grad", kernel_data_type);
  }
  VLOG(6) << "gelu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["approximate"] = approximate;
     phi::RecordOpInfoSupplement("gelu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gelu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gelu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, approximate, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void grid_sample_grad(const Tensor& x, const Tensor& grid, const Tensor& out_grad, const std::string& mode, const std::string& padding_mode, bool align_corners, Tensor* x_grad, Tensor* grid_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grid, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grid, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_grid = MakeDistMetaTensor(*grid.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_grid, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("grid_sample_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(grid_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*grid.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "grid_sample_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "grid_sample_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "grid_sample_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_grid = ReshardApiInputToKernelInput(dev_ctx, grid, spmd_info.first[1], "grid");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_grid = PrepareDataForDistTensor(dist_input_grid, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_grid = &dist_input_grid->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"grid", {
         (*input_grid).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["mode"] = mode;
         attrs["padding_mode"] = padding_mode;
         attrs["align_corners"] = align_corners;
         phi::RecordOpInfoSupplement("grid_sample_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_grid), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("grid_sample_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_grid, *input_out_grad, mode, padding_mode, align_corners, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, grid_grad, "grid_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "grid_sample_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "grid_sample_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("grid_sample_grad", kernel_data_type);
  }
  VLOG(6) << "grid_sample_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grid = PrepareData(grid, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grid", {
     (*input_grid).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["mode"] = mode;
     attrs["padding_mode"] = padding_mode;
     attrs["align_corners"] = align_corners;
     phi::RecordOpInfoSupplement("grid_sample_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grid_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("grid_sample_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_grid), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("grid_sample_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grid, *input_out_grad, mode, padding_mode, align_corners, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void group_norm_grad(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, const Tensor& y, const Tensor& mean, const Tensor& variance, const Tensor& y_grad, float epsilon, int groups, const std::string& data_format, Tensor* x_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, bias, y, mean, variance, y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(y_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(y_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, y, mean, variance, y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = scale ? MakeDistMetaTensor(*(*scale).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_mean = MakeDistMetaTensor(*mean.impl());
    auto meta_dist_input_variance = MakeDistMetaTensor(*variance.impl());
    auto meta_dist_input_y_grad = MakeDistMetaTensor(*y_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_bias, meta_dist_input_y, meta_dist_input_mean, meta_dist_input_variance, meta_dist_input_y_grad);
    DebugInfoForInferSpmd("group_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_scale = scale ? MakeMetaTensor(*(*scale).impl()) : phi::MetaTensor();

    phi::MetaTensor meta_dist_bias = bias ? MakeMetaTensor(*(*bias).impl()) : phi::MetaTensor();

    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*y.impl()), meta_dist_scale, meta_dist_bias, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "group_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "group_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "group_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[3], "y");
      auto dist_input_mean = ReshardApiInputToKernelInput(dev_ctx, mean, spmd_info.first[4], "mean");
      auto dist_input_variance = ReshardApiInputToKernelInput(dev_ctx, variance, spmd_info.first[5], "variance");
      auto dist_input_y_grad = ReshardApiInputToKernelInput(dev_ctx, y_grad, spmd_info.first[6], "y_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale = dist_input_scale ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale)->value()) : paddle::none;

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_mean = PrepareDataForDistTensor(dist_input_mean, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mean = &dist_input_mean->value();

      dist_input_variance = PrepareDataForDistTensor(dist_input_variance, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_variance = &dist_input_variance->value();

      dist_input_y_grad = PrepareDataForDistTensor(dist_input_y_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y_grad = &dist_input_y_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> scale_record_shapes;
         if(input_scale){
           scale_record_shapes.push_back((*input_scale).dims());
         }
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", scale_record_shapes},
         {"bias", bias_record_shapes},
         {"y", {
         (*input_y).dims()}},
         {"mean", {
         (*input_mean).dims()}},
         {"variance", {
         (*input_variance).dims()}},
         {"y_grad", {
         (*input_y_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["epsilon"] = epsilon;
         attrs["groups"] = groups;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("group_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_y), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("group_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, *input_y, *input_mean, *input_variance, *input_y_grad, epsilon, groups, data_format, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "group_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "group_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("group_norm_grad", kernel_data_type);
  }
  VLOG(6) << "group_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean = PrepareData(mean, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance = PrepareData(variance, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y_grad = PrepareData(y_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"bias", bias_record_shapes},
     {"y", {
     (*input_y).dims()}},
     {"mean", {
     (*input_mean).dims()}},
     {"variance", {
     (*input_variance).dims()}},
     {"y_grad", {
     (*input_y_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("group_norm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("group_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_y), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("group_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, *input_y, *input_mean, *input_variance, *input_y_grad, epsilon, groups, data_format, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void gumbel_softmax_grad(const Tensor& out, const Tensor& out_grad, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("gumbel_softmax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GumbelSoftmaxGradInferMeta(MakeMetaTensor(*out.impl()), MakeMetaTensor(*out_grad.impl()), axis, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "gumbel_softmax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "gumbel_softmax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "gumbel_softmax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("gumbel_softmax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GumbelSoftmaxGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out_grad), axis, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("gumbel_softmax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "gumbel_softmax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "gumbel_softmax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("gumbel_softmax_grad", kernel_data_type);
  }
  VLOG(6) << "gumbel_softmax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("gumbel_softmax_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("gumbel_softmax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GumbelSoftmaxGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out_grad), axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("gumbel_softmax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void hardshrink_grad(const Tensor& x, const Tensor& out_grad, float threshold, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("hardshrink_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "hardshrink_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "hard_shrink_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "hard_shrink_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["threshold"] = threshold;
         phi::RecordOpInfoSupplement("hardshrink_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("hardshrink_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, threshold, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "hardshrink_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hard_shrink_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardshrink_grad", kernel_data_type);
  }
  VLOG(6) << "hard_shrink_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("hardshrink_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardshrink_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardshrink_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void hardsigmoid_grad(const Tensor& out, const Tensor& out_grad, float slope, float offset, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("hardsigmoid_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "hardsigmoid_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "hardsigmoid_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "hardsigmoid_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["slope"] = slope;
         attrs["offset"] = offset;
         phi::RecordOpInfoSupplement("hardsigmoid_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("hardsigmoid_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, slope, offset, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "hardsigmoid_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardsigmoid_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardsigmoid_grad", kernel_data_type);
  }
  VLOG(6) << "hardsigmoid_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["slope"] = slope;
     attrs["offset"] = offset;
     phi::RecordOpInfoSupplement("hardsigmoid_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardsigmoid_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardsigmoid_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, slope, offset, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void hardtanh_grad(const Tensor& x, const Tensor& out_grad, float t_min, float t_max, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("hardtanh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "hardtanh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "hardtanh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "hardtanh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["t_min"] = t_min;
         attrs["t_max"] = t_max;
         phi::RecordOpInfoSupplement("hardtanh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("hardtanh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, t_min, t_max, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "hardtanh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardtanh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardtanh_grad", kernel_data_type);
  }
  VLOG(6) << "hardtanh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["t_min"] = t_min;
     attrs["t_max"] = t_max;
     phi::RecordOpInfoSupplement("hardtanh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardtanh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardtanh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, t_min, t_max, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void heaviside_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("heaviside_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "heaviside_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "heaviside_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "heaviside_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("heaviside_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("heaviside_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "heaviside_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "heaviside_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("heaviside_grad", kernel_data_type);
  }
  VLOG(6) << "heaviside_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("heaviside_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("heaviside_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("heaviside_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void huber_loss_grad(const Tensor& residual, const Tensor& out_grad, float delta, Tensor* input_grad, Tensor* label_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(residual, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(residual, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_residual = MakeDistMetaTensor(*residual.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_residual, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("huber_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(label_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*residual.impl()), MakeMetaTensor(*residual.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "huber_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "huber_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "huber_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_residual = ReshardApiInputToKernelInput(dev_ctx, residual, spmd_info.first[0], "residual");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_residual = PrepareDataForDistTensor(dist_input_residual, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_residual = &dist_input_residual->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"residual", {
         (*input_residual).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["delta"] = delta;
         phi::RecordOpInfoSupplement("huber_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_residual), MakeMetaTensor(*input_residual), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("huber_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_residual, *input_out_grad, delta, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, input_grad, "input_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, label_grad, "label_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "huber_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "huber_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("huber_loss_grad", kernel_data_type);
  }
  VLOG(6) << "huber_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_residual = PrepareData(residual, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"residual", {
     (*input_residual).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["delta"] = delta;
     phi::RecordOpInfoSupplement("huber_loss_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(input_grad);
  auto kernel_out_1 = SetKernelOutput(label_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("huber_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_residual), MakeMetaTensor(*input_residual), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("huber_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_residual, *input_out_grad, delta, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void i0_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("i0_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "i0_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "i0_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "i0_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("i0_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("i0_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "i0_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i0_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i0_grad", kernel_data_type);
  }
  VLOG(6) << "i0_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i0_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i0_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i0_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void i0e_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("i0e_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "i0e_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "i0e_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "i0e_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("i0e_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("i0e_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "i0e_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i0e_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i0e_grad", kernel_data_type);
  }
  VLOG(6) << "i0e_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i0e_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i0e_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i0e_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void i1_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("i1_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "i1_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "i1_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "i1_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("i1_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("i1_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "i1_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i1_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i1_grad", kernel_data_type);
  }
  VLOG(6) << "i1_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i1_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i1_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i1_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void i1e_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("i1e_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "i1e_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "i1e_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "i1e_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("i1e_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("i1e_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "i1e_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "i1e_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("i1e_grad", kernel_data_type);
  }
  VLOG(6) << "i1e_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("i1e_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("i1e_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("i1e_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void identity_loss_grad(const Tensor& x, const Tensor& out_grad, int reduction, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("identity_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::IdentityLossGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*out_grad.impl()), reduction, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "identity_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "identity_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "identity_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["reduction"] = reduction;
         phi::RecordOpInfoSupplement("identity_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::IdentityLossGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), reduction, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("identity_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, reduction, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "identity_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "identity_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("identity_loss_grad", kernel_data_type);
  }
  VLOG(6) << "identity_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduction"] = reduction;
     phi::RecordOpInfoSupplement("identity_loss_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("identity_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::IdentityLossGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), reduction, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("identity_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, reduction, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void imag_grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = phi::dtype::ToComplex(ParseDataType(out_grad));

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("imag_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::RealAndImagGradInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "imag_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "imag_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "imag_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("imag_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::RealAndImagGradInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("imag_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "imag_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "imag_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("imag_grad", kernel_data_type);
  }
  VLOG(6) << "imag_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("imag_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("imag_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagGradInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("imag_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void index_add_grad(const Tensor& index, const Tensor& add_value, const Tensor& out_grad, int axis, Tensor* x_grad, Tensor* add_value_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(index, add_value, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(index, add_value, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_add_value = MakeDistMetaTensor(*add_value.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_index, meta_dist_input_add_value, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("index_add_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(add_value_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::IndexAddGradInferMeta(MakeMetaTensor(*index.impl()), MakeMetaTensor(*add_value.impl()), MakeMetaTensor(*out_grad.impl()), axis, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "index_add_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "index_add_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "index_add_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[0], "index");
      auto dist_input_add_value = ReshardApiInputToKernelInput(dev_ctx, add_value, spmd_info.first[1], "add_value");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_add_value = PrepareDataForDistTensor(dist_input_add_value, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_add_value = &dist_input_add_value->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"index", {
         (*input_index).dims()}},
         {"add_value", {
         (*input_add_value).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("index_add_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::IndexAddGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_add_value), MakeMetaTensor(*input_out_grad), axis, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("index_add_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_index, *input_add_value, *input_out_grad, axis, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, add_value_grad, "add_value_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "index_add_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_add_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_add_grad", kernel_data_type);
  }
  VLOG(6) << "index_add_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_add_value = PrepareData(add_value, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"index", {
     (*input_index).dims()}},
     {"add_value", {
     (*input_add_value).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_add_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(add_value_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_add_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::IndexAddGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_add_value), MakeMetaTensor(*input_out_grad), axis, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_add_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_index, *input_add_value, *input_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void index_put_grad(const Tensor& x, const std::vector<Tensor>& indices, const Tensor& value, const Tensor& out_grad, bool accumulate, Tensor* x_grad, Tensor* value_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, value, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, value, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_indices;
    for(auto& e : indices) {
        meta_dist_input_indices.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto meta_dist_input_value = MakeDistMetaTensor(*value.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_value, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("index_put_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(value_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    std::vector<phi::MetaTensor> indices_meta_vec;
    for (auto tmp : indices) {
      indices_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> indices_meta_ptr_vec(indices_meta_vec.size());
    for (size_t i=0; i < indices_meta_ptr_vec.size(); ++i) {
      indices_meta_ptr_vec[i] = &indices_meta_vec[i];
    }

    phi::IndexPutGradInferMeta(MakeMetaTensor(*x.impl()), indices_meta_ptr_vec, MakeMetaTensor(*value.impl()), MakeMetaTensor(*out_grad.impl()), accumulate, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "index_put_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "index_put_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "index_put_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_value = ReshardApiInputToKernelInput(dev_ctx, value, spmd_info.first[2], "value");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      auto dist_input_indices_vec = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_indices_vec;
      for (auto tmp : dist_input_indices_vec) {
        dense_input_indices_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_indices_meta_vec = MakeMetaTensor(dense_input_indices_vec);
      std::vector<const phi::MetaTensor*> dense_input_indices_meta_ptr_vec(dense_input_indices_meta_vec.size());
      for (size_t i = 0; i < dense_input_indices_meta_ptr_vec.size(); ++i) {
        dense_input_indices_meta_ptr_vec[i] = &dense_input_indices_meta_vec[i];
      }

      dist_input_value = PrepareDataForDistTensor(dist_input_value, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_value = &dist_input_value->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"value", {
         (*input_value).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_indices_vec.size());
         for (size_t i = 0; i < dense_input_indices_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_indices_vec[i]).dims());
         }
         input_shapes.emplace_back("indices", ddims_vec);
         phi::AttributeMap attrs;
         attrs["accumulate"] = accumulate;
         phi::RecordOpInfoSupplement("index_put_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::IndexPutGradInferMeta(MakeMetaTensor(*input_x), dense_input_indices_meta_ptr_vec, MakeMetaTensor(*input_value), MakeMetaTensor(*input_out_grad), accumulate, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("index_put_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, dense_input_indices_vec, *input_value, *input_out_grad, accumulate, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, value_grad, "value_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "index_put_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_put_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_put_grad", kernel_data_type);
  }
  VLOG(6) << "index_put_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices_vec = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_indices(input_indices_vec->size());
  for (size_t i = 0; i < input_indices.size(); ++i) {
    input_indices[i] = &input_indices_vec->at(i);
  }
  auto input_value = PrepareData(value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"value", {
     (*input_value).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_indices.size());
     for (size_t i = 0; i < input_indices.size(); ++i) {
       ddims_vec.emplace_back((*input_indices[i]).dims());
     }
     input_shapes.emplace_back("indices", ddims_vec);
     phi::AttributeMap attrs;
     attrs["accumulate"] = accumulate;
     phi::RecordOpInfoSupplement("index_put_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(value_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_put_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto indices_meta_vec = MakeMetaTensor(input_indices);
  std::vector<const phi::MetaTensor*> indices_metas(indices_meta_vec.size());
  for (size_t i = 0; i < indices_meta_vec.size(); ++i) {
    indices_metas[i] = &indices_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::IndexPutGradInferMeta(MakeMetaTensor(*input_x), indices_metas, MakeMetaTensor(*input_value), MakeMetaTensor(*input_out_grad), accumulate, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_put_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_indices, *input_value, *input_out_grad, accumulate, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void index_sample_grad(const Tensor& x, const Tensor& index, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, index, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_index, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("index_sample_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "index_sample_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "index_sample_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "index_sample_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[1], "index");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"index", {
         (*input_index).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("index_sample_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("index_sample_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "index_sample_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_sample_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_sample_grad", kernel_data_type);
  }
  VLOG(6) << "index_sample_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("index_sample_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_sample_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_sample_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void index_select_grad(const Tensor& x, const Tensor& index, const Tensor& out_grad, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, index, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, index, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_index, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("index_select_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "index_select_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "index_select_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "index_select_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[1], "index");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"index", {
         (*input_index).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("index_select_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("index_select_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "index_select_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_select_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_select_grad", kernel_data_type);
  }
  VLOG(6) << "index_select_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"index", {
     (*input_index).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_select_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_select_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_select_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_index, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void index_select_strided_grad(const Tensor& x, const Tensor& out_grad, int64_t index, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("index_select_strided_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "index_select_strided_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "index_select_strided_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "index_select_strided_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["index"] = index;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("index_select_strided_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("index_select_strided_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, index, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "index_select_strided_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "index_select_strided_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("index_select_strided_grad", kernel_data_type);
  }
  VLOG(6) << "index_select_strided_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["index"] = index;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("index_select_strided_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("index_select_strided_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("index_select_strided_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, index, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void instance_norm_double_grad(const Tensor& x, const paddle::optional<Tensor>& fwd_scale, const Tensor& saved_mean, const Tensor& saved_variance, const Tensor& grad_y, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_scale_grad, const paddle::optional<Tensor>& grad_bias_grad, float epsilon, Tensor* x_grad, Tensor* fwd_scale_grad, Tensor* grad_y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, fwd_scale, saved_mean, saved_variance, grad_y, grad_x_grad, grad_scale_grad, grad_bias_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_y.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, fwd_scale, saved_mean, saved_variance, grad_y, grad_x_grad, grad_scale_grad, grad_bias_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "instance_norm_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "instance_norm_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("instance_norm_double_grad", kernel_data_type);
  }
  VLOG(6) << "instance_norm_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fwd_scale = PrepareData(fwd_scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y = PrepareData(grad_y, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_scale_grad = PrepareData(grad_scale_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_bias_grad = PrepareData(grad_bias_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fwd_scale_record_shapes;
     if(input_fwd_scale){
       fwd_scale_record_shapes.push_back((*input_fwd_scale).dims());
     }
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_scale_grad_record_shapes;
     if(input_grad_scale_grad){
       grad_scale_grad_record_shapes.push_back((*input_grad_scale_grad).dims());
     }
     std::vector<phi::DDim> grad_bias_grad_record_shapes;
     if(input_grad_bias_grad){
       grad_bias_grad_record_shapes.push_back((*input_grad_bias_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"fwd_scale", fwd_scale_record_shapes},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"grad_y", {
     (*input_grad_y).dims()}},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_scale_grad", grad_scale_grad_record_shapes},
     {"grad_bias_grad",
     grad_bias_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("instance_norm_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(fwd_scale_grad);
  auto kernel_out_2 = SetKernelOutput(grad_y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("instance_norm_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::InstanceNormDoubleGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_fwd_scale), MakeMetaTensor(*input_saved_mean), MakeMetaTensor(*input_saved_variance), MakeMetaTensor(*input_grad_y), MakeMetaTensor(input_grad_x_grad), MakeMetaTensor(input_grad_scale_grad), MakeMetaTensor(input_grad_bias_grad), epsilon, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("instance_norm_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_fwd_scale, *input_saved_mean, *input_saved_variance, *input_grad_y, input_grad_x_grad, input_grad_scale_grad, input_grad_bias_grad, epsilon, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void instance_norm_grad(const Tensor& x, const paddle::optional<Tensor>& scale, const Tensor& saved_mean, const Tensor& saved_variance, const Tensor& y_grad, float epsilon, Tensor* x_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, saved_mean, saved_variance, y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(y_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, saved_mean, saved_variance, y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = scale ? MakeDistMetaTensor(*(*scale).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_saved_mean = MakeDistMetaTensor(*saved_mean.impl());
    auto meta_dist_input_saved_variance = MakeDistMetaTensor(*saved_variance.impl());
    auto meta_dist_input_y_grad = MakeDistMetaTensor(*y_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_saved_mean, meta_dist_input_saved_variance, meta_dist_input_y_grad);
    DebugInfoForInferSpmd("instance_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_scale = scale ? MakeMetaTensor(*(*scale).impl()) : phi::MetaTensor();

    phi::InstanceNormGradInferMeta(MakeMetaTensor(*x.impl()), meta_dist_scale, MakeMetaTensor(*saved_mean.impl()), MakeMetaTensor(*saved_variance.impl()), MakeMetaTensor(*y_grad.impl()), epsilon, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "instance_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "instance_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "instance_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_saved_mean = ReshardApiInputToKernelInput(dev_ctx, saved_mean, spmd_info.first[2], "saved_mean");
      auto dist_input_saved_variance = ReshardApiInputToKernelInput(dev_ctx, saved_variance, spmd_info.first[3], "saved_variance");
      auto dist_input_y_grad = ReshardApiInputToKernelInput(dev_ctx, y_grad, spmd_info.first[4], "y_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale = dist_input_scale ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale)->value()) : paddle::none;

      dist_input_saved_mean = PrepareDataForDistTensor(dist_input_saved_mean, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_mean = &dist_input_saved_mean->value();

      dist_input_saved_variance = PrepareDataForDistTensor(dist_input_saved_variance, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_variance = &dist_input_saved_variance->value();

      dist_input_y_grad = PrepareDataForDistTensor(dist_input_y_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y_grad = &dist_input_y_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> scale_record_shapes;
         if(input_scale){
           scale_record_shapes.push_back((*input_scale).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", scale_record_shapes},
         {"saved_mean", {
         (*input_saved_mean).dims()}},
         {"saved_variance", {
         (*input_saved_variance).dims()}},
         {"y_grad", {
         (*input_y_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["epsilon"] = epsilon;
         phi::RecordOpInfoSupplement("instance_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::InstanceNormGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(*input_saved_mean), MakeMetaTensor(*input_saved_variance), MakeMetaTensor(*input_y_grad), epsilon, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("instance_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_scale, *input_saved_mean, *input_saved_variance, *input_y_grad, epsilon, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "instance_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "instance_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("instance_norm_grad", kernel_data_type);
  }
  VLOG(6) << "instance_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y_grad = PrepareData(y_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"y_grad", {
     (*input_y_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("instance_norm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("instance_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::InstanceNormGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(*input_saved_mean), MakeMetaTensor(*input_saved_variance), MakeMetaTensor(*input_y_grad), epsilon, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("instance_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, *input_saved_mean, *input_saved_variance, *input_y_grad, epsilon, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void inverse_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("inverse_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::InverseGradInferMeta(MakeMetaTensor(*out.impl()), MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "inverse_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "inverse_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "inverse_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("inverse_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::InverseGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("inverse_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "inverse_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "inverse_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("inverse_grad", kernel_data_type);
  }
  VLOG(6) << "inverse_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("inverse_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("inverse_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::InverseGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("inverse_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void kldiv_loss_grad(const Tensor& x, const Tensor& label, const Tensor& out_grad, const std::string& reduction, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, label, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, label, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_label, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("kldiv_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "kldiv_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "kldiv_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "kldiv_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[1], "label");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["reduction"] = reduction;
         phi::RecordOpInfoSupplement("kldiv_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("kldiv_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_label, *input_out_grad, reduction, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "kldiv_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kldiv_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("kldiv_loss_grad", kernel_data_type);
  }
  VLOG(6) << "kldiv_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduction"] = reduction;
     phi::RecordOpInfoSupplement("kldiv_loss_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("kldiv_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("kldiv_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_label, *input_out_grad, reduction, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void kron_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("kron_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "kron_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "kron_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "kron_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("kron_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("kron_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "kron_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kron_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("kron_grad", kernel_data_type);
  }
  VLOG(6) << "kron_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("kron_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("kron_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("kron_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void kthvalue_grad(const Tensor& x, const Tensor& indices, const Tensor& out_grad, int k, int axis, bool keepdim, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("kthvalue_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "kthvalue_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "kthvalue_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "kthvalue_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["k"] = k;
         attrs["axis"] = axis;
         attrs["keepdim"] = keepdim;
         phi::RecordOpInfoSupplement("kthvalue_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("kthvalue_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, k, axis, keepdim, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "kthvalue_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "kthvalue_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("kthvalue_grad", kernel_data_type);
  }
  VLOG(6) << "kthvalue_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["k"] = k;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("kthvalue_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("kthvalue_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("kthvalue_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, k, axis, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void label_smooth_grad(const Tensor& out_grad, float epsilon, Tensor* label_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("label_smooth_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(label_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "label_smooth_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "label_smooth_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "label_smooth_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["epsilon"] = epsilon;
         phi::RecordOpInfoSupplement("label_smooth_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("label_smooth_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, epsilon, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, label_grad, "label_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "label_smooth_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "label_smooth_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("label_smooth_grad", kernel_data_type);
  }
  VLOG(6) << "label_smooth_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("label_smooth_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(label_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("label_smooth_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("label_smooth_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, epsilon, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void layer_norm_grad(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, const Tensor& mean, const Tensor& variance, const Tensor& out_grad, float epsilon, int begin_norm_axis, Tensor* x_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, bias, mean, variance, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, mean, variance, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = scale ? MakeDistMetaTensor(*(*scale).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_mean = MakeDistMetaTensor(*mean.impl());
    auto meta_dist_input_variance = MakeDistMetaTensor(*variance.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::LayerNormGradInferSpmd(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_bias, meta_dist_input_mean, meta_dist_input_variance, meta_dist_input_out_grad, epsilon, begin_norm_axis);
    DebugInfoForInferSpmd("layer_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh, spmd_info.second[2]);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_scale = scale ? MakeMetaTensor(*(*scale).impl()) : phi::MetaTensor();

    phi::MetaTensor meta_dist_bias = bias ? MakeMetaTensor(*(*bias).impl()) : phi::MetaTensor();

    phi::LayerNormGradInferMeta(MakeMetaTensor(*x.impl()), meta_dist_scale, meta_dist_bias, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `layer_norm_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "layer_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "layer_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "layer_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_mean = ReshardApiInputToKernelInput(dev_ctx, mean, spmd_info.first[3], "mean");
      auto dist_input_variance = ReshardApiInputToKernelInput(dev_ctx, variance, spmd_info.first[4], "variance");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[5], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale = dist_input_scale ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale)->value()) : paddle::none;

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_mean = PrepareDataForDistTensor(dist_input_mean, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mean = &dist_input_mean->value();

      dist_input_variance = PrepareDataForDistTensor(dist_input_variance, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_variance = &dist_input_variance->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> scale_record_shapes;
         if(input_scale){
           scale_record_shapes.push_back((*input_scale).dims());
         }
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", scale_record_shapes},
         {"bias", bias_record_shapes},
         {"mean", {
         (*input_mean).dims()}},
         {"variance", {
         (*input_variance).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["epsilon"] = epsilon;
         attrs["begin_norm_axis"] = begin_norm_axis;
         phi::RecordOpInfoSupplement("layer_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::LayerNormGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("layer_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, *input_mean, *input_variance, *input_out_grad, epsilon, begin_norm_axis, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "layer_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "layer_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("layer_norm_grad", kernel_data_type);
  }
  VLOG(6) << "layer_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean = PrepareData(mean, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance = PrepareData(variance, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"bias", bias_record_shapes},
     {"mean", {
     (*input_mean).dims()}},
     {"variance", {
     (*input_variance).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     attrs["begin_norm_axis"] = begin_norm_axis;
     phi::RecordOpInfoSupplement("layer_norm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("layer_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::LayerNormGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("layer_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, *input_mean, *input_variance, *input_out_grad, epsilon, begin_norm_axis, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void leaky_relu_double_grad(const Tensor& x, const Tensor& grad_x_grad, float negative_slope, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "leaky_relu_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "leaky_relu_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("leaky_relu_double_grad", kernel_data_type);
  }
  VLOG(6) << "leaky_relu_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["negative_slope"] = negative_slope;
     phi::RecordOpInfoSupplement("leaky_relu_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("leaky_relu_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_grad_x_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("leaky_relu_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_x_grad, negative_slope, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void leaky_relu_grad(const Tensor& x, const Tensor& out_grad, float negative_slope, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("leaky_relu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "leaky_relu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "leaky_relu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "leaky_relu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["negative_slope"] = negative_slope;
         phi::RecordOpInfoSupplement("leaky_relu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("leaky_relu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, negative_slope, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "leaky_relu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "leaky_relu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("leaky_relu_grad", kernel_data_type);
  }
  VLOG(6) << "leaky_relu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["negative_slope"] = negative_slope;
     phi::RecordOpInfoSupplement("leaky_relu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("leaky_relu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("leaky_relu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, negative_slope, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void lerp_grad(const Tensor& x, const Tensor& y, const Tensor& weight, const Tensor& out, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, weight, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, weight, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_weight = MakeDistMetaTensor(*weight.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_weight, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("lerp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "lerp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "lerp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "lerp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_weight = ReshardApiInputToKernelInput(dev_ctx, weight, spmd_info.first[2], "weight");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[3], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_weight = PrepareDataForDistTensor(dist_input_weight, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_weight = &dist_input_weight->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"weight", {
         (*input_weight).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("lerp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("lerp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, *input_out, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "lerp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lerp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lerp_grad", kernel_data_type);
  }
  VLOG(6) << "lerp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("lerp_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lerp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lerp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_weight, *input_out, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void lgamma_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("lgamma_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "lgamma_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "lgamma_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "lgamma_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("lgamma_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("lgamma_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "lgamma_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lgamma_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lgamma_grad", kernel_data_type);
  }
  VLOG(6) << "lgamma_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("lgamma_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lgamma_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lgamma_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void linear_interp_grad(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const Tensor& output_grad, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_size, size_tensor, scale_tensor, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(output_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_size = out_size ? MakeDistMetaTensor(*(*out_size).impl()) : phi::distributed::DistMetaTensor();
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_size_tensor;
    if (size_tensor) {
        for(auto& e : *size_tensor) {
            meta_dist_input_size_tensor.push_back(MakeDistMetaTensor(*e.impl()));
        }
    }
    auto meta_dist_input_scale_tensor = scale_tensor ? MakeDistMetaTensor(*(*scale_tensor).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_size, meta_dist_input_size_tensor, meta_dist_input_scale_tensor, meta_dist_input_output_grad);
    DebugInfoForInferSpmd("linear_interp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "linear_interp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "linear_interp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "linear_interp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_size = ReshardApiInputToKernelInput(dev_ctx, out_size, spmd_info.first[1], "out_size");
      auto dist_input_size_tensor = ReshardApiInputToKernelInput(dev_ctx, size_tensor, spmd_info.first[2], "size_tensor");
      auto dist_input_scale_tensor = ReshardApiInputToKernelInput(dev_ctx, scale_tensor, spmd_info.first[3], "scale_tensor");
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[4], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_size = PrepareDataForDistTensor(dist_input_out_size, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out_size = dist_input_out_size ? paddle::make_optional<phi::DenseTensor>((*dist_input_out_size)->value()) : paddle::none;

      auto dist_input_size_tensor_vec = PrepareDataForDistTensor(dist_input_size_tensor, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {true}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_size_tensor_vec;
      if (size_tensor) {
        for (auto tmp : *dist_input_size_tensor_vec) {
          dense_input_size_tensor_vec.emplace_back(&tmp->value());
      }
    }
    paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor(dense_input_size_tensor_vec);
    std::vector<phi::MetaTensor> dense_input_size_tensor_meta_vec = MakeMetaTensor(dense_input_size_tensor_vec);
    std::vector<const phi::MetaTensor*> dense_input_size_tensor_meta_ptr_vec_tmp(dense_input_size_tensor_meta_vec.size());
    for (size_t i = 0; i < dense_input_size_tensor_meta_ptr_vec_tmp.size(); ++i) {
      dense_input_size_tensor_meta_ptr_vec_tmp[i] = &dense_input_size_tensor_meta_vec[i];
    }
    paddle::optional<std::vector<const phi::MetaTensor*>> dense_input_size_tensor_meta_ptr_vec =
            size_tensor ? paddle::make_optional<std::vector<const phi::MetaTensor*>>(dense_input_size_tensor_meta_ptr_vec_tmp) : paddle::none;

      dist_input_scale_tensor = PrepareDataForDistTensor(dist_input_scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale_tensor = dist_input_scale_tensor ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale_tensor)->value()) : paddle::none;

      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_size_record_shapes;
         if(input_out_size){
           out_size_record_shapes.push_back((*input_out_size).dims());
         }
         std::vector<phi::DDim> scale_tensor_record_shapes;
         if(input_scale_tensor){
           scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_size", out_size_record_shapes},
         {"scale_tensor", scale_tensor_record_shapes},
         {"output_grad", {
         (*input_output_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         if (input_size_tensor){
           ddims_vec.reserve(input_size_tensor->size());
           for (size_t i = 0; i < input_size_tensor->size(); ++i) {
             ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
           }
         }
         input_shapes.emplace_back("size_tensor", ddims_vec);
         phi::AttributeMap attrs;
         attrs["data_format"] = data_format;
         attrs["out_d"] = out_d;
         attrs["out_h"] = out_h;
         attrs["out_w"] = out_w;
         attrs["scale"] = scale;
         attrs["interp_method"] = interp_method;
         attrs["align_corners"] = align_corners;
         attrs["align_mode"] = align_mode;
         phi::RecordOpInfoSupplement("linear_interp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("linear_interp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "linear_interp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "linear_interp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("linear_interp_grad", kernel_data_type);
  }
  VLOG(6) << "linear_interp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor", scale_tensor_record_shapes},
     {"output_grad", {
     (*input_output_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("linear_interp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("linear_interp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("linear_interp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void log10_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("log10_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "log10_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "log10_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "log10_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("log10_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("log10_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "log10_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log10_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log10_grad", kernel_data_type);
  }
  VLOG(6) << "log10_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log10_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log10_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log10_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void log1p_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("log1p_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "log1p_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "log1p_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "log1p_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("log1p_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("log1p_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "log1p_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log1p_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log1p_grad", kernel_data_type);
  }
  VLOG(6) << "log1p_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log1p_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log1p_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log1p_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void log2_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("log2_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "log2_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "log2_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "log2_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("log2_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("log2_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "log2_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log2_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log2_grad", kernel_data_type);
  }
  VLOG(6) << "log2_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log2_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log2_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log2_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void log_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "log_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log_double_grad", kernel_data_type);
  }
  VLOG(6) << "log_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void log_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("log_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "log_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "log_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "log_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("log_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("log_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "log_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log_grad", kernel_data_type);
  }
  VLOG(6) << "log_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("log_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void log_loss_grad(const Tensor& input, const Tensor& label, const Tensor& out_grad, float epsilon, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, label, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, label, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_label, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("log_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "log_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "log_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "log_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[1], "label");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["epsilon"] = epsilon;
         phi::RecordOpInfoSupplement("log_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("log_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_label, *input_out_grad, epsilon, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "log_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log_loss_grad", kernel_data_type);
  }
  VLOG(6) << "log_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     phi::RecordOpInfoSupplement("log_loss_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, *input_out_grad, epsilon, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void log_softmax_grad(const Tensor& out, const Tensor& out_grad, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("log_softmax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "log_softmax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "log_softmax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "log_softmax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("log_softmax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("log_softmax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "log_softmax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "log_softmax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("log_softmax_grad", kernel_data_type);
  }
  VLOG(6) << "log_softmax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("log_softmax_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("log_softmax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("log_softmax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void logcumsumexp_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, int axis, bool flatten, bool exclusive, bool reverse, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("logcumsumexp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "logcumsumexp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "logcumsumexp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "logcumsumexp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["flatten"] = flatten;
         attrs["exclusive"] = exclusive;
         attrs["reverse"] = reverse;
         phi::RecordOpInfoSupplement("logcumsumexp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("logcumsumexp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, flatten, exclusive, reverse, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "logcumsumexp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logcumsumexp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logcumsumexp_grad", kernel_data_type);
  }
  VLOG(6) << "logcumsumexp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["flatten"] = flatten;
     attrs["exclusive"] = exclusive;
     attrs["reverse"] = reverse;
     phi::RecordOpInfoSupplement("logcumsumexp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logcumsumexp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logcumsumexp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, flatten, exclusive, reverse, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void logit_grad(const Tensor& x, const Tensor& out_grad, float eps, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("logit_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "logit_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "logit_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "logit_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["eps"] = eps;
         phi::RecordOpInfoSupplement("logit_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("logit_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, eps, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "logit_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logit_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logit_grad", kernel_data_type);
  }
  VLOG(6) << "logit_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["eps"] = eps;
     phi::RecordOpInfoSupplement("logit_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logit_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logit_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, eps, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void logsigmoid_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("logsigmoid_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "logsigmoid_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "logsigmoid_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "logsigmoid_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("logsigmoid_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("logsigmoid_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "logsigmoid_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logsigmoid_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logsigmoid_grad", kernel_data_type);
  }
  VLOG(6) << "logsigmoid_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("logsigmoid_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logsigmoid_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logsigmoid_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void lu_grad(const Tensor& x, const Tensor& out, const Tensor& pivots, const Tensor& out_grad, bool pivot, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, pivots, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, pivots, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_pivots = MakeDistMetaTensor(*pivots.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_pivots, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("lu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::LUGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*out.impl()), MakeMetaTensor(*pivots.impl()), MakeMetaTensor(*out_grad.impl()), pivot, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "lu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "lu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "lu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_pivots = ReshardApiInputToKernelInput(dev_ctx, pivots, spmd_info.first[2], "pivots");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_pivots = PrepareDataForDistTensor(dist_input_pivots, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_pivots = &dist_input_pivots->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"pivots", {
         (*input_pivots).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["pivot"] = pivot;
         phi::RecordOpInfoSupplement("lu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::LUGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out), MakeMetaTensor(*input_pivots), MakeMetaTensor(*input_out_grad), pivot, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("lu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_pivots, *input_out_grad, pivot, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "lu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lu_grad", kernel_data_type);
  }
  VLOG(6) << "lu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pivots = PrepareData(pivots, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"pivots", {
     (*input_pivots).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["pivot"] = pivot;
     phi::RecordOpInfoSupplement("lu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LUGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out), MakeMetaTensor(*input_pivots), MakeMetaTensor(*input_out_grad), pivot, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_pivots, *input_out_grad, pivot, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void lu_unpack_grad(const Tensor& x, const Tensor& y, const Tensor& l, const Tensor& u, const Tensor& pmat, const Tensor& l_grad, const Tensor& u_grad, bool unpack_ludata, bool unpack_pivots, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, l, u, pmat, l_grad, u_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(u_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, l, u, pmat, l_grad, u_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_l = MakeDistMetaTensor(*l.impl());
    auto meta_dist_input_u = MakeDistMetaTensor(*u.impl());
    auto meta_dist_input_pmat = MakeDistMetaTensor(*pmat.impl());
    auto meta_dist_input_l_grad = MakeDistMetaTensor(*l_grad.impl());
    auto meta_dist_input_u_grad = MakeDistMetaTensor(*u_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_l, meta_dist_input_u, meta_dist_input_pmat, meta_dist_input_l_grad, meta_dist_input_u_grad);
    DebugInfoForInferSpmd("lu_unpack_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::LUUnpackGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), MakeMetaTensor(*l.impl()), MakeMetaTensor(*u.impl()), MakeMetaTensor(*pmat.impl()), MakeMetaTensor(*l_grad.impl()), MakeMetaTensor(*u_grad.impl()), unpack_ludata, unpack_pivots, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "lu_unpack_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "lu_unpack_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "lu_unpack_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_l = ReshardApiInputToKernelInput(dev_ctx, l, spmd_info.first[2], "l");
      auto dist_input_u = ReshardApiInputToKernelInput(dev_ctx, u, spmd_info.first[3], "u");
      auto dist_input_pmat = ReshardApiInputToKernelInput(dev_ctx, pmat, spmd_info.first[4], "pmat");
      auto dist_input_l_grad = ReshardApiInputToKernelInput(dev_ctx, l_grad, spmd_info.first[5], "l_grad");
      auto dist_input_u_grad = ReshardApiInputToKernelInput(dev_ctx, u_grad, spmd_info.first[6], "u_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_l = PrepareDataForDistTensor(dist_input_l, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_l = &dist_input_l->value();

      dist_input_u = PrepareDataForDistTensor(dist_input_u, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_u = &dist_input_u->value();

      dist_input_pmat = PrepareDataForDistTensor(dist_input_pmat, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_pmat = &dist_input_pmat->value();

      dist_input_l_grad = PrepareDataForDistTensor(dist_input_l_grad, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_l_grad = &dist_input_l_grad->value();

      dist_input_u_grad = PrepareDataForDistTensor(dist_input_u_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_u_grad = &dist_input_u_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"l", {
         (*input_l).dims()}},
         {"u", {
         (*input_u).dims()}},
         {"pmat", {
         (*input_pmat).dims()}},
         {"l_grad", {
         (*input_l_grad).dims()}},
         {"u_grad", {
         (*input_u_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["unpack_ludata"] = unpack_ludata;
         attrs["unpack_pivots"] = unpack_pivots;
         phi::RecordOpInfoSupplement("lu_unpack_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::LUUnpackGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_l), MakeMetaTensor(*input_u), MakeMetaTensor(*input_pmat), MakeMetaTensor(*input_l_grad), MakeMetaTensor(*input_u_grad), unpack_ludata, unpack_pivots, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("lu_unpack_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_l, *input_u, *input_pmat, *input_l_grad, *input_u_grad, unpack_ludata, unpack_pivots, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "lu_unpack_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "lu_unpack_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("lu_unpack_grad", kernel_data_type);
  }
  VLOG(6) << "lu_unpack_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_l = PrepareData(l, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_u = PrepareData(u, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pmat = PrepareData(pmat, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_l_grad = PrepareData(l_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_u_grad = PrepareData(u_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"l", {
     (*input_l).dims()}},
     {"u", {
     (*input_u).dims()}},
     {"pmat", {
     (*input_pmat).dims()}},
     {"l_grad", {
     (*input_l_grad).dims()}},
     {"u_grad", {
     (*input_u_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["unpack_ludata"] = unpack_ludata;
     attrs["unpack_pivots"] = unpack_pivots;
     phi::RecordOpInfoSupplement("lu_unpack_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("lu_unpack_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::LUUnpackGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_l), MakeMetaTensor(*input_u), MakeMetaTensor(*input_pmat), MakeMetaTensor(*input_l_grad), MakeMetaTensor(*input_u_grad), unpack_ludata, unpack_pivots, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("lu_unpack_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_l, *input_u, *input_pmat, *input_l_grad, *input_u_grad, unpack_ludata, unpack_pivots, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void margin_cross_entropy_grad(const Tensor& logits, const Tensor& label, const Tensor& softmax, const Tensor& loss_grad, bool return_softmax, int ring_id, int rank, int nranks, float margin1, float margin2, float margin3, float scale, Tensor* logits_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(logits, label, softmax, loss_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(loss_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(softmax);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(logits, label, softmax, loss_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_logits = MakeDistMetaTensor(*logits.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_softmax = MakeDistMetaTensor(*softmax.impl());
    auto meta_dist_input_loss_grad = MakeDistMetaTensor(*loss_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_logits, meta_dist_input_label, meta_dist_input_softmax, meta_dist_input_loss_grad);
    DebugInfoForInferSpmd("margin_cross_entropy_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(logits_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::MarginCrossEntropyGradInferMeta(MakeMetaTensor(*logits.impl()), MakeMetaTensor(*label.impl()), MakeMetaTensor(*softmax.impl()), MakeMetaTensor(*loss_grad.impl()), return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "margin_cross_entropy_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "margin_cross_entropy_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "margin_cross_entropy_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_logits = ReshardApiInputToKernelInput(dev_ctx, logits, spmd_info.first[0], "logits");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[1], "label");
      auto dist_input_softmax = ReshardApiInputToKernelInput(dev_ctx, softmax, spmd_info.first[2], "softmax");
      auto dist_input_loss_grad = ReshardApiInputToKernelInput(dev_ctx, loss_grad, spmd_info.first[3], "loss_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_logits = PrepareDataForDistTensor(dist_input_logits, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_logits = &dist_input_logits->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_softmax = PrepareDataForDistTensor(dist_input_softmax, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_softmax = &dist_input_softmax->value();

      dist_input_loss_grad = PrepareDataForDistTensor(dist_input_loss_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_loss_grad = &dist_input_loss_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"logits", {
         (*input_logits).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"softmax", {
         (*input_softmax).dims()}},
         {"loss_grad", {
         (*input_loss_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["return_softmax"] = return_softmax;
         attrs["ring_id"] = ring_id;
         attrs["rank"] = rank;
         attrs["nranks"] = nranks;
         attrs["margin1"] = margin1;
         attrs["margin2"] = margin2;
         attrs["margin3"] = margin3;
         attrs["scale"] = scale;
         phi::RecordOpInfoSupplement("margin_cross_entropy_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::MarginCrossEntropyGradInferMeta(MakeMetaTensor(*input_logits), MakeMetaTensor(*input_label), MakeMetaTensor(*input_softmax), MakeMetaTensor(*input_loss_grad), return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("margin_cross_entropy_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, int, int, int, float, float, float, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_logits, *input_label, *input_softmax, *input_loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, logits_grad, "logits_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "margin_cross_entropy_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "margin_cross_entropy_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("margin_cross_entropy_grad", kernel_data_type);
  }
  VLOG(6) << "margin_cross_entropy_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_logits = PrepareData(logits, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_softmax = PrepareData(softmax, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_loss_grad = PrepareData(loss_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"logits", {
     (*input_logits).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"softmax", {
     (*input_softmax).dims()}},
     {"loss_grad", {
     (*input_loss_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["return_softmax"] = return_softmax;
     attrs["ring_id"] = ring_id;
     attrs["rank"] = rank;
     attrs["nranks"] = nranks;
     attrs["margin1"] = margin1;
     attrs["margin2"] = margin2;
     attrs["margin3"] = margin3;
     attrs["scale"] = scale;
     phi::RecordOpInfoSupplement("margin_cross_entropy_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(logits_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("margin_cross_entropy_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MarginCrossEntropyGradInferMeta(MakeMetaTensor(*input_logits), MakeMetaTensor(*input_label), MakeMetaTensor(*input_softmax), MakeMetaTensor(*input_loss_grad), return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, int, int, int, float, float, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("margin_cross_entropy_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_logits, *input_label, *input_softmax, *input_loss_grad, return_softmax, ring_id, rank, nranks, margin1, margin2, margin3, scale, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void masked_select_grad(const Tensor& x, const Tensor& mask, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_mask = MakeDistMetaTensor(*mask.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("masked_select_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "masked_select_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "masked_select_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "masked_select_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[1], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mask = &dist_input_mask->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"mask", {
         (*input_mask).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("masked_select_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("masked_select_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "masked_select_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "masked_select_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("masked_select_grad", kernel_data_type);
  }
  VLOG(6) << "masked_select_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("masked_select_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("masked_select_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("masked_select_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void matrix_power_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, int n, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("matrix_power_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "matrix_power_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "matrix_power_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "matrix_power_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["n"] = n;
         phi::RecordOpInfoSupplement("matrix_power_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("matrix_power_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, n, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "matrix_power_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matrix_power_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matrix_power_grad", kernel_data_type);
  }
  VLOG(6) << "matrix_power_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["n"] = n;
     phi::RecordOpInfoSupplement("matrix_power_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matrix_power_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matrix_power_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, n, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void max_pool2d_with_index_grad(const Tensor& x, const Tensor& mask, const Tensor& out_grad, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_mask = MakeDistMetaTensor(*mask.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("max_pool2d_with_index_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::MaxPoolWithIndexGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*mask.impl()), MakeMetaTensor(*out_grad.impl()), kernel_size, strides, paddings, global_pooling, adaptive, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "max_pool2d_with_index_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "max_pool2d_with_index_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "max_pool2d_with_index_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[1], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mask = &dist_input_mask->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"mask", {
         (*input_mask).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["kernel_size"] = kernel_size;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["global_pooling"] = global_pooling;
         attrs["adaptive"] = adaptive;
         phi::RecordOpInfoSupplement("max_pool2d_with_index_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::MaxPoolWithIndexGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), MakeMetaTensor(*input_out_grad), kernel_size, strides, paddings, global_pooling, adaptive, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("max_pool2d_with_index_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "max_pool2d_with_index_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_pool2d_with_index_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("max_pool2d_with_index_grad", kernel_data_type);
  }
  VLOG(6) << "max_pool2d_with_index_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     phi::RecordOpInfoSupplement("max_pool2d_with_index_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("max_pool2d_with_index_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MaxPoolWithIndexGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), MakeMetaTensor(*input_out_grad), kernel_size, strides, paddings, global_pooling, adaptive, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("max_pool2d_with_index_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void max_pool3d_with_index_grad(const Tensor& x, const Tensor& mask, const Tensor& out_grad, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool global_pooling, bool adaptive, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_mask = MakeDistMetaTensor(*mask.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("max_pool3d_with_index_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::MaxPoolWithIndexGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*mask.impl()), MakeMetaTensor(*out_grad.impl()), kernel_size, strides, paddings, global_pooling, adaptive, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "max_pool3d_with_index_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "max_pool3d_with_index_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "max_pool3d_with_index_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[1], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mask = &dist_input_mask->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"mask", {
         (*input_mask).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["kernel_size"] = kernel_size;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["global_pooling"] = global_pooling;
         attrs["adaptive"] = adaptive;
         phi::RecordOpInfoSupplement("max_pool3d_with_index_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::MaxPoolWithIndexGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), MakeMetaTensor(*input_out_grad), kernel_size, strides, paddings, global_pooling, adaptive, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("max_pool3d_with_index_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "max_pool3d_with_index_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_pool3d_with_index_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("max_pool3d_with_index_grad", kernel_data_type);
  }
  VLOG(6) << "max_pool3d_with_index_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"mask", {
     (*input_mask).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     phi::RecordOpInfoSupplement("max_pool3d_with_index_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("max_pool3d_with_index_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::MaxPoolWithIndexGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_mask), MakeMetaTensor(*input_out_grad), kernel_size, strides, paddings, global_pooling, adaptive, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("max_pool3d_with_index_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_mask, *input_out_grad, kernel_size, strides, paddings, global_pooling, adaptive, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void maxout_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, int groups, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("maxout_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "maxout_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "maxout_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "maxout_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["groups"] = groups;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("maxout_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("maxout_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, groups, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "maxout_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "maxout_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("maxout_grad", kernel_data_type);
  }
  VLOG(6) << "maxout_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["groups"] = groups;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("maxout_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("maxout_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("maxout_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, groups, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void mean_all_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("mean_all_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedExceptLayoutInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "mean_all_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "mean_all_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "mean_all_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("mean_all_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedExceptLayoutInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("mean_all_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "mean_all_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mean_all_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mean_all_grad", kernel_data_type);
  }
  VLOG(6) << "mean_all_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("mean_all_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mean_all_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedExceptLayoutInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mean_all_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void memory_efficient_attention_grad(const Tensor& query, const Tensor& key, const Tensor& value, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& cu_seqlens_q, const paddle::optional<Tensor>& cu_seqlens_k, const Tensor& output, const Tensor& logsumexp, const Tensor& seed_and_offset, const Tensor& output_grad, const Scalar& max_seqlen_q, const Scalar& max_seqlen_k, bool causal, double dropout_p, float scale, Tensor* query_grad, Tensor* key_grad, Tensor* value_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(query, key, value, bias, cu_seqlens_q, cu_seqlens_k, output, logsumexp, seed_and_offset, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(output_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(query, key, value, bias, cu_seqlens_q, cu_seqlens_k, output, logsumexp, seed_and_offset, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_query = MakeDistMetaTensor(*query.impl());
    auto meta_dist_input_key = MakeDistMetaTensor(*key.impl());
    auto meta_dist_input_value = MakeDistMetaTensor(*value.impl());
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_cu_seqlens_q = cu_seqlens_q ? MakeDistMetaTensor(*(*cu_seqlens_q).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_cu_seqlens_k = cu_seqlens_k ? MakeDistMetaTensor(*(*cu_seqlens_k).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_output = MakeDistMetaTensor(*output.impl());
    auto meta_dist_input_logsumexp = MakeDistMetaTensor(*logsumexp.impl());
    auto meta_dist_input_seed_and_offset = MakeDistMetaTensor(*seed_and_offset.impl());
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_query, meta_dist_input_key, meta_dist_input_value, meta_dist_input_bias, meta_dist_input_cu_seqlens_q, meta_dist_input_cu_seqlens_k, meta_dist_input_output, meta_dist_input_logsumexp, meta_dist_input_seed_and_offset, meta_dist_input_output_grad);
    DebugInfoForInferSpmd("memory_efficient_attention_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(query_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(key_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(value_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_3 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_3 = shared_dist_out_3.get();
    phi::DenseTensor* dense_out_3 = dist_out_3 ? dist_out_3->unsafe_mutable_value() : nullptr;
    if (dense_out_3 && !rank_is_in_current_mesh && !dist_out_3->defined()) {
      *dense_out_3 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_out_3(dist_out_3);
    phi::MetaTensor meta_dist_bias = bias ? MakeMetaTensor(*(*bias).impl()) : phi::MetaTensor();

    phi::MetaTensor meta_dist_cu_seqlens_q = cu_seqlens_q ? MakeMetaTensor(*(*cu_seqlens_q).impl()) : phi::MetaTensor();

    phi::MetaTensor meta_dist_cu_seqlens_k = cu_seqlens_k ? MakeMetaTensor(*(*cu_seqlens_k).impl()) : phi::MetaTensor();

    phi::MemoryEfficientAttentionGradInferMeta(MakeMetaTensor(*query.impl()), MakeMetaTensor(*key.impl()), MakeMetaTensor(*value.impl()), meta_dist_bias, meta_dist_cu_seqlens_q, meta_dist_cu_seqlens_k, MakeMetaTensor(*output.impl()), MakeMetaTensor(*logsumexp.impl()), MakeMetaTensor(*seed_and_offset.impl()), MakeMetaTensor(*output_grad.impl()), max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr, dist_out_3 ? &meta_dist_out_3 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_3, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "memory_efficient_attention_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "memory_efficient_attention_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "memory_efficient_attention_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_query = ReshardApiInputToKernelInput(dev_ctx, query, spmd_info.first[0], "query");
      auto dist_input_key = ReshardApiInputToKernelInput(dev_ctx, key, spmd_info.first[1], "key");
      auto dist_input_value = ReshardApiInputToKernelInput(dev_ctx, value, spmd_info.first[2], "value");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[3], "bias");
      auto dist_input_cu_seqlens_q = ReshardApiInputToKernelInput(dev_ctx, cu_seqlens_q, spmd_info.first[4], "cu_seqlens_q");
      auto dist_input_cu_seqlens_k = ReshardApiInputToKernelInput(dev_ctx, cu_seqlens_k, spmd_info.first[5], "cu_seqlens_k");
      auto dist_input_output = ReshardApiInputToKernelInput(dev_ctx, output, spmd_info.first[6], "output");
      auto dist_input_logsumexp = ReshardApiInputToKernelInput(dev_ctx, logsumexp, spmd_info.first[7], "logsumexp");
      auto dist_input_seed_and_offset = ReshardApiInputToKernelInput(dev_ctx, seed_and_offset, spmd_info.first[8], "seed_and_offset");
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[9], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_query = PrepareDataForDistTensor(dist_input_query, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_query = &dist_input_query->value();

      dist_input_key = PrepareDataForDistTensor(dist_input_key, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_key = &dist_input_key->value();

      dist_input_value = PrepareDataForDistTensor(dist_input_value, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_value = &dist_input_value->value();

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_cu_seqlens_q = PrepareDataForDistTensor(dist_input_cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_cu_seqlens_q = dist_input_cu_seqlens_q ? paddle::make_optional<phi::DenseTensor>((*dist_input_cu_seqlens_q)->value()) : paddle::none;

      dist_input_cu_seqlens_k = PrepareDataForDistTensor(dist_input_cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_cu_seqlens_k = dist_input_cu_seqlens_k ? paddle::make_optional<phi::DenseTensor>((*dist_input_cu_seqlens_k)->value()) : paddle::none;

      dist_input_output = PrepareDataForDistTensor(dist_input_output, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output = &dist_input_output->value();

      dist_input_logsumexp = PrepareDataForDistTensor(dist_input_logsumexp, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_logsumexp = &dist_input_logsumexp->value();

      dist_input_seed_and_offset = PrepareDataForDistTensor(dist_input_seed_and_offset, GetKernelInputArgDef(kernel.InputAt(8), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_seed_and_offset = &dist_input_seed_and_offset->value();

      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(9), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<phi::DDim> cu_seqlens_q_record_shapes;
         if(input_cu_seqlens_q){
           cu_seqlens_q_record_shapes.push_back((*input_cu_seqlens_q).dims());
         }
         std::vector<phi::DDim> cu_seqlens_k_record_shapes;
         if(input_cu_seqlens_k){
           cu_seqlens_k_record_shapes.push_back((*input_cu_seqlens_k).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"query", {
         (*input_query).dims()}},
         {"key", {
         (*input_key).dims()}},
         {"value", {
         (*input_value).dims()}},
         {"bias", bias_record_shapes},
         {"cu_seqlens_q", cu_seqlens_q_record_shapes},
         {"cu_seqlens_k", cu_seqlens_k_record_shapes},
         {"output", {
         (*input_output).dims()}},
         {"logsumexp", {
         (*input_logsumexp).dims()}},
         {"seed_and_offset", {
         (*input_seed_and_offset).dims()}},
         {"output_grad", {
         (*input_output_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (max_seqlen_q.dtype()) {
          case DataType::FLOAT32:
              attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["max_seqlen_q"] = static_cast<double>(max_seqlen_q.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["max_seqlen_q"] = static_cast<int32_t>(max_seqlen_q.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["max_seqlen_q"] = static_cast<int64_t>(max_seqlen_q.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["max_seqlen_q"] = static_cast<int16_t>(max_seqlen_q.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["max_seqlen_q"] = static_cast<int8_t>(max_seqlen_q.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["max_seqlen_q"] = static_cast<uint16_t>(max_seqlen_q.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["max_seqlen_q"] = static_cast<uint8_t>(max_seqlen_q.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["max_seqlen_q"] = static_cast<bool>(max_seqlen_q.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["max_seqlen_q"] = static_cast<double>(max_seqlen_q.to<complex128>());
              break;
          default:
              attrs["max_seqlen_q"] = "";
              break;
        }
        switch (max_seqlen_k.dtype()) {
          case DataType::FLOAT32:
              attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["max_seqlen_k"] = static_cast<double>(max_seqlen_k.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["max_seqlen_k"] = static_cast<int32_t>(max_seqlen_k.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["max_seqlen_k"] = static_cast<int64_t>(max_seqlen_k.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["max_seqlen_k"] = static_cast<int16_t>(max_seqlen_k.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["max_seqlen_k"] = static_cast<int8_t>(max_seqlen_k.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["max_seqlen_k"] = static_cast<uint16_t>(max_seqlen_k.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["max_seqlen_k"] = static_cast<uint8_t>(max_seqlen_k.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["max_seqlen_k"] = static_cast<bool>(max_seqlen_k.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["max_seqlen_k"] = static_cast<double>(max_seqlen_k.to<complex128>());
              break;
          default:
              attrs["max_seqlen_k"] = "";
              break;
        }
         attrs["causal"] = causal;
         attrs["dropout_p"] = dropout_p;
         attrs["scale"] = scale;
         phi::RecordOpInfoSupplement("memory_efficient_attention_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::MetaTensor meta_dense_out_3(dense_out_3);
      phi::MemoryEfficientAttentionGradInferMeta(MakeMetaTensor(*input_query), MakeMetaTensor(*input_key), MakeMetaTensor(*input_value), MakeMetaTensor(input_bias), MakeMetaTensor(input_cu_seqlens_q), MakeMetaTensor(input_cu_seqlens_k), MakeMetaTensor(*input_output), MakeMetaTensor(*input_logsumexp), MakeMetaTensor(*input_seed_and_offset), MakeMetaTensor(*input_output_grad), max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr, dense_out_3 ? &meta_dense_out_3 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("memory_efficient_attention_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, bool, double, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_query, *input_key, *input_value, input_bias, input_cu_seqlens_q, input_cu_seqlens_k, *input_output, *input_logsumexp, *input_seed_and_offset, *input_output_grad, phi::Scalar(max_seqlen_q), phi::Scalar(max_seqlen_k), causal, dropout_p, scale, dense_out_0, dense_out_1, dense_out_2, dense_out_3);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
        TransDataBackend(dense_out_3, kernel_backend, dense_out_3);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, query_grad, "query_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, key_grad, "key_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, value_grad, "value_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_3, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "memory_efficient_attention_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "memory_efficient_attention_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("memory_efficient_attention_grad", kernel_data_type);
  }
  VLOG(6) << "memory_efficient_attention_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_query = PrepareData(query, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_key = PrepareData(key, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_value = PrepareData(value, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_q = PrepareData(cu_seqlens_q, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_cu_seqlens_k = PrepareData(cu_seqlens_k, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_output = PrepareData(output, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_logsumexp = PrepareData(logsumexp, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_seed_and_offset = PrepareData(seed_and_offset, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(9), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> cu_seqlens_q_record_shapes;
     if(input_cu_seqlens_q){
       cu_seqlens_q_record_shapes.push_back((*input_cu_seqlens_q).dims());
     }
     std::vector<phi::DDim> cu_seqlens_k_record_shapes;
     if(input_cu_seqlens_k){
       cu_seqlens_k_record_shapes.push_back((*input_cu_seqlens_k).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"query", {
     (*input_query).dims()}},
     {"key", {
     (*input_key).dims()}},
     {"value", {
     (*input_value).dims()}},
     {"bias", bias_record_shapes},
     {"cu_seqlens_q", cu_seqlens_q_record_shapes},
     {"cu_seqlens_k", cu_seqlens_k_record_shapes},
     {"output", {
     (*input_output).dims()}},
     {"logsumexp", {
     (*input_logsumexp).dims()}},
     {"seed_and_offset", {
     (*input_seed_and_offset).dims()}},
     {"output_grad", {
     (*input_output_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (max_seqlen_q.dtype()) {
      case DataType::FLOAT32:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max_seqlen_q"] = static_cast<double>(max_seqlen_q.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max_seqlen_q"] = static_cast<int32_t>(max_seqlen_q.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max_seqlen_q"] = static_cast<int64_t>(max_seqlen_q.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max_seqlen_q"] = static_cast<int16_t>(max_seqlen_q.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max_seqlen_q"] = static_cast<int8_t>(max_seqlen_q.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max_seqlen_q"] = static_cast<uint16_t>(max_seqlen_q.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max_seqlen_q"] = static_cast<uint8_t>(max_seqlen_q.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max_seqlen_q"] = static_cast<bool>(max_seqlen_q.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max_seqlen_q"] = static_cast<float>(max_seqlen_q.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max_seqlen_q"] = static_cast<double>(max_seqlen_q.to<complex128>());
          break;
      default:
          attrs["max_seqlen_q"] = "";
          break;
    }
    switch (max_seqlen_k.dtype()) {
      case DataType::FLOAT32:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["max_seqlen_k"] = static_cast<double>(max_seqlen_k.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["max_seqlen_k"] = static_cast<int32_t>(max_seqlen_k.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["max_seqlen_k"] = static_cast<int64_t>(max_seqlen_k.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["max_seqlen_k"] = static_cast<int16_t>(max_seqlen_k.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["max_seqlen_k"] = static_cast<int8_t>(max_seqlen_k.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["max_seqlen_k"] = static_cast<uint16_t>(max_seqlen_k.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["max_seqlen_k"] = static_cast<uint8_t>(max_seqlen_k.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["max_seqlen_k"] = static_cast<bool>(max_seqlen_k.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["max_seqlen_k"] = static_cast<float>(max_seqlen_k.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["max_seqlen_k"] = static_cast<double>(max_seqlen_k.to<complex128>());
          break;
      default:
          attrs["max_seqlen_k"] = "";
          break;
    }
     attrs["causal"] = causal;
     attrs["dropout_p"] = dropout_p;
     attrs["scale"] = scale;
     phi::RecordOpInfoSupplement("memory_efficient_attention_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(query_grad);
  auto kernel_out_1 = SetKernelOutput(key_grad);
  auto kernel_out_2 = SetKernelOutput(value_grad);
  auto kernel_out_3 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("memory_efficient_attention_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::MemoryEfficientAttentionGradInferMeta(MakeMetaTensor(*input_query), MakeMetaTensor(*input_key), MakeMetaTensor(*input_value), MakeMetaTensor(input_bias), MakeMetaTensor(input_cu_seqlens_q), MakeMetaTensor(input_cu_seqlens_k), MakeMetaTensor(*input_output), MakeMetaTensor(*input_logsumexp), MakeMetaTensor(*input_seed_and_offset), MakeMetaTensor(*input_output_grad), max_seqlen_q, max_seqlen_k, causal, dropout_p, scale, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, const phi::Scalar&, bool, double, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("memory_efficient_attention_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_query, *input_key, *input_value, input_bias, input_cu_seqlens_q, input_cu_seqlens_k, *input_output, *input_logsumexp, *input_seed_and_offset, *input_output_grad, phi::Scalar(max_seqlen_q), phi::Scalar(max_seqlen_k), causal, dropout_p, scale, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  
}

PADDLE_API void meshgrid_grad(const std::vector<Tensor>& inputs, const std::vector<Tensor>& outputs_grad, std::vector<Tensor*> inputs_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(inputs, outputs_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(inputs[0].impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(outputs_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(inputs, outputs_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_inputs;
    for(auto& e : inputs) {
        meta_dist_input_inputs.push_back(MakeDistMetaTensor(*e.impl()));
    }
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_outputs_grad;
    for(auto& e : outputs_grad) {
        meta_dist_input_outputs_grad.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_inputs, meta_dist_input_outputs_grad);
    DebugInfoForInferSpmd("meshgrid_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(inputs_grad, !rank_is_in_current_mesh);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    std::vector<phi::MetaTensor> inputs_meta_vec;
    for (auto tmp : inputs) {
      inputs_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> inputs_meta_ptr_vec(inputs_meta_vec.size());
    for (size_t i=0; i < inputs_meta_ptr_vec.size(); ++i) {
      inputs_meta_ptr_vec[i] = &inputs_meta_vec[i];
    }

    std::vector<phi::MetaTensor> outputs_grad_meta_vec;
    for (auto tmp : outputs_grad) {
      outputs_grad_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> outputs_grad_meta_ptr_vec(outputs_grad_meta_vec.size());
    for (size_t i=0; i < outputs_grad_meta_ptr_vec.size(); ++i) {
      outputs_grad_meta_ptr_vec[i] = &outputs_grad_meta_vec[i];
    }

    phi::MeshgridGradInferMeta(inputs_meta_ptr_vec, outputs_grad_meta_ptr_vec, dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    for (size_t i = 0; i < dist_out.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out[i], current_process_mesh);
    }


    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "meshgrid_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "meshgrid_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "meshgrid_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_inputs = ReshardApiInputToKernelInput(dev_ctx, inputs, spmd_info.first[0], "inputs");
      auto dist_input_outputs_grad = ReshardApiInputToKernelInput(dev_ctx, outputs_grad, spmd_info.first[1], "outputs_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      auto dist_input_inputs_vec = PrepareDataForDistTensor(dist_input_inputs, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_inputs_vec;
      for (auto tmp : dist_input_inputs_vec) {
        dense_input_inputs_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_inputs_meta_vec = MakeMetaTensor(dense_input_inputs_vec);
      std::vector<const phi::MetaTensor*> dense_input_inputs_meta_ptr_vec(dense_input_inputs_meta_vec.size());
      for (size_t i = 0; i < dense_input_inputs_meta_ptr_vec.size(); ++i) {
        dense_input_inputs_meta_ptr_vec[i] = &dense_input_inputs_meta_vec[i];
      }

      auto dist_input_outputs_grad_vec = PrepareDataForDistTensor(dist_input_outputs_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_outputs_grad_vec;
      for (auto tmp : dist_input_outputs_grad_vec) {
        dense_input_outputs_grad_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_outputs_grad_meta_vec = MakeMetaTensor(dense_input_outputs_grad_vec);
      std::vector<const phi::MetaTensor*> dense_input_outputs_grad_meta_ptr_vec(dense_input_outputs_grad_meta_vec.size());
      for (size_t i = 0; i < dense_input_outputs_grad_meta_ptr_vec.size(); ++i) {
        dense_input_outputs_grad_meta_ptr_vec[i] = &dense_input_outputs_grad_meta_vec[i];
      }

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_inputs_vec.size());
         for (size_t i = 0; i < dense_input_inputs_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_inputs_vec[i]).dims());
         }
         input_shapes.emplace_back("inputs", ddims_vec);
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_outputs_grad_vec.size());
         for (size_t i = 0; i < dense_input_outputs_grad_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_outputs_grad_vec[i]).dims());
         }
         input_shapes.emplace_back("outputs_grad", ddims_vec);
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("meshgrid_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::MeshgridGradInferMeta(dense_input_inputs_meta_ptr_vec, dense_input_outputs_grad_meta_ptr_vec, dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("meshgrid_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, dense_input_inputs_vec, dense_input_outputs_grad_vec, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, inputs_grad, "inputs_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "meshgrid_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "meshgrid_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("meshgrid_grad", kernel_data_type);
  }
  VLOG(6) << "meshgrid_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_inputs_vec = PrepareData(inputs, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_inputs(input_inputs_vec->size());
  for (size_t i = 0; i < input_inputs.size(); ++i) {
    input_inputs[i] = &input_inputs_vec->at(i);
  }
  auto input_outputs_grad_vec = PrepareData(outputs_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_outputs_grad(input_outputs_grad_vec->size());
  for (size_t i = 0; i < input_outputs_grad.size(); ++i) {
    input_outputs_grad[i] = &input_outputs_grad_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_inputs.size());
     for (size_t i = 0; i < input_inputs.size(); ++i) {
       ddims_vec.emplace_back((*input_inputs[i]).dims());
     }
     input_shapes.emplace_back("inputs", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_outputs_grad.size());
     for (size_t i = 0; i < input_outputs_grad.size(); ++i) {
       ddims_vec.emplace_back((*input_outputs_grad[i]).dims());
     }
     input_shapes.emplace_back("outputs_grad", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("meshgrid_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&inputs_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("meshgrid_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto inputs_meta_vec = MakeMetaTensor(input_inputs);
  std::vector<const phi::MetaTensor*> inputs_metas(inputs_meta_vec.size());
  for (size_t i = 0; i < inputs_meta_vec.size(); ++i) {
    inputs_metas[i] = &inputs_meta_vec[i];
  }

  auto outputs_grad_meta_vec = MakeMetaTensor(input_outputs_grad);
  std::vector<const phi::MetaTensor*> outputs_grad_metas(outputs_grad_meta_vec.size());
  for (size_t i = 0; i < outputs_grad_meta_vec.size(); ++i) {
    outputs_grad_metas[i] = &outputs_grad_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::MeshgridGradInferMeta(inputs_metas, outputs_grad_metas, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("meshgrid_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_inputs, input_outputs_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void mode_grad(const Tensor& x, const Tensor& indices, const Tensor& out_grad, int axis, bool keepdim, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("mode_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "mode_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "mode_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "mode_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["keepdim"] = keepdim;
         phi::RecordOpInfoSupplement("mode_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("mode_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, axis, keepdim, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "mode_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mode_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mode_grad", kernel_data_type);
  }
  VLOG(6) << "mode_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     phi::RecordOpInfoSupplement("mode_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mode_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mode_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, axis, keepdim, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void multi_dot_grad(const std::vector<Tensor>& x, const Tensor& out_grad, std::vector<Tensor*> x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_x;
    for(auto& e : x) {
        meta_dist_input_x.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("multi_dot_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    std::vector<phi::MetaTensor> x_meta_vec;
    for (auto tmp : x) {
      x_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> x_meta_ptr_vec(x_meta_vec.size());
    for (size_t i=0; i < x_meta_ptr_vec.size(); ++i) {
      x_meta_ptr_vec[i] = &x_meta_vec[i];
    }

    phi::MultiDotGradInferMeta(x_meta_ptr_vec, MakeMetaTensor(*out_grad.impl()), dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    for (size_t i = 0; i < dist_out.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out[i], current_process_mesh);
    }


    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "multi_dot_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "multi_dot_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "multi_dot_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      auto dist_input_x_vec = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_x_vec;
      for (auto tmp : dist_input_x_vec) {
        dense_input_x_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_x_meta_vec = MakeMetaTensor(dense_input_x_vec);
      std::vector<const phi::MetaTensor*> dense_input_x_meta_ptr_vec(dense_input_x_meta_vec.size());
      for (size_t i = 0; i < dense_input_x_meta_ptr_vec.size(); ++i) {
        dense_input_x_meta_ptr_vec[i] = &dense_input_x_meta_vec[i];
      }

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_x_vec.size());
         for (size_t i = 0; i < dense_input_x_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_x_vec[i]).dims());
         }
         input_shapes.emplace_back("x", ddims_vec);
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("multi_dot_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::MultiDotGradInferMeta(dense_input_x_meta_ptr_vec, MakeMetaTensor(*input_out_grad), dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("multi_dot_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, dense_input_x_vec, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "multi_dot_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multi_dot_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multi_dot_grad", kernel_data_type);
  }
  VLOG(6) << "multi_dot_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_vec = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x(input_x_vec->size());
  for (size_t i = 0; i < input_x.size(); ++i) {
    input_x[i] = &input_x_vec->at(i);
  }
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x.size());
     for (size_t i = 0; i < input_x.size(); ++i) {
       ddims_vec.emplace_back((*input_x[i]).dims());
     }
     input_shapes.emplace_back("x", ddims_vec);
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("multi_dot_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multi_dot_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_meta_vec = MakeMetaTensor(input_x);
  std::vector<const phi::MetaTensor*> x_metas(x_meta_vec.size());
  for (size_t i = 0; i < x_meta_vec.size(); ++i) {
    x_metas[i] = &x_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::MultiDotGradInferMeta(x_metas, MakeMetaTensor(*input_out_grad), kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multi_dot_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void multiplex_grad(const std::vector<Tensor>& inputs, const Tensor& index, const Tensor& out_grad, std::vector<Tensor*> inputs_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(inputs, index, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(inputs, index, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_index, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("multiplex_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(inputs_grad, !rank_is_in_current_mesh);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    phi::MultiplexGradInferMeta(MakeMetaTensor(*index.impl()), MakeMetaTensor(*out_grad.impl()), dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    for (size_t i = 0; i < dist_out.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out[i], current_process_mesh);
    }


    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "multiplex_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "multiplex_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "multiplex_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[0], "index");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {true}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"index", {
         (*input_index).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("multiplex_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::MultiplexGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_out_grad), dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("multiplex_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_index, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, inputs_grad, "inputs_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "multiplex_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiplex_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiplex_grad", kernel_data_type);
  }
  VLOG(6) << "multiplex_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"index", {
     (*input_index).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("multiplex_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&inputs_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multiplex_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::MultiplexGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_out_grad), kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multiplex_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_index, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void mv_grad(const Tensor& x, const Tensor& vec, const Tensor& out_grad, Tensor* x_grad, Tensor* vec_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, vec, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, vec, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_vec = MakeDistMetaTensor(*vec.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_vec, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("mv_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(vec_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*vec.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "mv_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "mv_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "mv_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_vec = ReshardApiInputToKernelInput(dev_ctx, vec, spmd_info.first[1], "vec");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_vec = PrepareDataForDistTensor(dist_input_vec, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_vec = &dist_input_vec->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"vec", {
         (*input_vec).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("mv_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_vec), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("mv_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_vec, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, vec_grad, "vec_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "mv_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mv_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mv_grad", kernel_data_type);
  }
  VLOG(6) << "mv_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_vec = PrepareData(vec, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"vec", {
     (*input_vec).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("mv_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(vec_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mv_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_vec), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mv_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_vec, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void nanmedian_grad(const Tensor& x, const Tensor& medians, const Tensor& out_grad, const IntArray& axis, bool keepdim, const std::string& mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, medians, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, medians, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_medians = MakeDistMetaTensor(*medians.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_medians, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("nanmedian_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::NanmedianGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*medians.impl()), MakeMetaTensor(*out_grad.impl()), axis, keepdim, mode, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "nanmedian_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "nanmedian_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "nanmedian_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_medians = ReshardApiInputToKernelInput(dev_ctx, medians, spmd_info.first[1], "medians");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_medians = PrepareDataForDistTensor(dist_input_medians, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_medians = &dist_input_medians->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"medians", {
         (*input_medians).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         attrs["keepdim"] = keepdim;
         attrs["mode"] = mode;
         phi::RecordOpInfoSupplement("nanmedian_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::NanmedianGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_medians), MakeMetaTensor(*input_out_grad), axis, keepdim, mode, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("nanmedian_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_medians, *input_out_grad, phi::IntArray(axis), keepdim, mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "nanmedian_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nanmedian_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nanmedian_grad", kernel_data_type);
  }
  VLOG(6) << "nanmedian_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_medians = PrepareData(medians, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"medians", {
     (*input_medians).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("nanmedian_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nanmedian_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::NanmedianGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_medians), MakeMetaTensor(*input_out_grad), axis, keepdim, mode, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nanmedian_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_medians, *input_out_grad, phi::IntArray(axis), keepdim, mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void nearest_interp_grad(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const Tensor& output_grad, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_size, size_tensor, scale_tensor, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(output_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_size = out_size ? MakeDistMetaTensor(*(*out_size).impl()) : phi::distributed::DistMetaTensor();
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_size_tensor;
    if (size_tensor) {
        for(auto& e : *size_tensor) {
            meta_dist_input_size_tensor.push_back(MakeDistMetaTensor(*e.impl()));
        }
    }
    auto meta_dist_input_scale_tensor = scale_tensor ? MakeDistMetaTensor(*(*scale_tensor).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_size, meta_dist_input_size_tensor, meta_dist_input_scale_tensor, meta_dist_input_output_grad);
    DebugInfoForInferSpmd("nearest_interp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "nearest_interp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "nearest_interp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "nearest_interp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_size = ReshardApiInputToKernelInput(dev_ctx, out_size, spmd_info.first[1], "out_size");
      auto dist_input_size_tensor = ReshardApiInputToKernelInput(dev_ctx, size_tensor, spmd_info.first[2], "size_tensor");
      auto dist_input_scale_tensor = ReshardApiInputToKernelInput(dev_ctx, scale_tensor, spmd_info.first[3], "scale_tensor");
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[4], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_size = PrepareDataForDistTensor(dist_input_out_size, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out_size = dist_input_out_size ? paddle::make_optional<phi::DenseTensor>((*dist_input_out_size)->value()) : paddle::none;

      auto dist_input_size_tensor_vec = PrepareDataForDistTensor(dist_input_size_tensor, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {true}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_size_tensor_vec;
      if (size_tensor) {
        for (auto tmp : *dist_input_size_tensor_vec) {
          dense_input_size_tensor_vec.emplace_back(&tmp->value());
      }
    }
    paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor(dense_input_size_tensor_vec);
    std::vector<phi::MetaTensor> dense_input_size_tensor_meta_vec = MakeMetaTensor(dense_input_size_tensor_vec);
    std::vector<const phi::MetaTensor*> dense_input_size_tensor_meta_ptr_vec_tmp(dense_input_size_tensor_meta_vec.size());
    for (size_t i = 0; i < dense_input_size_tensor_meta_ptr_vec_tmp.size(); ++i) {
      dense_input_size_tensor_meta_ptr_vec_tmp[i] = &dense_input_size_tensor_meta_vec[i];
    }
    paddle::optional<std::vector<const phi::MetaTensor*>> dense_input_size_tensor_meta_ptr_vec =
            size_tensor ? paddle::make_optional<std::vector<const phi::MetaTensor*>>(dense_input_size_tensor_meta_ptr_vec_tmp) : paddle::none;

      dist_input_scale_tensor = PrepareDataForDistTensor(dist_input_scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale_tensor = dist_input_scale_tensor ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale_tensor)->value()) : paddle::none;

      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_size_record_shapes;
         if(input_out_size){
           out_size_record_shapes.push_back((*input_out_size).dims());
         }
         std::vector<phi::DDim> scale_tensor_record_shapes;
         if(input_scale_tensor){
           scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_size", out_size_record_shapes},
         {"scale_tensor", scale_tensor_record_shapes},
         {"output_grad", {
         (*input_output_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         if (input_size_tensor){
           ddims_vec.reserve(input_size_tensor->size());
           for (size_t i = 0; i < input_size_tensor->size(); ++i) {
             ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
           }
         }
         input_shapes.emplace_back("size_tensor", ddims_vec);
         phi::AttributeMap attrs;
         attrs["data_format"] = data_format;
         attrs["out_d"] = out_d;
         attrs["out_h"] = out_h;
         attrs["out_w"] = out_w;
         attrs["scale"] = scale;
         attrs["interp_method"] = interp_method;
         attrs["align_corners"] = align_corners;
         attrs["align_mode"] = align_mode;
         phi::RecordOpInfoSupplement("nearest_interp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("nearest_interp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "nearest_interp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nearest_interp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nearest_interp_grad", kernel_data_type);
  }
  VLOG(6) << "nearest_interp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor", scale_tensor_record_shapes},
     {"output_grad", {
     (*input_output_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("nearest_interp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nearest_interp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nearest_interp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void nll_loss_grad(const Tensor& input, const Tensor& label, const paddle::optional<Tensor>& weight, const Tensor& total_weight, const Tensor& out_grad, int64_t ignore_index, const std::string& reduction, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, label, weight, total_weight, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(input);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, label, weight, total_weight, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_weight = weight ? MakeDistMetaTensor(*(*weight).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_total_weight = MakeDistMetaTensor(*total_weight.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_label, meta_dist_input_weight, meta_dist_input_total_weight, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("nll_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::MetaTensor meta_dist_weight = weight ? MakeMetaTensor(*(*weight).impl()) : phi::MetaTensor();

    phi::NllLossGradInferMeta(MakeMetaTensor(*input.impl()), MakeMetaTensor(*label.impl()), meta_dist_weight, MakeMetaTensor(*total_weight.impl()), MakeMetaTensor(*out_grad.impl()), ignore_index, reduction, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "nll_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "nll_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "nll_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[1], "label");
      auto dist_input_weight = ReshardApiInputToKernelInput(dev_ctx, weight, spmd_info.first[2], "weight");
      auto dist_input_total_weight = ReshardApiInputToKernelInput(dev_ctx, total_weight, spmd_info.first[3], "total_weight");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_weight = PrepareDataForDistTensor(dist_input_weight, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_weight = dist_input_weight ? paddle::make_optional<phi::DenseTensor>((*dist_input_weight)->value()) : paddle::none;

      dist_input_total_weight = PrepareDataForDistTensor(dist_input_total_weight, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_total_weight = &dist_input_total_weight->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> weight_record_shapes;
         if(input_weight){
           weight_record_shapes.push_back((*input_weight).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"weight", weight_record_shapes},
         {"total_weight", {
         (*input_total_weight).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["ignore_index"] = ignore_index;
         attrs["reduction"] = reduction;
         phi::RecordOpInfoSupplement("nll_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::NllLossGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), MakeMetaTensor(input_weight), MakeMetaTensor(*input_total_weight), MakeMetaTensor(*input_out_grad), ignore_index, reduction, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("nll_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_label, input_weight, *input_total_weight, *input_out_grad, ignore_index, reduction, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "nll_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "nll_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("nll_loss_grad", kernel_data_type);
  }
  VLOG(6) << "nll_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_total_weight = PrepareData(total_weight, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> weight_record_shapes;
     if(input_weight){
       weight_record_shapes.push_back((*input_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"weight", weight_record_shapes},
     {"total_weight", {
     (*input_total_weight).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["ignore_index"] = ignore_index;
     attrs["reduction"] = reduction;
     phi::RecordOpInfoSupplement("nll_loss_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("nll_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::NllLossGradInferMeta(MakeMetaTensor(*input_input), MakeMetaTensor(*input_label), MakeMetaTensor(input_weight), MakeMetaTensor(*input_total_weight), MakeMetaTensor(*input_out_grad), ignore_index, reduction, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("nll_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_label, input_weight, *input_total_weight, *input_out_grad, ignore_index, reduction, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void overlap_add_grad(const Tensor& x, const Tensor& out_grad, int hop_length, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("overlap_add_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::OverlapAddGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*out_grad.impl()), hop_length, axis, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "overlap_add_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "overlap_add_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "overlap_add_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["hop_length"] = hop_length;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("overlap_add_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::OverlapAddGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), hop_length, axis, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("overlap_add_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, hop_length, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "overlap_add_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "overlap_add_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("overlap_add_grad", kernel_data_type);
  }
  VLOG(6) << "overlap_add_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["hop_length"] = hop_length;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("overlap_add_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("overlap_add_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::OverlapAddGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), hop_length, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("overlap_add_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, hop_length, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void p_norm_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, float porder, int axis, float epsilon, bool keepdim, bool asvector, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("p_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "p_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "p_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "p_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["porder"] = porder;
         attrs["axis"] = axis;
         attrs["epsilon"] = epsilon;
         attrs["keepdim"] = keepdim;
         attrs["asvector"] = asvector;
         phi::RecordOpInfoSupplement("p_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("p_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, float, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, porder, axis, epsilon, keepdim, asvector, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "p_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "p_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("p_norm_grad", kernel_data_type);
  }
  VLOG(6) << "p_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["porder"] = porder;
     attrs["axis"] = axis;
     attrs["epsilon"] = epsilon;
     attrs["keepdim"] = keepdim;
     attrs["asvector"] = asvector;
     phi::RecordOpInfoSupplement("p_norm_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("p_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, float, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("p_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, porder, axis, epsilon, keepdim, asvector, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pad3d_double_grad(const Tensor& grad_x_grad, const IntArray& paddings, const std::string& mode, float pad_value, const std::string& data_format, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "pad3d_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad3d", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pad3d_double_grad", kernel_data_type);
  }
  VLOG(6) << "pad3d kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["paddings"] = paddings.GetData();
     attrs["mode"] = mode;
     attrs["pad_value"] = pad_value;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pad3d_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pad3d_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Pad3dInferMeta(MakeMetaTensor(*input_grad_x_grad), paddings, mode, pad_value, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const std::string&, float, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pad3d_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_grad_x_grad, phi::IntArray(paddings), mode, pad_value, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pad3d_grad(const Tensor& x, const Tensor& out_grad, const IntArray& paddings, const std::string& mode, float pad_value, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("pad3d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pad3d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pad3d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pad3d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["paddings"] = paddings.GetData();
         attrs["mode"] = mode;
         attrs["pad_value"] = pad_value;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("pad3d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pad3d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const std::string&, float, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(paddings), mode, pad_value, data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pad3d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad3d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pad3d_grad", kernel_data_type);
  }
  VLOG(6) << "pad3d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["paddings"] = paddings.GetData();
     attrs["mode"] = mode;
     attrs["pad_value"] = pad_value;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pad3d_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pad3d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const std::string&, float, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pad3d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(paddings), mode, pad_value, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pixel_shuffle_grad(const Tensor& out_grad, int upscale_factor, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("pixel_shuffle_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::PixelShuffleGradInferMeta(MakeMetaTensor(*out_grad.impl()), upscale_factor, data_format, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pixel_shuffle_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pixel_shuffle_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pixel_shuffle_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["upscale_factor"] = upscale_factor;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("pixel_shuffle_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::PixelShuffleGradInferMeta(MakeMetaTensor(*input_out_grad), upscale_factor, data_format, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pixel_shuffle_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, upscale_factor, data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pixel_shuffle_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pixel_shuffle_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pixel_shuffle_grad", kernel_data_type);
  }
  VLOG(6) << "pixel_shuffle_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["upscale_factor"] = upscale_factor;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pixel_shuffle_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pixel_shuffle_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PixelShuffleGradInferMeta(MakeMetaTensor(*input_out_grad), upscale_factor, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pixel_shuffle_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, upscale_factor, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pixel_unshuffle_grad(const Tensor& out_grad, int downscale_factor, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("pixel_unshuffle_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::PixelUnshuffleGradInferMeta(MakeMetaTensor(*out_grad.impl()), downscale_factor, data_format, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pixel_unshuffle_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pixel_unshuffle_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pixel_unshuffle_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["downscale_factor"] = downscale_factor;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("pixel_unshuffle_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::PixelUnshuffleGradInferMeta(MakeMetaTensor(*input_out_grad), downscale_factor, data_format, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pixel_unshuffle_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, downscale_factor, data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pixel_unshuffle_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pixel_unshuffle_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pixel_unshuffle_grad", kernel_data_type);
  }
  VLOG(6) << "pixel_unshuffle_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["downscale_factor"] = downscale_factor;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("pixel_unshuffle_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pixel_unshuffle_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PixelUnshuffleGradInferMeta(MakeMetaTensor(*input_out_grad), downscale_factor, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pixel_unshuffle_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, downscale_factor, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void poisson_grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("poisson_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "poisson_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "poisson_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "poisson_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("poisson_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("poisson_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "poisson_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "poisson_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("poisson_grad", kernel_data_type);
  }
  VLOG(6) << "poisson_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("poisson_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("poisson_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("poisson_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void polygamma_grad(const Tensor& x, const Tensor& out_grad, int n, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("polygamma_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "polygamma_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "polygamma_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "polygamma_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["n"] = n;
         phi::RecordOpInfoSupplement("polygamma_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("polygamma_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, n, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "polygamma_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "polygamma_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("polygamma_grad", kernel_data_type);
  }
  VLOG(6) << "polygamma_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["n"] = n;
     phi::RecordOpInfoSupplement("polygamma_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("polygamma_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("polygamma_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, n, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pow_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, const Scalar& y, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "pow_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pow_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pow_double_grad", kernel_data_type);
  }
  VLOG(6) << "pow_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (y.dtype()) {
      case DataType::FLOAT32:
          attrs["y"] = static_cast<float>(y.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["y"] = static_cast<double>(y.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["y"] = static_cast<float>(y.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["y"] = static_cast<float>(y.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["y"] = static_cast<int32_t>(y.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["y"] = static_cast<int64_t>(y.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["y"] = static_cast<int16_t>(y.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["y"] = static_cast<int8_t>(y.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["y"] = static_cast<uint16_t>(y.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["y"] = static_cast<uint8_t>(y.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["y"] = static_cast<bool>(y.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["y"] = static_cast<float>(y.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["y"] = static_cast<double>(y.to<complex128>());
          break;
      default:
          attrs["y"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pow_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pow_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pow_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, phi::Scalar(y), kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void pow_grad(const Tensor& x, const Tensor& out_grad, const Scalar& y, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::PowGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad, y);
    DebugInfoForInferSpmd("pow_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `pow_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pow_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pow_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pow_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (y.dtype()) {
          case DataType::FLOAT32:
              attrs["y"] = static_cast<float>(y.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["y"] = static_cast<double>(y.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["y"] = static_cast<float>(y.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["y"] = static_cast<float>(y.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["y"] = static_cast<int32_t>(y.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["y"] = static_cast<int64_t>(y.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["y"] = static_cast<int16_t>(y.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["y"] = static_cast<int8_t>(y.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["y"] = static_cast<uint16_t>(y.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["y"] = static_cast<uint8_t>(y.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["y"] = static_cast<bool>(y.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["y"] = static_cast<float>(y.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["y"] = static_cast<double>(y.to<complex128>());
              break;
          default:
              attrs["y"] = "";
              break;
        }
         phi::RecordOpInfoSupplement("pow_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pow_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::Scalar(y), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pow_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pow_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pow_grad", kernel_data_type);
  }
  VLOG(6) << "pow_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (y.dtype()) {
      case DataType::FLOAT32:
          attrs["y"] = static_cast<float>(y.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["y"] = static_cast<double>(y.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["y"] = static_cast<float>(y.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["y"] = static_cast<float>(y.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["y"] = static_cast<int32_t>(y.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["y"] = static_cast<int64_t>(y.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["y"] = static_cast<int16_t>(y.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["y"] = static_cast<int8_t>(y.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["y"] = static_cast<uint16_t>(y.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["y"] = static_cast<uint8_t>(y.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["y"] = static_cast<bool>(y.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["y"] = static_cast<float>(y.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["y"] = static_cast<double>(y.to<complex128>());
          break;
      default:
          attrs["y"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pow_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pow_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pow_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::Scalar(y), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pow_triple_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_grad_x, const Tensor& grad_x_grad, const paddle::optional<Tensor>& grad_grad_out_grad, const Scalar& y, Tensor* x_grad, Tensor* grad_out_grad, Tensor* grad_grad_x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_grad_x, grad_x_grad, grad_grad_out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_grad_x, grad_x_grad, grad_grad_out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "pow_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pow_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pow_triple_grad", kernel_data_type);
  }
  VLOG(6) << "pow_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_x = PrepareData(grad_grad_x, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_out_grad = PrepareData(grad_grad_out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_grad_out_grad_record_shapes;
     if(input_grad_grad_out_grad){
       grad_grad_out_grad_record_shapes.push_back((*input_grad_grad_out_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_grad_x", {
     (*input_grad_grad_x).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}},
     {"grad_grad_out_grad",
     grad_grad_out_grad_record_shapes}};
     phi::AttributeMap attrs;
    switch (y.dtype()) {
      case DataType::FLOAT32:
          attrs["y"] = static_cast<float>(y.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["y"] = static_cast<double>(y.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["y"] = static_cast<float>(y.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["y"] = static_cast<float>(y.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["y"] = static_cast<int32_t>(y.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["y"] = static_cast<int64_t>(y.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["y"] = static_cast<int16_t>(y.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["y"] = static_cast<int8_t>(y.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["y"] = static_cast<uint16_t>(y.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["y"] = static_cast<uint8_t>(y.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["y"] = static_cast<bool>(y.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["y"] = static_cast<float>(y.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["y"] = static_cast<double>(y.to<complex128>());
          break;
      default:
          attrs["y"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pow_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);
  auto kernel_out_2 = SetKernelOutput(grad_grad_x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pow_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_grad_out), MakeMetaTensor(*input_grad_grad_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::Scalar&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pow_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_grad_x, *input_grad_x_grad, input_grad_grad_out_grad, phi::Scalar(y), kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void prelu_grad(const Tensor& x, const Tensor& alpha, const Tensor& out_grad, const std::string& data_format, const std::string& mode, Tensor* x_grad, Tensor* alpha_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, alpha, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, alpha, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_alpha = MakeDistMetaTensor(*alpha.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_alpha, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("prelu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(alpha_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::PreluGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*alpha.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "prelu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "prelu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "prelu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_alpha = ReshardApiInputToKernelInput(dev_ctx, alpha, spmd_info.first[1], "alpha");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_alpha = PrepareDataForDistTensor(dist_input_alpha, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_alpha = &dist_input_alpha->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"alpha", {
         (*input_alpha).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["data_format"] = data_format;
         attrs["mode"] = mode;
         phi::RecordOpInfoSupplement("prelu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::PreluGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_alpha), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("prelu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_alpha, *input_out_grad, data_format, mode, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, alpha_grad, "alpha_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "prelu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prelu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("prelu_grad", kernel_data_type);
  }
  VLOG(6) << "prelu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_alpha = PrepareData(alpha, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"alpha", {
     (*input_alpha).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("prelu_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(alpha_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("prelu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::PreluGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_alpha), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("prelu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_alpha, *input_out_grad, data_format, mode, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void psroi_pool_grad(const Tensor& x, const Tensor& boxes, const paddle::optional<Tensor>& boxes_num, const Tensor& out_grad, int pooled_height, int pooled_width, int output_channels, float spatial_scale, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, boxes, boxes_num, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_boxes = MakeDistMetaTensor(*boxes.impl());
    auto meta_dist_input_boxes_num = boxes_num ? MakeDistMetaTensor(*(*boxes_num).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_boxes, meta_dist_input_boxes_num, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("psroi_pool_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "psroi_pool_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "psroi_pool_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "psroi_pool_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_boxes = ReshardApiInputToKernelInput(dev_ctx, boxes, spmd_info.first[1], "boxes");
      auto dist_input_boxes_num = ReshardApiInputToKernelInput(dev_ctx, boxes_num, spmd_info.first[2], "boxes_num");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_boxes = PrepareDataForDistTensor(dist_input_boxes, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_boxes = &dist_input_boxes->value();

      dist_input_boxes_num = PrepareDataForDistTensor(dist_input_boxes_num, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_boxes_num = dist_input_boxes_num ? paddle::make_optional<phi::DenseTensor>((*dist_input_boxes_num)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> boxes_num_record_shapes;
         if(input_boxes_num){
           boxes_num_record_shapes.push_back((*input_boxes_num).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"boxes", {
         (*input_boxes).dims()}},
         {"boxes_num", boxes_num_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["pooled_height"] = pooled_height;
         attrs["pooled_width"] = pooled_width;
         attrs["output_channels"] = output_channels;
         attrs["spatial_scale"] = spatial_scale;
         phi::RecordOpInfoSupplement("psroi_pool_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("psroi_pool_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int, int, int, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, *input_out_grad, pooled_height, pooled_width, output_channels, spatial_scale, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "psroi_pool_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "psroi_pool_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("psroi_pool_grad", kernel_data_type);
  }
  VLOG(6) << "psroi_pool_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes = PrepareData(boxes, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes_num = PrepareData(boxes_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> boxes_num_record_shapes;
     if(input_boxes_num){
       boxes_num_record_shapes.push_back((*input_boxes_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"boxes", {
     (*input_boxes).dims()}},
     {"boxes_num", boxes_num_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["pooled_height"] = pooled_height;
     attrs["pooled_width"] = pooled_width;
     attrs["output_channels"] = output_channels;
     attrs["spatial_scale"] = spatial_scale;
     phi::RecordOpInfoSupplement("psroi_pool_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("psroi_pool_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("psroi_pool_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, *input_out_grad, pooled_height, pooled_width, output_channels, spatial_scale, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void put_along_axis_grad(const Tensor& arr, const Tensor& indices, const Tensor& values, const Tensor& out, const Tensor& out_grad, int axis, const std::string& reduce, bool include_self, Tensor* arr_grad, Tensor* values_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(arr, indices, values, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(arr, indices, values, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_arr = MakeDistMetaTensor(*arr.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_values = MakeDistMetaTensor(*values.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_arr, meta_dist_input_indices, meta_dist_input_values, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("put_along_axis_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(arr_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(values_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*arr.impl()), MakeMetaTensor(*indices.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "put_along_axis_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "put_along_axis_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "put_along_axis_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_arr = ReshardApiInputToKernelInput(dev_ctx, arr, spmd_info.first[0], "arr");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_values = ReshardApiInputToKernelInput(dev_ctx, values, spmd_info.first[2], "values");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[3], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_arr = PrepareDataForDistTensor(dist_input_arr, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_arr = &dist_input_arr->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_values = PrepareDataForDistTensor(dist_input_values, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_values = &dist_input_values->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"arr", {
         (*input_arr).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"values", {
         (*input_values).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["reduce"] = reduce;
         attrs["include_self"] = include_self;
         phi::RecordOpInfoSupplement("put_along_axis_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_arr), MakeMetaTensor(*input_indices), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("put_along_axis_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, const std::string&, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_arr, *input_indices, *input_values, *input_out, *input_out_grad, axis, reduce, include_self, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, arr_grad, "arr_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, values_grad, "values_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "put_along_axis_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "put_along_axis_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("put_along_axis_grad", kernel_data_type);
  }
  VLOG(6) << "put_along_axis_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_arr = PrepareData(arr, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_values = PrepareData(values, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"arr", {
     (*input_arr).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"values", {
     (*input_values).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["reduce"] = reduce;
     attrs["include_self"] = include_self;
     phi::RecordOpInfoSupplement("put_along_axis_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(arr_grad);
  auto kernel_out_1 = SetKernelOutput(values_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("put_along_axis_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_arr), MakeMetaTensor(*input_indices), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, const std::string&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("put_along_axis_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_arr, *input_indices, *input_values, *input_out, *input_out_grad, axis, reduce, include_self, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void qr_grad(const Tensor& x, const Tensor& q, const Tensor& r, const Tensor& q_grad, const Tensor& r_grad, const std::string& mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, q, r, q_grad, r_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(r_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, q, r, q_grad, r_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_q = MakeDistMetaTensor(*q.impl());
    auto meta_dist_input_r = MakeDistMetaTensor(*r.impl());
    auto meta_dist_input_q_grad = MakeDistMetaTensor(*q_grad.impl());
    auto meta_dist_input_r_grad = MakeDistMetaTensor(*r_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_q, meta_dist_input_r, meta_dist_input_q_grad, meta_dist_input_r_grad);
    DebugInfoForInferSpmd("qr_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "qr_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "qr_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "qr_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_q = ReshardApiInputToKernelInput(dev_ctx, q, spmd_info.first[1], "q");
      auto dist_input_r = ReshardApiInputToKernelInput(dev_ctx, r, spmd_info.first[2], "r");
      auto dist_input_q_grad = ReshardApiInputToKernelInput(dev_ctx, q_grad, spmd_info.first[3], "q_grad");
      auto dist_input_r_grad = ReshardApiInputToKernelInput(dev_ctx, r_grad, spmd_info.first[4], "r_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_q = PrepareDataForDistTensor(dist_input_q, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_q = &dist_input_q->value();

      dist_input_r = PrepareDataForDistTensor(dist_input_r, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_r = &dist_input_r->value();

      dist_input_q_grad = PrepareDataForDistTensor(dist_input_q_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_q_grad = &dist_input_q_grad->value();

      dist_input_r_grad = PrepareDataForDistTensor(dist_input_r_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_r_grad = &dist_input_r_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"q", {
         (*input_q).dims()}},
         {"r", {
         (*input_r).dims()}},
         {"q_grad", {
         (*input_q_grad).dims()}},
         {"r_grad", {
         (*input_r_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["mode"] = mode;
         phi::RecordOpInfoSupplement("qr_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("qr_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_q, *input_r, *input_q_grad, *input_r_grad, mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "qr_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "qr_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("qr_grad", kernel_data_type);
  }
  VLOG(6) << "qr_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_q = PrepareData(q, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_r = PrepareData(r, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_q_grad = PrepareData(q_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_r_grad = PrepareData(r_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"q", {
     (*input_q).dims()}},
     {"r", {
     (*input_r).dims()}},
     {"q_grad", {
     (*input_q_grad).dims()}},
     {"r_grad", {
     (*input_r_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("qr_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("qr_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("qr_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_q, *input_r, *input_q_grad, *input_r_grad, mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void real_grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = phi::dtype::ToComplex(ParseDataType(out_grad));

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("real_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::RealAndImagGradInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "real_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "real_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "real_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("real_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::RealAndImagGradInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("real_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "real_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "real_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("real_grad", kernel_data_type);
  }
  VLOG(6) << "real_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("real_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("real_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RealAndImagGradInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("real_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void reciprocal_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("reciprocal_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "reciprocal_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "reciprocal_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "reciprocal_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("reciprocal_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("reciprocal_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "reciprocal_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reciprocal_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reciprocal_grad", kernel_data_type);
  }
  VLOG(6) << "reciprocal_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reciprocal_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reciprocal_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reciprocal_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void relu6_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("relu6_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "relu6_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "relu6_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "relu6_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("relu6_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("relu6_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "relu6_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu6_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("relu6_grad", kernel_data_type);
  }
  VLOG(6) << "relu6_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("relu6_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("relu6_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("relu6_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void relu_double_grad(const Tensor& out, const Tensor& grad_x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "relu_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("relu_double_grad", kernel_data_type);
  }
  VLOG(6) << "relu_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("relu_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("relu_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("relu_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_grad_x_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void relu_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("relu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `relu_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "relu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "relu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "relu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("relu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("relu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "relu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "relu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("relu_grad", kernel_data_type);
  }
  VLOG(6) << "relu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("relu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("relu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("relu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void renorm_grad(const Tensor& x, const Tensor& out_grad, float p, int axis, float max_norm, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("renorm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "renorm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "renorm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "renorm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["p"] = p;
         attrs["axis"] = axis;
         attrs["max_norm"] = max_norm;
         phi::RecordOpInfoSupplement("renorm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("renorm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, p, axis, max_norm, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "renorm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "renorm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("renorm_grad", kernel_data_type);
  }
  VLOG(6) << "renorm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["p"] = p;
     attrs["axis"] = axis;
     attrs["max_norm"] = max_norm;
     phi::RecordOpInfoSupplement("renorm_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("renorm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("renorm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, p, axis, max_norm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void rms_norm_grad(const Tensor& x, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& residual, const Tensor& norm_weight, const paddle::optional<Tensor>& norm_bias, const Tensor& inv_var, const Tensor& out_grad, float epsilon, int begin_norm_axis, float quant_scale, Tensor* x_grad, Tensor* norm_weight_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, bias, residual, norm_weight, norm_bias, inv_var, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, bias, residual, norm_weight, norm_bias, inv_var, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_residual = residual ? MakeDistMetaTensor(*(*residual).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_norm_weight = MakeDistMetaTensor(*norm_weight.impl());
    auto meta_dist_input_norm_bias = norm_bias ? MakeDistMetaTensor(*(*norm_bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_inv_var = MakeDistMetaTensor(*inv_var.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_bias, meta_dist_input_residual, meta_dist_input_norm_weight, meta_dist_input_norm_bias, meta_dist_input_inv_var, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("rms_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(norm_weight_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::RmsNormGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*norm_weight.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "rms_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "rms_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "rms_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[1], "bias");
      auto dist_input_residual = ReshardApiInputToKernelInput(dev_ctx, residual, spmd_info.first[2], "residual");
      auto dist_input_norm_weight = ReshardApiInputToKernelInput(dev_ctx, norm_weight, spmd_info.first[3], "norm_weight");
      auto dist_input_norm_bias = ReshardApiInputToKernelInput(dev_ctx, norm_bias, spmd_info.first[4], "norm_bias");
      auto dist_input_inv_var = ReshardApiInputToKernelInput(dev_ctx, inv_var, spmd_info.first[5], "inv_var");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[6], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_residual = PrepareDataForDistTensor(dist_input_residual, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_residual = dist_input_residual ? paddle::make_optional<phi::DenseTensor>((*dist_input_residual)->value()) : paddle::none;

      dist_input_norm_weight = PrepareDataForDistTensor(dist_input_norm_weight, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_norm_weight = &dist_input_norm_weight->value();

      dist_input_norm_bias = PrepareDataForDistTensor(dist_input_norm_bias, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_norm_bias = dist_input_norm_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_norm_bias)->value()) : paddle::none;

      dist_input_inv_var = PrepareDataForDistTensor(dist_input_inv_var, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_inv_var = &dist_input_inv_var->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<phi::DDim> residual_record_shapes;
         if(input_residual){
           residual_record_shapes.push_back((*input_residual).dims());
         }
         std::vector<phi::DDim> norm_bias_record_shapes;
         if(input_norm_bias){
           norm_bias_record_shapes.push_back((*input_norm_bias).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"bias", bias_record_shapes},
         {"residual", residual_record_shapes},
         {"norm_weight", {
         (*input_norm_weight).dims()}},
         {"norm_bias", norm_bias_record_shapes},
         {"inv_var", {
         (*input_inv_var).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["epsilon"] = epsilon;
         attrs["begin_norm_axis"] = begin_norm_axis;
         attrs["quant_scale"] = quant_scale;
         phi::RecordOpInfoSupplement("rms_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::RmsNormGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_norm_weight), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("rms_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, float, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_bias, input_residual, *input_norm_weight, input_norm_bias, *input_inv_var, *input_out_grad, epsilon, begin_norm_axis, quant_scale, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, norm_weight_grad, "norm_weight_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "rms_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rms_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rms_norm_grad", kernel_data_type);
  }
  VLOG(6) << "rms_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_residual = PrepareData(residual, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_norm_weight = PrepareData(norm_weight, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_norm_bias = PrepareData(norm_bias, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_inv_var = PrepareData(inv_var, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> residual_record_shapes;
     if(input_residual){
       residual_record_shapes.push_back((*input_residual).dims());
     }
     std::vector<phi::DDim> norm_bias_record_shapes;
     if(input_norm_bias){
       norm_bias_record_shapes.push_back((*input_norm_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"bias", bias_record_shapes},
     {"residual", residual_record_shapes},
     {"norm_weight", {
     (*input_norm_weight).dims()}},
     {"norm_bias", norm_bias_record_shapes},
     {"inv_var", {
     (*input_inv_var).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["epsilon"] = epsilon;
     attrs["begin_norm_axis"] = begin_norm_axis;
     attrs["quant_scale"] = quant_scale;
     phi::RecordOpInfoSupplement("rms_norm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(norm_weight_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rms_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::RmsNormGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_norm_weight), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, float, int, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rms_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_bias, input_residual, *input_norm_weight, input_norm_bias, *input_inv_var, *input_out_grad, epsilon, begin_norm_axis, quant_scale, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void roi_align_grad(const Tensor& x, const Tensor& boxes, const paddle::optional<Tensor>& boxes_num, const Tensor& out_grad, int pooled_height, int pooled_width, float spatial_scale, int sampling_ratio, bool aligned, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, boxes, boxes_num, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(boxes);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_boxes = MakeDistMetaTensor(*boxes.impl());
    auto meta_dist_input_boxes_num = boxes_num ? MakeDistMetaTensor(*(*boxes_num).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_boxes, meta_dist_input_boxes_num, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("roi_align_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "roi_align_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "roi_align_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "roi_align_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_boxes = ReshardApiInputToKernelInput(dev_ctx, boxes, spmd_info.first[1], "boxes");
      auto dist_input_boxes_num = ReshardApiInputToKernelInput(dev_ctx, boxes_num, spmd_info.first[2], "boxes_num");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_boxes = PrepareDataForDistTensor(dist_input_boxes, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_boxes = &dist_input_boxes->value();

      dist_input_boxes_num = PrepareDataForDistTensor(dist_input_boxes_num, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_boxes_num = dist_input_boxes_num ? paddle::make_optional<phi::DenseTensor>((*dist_input_boxes_num)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> boxes_num_record_shapes;
         if(input_boxes_num){
           boxes_num_record_shapes.push_back((*input_boxes_num).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"boxes", {
         (*input_boxes).dims()}},
         {"boxes_num", boxes_num_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["pooled_height"] = pooled_height;
         attrs["pooled_width"] = pooled_width;
         attrs["spatial_scale"] = spatial_scale;
         attrs["sampling_ratio"] = sampling_ratio;
         attrs["aligned"] = aligned;
         phi::RecordOpInfoSupplement("roi_align_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("roi_align_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int, int, float, int, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, *input_out_grad, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "roi_align_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roi_align_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("roi_align_grad", kernel_data_type);
  }
  VLOG(6) << "roi_align_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes = PrepareData(boxes, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes_num = PrepareData(boxes_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> boxes_num_record_shapes;
     if(input_boxes_num){
       boxes_num_record_shapes.push_back((*input_boxes_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"boxes", {
     (*input_boxes).dims()}},
     {"boxes_num", boxes_num_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["pooled_height"] = pooled_height;
     attrs["pooled_width"] = pooled_width;
     attrs["spatial_scale"] = spatial_scale;
     attrs["sampling_ratio"] = sampling_ratio;
     attrs["aligned"] = aligned;
     phi::RecordOpInfoSupplement("roi_align_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("roi_align_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, int, int, float, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("roi_align_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, *input_out_grad, pooled_height, pooled_width, spatial_scale, sampling_ratio, aligned, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void roi_pool_grad(const Tensor& x, const Tensor& boxes, const paddle::optional<Tensor>& boxes_num, const Tensor& arg_max, const Tensor& out_grad, int pooled_height, int pooled_width, float spatial_scale, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, boxes, boxes_num, arg_max, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, boxes, boxes_num, arg_max, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_boxes = MakeDistMetaTensor(*boxes.impl());
    auto meta_dist_input_boxes_num = boxes_num ? MakeDistMetaTensor(*(*boxes_num).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_arg_max = MakeDistMetaTensor(*arg_max.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_boxes, meta_dist_input_boxes_num, meta_dist_input_arg_max, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("roi_pool_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "roi_pool_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "roi_pool_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "roi_pool_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_boxes = ReshardApiInputToKernelInput(dev_ctx, boxes, spmd_info.first[1], "boxes");
      auto dist_input_boxes_num = ReshardApiInputToKernelInput(dev_ctx, boxes_num, spmd_info.first[2], "boxes_num");
      auto dist_input_arg_max = ReshardApiInputToKernelInput(dev_ctx, arg_max, spmd_info.first[3], "arg_max");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_boxes = PrepareDataForDistTensor(dist_input_boxes, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_boxes = &dist_input_boxes->value();

      dist_input_boxes_num = PrepareDataForDistTensor(dist_input_boxes_num, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_boxes_num = dist_input_boxes_num ? paddle::make_optional<phi::DenseTensor>((*dist_input_boxes_num)->value()) : paddle::none;

      dist_input_arg_max = PrepareDataForDistTensor(dist_input_arg_max, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_arg_max = &dist_input_arg_max->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> boxes_num_record_shapes;
         if(input_boxes_num){
           boxes_num_record_shapes.push_back((*input_boxes_num).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"boxes", {
         (*input_boxes).dims()}},
         {"boxes_num", boxes_num_record_shapes},
         {"arg_max", {
         (*input_arg_max).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["pooled_height"] = pooled_height;
         attrs["pooled_width"] = pooled_width;
         attrs["spatial_scale"] = spatial_scale;
         phi::RecordOpInfoSupplement("roi_pool_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("roi_pool_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, *input_arg_max, *input_out_grad, pooled_height, pooled_width, spatial_scale, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "roi_pool_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roi_pool_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("roi_pool_grad", kernel_data_type);
  }
  VLOG(6) << "roi_pool_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes = PrepareData(boxes, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_boxes_num = PrepareData(boxes_num, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_arg_max = PrepareData(arg_max, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> boxes_num_record_shapes;
     if(input_boxes_num){
       boxes_num_record_shapes.push_back((*input_boxes_num).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"boxes", {
     (*input_boxes).dims()}},
     {"boxes_num", boxes_num_record_shapes},
     {"arg_max", {
     (*input_arg_max).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["pooled_height"] = pooled_height;
     attrs["pooled_width"] = pooled_width;
     attrs["spatial_scale"] = spatial_scale;
     phi::RecordOpInfoSupplement("roi_pool_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("roi_pool_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("roi_pool_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_boxes, input_boxes_num, *input_arg_max, *input_out_grad, pooled_height, pooled_width, spatial_scale, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void roll_grad(const Tensor& x, const Tensor& out_grad, const IntArray& shifts, const std::vector<int64_t>& axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("roll_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "roll_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "roll_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "roll_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["shifts"] = shifts.GetData();
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("roll_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("roll_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int64_t>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(shifts), axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "roll_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "roll_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("roll_grad", kernel_data_type);
  }
  VLOG(6) << "roll_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["shifts"] = shifts.GetData();
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("roll_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("roll_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("roll_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(shifts), axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void round_grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("round_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "round_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "round_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "round_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("round_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("round_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "round_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "round_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("round_grad", kernel_data_type);
  }
  VLOG(6) << "round_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("round_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("round_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("round_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void rsqrt_double_grad(const Tensor& out, const Tensor& grad_x, const Tensor& grad_x_grad, Tensor* out_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, grad_x, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, grad_x, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "rsqrt_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rsqrt_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rsqrt_double_grad", kernel_data_type);
  }
  VLOG(6) << "rsqrt_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x = PrepareData(grad_x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"grad_x", {
     (*input_grad_x).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("rsqrt_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(out_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rsqrt_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rsqrt_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_grad_x, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void rsqrt_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("rsqrt_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `rsqrt_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "rsqrt_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "rsqrt_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "rsqrt_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("rsqrt_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("rsqrt_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "rsqrt_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rsqrt_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rsqrt_grad", kernel_data_type);
  }
  VLOG(6) << "rsqrt_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("rsqrt_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rsqrt_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rsqrt_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void scatter_grad(const Tensor& index, const Tensor& updates, const Tensor& out_grad, bool overwrite, Tensor* x_grad, Tensor* updates_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(index, updates, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(index, updates, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_updates = MakeDistMetaTensor(*updates.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_index, meta_dist_input_updates, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("scatter_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(updates_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::ScatterGradInferMeta(MakeMetaTensor(*index.impl()), MakeMetaTensor(*updates.impl()), MakeMetaTensor(*out_grad.impl()), overwrite, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "scatter_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "scatter_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "scatter_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[0], "index");
      auto dist_input_updates = ReshardApiInputToKernelInput(dev_ctx, updates, spmd_info.first[1], "updates");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_updates = PrepareDataForDistTensor(dist_input_updates, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_updates = &dist_input_updates->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"index", {
         (*input_index).dims()}},
         {"updates", {
         (*input_updates).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["overwrite"] = overwrite;
         phi::RecordOpInfoSupplement("scatter_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::ScatterGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), MakeMetaTensor(*input_out_grad), overwrite, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("scatter_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_index, *input_updates, *input_out_grad, overwrite, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, updates_grad, "updates_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "scatter_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scatter_grad", kernel_data_type);
  }
  VLOG(6) << "scatter_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_updates = PrepareData(updates, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"index", {
     (*input_index).dims()}},
     {"updates", {
     (*input_updates).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["overwrite"] = overwrite;
     phi::RecordOpInfoSupplement("scatter_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(updates_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("scatter_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ScatterGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), MakeMetaTensor(*input_out_grad), overwrite, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("scatter_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_index, *input_updates, *input_out_grad, overwrite, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void scatter_nd_add_grad(const Tensor& index, const Tensor& updates, const Tensor& out_grad, Tensor* x_grad, Tensor* updates_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(index, updates, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(index, updates, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_index = MakeDistMetaTensor(*index.impl());
    auto meta_dist_input_updates = MakeDistMetaTensor(*updates.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_index, meta_dist_input_updates, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("scatter_nd_add_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(updates_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::ScatterNdAddGradInferMeta(MakeMetaTensor(*index.impl()), MakeMetaTensor(*updates.impl()), MakeMetaTensor(*out_grad.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "scatter_nd_add_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "scatter_nd_add_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "scatter_nd_add_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_index = ReshardApiInputToKernelInput(dev_ctx, index, spmd_info.first[0], "index");
      auto dist_input_updates = ReshardApiInputToKernelInput(dev_ctx, updates, spmd_info.first[1], "updates");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_index = PrepareDataForDistTensor(dist_input_index, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_index = &dist_input_index->value();

      dist_input_updates = PrepareDataForDistTensor(dist_input_updates, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_updates = &dist_input_updates->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"index", {
         (*input_index).dims()}},
         {"updates", {
         (*input_updates).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("scatter_nd_add_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::ScatterNdAddGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), MakeMetaTensor(*input_out_grad), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("scatter_nd_add_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_index, *input_updates, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, updates_grad, "updates_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "scatter_nd_add_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "scatter_nd_add_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("scatter_nd_add_grad", kernel_data_type);
  }
  VLOG(6) << "scatter_nd_add_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_index = PrepareData(index, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_updates = PrepareData(updates, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"index", {
     (*input_index).dims()}},
     {"updates", {
     (*input_updates).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("scatter_nd_add_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(updates_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("scatter_nd_add_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::ScatterNdAddGradInferMeta(MakeMetaTensor(*input_index), MakeMetaTensor(*input_updates), MakeMetaTensor(*input_out_grad), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("scatter_nd_add_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_index, *input_updates, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void segment_pool_grad(const Tensor& x, const Tensor& segment_ids, const Tensor& out, const paddle::optional<Tensor>& summed_ids, const Tensor& out_grad, const std::string& pooltype, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, segment_ids, out, summed_ids, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, segment_ids, out, summed_ids, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_segment_ids = MakeDistMetaTensor(*segment_ids.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_summed_ids = summed_ids ? MakeDistMetaTensor(*(*summed_ids).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_segment_ids, meta_dist_input_out, meta_dist_input_summed_ids, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("segment_pool_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "segment_pool_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "segment_pool_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "segment_pool_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_segment_ids = ReshardApiInputToKernelInput(dev_ctx, segment_ids, spmd_info.first[1], "segment_ids");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_summed_ids = ReshardApiInputToKernelInput(dev_ctx, summed_ids, spmd_info.first[3], "summed_ids");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_segment_ids = PrepareDataForDistTensor(dist_input_segment_ids, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_segment_ids = &dist_input_segment_ids->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_summed_ids = PrepareDataForDistTensor(dist_input_summed_ids, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_summed_ids = dist_input_summed_ids ? paddle::make_optional<phi::DenseTensor>((*dist_input_summed_ids)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> summed_ids_record_shapes;
         if(input_summed_ids){
           summed_ids_record_shapes.push_back((*input_summed_ids).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"segment_ids", {
         (*input_segment_ids).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"summed_ids", summed_ids_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["pooltype"] = pooltype;
         phi::RecordOpInfoSupplement("segment_pool_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("segment_pool_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_segment_ids, *input_out, input_summed_ids, *input_out_grad, pooltype, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "segment_pool_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "segment_pool_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("segment_pool_grad", kernel_data_type);
  }
  VLOG(6) << "segment_pool_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_segment_ids = PrepareData(segment_ids, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_summed_ids = PrepareData(summed_ids, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> summed_ids_record_shapes;
     if(input_summed_ids){
       summed_ids_record_shapes.push_back((*input_summed_ids).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"segment_ids", {
     (*input_segment_ids).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"summed_ids", summed_ids_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["pooltype"] = pooltype;
     phi::RecordOpInfoSupplement("segment_pool_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("segment_pool_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("segment_pool_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_segment_ids, *input_out, input_summed_ids, *input_out_grad, pooltype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void selu_grad(const Tensor& out, const Tensor& out_grad, float scale, float alpha, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("selu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "selu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "selu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "selu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["scale"] = scale;
         attrs["alpha"] = alpha;
         phi::RecordOpInfoSupplement("selu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("selu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, scale, alpha, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "selu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "selu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("selu_grad", kernel_data_type);
  }
  VLOG(6) << "selu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["scale"] = scale;
     attrs["alpha"] = alpha;
     phi::RecordOpInfoSupplement("selu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("selu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("selu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, scale, alpha, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void send_u_recv_grad(const Tensor& x, const Tensor& src_index, const Tensor& dst_index, const paddle::optional<Tensor>& out, const paddle::optional<Tensor>& dst_count, const Tensor& out_grad, const std::string& reduce_op, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, src_index, dst_index, out, dst_count, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, src_index, dst_index, out, dst_count, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_src_index = MakeDistMetaTensor(*src_index.impl());
    auto meta_dist_input_dst_index = MakeDistMetaTensor(*dst_index.impl());
    auto meta_dist_input_out = out ? MakeDistMetaTensor(*(*out).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_dst_count = dst_count ? MakeDistMetaTensor(*(*dst_count).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_src_index, meta_dist_input_dst_index, meta_dist_input_out, meta_dist_input_dst_count, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("send_u_recv_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "send_u_recv_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "send_u_recv_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "send_u_recv_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_src_index = ReshardApiInputToKernelInput(dev_ctx, src_index, spmd_info.first[1], "src_index");
      auto dist_input_dst_index = ReshardApiInputToKernelInput(dev_ctx, dst_index, spmd_info.first[2], "dst_index");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[3], "out");
      auto dist_input_dst_count = ReshardApiInputToKernelInput(dev_ctx, dst_count, spmd_info.first[4], "dst_count");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[5], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_src_index = PrepareDataForDistTensor(dist_input_src_index, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_src_index = &dist_input_src_index->value();

      dist_input_dst_index = PrepareDataForDistTensor(dist_input_dst_index, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_dst_index = &dist_input_dst_index->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out = dist_input_out ? paddle::make_optional<phi::DenseTensor>((*dist_input_out)->value()) : paddle::none;

      dist_input_dst_count = PrepareDataForDistTensor(dist_input_dst_count, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_dst_count = dist_input_dst_count ? paddle::make_optional<phi::DenseTensor>((*dist_input_dst_count)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_record_shapes;
         if(input_out){
           out_record_shapes.push_back((*input_out).dims());
         }
         std::vector<phi::DDim> dst_count_record_shapes;
         if(input_dst_count){
           dst_count_record_shapes.push_back((*input_dst_count).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"src_index", {
         (*input_src_index).dims()}},
         {"dst_index", {
         (*input_dst_index).dims()}},
         {"out", out_record_shapes},
         {"dst_count", dst_count_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["reduce_op"] = reduce_op;
         phi::RecordOpInfoSupplement("send_u_recv_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("send_u_recv_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_src_index, *input_dst_index, input_out, input_dst_count, *input_out_grad, reduce_op, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "send_u_recv_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "send_u_recv_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("send_u_recv_grad", kernel_data_type);
  }
  VLOG(6) << "send_u_recv_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_index = PrepareData(src_index, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_index = PrepareData(dst_index, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_count = PrepareData(dst_count, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_record_shapes;
     if(input_out){
       out_record_shapes.push_back((*input_out).dims());
     }
     std::vector<phi::DDim> dst_count_record_shapes;
     if(input_dst_count){
       dst_count_record_shapes.push_back((*input_dst_count).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"src_index", {
     (*input_src_index).dims()}},
     {"dst_index", {
     (*input_dst_index).dims()}},
     {"out", out_record_shapes},
     {"dst_count", dst_count_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["reduce_op"] = reduce_op;
     phi::RecordOpInfoSupplement("send_u_recv_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("send_u_recv_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("send_u_recv_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_src_index, *input_dst_index, input_out, input_dst_count, *input_out_grad, reduce_op, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void send_ue_recv_grad(const Tensor& x, const Tensor& y, const Tensor& src_index, const Tensor& dst_index, const paddle::optional<Tensor>& out, const paddle::optional<Tensor>& dst_count, const Tensor& out_grad, const std::string& message_op, const std::string& reduce_op, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, src_index, dst_index, out, dst_count, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, src_index, dst_index, out, dst_count, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_src_index = MakeDistMetaTensor(*src_index.impl());
    auto meta_dist_input_dst_index = MakeDistMetaTensor(*dst_index.impl());
    auto meta_dist_input_out = out ? MakeDistMetaTensor(*(*out).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_dst_count = dst_count ? MakeDistMetaTensor(*(*dst_count).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_src_index, meta_dist_input_dst_index, meta_dist_input_out, meta_dist_input_dst_count, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("send_ue_recv_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "send_ue_recv_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "send_ue_recv_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "send_ue_recv_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_src_index = ReshardApiInputToKernelInput(dev_ctx, src_index, spmd_info.first[2], "src_index");
      auto dist_input_dst_index = ReshardApiInputToKernelInput(dev_ctx, dst_index, spmd_info.first[3], "dst_index");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[4], "out");
      auto dist_input_dst_count = ReshardApiInputToKernelInput(dev_ctx, dst_count, spmd_info.first[5], "dst_count");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[6], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_src_index = PrepareDataForDistTensor(dist_input_src_index, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_src_index = &dist_input_src_index->value();

      dist_input_dst_index = PrepareDataForDistTensor(dist_input_dst_index, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_dst_index = &dist_input_dst_index->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out = dist_input_out ? paddle::make_optional<phi::DenseTensor>((*dist_input_out)->value()) : paddle::none;

      dist_input_dst_count = PrepareDataForDistTensor(dist_input_dst_count, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_dst_count = dist_input_dst_count ? paddle::make_optional<phi::DenseTensor>((*dist_input_dst_count)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_record_shapes;
         if(input_out){
           out_record_shapes.push_back((*input_out).dims());
         }
         std::vector<phi::DDim> dst_count_record_shapes;
         if(input_dst_count){
           dst_count_record_shapes.push_back((*input_dst_count).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"src_index", {
         (*input_src_index).dims()}},
         {"dst_index", {
         (*input_dst_index).dims()}},
         {"out", out_record_shapes},
         {"dst_count", dst_count_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["message_op"] = message_op;
         attrs["reduce_op"] = reduce_op;
         phi::RecordOpInfoSupplement("send_ue_recv_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("send_ue_recv_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_src_index, *input_dst_index, input_out, input_dst_count, *input_out_grad, message_op, reduce_op, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "send_ue_recv_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "send_ue_recv_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("send_ue_recv_grad", kernel_data_type);
  }
  VLOG(6) << "send_ue_recv_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_index = PrepareData(src_index, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_index = PrepareData(dst_index, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_count = PrepareData(dst_count, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_record_shapes;
     if(input_out){
       out_record_shapes.push_back((*input_out).dims());
     }
     std::vector<phi::DDim> dst_count_record_shapes;
     if(input_dst_count){
       dst_count_record_shapes.push_back((*input_dst_count).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"src_index", {
     (*input_src_index).dims()}},
     {"dst_index", {
     (*input_dst_index).dims()}},
     {"out", out_record_shapes},
     {"dst_count", dst_count_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["message_op"] = message_op;
     attrs["reduce_op"] = reduce_op;
     phi::RecordOpInfoSupplement("send_ue_recv_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("send_ue_recv_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("send_ue_recv_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_src_index, *input_dst_index, input_out, input_dst_count, *input_out_grad, message_op, reduce_op, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void send_uv_grad(const Tensor& x, const Tensor& y, const Tensor& src_index, const Tensor& dst_index, const Tensor& out_grad, const std::string& message_op, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, src_index, dst_index, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, src_index, dst_index, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_src_index = MakeDistMetaTensor(*src_index.impl());
    auto meta_dist_input_dst_index = MakeDistMetaTensor(*dst_index.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_src_index, meta_dist_input_dst_index, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("send_uv_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "send_uv_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "send_uv_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "send_uv_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_src_index = ReshardApiInputToKernelInput(dev_ctx, src_index, spmd_info.first[2], "src_index");
      auto dist_input_dst_index = ReshardApiInputToKernelInput(dev_ctx, dst_index, spmd_info.first[3], "dst_index");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_src_index = PrepareDataForDistTensor(dist_input_src_index, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_src_index = &dist_input_src_index->value();

      dist_input_dst_index = PrepareDataForDistTensor(dist_input_dst_index, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_dst_index = &dist_input_dst_index->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"src_index", {
         (*input_src_index).dims()}},
         {"dst_index", {
         (*input_dst_index).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["message_op"] = message_op;
         phi::RecordOpInfoSupplement("send_uv_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("send_uv_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_src_index, *input_dst_index, *input_out_grad, message_op, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "send_uv_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "send_uv_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("send_uv_grad", kernel_data_type);
  }
  VLOG(6) << "send_uv_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_src_index = PrepareData(src_index, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dst_index = PrepareData(dst_index, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"src_index", {
     (*input_src_index).dims()}},
     {"dst_index", {
     (*input_dst_index).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["message_op"] = message_op;
     phi::RecordOpInfoSupplement("send_uv_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("send_uv_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("send_uv_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_src_index, *input_dst_index, *input_out_grad, message_op, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void sigmoid_cross_entropy_with_logits_grad(const Tensor& x, const Tensor& label, const paddle::optional<Tensor>& pos_weight, const Tensor& out_grad, bool normalize, int ignore_index, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, label, pos_weight, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, label, pos_weight, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_pos_weight = pos_weight ? MakeDistMetaTensor(*(*pos_weight).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_label, meta_dist_input_pos_weight, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("sigmoid_cross_entropy_with_logits_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sigmoid_cross_entropy_with_logits_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sigmoid_cross_entropy_with_logits_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sigmoid_cross_entropy_with_logits_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[1], "label");
      auto dist_input_pos_weight = ReshardApiInputToKernelInput(dev_ctx, pos_weight, spmd_info.first[2], "pos_weight");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_pos_weight = PrepareDataForDistTensor(dist_input_pos_weight, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_pos_weight = dist_input_pos_weight ? paddle::make_optional<phi::DenseTensor>((*dist_input_pos_weight)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> pos_weight_record_shapes;
         if(input_pos_weight){
           pos_weight_record_shapes.push_back((*input_pos_weight).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"pos_weight", pos_weight_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["normalize"] = normalize;
         attrs["ignore_index"] = ignore_index;
         phi::RecordOpInfoSupplement("sigmoid_cross_entropy_with_logits_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, bool, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_label, input_pos_weight, *input_out_grad, normalize, ignore_index, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sigmoid_cross_entropy_with_logits_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_cross_entropy_with_logits_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid_cross_entropy_with_logits_grad", kernel_data_type);
  }
  VLOG(6) << "sigmoid_cross_entropy_with_logits_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pos_weight = PrepareData(pos_weight, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> pos_weight_record_shapes;
     if(input_pos_weight){
       pos_weight_record_shapes.push_back((*input_pos_weight).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"pos_weight", pos_weight_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["normalize"] = normalize;
     attrs["ignore_index"] = ignore_index;
     phi::RecordOpInfoSupplement("sigmoid_cross_entropy_with_logits_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid_cross_entropy_with_logits_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_label, input_pos_weight, *input_out_grad, normalize, ignore_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void sigmoid_double_grad(const Tensor& out, const Tensor& fwd_grad_out, const Tensor& grad_x_grad, Tensor* out_grad, Tensor* fwd_grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, fwd_grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, fwd_grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "sigmoid_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid_double_grad", kernel_data_type);
  }
  VLOG(6) << "sigmoid_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fwd_grad_out = PrepareData(fwd_grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"fwd_grad_out", {
     (*input_fwd_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sigmoid_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(out_grad);
  auto kernel_out_1 = SetKernelOutput(fwd_grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_fwd_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_fwd_grad_out, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void sigmoid_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("sigmoid_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sigmoid_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sigmoid_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sigmoid_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("sigmoid_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sigmoid_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sigmoid_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid_grad", kernel_data_type);
  }
  VLOG(6) << "sigmoid_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sigmoid_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void sigmoid_triple_grad(const Tensor& out, const Tensor& fwd_grad_out, const Tensor& grad_grad_x, const Tensor& grad_out_grad, const paddle::optional<Tensor>& grad_grad_out_grad, Tensor* out_grad, Tensor* fwd_grad_out_grad, Tensor* grad_grad_x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, fwd_grad_out, grad_grad_x, grad_out_grad, grad_grad_out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, fwd_grad_out, grad_grad_x, grad_out_grad, grad_grad_out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "sigmoid_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sigmoid_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sigmoid_triple_grad", kernel_data_type);
  }
  VLOG(6) << "sigmoid_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fwd_grad_out = PrepareData(fwd_grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_x = PrepareData(grad_grad_x, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_grad = PrepareData(grad_out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_out_grad = PrepareData(grad_grad_out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_grad_out_grad_record_shapes;
     if(input_grad_grad_out_grad){
       grad_grad_out_grad_record_shapes.push_back((*input_grad_grad_out_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"fwd_grad_out", {
     (*input_fwd_grad_out).dims()}},
     {"grad_grad_x", {
     (*input_grad_grad_x).dims()}},
     {"grad_out_grad", {
     (*input_grad_out_grad).dims()}},
     {"grad_grad_out_grad",
     grad_grad_out_grad_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sigmoid_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(out_grad);
  auto kernel_out_1 = SetKernelOutput(fwd_grad_out_grad);
  auto kernel_out_2 = SetKernelOutput(grad_grad_x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sigmoid_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_fwd_grad_out), MakeMetaTensor(*input_grad_grad_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sigmoid_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_fwd_grad_out, *input_grad_grad_x, *input_grad_out_grad, input_grad_grad_out_grad, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void silu_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("silu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `silu_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "silu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "silu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "silu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("silu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("silu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "silu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "silu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("silu_grad", kernel_data_type);
  }
  VLOG(6) << "silu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("silu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("silu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("silu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void sin_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "sin_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sin_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sin_double_grad", kernel_data_type);
  }
  VLOG(6) << "sin_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sin_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sin_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sin_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void sin_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("sin_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `sin_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sin_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sin_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sin_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("sin_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sin_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sin_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sin_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sin_grad", kernel_data_type);
  }
  VLOG(6) << "sin_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sin_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sin_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sin_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void sin_triple_grad(const Tensor& x, const paddle::optional<Tensor>& grad_out_forward, const paddle::optional<Tensor>& grad_x_grad_forward, const Tensor& grad_x_grad, const paddle::optional<Tensor>& grad_out_grad_grad, Tensor* x_grad, Tensor* grad_out_forward_grad, Tensor* grad_x_grad_forward_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out_forward, grad_x_grad_forward, grad_x_grad, grad_out_grad_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "sin_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sin_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sin_triple_grad", kernel_data_type);
  }
  VLOG(6) << "sin_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_forward = PrepareData(grad_out_forward, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad_forward = PrepareData(grad_x_grad_forward, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_grad_grad = PrepareData(grad_out_grad_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_out_forward_record_shapes;
     if(input_grad_out_forward){
       grad_out_forward_record_shapes.push_back((*input_grad_out_forward).dims());
     }
     std::vector<phi::DDim> grad_x_grad_forward_record_shapes;
     if(input_grad_x_grad_forward){
       grad_x_grad_forward_record_shapes.push_back((*input_grad_x_grad_forward).dims());
     }
     std::vector<phi::DDim> grad_out_grad_grad_record_shapes;
     if(input_grad_out_grad_grad){
       grad_out_grad_grad_record_shapes.push_back((*input_grad_out_grad_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out_forward", grad_out_forward_record_shapes},
     {"grad_x_grad_forward", grad_x_grad_forward_record_shapes},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}},
     {"grad_out_grad_grad",
     grad_out_grad_grad_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sin_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_forward_grad);
  auto kernel_out_2 = SetKernelOutput(grad_x_grad_forward_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sin_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), MakeMetaTensor(input_grad_x_grad_forward), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sin_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_grad_out_forward, input_grad_x_grad_forward, *input_grad_x_grad, input_grad_out_grad_grad, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void sinh_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("sinh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sinh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sinh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sinh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("sinh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sinh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sinh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sinh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sinh_grad", kernel_data_type);
  }
  VLOG(6) << "sinh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sinh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sinh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sinh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void slogdet_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("slogdet_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "slogdet_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "slogdet_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "slogdet_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("slogdet_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("slogdet_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "slogdet_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "slogdet_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("slogdet_grad", kernel_data_type);
  }
  VLOG(6) << "slogdet_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("slogdet_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("slogdet_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("slogdet_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void softplus_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, float beta, float threshold, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "softplus_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softplus_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softplus_double_grad", kernel_data_type);
  }
  VLOG(6) << "softplus_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["beta"] = beta;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("softplus_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softplus_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softplus_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, beta, threshold, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void softplus_grad(const Tensor& x, const Tensor& out_grad, float beta, float threshold, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("softplus_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "softplus_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "softplus_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "softplus_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["beta"] = beta;
         attrs["threshold"] = threshold;
         phi::RecordOpInfoSupplement("softplus_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("softplus_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, beta, threshold, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "softplus_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softplus_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softplus_grad", kernel_data_type);
  }
  VLOG(6) << "softplus_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["beta"] = beta;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("softplus_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softplus_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softplus_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, beta, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void softshrink_grad(const Tensor& x, const Tensor& out_grad, float threshold, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("softshrink_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "softshrink_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "softshrink_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "softshrink_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["threshold"] = threshold;
         phi::RecordOpInfoSupplement("softshrink_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("softshrink_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, threshold, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "softshrink_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softshrink_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softshrink_grad", kernel_data_type);
  }
  VLOG(6) << "softshrink_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("softshrink_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softshrink_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softshrink_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void softsign_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("softsign_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "softsign_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "softsign_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "softsign_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("softsign_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("softsign_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "softsign_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softsign_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softsign_grad", kernel_data_type);
  }
  VLOG(6) << "softsign_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("softsign_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softsign_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softsign_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void solve_grad(const Tensor& x, const Tensor& y, const Tensor& out, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("solve_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "solve_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "solve_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "solve_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("solve_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("solve_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "solve_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "solve_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("solve_grad", kernel_data_type);
  }
  VLOG(6) << "solve_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("solve_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("solve_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("solve_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void spectral_norm_grad(const Tensor& weight, const Tensor& u, const Tensor& v, const Tensor& out_grad, int dim, int power_iters, float eps, Tensor* weight_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(weight, u, v, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(weight);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(weight, u, v, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_weight = MakeDistMetaTensor(*weight.impl());
    auto meta_dist_input_u = MakeDistMetaTensor(*u.impl());
    auto meta_dist_input_v = MakeDistMetaTensor(*v.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_weight, meta_dist_input_u, meta_dist_input_v, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("spectral_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(weight_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::SpectralNormGradInferMeta(MakeMetaTensor(*weight.impl()), MakeMetaTensor(*u.impl()), MakeMetaTensor(*v.impl()), MakeMetaTensor(*out_grad.impl()), dim, power_iters, eps, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "spectral_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "spectral_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "spectral_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_weight = ReshardApiInputToKernelInput(dev_ctx, weight, spmd_info.first[0], "weight");
      auto dist_input_u = ReshardApiInputToKernelInput(dev_ctx, u, spmd_info.first[1], "u");
      auto dist_input_v = ReshardApiInputToKernelInput(dev_ctx, v, spmd_info.first[2], "v");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_weight = PrepareDataForDistTensor(dist_input_weight, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_weight = &dist_input_weight->value();

      dist_input_u = PrepareDataForDistTensor(dist_input_u, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_u = &dist_input_u->value();

      dist_input_v = PrepareDataForDistTensor(dist_input_v, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_v = &dist_input_v->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"weight", {
         (*input_weight).dims()}},
         {"u", {
         (*input_u).dims()}},
         {"v", {
         (*input_v).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dim"] = dim;
         attrs["power_iters"] = power_iters;
         attrs["eps"] = eps;
         phi::RecordOpInfoSupplement("spectral_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::SpectralNormGradInferMeta(MakeMetaTensor(*input_weight), MakeMetaTensor(*input_u), MakeMetaTensor(*input_v), MakeMetaTensor(*input_out_grad), dim, power_iters, eps, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("spectral_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_weight, *input_u, *input_v, *input_out_grad, dim, power_iters, eps, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, weight_grad, "weight_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "spectral_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "spectral_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("spectral_norm_grad", kernel_data_type);
  }
  VLOG(6) << "spectral_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_u = PrepareData(u, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_v = PrepareData(v, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"weight", {
     (*input_weight).dims()}},
     {"u", {
     (*input_u).dims()}},
     {"v", {
     (*input_v).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dim"] = dim;
     attrs["power_iters"] = power_iters;
     attrs["eps"] = eps;
     phi::RecordOpInfoSupplement("spectral_norm_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(weight_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("spectral_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::SpectralNormGradInferMeta(MakeMetaTensor(*input_weight), MakeMetaTensor(*input_u), MakeMetaTensor(*input_v), MakeMetaTensor(*input_out_grad), dim, power_iters, eps, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("spectral_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_weight, *input_u, *input_v, *input_out_grad, dim, power_iters, eps, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void sqrt_double_grad(const Tensor& out, const Tensor& grad_x, const Tensor& grad_x_grad, Tensor* out_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, grad_x, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, grad_x, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "sqrt_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sqrt_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sqrt_double_grad", kernel_data_type);
  }
  VLOG(6) << "sqrt_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x = PrepareData(grad_x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"grad_x", {
     (*input_grad_x).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sqrt_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(out_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sqrt_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sqrt_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_grad_x, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void sqrt_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("sqrt_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sqrt_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sqrt_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sqrt_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("sqrt_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sqrt_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sqrt_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sqrt_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sqrt_grad", kernel_data_type);
  }
  VLOG(6) << "sqrt_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("sqrt_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sqrt_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sqrt_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void square_double_grad(const Tensor& x, const Tensor& grad_out, const Tensor& grad_x_grad, Tensor* x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "square_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "square_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("square_double_grad", kernel_data_type);
  }
  VLOG(6) << "square_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("square_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("square_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("square_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_grad_out, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void square_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseUnaryGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("square_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `square_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "square_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "square_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "square_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("square_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("square_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "square_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "square_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("square_grad", kernel_data_type);
  }
  VLOG(6) << "square_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("square_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("square_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("square_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void squared_l2_norm_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("squared_l2_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "squared_l2_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "squared_l2_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "squared_l2_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("squared_l2_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("squared_l2_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "squared_l2_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squared_l2_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("squared_l2_norm_grad", kernel_data_type);
  }
  VLOG(6) << "squared_l2_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("squared_l2_norm_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("squared_l2_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("squared_l2_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void squeeze_grad(const Tensor& xshape, const Tensor& out_grad, const IntArray& axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(xshape, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(xshape, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_xshape = MakeDistMetaTensor(*xshape.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::SqueezeGradInferSpmd(meta_dist_input_xshape, meta_dist_input_out_grad, axis.GetData());
    DebugInfoForInferSpmd("squeeze_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::KernelWithXShapeInferMeta(MakeMetaTensor(*xshape.impl()), MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `squeeze_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "squeeze_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "squeeze_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "squeeze_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_xshape = ReshardApiInputToKernelInput(dev_ctx, xshape, spmd_info.first[0], "xshape");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_xshape = PrepareDataForDistTensor(dist_input_xshape, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_xshape = &dist_input_xshape->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"xshape", {
         (*input_xshape).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         phi::RecordOpInfoSupplement("squeeze_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("squeeze_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_xshape, *input_out_grad, phi::IntArray(axis), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "squeeze_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "squeeze_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("squeeze_grad", kernel_data_type);
  }
  VLOG(6) << "squeeze_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_xshape = PrepareData(xshape, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"xshape", {
     (*input_xshape).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("squeeze_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("squeeze_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("squeeze_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_xshape, *input_out_grad, phi::IntArray(axis), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void stack_grad(const std::vector<Tensor>& x, const Tensor& out_grad, int axis, std::vector<Tensor*> x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::StackGradInferSpmd(meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("stack_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    phi::StackGradInferMeta(MakeMetaTensor(*out_grad.impl()), axis, dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    // API `stack_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "stack_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "stack_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "stack_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("stack_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::StackGradInferMeta(MakeMetaTensor(*input_out_grad), axis, dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("stack_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "stack_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "stack_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("stack_grad", kernel_data_type);
  }
  VLOG(6) << "stack_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("stack_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("stack_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::StackGradInferMeta(MakeMetaTensor(*input_out_grad), axis, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("stack_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void stanh_grad(const Tensor& x, const Tensor& out_grad, float scale_a, float scale_b, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("stanh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "stanh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "stanh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "stanh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["scale_a"] = scale_a;
         attrs["scale_b"] = scale_b;
         phi::RecordOpInfoSupplement("stanh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("stanh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, scale_a, scale_b, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "stanh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "stanh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("stanh_grad", kernel_data_type);
  }
  VLOG(6) << "stanh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["scale_a"] = scale_a;
     attrs["scale_b"] = scale_b;
     phi::RecordOpInfoSupplement("stanh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("stanh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("stanh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, scale_a, scale_b, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void svd_grad(const Tensor& x, const Tensor& u, const Tensor& vh, const Tensor& s, const paddle::optional<Tensor>& u_grad, const paddle::optional<Tensor>& vh_grad, const paddle::optional<Tensor>& s_grad, bool full_matrices, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, u, vh, s, u_grad, vh_grad, s_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(s.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, u, vh, s, u_grad, vh_grad, s_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_u = MakeDistMetaTensor(*u.impl());
    auto meta_dist_input_vh = MakeDistMetaTensor(*vh.impl());
    auto meta_dist_input_s = MakeDistMetaTensor(*s.impl());
    auto meta_dist_input_u_grad = u_grad ? MakeDistMetaTensor(*(*u_grad).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_vh_grad = vh_grad ? MakeDistMetaTensor(*(*vh_grad).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_s_grad = s_grad ? MakeDistMetaTensor(*(*s_grad).impl()) : phi::distributed::DistMetaTensor();
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_u, meta_dist_input_vh, meta_dist_input_s, meta_dist_input_u_grad, meta_dist_input_vh_grad, meta_dist_input_s_grad);
    DebugInfoForInferSpmd("svd_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "svd_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "svd_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "svd_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_u = ReshardApiInputToKernelInput(dev_ctx, u, spmd_info.first[1], "u");
      auto dist_input_vh = ReshardApiInputToKernelInput(dev_ctx, vh, spmd_info.first[2], "vh");
      auto dist_input_s = ReshardApiInputToKernelInput(dev_ctx, s, spmd_info.first[3], "s");
      auto dist_input_u_grad = ReshardApiInputToKernelInput(dev_ctx, u_grad, spmd_info.first[4], "u_grad");
      auto dist_input_vh_grad = ReshardApiInputToKernelInput(dev_ctx, vh_grad, spmd_info.first[5], "vh_grad");
      auto dist_input_s_grad = ReshardApiInputToKernelInput(dev_ctx, s_grad, spmd_info.first[6], "s_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_u = PrepareDataForDistTensor(dist_input_u, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_u = &dist_input_u->value();

      dist_input_vh = PrepareDataForDistTensor(dist_input_vh, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_vh = &dist_input_vh->value();

      dist_input_s = PrepareDataForDistTensor(dist_input_s, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_s = &dist_input_s->value();

      dist_input_u_grad = PrepareDataForDistTensor(dist_input_u_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_u_grad = dist_input_u_grad ? paddle::make_optional<phi::DenseTensor>((*dist_input_u_grad)->value()) : paddle::none;

      dist_input_vh_grad = PrepareDataForDistTensor(dist_input_vh_grad, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_vh_grad = dist_input_vh_grad ? paddle::make_optional<phi::DenseTensor>((*dist_input_vh_grad)->value()) : paddle::none;

      dist_input_s_grad = PrepareDataForDistTensor(dist_input_s_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_s_grad = dist_input_s_grad ? paddle::make_optional<phi::DenseTensor>((*dist_input_s_grad)->value()) : paddle::none;

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> u_grad_record_shapes;
         if(input_u_grad){
           u_grad_record_shapes.push_back((*input_u_grad).dims());
         }
         std::vector<phi::DDim> vh_grad_record_shapes;
         if(input_vh_grad){
           vh_grad_record_shapes.push_back((*input_vh_grad).dims());
         }
         std::vector<phi::DDim> s_grad_record_shapes;
         if(input_s_grad){
           s_grad_record_shapes.push_back((*input_s_grad).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"u", {
         (*input_u).dims()}},
         {"vh", {
         (*input_vh).dims()}},
         {"s", {
         (*input_s).dims()}},
         {"u_grad", u_grad_record_shapes},
         {"vh_grad", vh_grad_record_shapes},
         {"s_grad",
         s_grad_record_shapes}};
         phi::AttributeMap attrs;
         attrs["full_matrices"] = full_matrices;
         phi::RecordOpInfoSupplement("svd_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("svd_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_u, *input_vh, *input_s, input_u_grad, input_vh_grad, input_s_grad, full_matrices, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "svd_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "svd_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("svd_grad", kernel_data_type);
  }
  VLOG(6) << "svd_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_u = PrepareData(u, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_vh = PrepareData(vh, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_s = PrepareData(s, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_u_grad = PrepareData(u_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_vh_grad = PrepareData(vh_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_s_grad = PrepareData(s_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> u_grad_record_shapes;
     if(input_u_grad){
       u_grad_record_shapes.push_back((*input_u_grad).dims());
     }
     std::vector<phi::DDim> vh_grad_record_shapes;
     if(input_vh_grad){
       vh_grad_record_shapes.push_back((*input_vh_grad).dims());
     }
     std::vector<phi::DDim> s_grad_record_shapes;
     if(input_s_grad){
       s_grad_record_shapes.push_back((*input_s_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"u", {
     (*input_u).dims()}},
     {"vh", {
     (*input_vh).dims()}},
     {"s", {
     (*input_s).dims()}},
     {"u_grad", u_grad_record_shapes},
     {"vh_grad", vh_grad_record_shapes},
     {"s_grad",
     s_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["full_matrices"] = full_matrices;
     phi::RecordOpInfoSupplement("svd_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("svd_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("svd_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_u, *input_vh, *input_s, input_u_grad, input_vh_grad, input_s_grad, full_matrices, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void swiglu_grad(const Tensor& x, const paddle::optional<Tensor>& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = y ? MakeDistMetaTensor(*(*y).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::SwiGLUGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("swiglu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_y = y ? MakeMetaTensor(*(*y).impl()) : phi::MetaTensor();

    phi::SwiGLUGradInferMeta(MakeMetaTensor(*x.impl()), meta_dist_y, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `swiglu_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "swiglu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "swiglu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "swiglu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_y = dist_input_y ? paddle::make_optional<phi::DenseTensor>((*dist_input_y)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> y_record_shapes;
         if(input_y){
           y_record_shapes.push_back((*input_y).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", y_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("swiglu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::SwiGLUGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("swiglu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "swiglu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "swiglu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("swiglu_grad", kernel_data_type);
  }
  VLOG(6) << "swiglu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> y_record_shapes;
     if(input_y){
       y_record_shapes.push_back((*input_y).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", y_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("swiglu_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("swiglu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::SwiGLUGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("swiglu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void take_along_axis_grad(const Tensor& arr, const Tensor& indices, const Tensor& out_grad, int axis, Tensor* arr_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(arr, indices, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(arr, indices, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_arr = MakeDistMetaTensor(*arr.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_arr, meta_dist_input_indices, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("take_along_axis_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(arr_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*arr.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "take_along_axis_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "take_along_axis_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "take_along_axis_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_arr = ReshardApiInputToKernelInput(dev_ctx, arr, spmd_info.first[0], "arr");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_arr = PrepareDataForDistTensor(dist_input_arr, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_arr = &dist_input_arr->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"arr", {
         (*input_arr).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("take_along_axis_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_arr), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("take_along_axis_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_arr, *input_indices, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, arr_grad, "arr_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "take_along_axis_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "take_along_axis_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("take_along_axis_grad", kernel_data_type);
  }
  VLOG(6) << "take_along_axis_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_arr = PrepareData(arr, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"arr", {
     (*input_arr).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("take_along_axis_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(arr_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("take_along_axis_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_arr), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("take_along_axis_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_arr, *input_indices, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void tan_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("tan_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "tan_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "tan_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "tan_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("tan_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("tan_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "tan_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tan_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tan_grad", kernel_data_type);
  }
  VLOG(6) << "tan_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tan_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tan_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tan_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void tanh_double_grad(const Tensor& out, const Tensor& grad_out, const Tensor& grad_x_grad, Tensor* out_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "tanh_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh_double_grad", kernel_data_type);
  }
  VLOG(6) << "tanh_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(out_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_grad_out, *input_grad_x_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void tanh_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("tanh_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "tanh_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "tanh_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "tanh_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("tanh_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("tanh_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "tanh_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh_grad", kernel_data_type);
  }
  VLOG(6) << "tanh_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void tanh_shrink_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("tanh_shrink_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "tanh_shrink_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "tanh_shrink_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "tanh_shrink_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("tanh_shrink_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("tanh_shrink_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "tanh_shrink_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh_shrink_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh_shrink_grad", kernel_data_type);
  }
  VLOG(6) << "tanh_shrink_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh_shrink_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh_shrink_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh_shrink_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void tanh_triple_grad(const Tensor& out, const Tensor& grad_out_forward, const Tensor& grad_x_grad_forward, const paddle::optional<Tensor>& grad_out_new_grad, const paddle::optional<Tensor>& grad_out_grad_grad, Tensor* out_grad, Tensor* grad_out_forward_grad, Tensor* grad_x_grad_forward_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, grad_out_forward, grad_x_grad_forward, grad_out_new_grad, grad_out_grad_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad_forward.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, grad_out_forward, grad_x_grad_forward, grad_out_new_grad, grad_out_grad_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "tanh_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tanh_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tanh_triple_grad", kernel_data_type);
  }
  VLOG(6) << "tanh_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_forward = PrepareData(grad_out_forward, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad_forward = PrepareData(grad_x_grad_forward, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_new_grad = PrepareData(grad_out_new_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out_grad_grad = PrepareData(grad_out_grad_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_out_new_grad_record_shapes;
     if(input_grad_out_new_grad){
       grad_out_new_grad_record_shapes.push_back((*input_grad_out_new_grad).dims());
     }
     std::vector<phi::DDim> grad_out_grad_grad_record_shapes;
     if(input_grad_out_grad_grad){
       grad_out_grad_grad_record_shapes.push_back((*input_grad_out_grad_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"grad_out_forward", {
     (*input_grad_out_forward).dims()}},
     {"grad_x_grad_forward", {
     (*input_grad_x_grad_forward).dims()}},
     {"grad_out_new_grad", grad_out_new_grad_record_shapes},
     {"grad_out_grad_grad",
     grad_out_grad_grad_record_shapes}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("tanh_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(out_grad);
  auto kernel_out_1 = SetKernelOutput(grad_out_forward_grad);
  auto kernel_out_2 = SetKernelOutput(grad_x_grad_forward_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tanh_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_out), MakeMetaTensor(*input_out), MakeMetaTensor(*input_grad_x_grad_forward), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tanh_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_grad_out_forward, *input_grad_x_grad_forward, input_grad_out_new_grad, input_grad_out_grad_grad, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void temporal_shift_grad(const Tensor& out_grad, int seg_num, float shift_ratio, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("temporal_shift_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "temporal_shift_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "temporal_shift_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "temporal_shift_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["seg_num"] = seg_num;
         attrs["shift_ratio"] = shift_ratio;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("temporal_shift_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("temporal_shift_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, float, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, seg_num, shift_ratio, data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "temporal_shift_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "temporal_shift_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("temporal_shift_grad", kernel_data_type);
  }
  VLOG(6) << "temporal_shift_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["seg_num"] = seg_num;
     attrs["shift_ratio"] = shift_ratio;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("temporal_shift_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("temporal_shift_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, float, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("temporal_shift_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, seg_num, shift_ratio, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void tensor_unfold_grad(const Tensor& input, const Tensor& out_grad, int64_t axis, int64_t size, int64_t step, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("tensor_unfold_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::StridedUnChangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "tensor_unfold_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "tensor_unfold_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "tensor_unfold_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["size"] = size;
         attrs["step"] = step;
         phi::RecordOpInfoSupplement("tensor_unfold_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("tensor_unfold_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int64_t, int64_t, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, axis, size, step, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "tensor_unfold_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tensor_unfold_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tensor_unfold_grad", kernel_data_type);
  }
  VLOG(6) << "tensor_unfold_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["size"] = size;
     attrs["step"] = step;
     phi::RecordOpInfoSupplement("tensor_unfold_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tensor_unfold_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, int64_t, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tensor_unfold_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, axis, size, step, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void thresholded_relu_grad(const Tensor& x, const Tensor& out_grad, float threshold, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("thresholded_relu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "thresholded_relu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "thresholded_relu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "thresholded_relu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["threshold"] = threshold;
         phi::RecordOpInfoSupplement("thresholded_relu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("thresholded_relu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, threshold, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "thresholded_relu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "thresholded_relu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("thresholded_relu_grad", kernel_data_type);
  }
  VLOG(6) << "thresholded_relu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["threshold"] = threshold;
     phi::RecordOpInfoSupplement("thresholded_relu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("thresholded_relu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("thresholded_relu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, threshold, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void topk_grad(const Tensor& x, const Tensor& indices, const Tensor& out_grad, const Scalar& k, int axis, bool largest, bool sorted, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("topk_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "topk_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "topk_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "topk_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (k.dtype()) {
          case DataType::FLOAT32:
              attrs["k"] = static_cast<float>(k.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["k"] = static_cast<double>(k.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["k"] = static_cast<float>(k.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["k"] = static_cast<float>(k.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["k"] = static_cast<int32_t>(k.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["k"] = static_cast<int64_t>(k.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["k"] = static_cast<int16_t>(k.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["k"] = static_cast<int8_t>(k.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["k"] = static_cast<uint16_t>(k.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["k"] = static_cast<uint8_t>(k.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["k"] = static_cast<bool>(k.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["k"] = static_cast<float>(k.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["k"] = static_cast<double>(k.to<complex128>());
              break;
          default:
              attrs["k"] = "";
              break;
        }
         attrs["axis"] = axis;
         attrs["largest"] = largest;
         attrs["sorted"] = sorted;
         phi::RecordOpInfoSupplement("topk_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("topk_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, int, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, phi::Scalar(k), axis, largest, sorted, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "topk_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "topk_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("topk_grad", kernel_data_type);
  }
  VLOG(6) << "topk_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (k.dtype()) {
      case DataType::FLOAT32:
          attrs["k"] = static_cast<float>(k.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["k"] = static_cast<double>(k.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["k"] = static_cast<float>(k.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["k"] = static_cast<float>(k.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["k"] = static_cast<int32_t>(k.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["k"] = static_cast<int64_t>(k.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["k"] = static_cast<int16_t>(k.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["k"] = static_cast<int8_t>(k.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["k"] = static_cast<uint16_t>(k.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["k"] = static_cast<uint8_t>(k.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["k"] = static_cast<bool>(k.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["k"] = static_cast<float>(k.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["k"] = static_cast<double>(k.to<complex128>());
          break;
      default:
          attrs["k"] = "";
          break;
    }
     attrs["axis"] = axis;
     attrs["largest"] = largest;
     attrs["sorted"] = sorted;
     phi::RecordOpInfoSupplement("topk_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("topk_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, int, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("topk_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out_grad, phi::Scalar(k), axis, largest, sorted, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void trace_grad(const Tensor& x, const Tensor& out_grad, int offset, int axis1, int axis2, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("trace_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "trace_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "trace_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "trace_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["offset"] = offset;
         attrs["axis1"] = axis1;
         attrs["axis2"] = axis2;
         phi::RecordOpInfoSupplement("trace_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("trace_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, offset, axis1, axis2, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "trace_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trace_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trace_grad", kernel_data_type);
  }
  VLOG(6) << "trace_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["offset"] = offset;
     attrs["axis1"] = axis1;
     attrs["axis2"] = axis2;
     phi::RecordOpInfoSupplement("trace_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trace_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trace_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, offset, axis1, axis2, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void triangular_solve_grad(const Tensor& x, const Tensor& y, const Tensor& out, const Tensor& out_grad, bool upper, bool transpose, bool unitriangular, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("triangular_solve_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "triangular_solve_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "triangular_solve_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "triangular_solve_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["upper"] = upper;
         attrs["transpose"] = transpose;
         attrs["unitriangular"] = unitriangular;
         phi::RecordOpInfoSupplement("triangular_solve_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("triangular_solve_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, upper, transpose, unitriangular, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "triangular_solve_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triangular_solve_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("triangular_solve_grad", kernel_data_type);
  }
  VLOG(6) << "triangular_solve_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["upper"] = upper;
     attrs["transpose"] = transpose;
     attrs["unitriangular"] = unitriangular;
     phi::RecordOpInfoSupplement("triangular_solve_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("triangular_solve_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("triangular_solve_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, upper, transpose, unitriangular, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void trilinear_interp_grad(const Tensor& x, const paddle::optional<Tensor>& out_size, const paddle::optional<std::vector<Tensor>>& size_tensor, const paddle::optional<Tensor>& scale_tensor, const Tensor& output_grad, const std::string& data_format, int out_d, int out_h, int out_w, const std::vector<float>& scale, const std::string& interp_method, bool align_corners, int align_mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_size, size_tensor, scale_tensor, output_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(output_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(output_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_size, size_tensor, scale_tensor, output_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_size = out_size ? MakeDistMetaTensor(*(*out_size).impl()) : phi::distributed::DistMetaTensor();
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_size_tensor;
    if (size_tensor) {
        for(auto& e : *size_tensor) {
            meta_dist_input_size_tensor.push_back(MakeDistMetaTensor(*e.impl()));
        }
    }
    auto meta_dist_input_scale_tensor = scale_tensor ? MakeDistMetaTensor(*(*scale_tensor).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_output_grad = MakeDistMetaTensor(*output_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_size, meta_dist_input_size_tensor, meta_dist_input_scale_tensor, meta_dist_input_output_grad);
    DebugInfoForInferSpmd("trilinear_interp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "trilinear_interp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "trilinear_interp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "trilinear_interp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_size = ReshardApiInputToKernelInput(dev_ctx, out_size, spmd_info.first[1], "out_size");
      auto dist_input_size_tensor = ReshardApiInputToKernelInput(dev_ctx, size_tensor, spmd_info.first[2], "size_tensor");
      auto dist_input_scale_tensor = ReshardApiInputToKernelInput(dev_ctx, scale_tensor, spmd_info.first[3], "scale_tensor");
      auto dist_input_output_grad = ReshardApiInputToKernelInput(dev_ctx, output_grad, spmd_info.first[4], "output_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_size = PrepareDataForDistTensor(dist_input_out_size, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_out_size = dist_input_out_size ? paddle::make_optional<phi::DenseTensor>((*dist_input_out_size)->value()) : paddle::none;

      auto dist_input_size_tensor_vec = PrepareDataForDistTensor(dist_input_size_tensor, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {true}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_size_tensor_vec;
      if (size_tensor) {
        for (auto tmp : *dist_input_size_tensor_vec) {
          dense_input_size_tensor_vec.emplace_back(&tmp->value());
      }
    }
    paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor(dense_input_size_tensor_vec);
    std::vector<phi::MetaTensor> dense_input_size_tensor_meta_vec = MakeMetaTensor(dense_input_size_tensor_vec);
    std::vector<const phi::MetaTensor*> dense_input_size_tensor_meta_ptr_vec_tmp(dense_input_size_tensor_meta_vec.size());
    for (size_t i = 0; i < dense_input_size_tensor_meta_ptr_vec_tmp.size(); ++i) {
      dense_input_size_tensor_meta_ptr_vec_tmp[i] = &dense_input_size_tensor_meta_vec[i];
    }
    paddle::optional<std::vector<const phi::MetaTensor*>> dense_input_size_tensor_meta_ptr_vec =
            size_tensor ? paddle::make_optional<std::vector<const phi::MetaTensor*>>(dense_input_size_tensor_meta_ptr_vec_tmp) : paddle::none;

      dist_input_scale_tensor = PrepareDataForDistTensor(dist_input_scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {true}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale_tensor = dist_input_scale_tensor ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale_tensor)->value()) : paddle::none;

      dist_input_output_grad = PrepareDataForDistTensor(dist_input_output_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_output_grad = &dist_input_output_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> out_size_record_shapes;
         if(input_out_size){
           out_size_record_shapes.push_back((*input_out_size).dims());
         }
         std::vector<phi::DDim> scale_tensor_record_shapes;
         if(input_scale_tensor){
           scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_size", out_size_record_shapes},
         {"scale_tensor", scale_tensor_record_shapes},
         {"output_grad", {
         (*input_output_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         if (input_size_tensor){
           ddims_vec.reserve(input_size_tensor->size());
           for (size_t i = 0; i < input_size_tensor->size(); ++i) {
             ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
           }
         }
         input_shapes.emplace_back("size_tensor", ddims_vec);
         phi::AttributeMap attrs;
         attrs["data_format"] = data_format;
         attrs["out_d"] = out_d;
         attrs["out_h"] = out_h;
         attrs["out_w"] = out_w;
         attrs["scale"] = scale;
         attrs["interp_method"] = interp_method;
         attrs["align_corners"] = align_corners;
         attrs["align_mode"] = align_mode;
         phi::RecordOpInfoSupplement("trilinear_interp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("trilinear_interp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "trilinear_interp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trilinear_interp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trilinear_interp_grad", kernel_data_type);
  }
  VLOG(6) << "trilinear_interp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_size = PrepareData(out_size, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_size_tensor_vec = PrepareData(size_tensor, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  paddle::optional<std::vector<const phi::DenseTensor*>> input_size_tensor;
  if (input_size_tensor_vec){
    input_size_tensor = paddle::optional<std::vector<const phi::DenseTensor*>>(input_size_tensor_vec->size());
    for (size_t i = 0; i < input_size_tensor_vec->size(); ++i) {
      input_size_tensor->at(i) = &input_size_tensor_vec->at(i);
    }
  }
  auto input_scale_tensor = PrepareData(scale_tensor, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {true}, kernel_result.is_stride_kernel);
  auto input_output_grad = PrepareData(output_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> out_size_record_shapes;
     if(input_out_size){
       out_size_record_shapes.push_back((*input_out_size).dims());
     }
     std::vector<phi::DDim> scale_tensor_record_shapes;
     if(input_scale_tensor){
       scale_tensor_record_shapes.push_back((*input_scale_tensor).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_size", out_size_record_shapes},
     {"scale_tensor", scale_tensor_record_shapes},
     {"output_grad", {
     (*input_output_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     if (input_size_tensor){
       ddims_vec.reserve(input_size_tensor->size());
       for (size_t i = 0; i < input_size_tensor->size(); ++i) {
         ddims_vec.emplace_back((*input_size_tensor->at(i)).dims());
       }
     }
     input_shapes.emplace_back("size_tensor", ddims_vec);
     phi::AttributeMap attrs;
     attrs["data_format"] = data_format;
     attrs["out_d"] = out_d;
     attrs["out_h"] = out_h;
     attrs["out_w"] = out_w;
     attrs["scale"] = scale;
     attrs["interp_method"] = interp_method;
     attrs["align_corners"] = align_corners;
     attrs["align_mode"] = align_mode;
     phi::RecordOpInfoSupplement("trilinear_interp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trilinear_interp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<std::vector<const phi::DenseTensor*>>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::string&, int, int, int, const std::vector<float>&, const std::string&, bool, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trilinear_interp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_out_size, input_size_tensor, input_scale_tensor, *input_output_grad, data_format, out_d, out_h, out_w, scale, interp_method, align_corners, align_mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void trunc_grad(const Tensor& out_grad, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("trunc_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "trunc_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "trunc_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "trunc_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("trunc_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("trunc_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "trunc_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trunc_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trunc_grad", kernel_data_type);
  }
  VLOG(6) << "trunc_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("trunc_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trunc_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trunc_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void unfold_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int>& kernel_sizes, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("unfold_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "unfold_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "unfold_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "unfold_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["kernel_sizes"] = kernel_sizes;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["dilations"] = dilations;
         phi::RecordOpInfoSupplement("unfold_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("unfold_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_sizes, strides, paddings, dilations, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "unfold_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unfold_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unfold_grad", kernel_data_type);
  }
  VLOG(6) << "unfold_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_sizes"] = kernel_sizes;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["dilations"] = dilations;
     phi::RecordOpInfoSupplement("unfold_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unfold_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unfold_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_sizes, strides, paddings, dilations, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void uniform_inplace_grad(const Tensor& out_grad, float min, float max, int seed, int diag_num, int diag_step, float diag_val, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("uniform_inplace_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UniformRandomInplaceGradInferMeta(MakeMetaTensor(*out_grad.impl()), min, max, seed, diag_num, diag_step, diag_val, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "uniform_inplace_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "uniform_inplace_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "uniform_inplace_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["min"] = min;
         attrs["max"] = max;
         attrs["seed"] = seed;
         attrs["diag_num"] = diag_num;
         attrs["diag_step"] = diag_step;
         attrs["diag_val"] = diag_val;
         phi::RecordOpInfoSupplement("uniform_inplace_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UniformRandomInplaceGradInferMeta(MakeMetaTensor(*input_out_grad), min, max, seed, diag_num, diag_step, diag_val, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("uniform_inplace_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, int, int, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, min, max, seed, diag_num, diag_step, diag_val, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "uniform_inplace_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "uniform_inplace_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("uniform_inplace_grad", kernel_data_type);
  }
  VLOG(6) << "uniform_inplace_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["min"] = min;
     attrs["max"] = max;
     attrs["seed"] = seed;
     attrs["diag_num"] = diag_num;
     attrs["diag_step"] = diag_step;
     attrs["diag_val"] = diag_val;
     phi::RecordOpInfoSupplement("uniform_inplace_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("uniform_inplace_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UniformRandomInplaceGradInferMeta(MakeMetaTensor(*input_out_grad), min, max, seed, diag_num, diag_step, diag_val, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, float, float, int, int, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("uniform_inplace_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, min, max, seed, diag_num, diag_step, diag_val, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void unsqueeze_grad(const Tensor& xshape, const Tensor& out_grad, const IntArray& axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(xshape, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(xshape, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_xshape = MakeDistMetaTensor(*xshape.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::UnsqueezeGradInferSpmd(meta_dist_input_xshape, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("unsqueeze_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::KernelWithXShapeInferMeta(MakeMetaTensor(*xshape.impl()), MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `unsqueeze_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "unsqueeze_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "unsqueeze_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "unsqueeze_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_xshape = ReshardApiInputToKernelInput(dev_ctx, xshape, spmd_info.first[0], "xshape");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_xshape = PrepareDataForDistTensor(dist_input_xshape, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_xshape = &dist_input_xshape->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"xshape", {
         (*input_xshape).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         phi::RecordOpInfoSupplement("unsqueeze_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("unsqueeze_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_xshape, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "unsqueeze_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unsqueeze_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unsqueeze_grad", kernel_data_type);
  }
  VLOG(6) << "unsqueeze_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_xshape = PrepareData(xshape, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"xshape", {
     (*input_xshape).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     phi::RecordOpInfoSupplement("unsqueeze_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unsqueeze_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unsqueeze_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_xshape, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void unstack_grad(const std::vector<Tensor>& out_grad, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad[0].impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_out_grad;
    for(auto& e : out_grad) {
        meta_dist_input_out_grad.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("unstack_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    std::vector<phi::MetaTensor> out_grad_meta_vec;
    for (auto tmp : out_grad) {
      out_grad_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> out_grad_meta_ptr_vec(out_grad_meta_vec.size());
    for (size_t i=0; i < out_grad_meta_ptr_vec.size(); ++i) {
      out_grad_meta_ptr_vec[i] = &out_grad_meta_vec[i];
    }

    phi::UnStackGradInferMeta(out_grad_meta_ptr_vec, axis, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "unstack_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "unstack_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "unstack_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      auto dist_input_out_grad_vec = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_out_grad_vec;
      for (auto tmp : dist_input_out_grad_vec) {
        dense_input_out_grad_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_out_grad_meta_vec = MakeMetaTensor(dense_input_out_grad_vec);
      std::vector<const phi::MetaTensor*> dense_input_out_grad_meta_ptr_vec(dense_input_out_grad_meta_vec.size());
      for (size_t i = 0; i < dense_input_out_grad_meta_ptr_vec.size(); ++i) {
        dense_input_out_grad_meta_ptr_vec[i] = &dense_input_out_grad_meta_vec[i];
      }

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_out_grad_vec.size());
         for (size_t i = 0; i < dense_input_out_grad_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_out_grad_vec[i]).dims());
         }
         input_shapes.emplace_back("out_grad", ddims_vec);
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("unstack_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnStackGradInferMeta(dense_input_out_grad_meta_ptr_vec, axis, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("unstack_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, dense_input_out_grad_vec, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "unstack_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unstack_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unstack_grad", kernel_data_type);
  }
  VLOG(6) << "unstack_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad_vec = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_out_grad(input_out_grad_vec->size());
  for (size_t i = 0; i < input_out_grad.size(); ++i) {
    input_out_grad[i] = &input_out_grad_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes;
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_out_grad.size());
     for (size_t i = 0; i < input_out_grad.size(); ++i) {
       ddims_vec.emplace_back((*input_out_grad[i]).dims());
     }
     input_shapes.emplace_back("out_grad", ddims_vec);
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("unstack_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unstack_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto out_grad_meta_vec = MakeMetaTensor(input_out_grad);
  std::vector<const phi::MetaTensor*> out_grad_metas(out_grad_meta_vec.size());
  for (size_t i = 0; i < out_grad_meta_vec.size(); ++i) {
    out_grad_metas[i] = &out_grad_meta_vec[i];
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnStackGradInferMeta(out_grad_metas, axis, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unstack_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void view_dtype_grad(const Tensor& input, const Tensor& out_grad, DataType dtype, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("view_dtype_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::StridedUnChangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "view_dtype_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "view_dtype_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "view_dtype_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("view_dtype_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("view_dtype_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, dtype, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "view_dtype_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "view_dtype_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("view_dtype_grad", kernel_data_type);
  }
  VLOG(6) << "view_dtype_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("view_dtype_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("view_dtype_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, DataType, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("view_dtype_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, dtype, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void view_shape_grad(const Tensor& input, const Tensor& out_grad, const std::vector<int64_t>& dims, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("view_shape_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::StridedUnChangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "view_shape_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "view_shape_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "view_shape_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dims"] = dims;
         phi::RecordOpInfoSupplement("view_shape_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("view_shape_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, dims, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "view_shape_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "view_shape_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("view_shape_grad", kernel_data_type);
  }
  VLOG(6) << "view_shape_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dims"] = dims;
     phi::RecordOpInfoSupplement("view_shape_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("view_shape_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::StridedUnChangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("view_shape_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, dims, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void warpctc_grad(const Tensor& logits, const paddle::optional<Tensor>& logits_length, const Tensor& warpctcgrad, const Tensor& loss_grad, int blank, bool norm_by_times, Tensor* logits_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(logits, logits_length, warpctcgrad, loss_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(loss_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(loss_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(logits, logits_length, warpctcgrad, loss_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_logits = MakeDistMetaTensor(*logits.impl());
    auto meta_dist_input_logits_length = logits_length ? MakeDistMetaTensor(*(*logits_length).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_warpctcgrad = MakeDistMetaTensor(*warpctcgrad.impl());
    auto meta_dist_input_loss_grad = MakeDistMetaTensor(*loss_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_logits, meta_dist_input_logits_length, meta_dist_input_warpctcgrad, meta_dist_input_loss_grad);
    DebugInfoForInferSpmd("warpctc_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(logits_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*logits.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "warpctc_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "warpctc_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "warpctc_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_logits = ReshardApiInputToKernelInput(dev_ctx, logits, spmd_info.first[0], "logits");
      auto dist_input_logits_length = ReshardApiInputToKernelInput(dev_ctx, logits_length, spmd_info.first[1], "logits_length");
      auto dist_input_warpctcgrad = ReshardApiInputToKernelInput(dev_ctx, warpctcgrad, spmd_info.first[2], "warpctcgrad");
      auto dist_input_loss_grad = ReshardApiInputToKernelInput(dev_ctx, loss_grad, spmd_info.first[3], "loss_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_logits = PrepareDataForDistTensor(dist_input_logits, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_logits = &dist_input_logits->value();

      dist_input_logits_length = PrepareDataForDistTensor(dist_input_logits_length, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_logits_length = dist_input_logits_length ? paddle::make_optional<phi::DenseTensor>((*dist_input_logits_length)->value()) : paddle::none;

      dist_input_warpctcgrad = PrepareDataForDistTensor(dist_input_warpctcgrad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_warpctcgrad = &dist_input_warpctcgrad->value();

      dist_input_loss_grad = PrepareDataForDistTensor(dist_input_loss_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_loss_grad = &dist_input_loss_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> logits_length_record_shapes;
         if(input_logits_length){
           logits_length_record_shapes.push_back((*input_logits_length).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"logits", {
         (*input_logits).dims()}},
         {"logits_length", logits_length_record_shapes},
         {"warpctcgrad", {
         (*input_warpctcgrad).dims()}},
         {"loss_grad", {
         (*input_loss_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["blank"] = blank;
         attrs["norm_by_times"] = norm_by_times;
         phi::RecordOpInfoSupplement("warpctc_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_logits), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("warpctc_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_logits, input_logits_length, *input_warpctcgrad, *input_loss_grad, blank, norm_by_times, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, logits_grad, "logits_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "warpctc_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "warpctc_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("warpctc_grad", kernel_data_type);
  }
  VLOG(6) << "warpctc_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_logits = PrepareData(logits, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_logits_length = PrepareData(logits_length, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_warpctcgrad = PrepareData(warpctcgrad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_loss_grad = PrepareData(loss_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> logits_length_record_shapes;
     if(input_logits_length){
       logits_length_record_shapes.push_back((*input_logits_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"logits", {
     (*input_logits).dims()}},
     {"logits_length", logits_length_record_shapes},
     {"warpctcgrad", {
     (*input_warpctcgrad).dims()}},
     {"loss_grad", {
     (*input_loss_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["blank"] = blank;
     attrs["norm_by_times"] = norm_by_times;
     phi::RecordOpInfoSupplement("warpctc_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(logits_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("warpctc_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_logits), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("warpctc_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_logits, input_logits_length, *input_warpctcgrad, *input_loss_grad, blank, norm_by_times, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void warprnnt_grad(const Tensor& input, const Tensor& input_lengths, const Tensor& warprnntgrad, const Tensor& loss_grad, int blank, float fastemit_lambda, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, input_lengths, warprnntgrad, loss_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(loss_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, input_lengths, warprnntgrad, loss_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_input_lengths = MakeDistMetaTensor(*input_lengths.impl());
    auto meta_dist_input_warprnntgrad = MakeDistMetaTensor(*warprnntgrad.impl());
    auto meta_dist_input_loss_grad = MakeDistMetaTensor(*loss_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_input, meta_dist_input_input_lengths, meta_dist_input_warprnntgrad, meta_dist_input_loss_grad);
    DebugInfoForInferSpmd("warprnnt_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "warprnnt_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "warprnnt_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "warprnnt_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_input_lengths = ReshardApiInputToKernelInput(dev_ctx, input_lengths, spmd_info.first[1], "input_lengths");
      auto dist_input_warprnntgrad = ReshardApiInputToKernelInput(dev_ctx, warprnntgrad, spmd_info.first[2], "warprnntgrad");
      auto dist_input_loss_grad = ReshardApiInputToKernelInput(dev_ctx, loss_grad, spmd_info.first[3], "loss_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_input_lengths = PrepareDataForDistTensor(dist_input_input_lengths, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input_lengths = &dist_input_input_lengths->value();

      dist_input_warprnntgrad = PrepareDataForDistTensor(dist_input_warprnntgrad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_warprnntgrad = &dist_input_warprnntgrad->value();

      dist_input_loss_grad = PrepareDataForDistTensor(dist_input_loss_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_loss_grad = &dist_input_loss_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"input_lengths", {
         (*input_input_lengths).dims()}},
         {"warprnntgrad", {
         (*input_warprnntgrad).dims()}},
         {"loss_grad", {
         (*input_loss_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["blank"] = blank;
         attrs["fastemit_lambda"] = fastemit_lambda;
         phi::RecordOpInfoSupplement("warprnnt_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("warprnnt_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_input_lengths, *input_warprnntgrad, *input_loss_grad, blank, fastemit_lambda, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "warprnnt_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "warprnnt_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("warprnnt_grad", kernel_data_type);
  }
  VLOG(6) << "warprnnt_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_input_lengths = PrepareData(input_lengths, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_warprnntgrad = PrepareData(warprnntgrad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_loss_grad = PrepareData(loss_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"input_lengths", {
     (*input_input_lengths).dims()}},
     {"warprnntgrad", {
     (*input_warprnntgrad).dims()}},
     {"loss_grad", {
     (*input_loss_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["blank"] = blank;
     attrs["fastemit_lambda"] = fastemit_lambda;
     phi::RecordOpInfoSupplement("warprnnt_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("warprnnt_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("warprnnt_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_input_lengths, *input_warprnntgrad, *input_loss_grad, blank, fastemit_lambda, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void weight_only_linear_grad(const Tensor& x, const Tensor& weight, const paddle::optional<Tensor>& bias, const Tensor& weight_scale, const Tensor& out_grad, const std::string& weight_dtype, int arch, int group_size, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, weight, bias, weight_scale, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, weight, bias, weight_scale, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_weight = MakeDistMetaTensor(*weight.impl());
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_weight_scale = MakeDistMetaTensor(*weight_scale.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_weight, meta_dist_input_bias, meta_dist_input_weight_scale, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("weight_only_linear_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::MetaTensor meta_dist_bias = bias ? MakeMetaTensor(*(*bias).impl()) : phi::MetaTensor();

    phi::WeightOnlyLinearGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*weight.impl()), meta_dist_bias, MakeMetaTensor(*weight_scale.impl()), MakeMetaTensor(*out_grad.impl()), weight_dtype, arch, group_size, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "weight_only_linear_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "weight_only_linear_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "weight_only_linear_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_weight = ReshardApiInputToKernelInput(dev_ctx, weight, spmd_info.first[1], "weight");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_weight_scale = ReshardApiInputToKernelInput(dev_ctx, weight_scale, spmd_info.first[3], "weight_scale");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_weight = PrepareDataForDistTensor(dist_input_weight, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_weight = &dist_input_weight->value();

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_weight_scale = PrepareDataForDistTensor(dist_input_weight_scale, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_weight_scale = &dist_input_weight_scale->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"weight", {
         (*input_weight).dims()}},
         {"bias", bias_record_shapes},
         {"weight_scale", {
         (*input_weight_scale).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["weight_dtype"] = weight_dtype;
         attrs["arch"] = arch;
         attrs["group_size"] = group_size;
         phi::RecordOpInfoSupplement("weight_only_linear_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::WeightOnlyLinearGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), MakeMetaTensor(*input_weight_scale), MakeMetaTensor(*input_out_grad), weight_dtype, arch, group_size, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("weight_only_linear_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_weight, input_bias, *input_weight_scale, *input_out_grad, weight_dtype, arch, group_size, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "weight_only_linear_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "weight_only_linear_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("weight_only_linear_grad", kernel_data_type);
  }
  VLOG(6) << "weight_only_linear_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_weight_scale = PrepareData(weight_scale, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"weight", {
     (*input_weight).dims()}},
     {"bias", bias_record_shapes},
     {"weight_scale", {
     (*input_weight_scale).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["weight_dtype"] = weight_dtype;
     attrs["arch"] = arch;
     attrs["group_size"] = group_size;
     phi::RecordOpInfoSupplement("weight_only_linear_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("weight_only_linear_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::WeightOnlyLinearGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), MakeMetaTensor(input_bias), MakeMetaTensor(*input_weight_scale), MakeMetaTensor(*input_out_grad), weight_dtype, arch, group_size, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const std::string&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("weight_only_linear_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_weight, input_bias, *input_weight_scale, *input_out_grad, weight_dtype, arch, group_size, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void where_grad(const Tensor& condition, const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(condition, x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(condition, x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_condition = MakeDistMetaTensor(*condition.impl());
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::WhereGradInferSpmd(meta_dist_input_condition, meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("where_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `where_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "where_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "where_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "where_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_condition = ReshardApiInputToKernelInput(dev_ctx, condition, spmd_info.first[0], "condition");
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[1], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[2], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_condition = PrepareDataForDistTensor(dist_input_condition, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_condition = &dist_input_condition->value();

      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"condition", {
         (*input_condition).dims()}},
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("where_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("where_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_condition, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "where_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "where_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("where_grad", kernel_data_type);
  }
  VLOG(6) << "where_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_condition = PrepareData(condition, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"condition", {
     (*input_condition).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("where_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("where_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("where_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_condition, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void yolo_loss_grad(const Tensor& x, const Tensor& gt_box, const Tensor& gt_label, const paddle::optional<Tensor>& gt_score, const Tensor& objectness_mask, const Tensor& gt_match_mask, const Tensor& loss_grad, const std::vector<int>& anchors, const std::vector<int>& anchor_mask, int class_num, float ignore_thresh, int downsample_ratio, bool use_label_smooth, float scale_x_y, Tensor* x_grad, Tensor* gt_box_grad, Tensor* gt_label_grad, Tensor* gt_score_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, gt_box, gt_label, gt_score, objectness_mask, gt_match_mask, loss_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(loss_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, gt_box, gt_label, gt_score, objectness_mask, gt_match_mask, loss_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_gt_box = MakeDistMetaTensor(*gt_box.impl());
    auto meta_dist_input_gt_label = MakeDistMetaTensor(*gt_label.impl());
    auto meta_dist_input_gt_score = gt_score ? MakeDistMetaTensor(*(*gt_score).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_objectness_mask = MakeDistMetaTensor(*objectness_mask.impl());
    auto meta_dist_input_gt_match_mask = MakeDistMetaTensor(*gt_match_mask.impl());
    auto meta_dist_input_loss_grad = MakeDistMetaTensor(*loss_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_gt_box, meta_dist_input_gt_label, meta_dist_input_gt_score, meta_dist_input_objectness_mask, meta_dist_input_gt_match_mask, meta_dist_input_loss_grad);
    DebugInfoForInferSpmd("yolo_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(gt_box_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(gt_label_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_3 =
        CreateKernelDistOutput(gt_score_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_3 = shared_dist_out_3.get();
    phi::DenseTensor* dense_out_3 = dist_out_3 ? dist_out_3->unsafe_mutable_value() : nullptr;
    if (dense_out_3 && !rank_is_in_current_mesh && !dist_out_3->defined()) {
      *dense_out_3 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_out_3(dist_out_3);
    phi::MetaTensor meta_dist_gt_score = gt_score ? MakeMetaTensor(*(*gt_score).impl()) : phi::MetaTensor();

    phi::YoloLossGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*gt_box.impl()), MakeMetaTensor(*gt_label.impl()), meta_dist_gt_score, MakeMetaTensor(*objectness_mask.impl()), MakeMetaTensor(*gt_match_mask.impl()), MakeMetaTensor(*loss_grad.impl()), anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr, dist_out_3 ? &meta_dist_out_3 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_3, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "yolo_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "yolo_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "yolo_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_gt_box = ReshardApiInputToKernelInput(dev_ctx, gt_box, spmd_info.first[1], "gt_box");
      auto dist_input_gt_label = ReshardApiInputToKernelInput(dev_ctx, gt_label, spmd_info.first[2], "gt_label");
      auto dist_input_gt_score = ReshardApiInputToKernelInput(dev_ctx, gt_score, spmd_info.first[3], "gt_score");
      auto dist_input_objectness_mask = ReshardApiInputToKernelInput(dev_ctx, objectness_mask, spmd_info.first[4], "objectness_mask");
      auto dist_input_gt_match_mask = ReshardApiInputToKernelInput(dev_ctx, gt_match_mask, spmd_info.first[5], "gt_match_mask");
      auto dist_input_loss_grad = ReshardApiInputToKernelInput(dev_ctx, loss_grad, spmd_info.first[6], "loss_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_gt_box = PrepareDataForDistTensor(dist_input_gt_box, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_gt_box = &dist_input_gt_box->value();

      dist_input_gt_label = PrepareDataForDistTensor(dist_input_gt_label, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_gt_label = &dist_input_gt_label->value();

      dist_input_gt_score = PrepareDataForDistTensor(dist_input_gt_score, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_gt_score = dist_input_gt_score ? paddle::make_optional<phi::DenseTensor>((*dist_input_gt_score)->value()) : paddle::none;

      dist_input_objectness_mask = PrepareDataForDistTensor(dist_input_objectness_mask, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_objectness_mask = &dist_input_objectness_mask->value();

      dist_input_gt_match_mask = PrepareDataForDistTensor(dist_input_gt_match_mask, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_gt_match_mask = &dist_input_gt_match_mask->value();

      dist_input_loss_grad = PrepareDataForDistTensor(dist_input_loss_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_loss_grad = &dist_input_loss_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> gt_score_record_shapes;
         if(input_gt_score){
           gt_score_record_shapes.push_back((*input_gt_score).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"gt_box", {
         (*input_gt_box).dims()}},
         {"gt_label", {
         (*input_gt_label).dims()}},
         {"gt_score", gt_score_record_shapes},
         {"objectness_mask", {
         (*input_objectness_mask).dims()}},
         {"gt_match_mask", {
         (*input_gt_match_mask).dims()}},
         {"loss_grad", {
         (*input_loss_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["anchors"] = anchors;
         attrs["anchor_mask"] = anchor_mask;
         attrs["class_num"] = class_num;
         attrs["ignore_thresh"] = ignore_thresh;
         attrs["downsample_ratio"] = downsample_ratio;
         attrs["use_label_smooth"] = use_label_smooth;
         attrs["scale_x_y"] = scale_x_y;
         phi::RecordOpInfoSupplement("yolo_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::MetaTensor meta_dense_out_3(dense_out_3);
      phi::YoloLossGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_gt_box), MakeMetaTensor(*input_gt_label), MakeMetaTensor(input_gt_score), MakeMetaTensor(*input_objectness_mask), MakeMetaTensor(*input_gt_match_mask), MakeMetaTensor(*input_loss_grad), anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr, dense_out_3 ? &meta_dense_out_3 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("yolo_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, int, float, int, bool, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_gt_box, *input_gt_label, input_gt_score, *input_objectness_mask, *input_gt_match_mask, *input_loss_grad, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, dense_out_0, dense_out_1, dense_out_2, dense_out_3);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
        TransDataBackend(dense_out_3, kernel_backend, dense_out_3);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, gt_box_grad, "gt_box_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, gt_label_grad, "gt_label_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_3, gt_score_grad, "gt_score_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "yolo_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "yolo_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("yolo_loss_grad", kernel_data_type);
  }
  VLOG(6) << "yolo_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_box = PrepareData(gt_box, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_label = PrepareData(gt_label, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_score = PrepareData(gt_score, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_objectness_mask = PrepareData(objectness_mask, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_gt_match_mask = PrepareData(gt_match_mask, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_loss_grad = PrepareData(loss_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> gt_score_record_shapes;
     if(input_gt_score){
       gt_score_record_shapes.push_back((*input_gt_score).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"gt_box", {
     (*input_gt_box).dims()}},
     {"gt_label", {
     (*input_gt_label).dims()}},
     {"gt_score", gt_score_record_shapes},
     {"objectness_mask", {
     (*input_objectness_mask).dims()}},
     {"gt_match_mask", {
     (*input_gt_match_mask).dims()}},
     {"loss_grad", {
     (*input_loss_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["anchors"] = anchors;
     attrs["anchor_mask"] = anchor_mask;
     attrs["class_num"] = class_num;
     attrs["ignore_thresh"] = ignore_thresh;
     attrs["downsample_ratio"] = downsample_ratio;
     attrs["use_label_smooth"] = use_label_smooth;
     attrs["scale_x_y"] = scale_x_y;
     phi::RecordOpInfoSupplement("yolo_loss_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(gt_box_grad);
  auto kernel_out_2 = SetKernelOutput(gt_label_grad);
  auto kernel_out_3 = SetKernelOutput(gt_score_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("yolo_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::YoloLossGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_gt_box), MakeMetaTensor(*input_gt_label), MakeMetaTensor(input_gt_score), MakeMetaTensor(*input_objectness_mask), MakeMetaTensor(*input_gt_match_mask), MakeMetaTensor(*input_loss_grad), anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, int, float, int, bool, float, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("yolo_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_gt_box, *input_gt_label, input_gt_score, *input_objectness_mask, *input_gt_match_mask, *input_loss_grad, anchors, anchor_mask, class_num, ignore_thresh, downsample_ratio, use_label_smooth, scale_x_y, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  
}

PADDLE_API void unpool3d_grad(const Tensor& x, const Tensor& indices, const Tensor& out, const Tensor& out_grad, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_size, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("unpool3d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "unpool3d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "unpool3d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "unpool3d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["ksize"] = ksize;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["output_size"] = output_size;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("unpool3d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("unpool3d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out, *input_out_grad, ksize, strides, paddings, output_size, data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "unpool3d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unpool3d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unpool3d_grad", kernel_data_type);
  }
  VLOG(6) << "unpool3d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["ksize"] = ksize;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_size"] = output_size;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("unpool3d_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unpool3d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unpool3d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out, *input_out_grad, ksize, strides, paddings, output_size, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void add_double_grad(const Tensor& y, const Tensor& grad_out, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_y_grad, int axis, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(y, grad_out, grad_x_grad, grad_y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(y, grad_out, grad_x_grad, grad_y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "add_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("add_double_grad", kernel_data_type);
  }
  VLOG(6) << "add_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y_grad = PrepareData(grad_y_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_y_grad_record_shapes;
     if(input_grad_y_grad){
       grad_y_grad_record_shapes.push_back((*input_grad_y_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"y", {
     (*input_y).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_y_grad",
     grad_y_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("add_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("add_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_grad_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("add_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_y, *input_grad_out, input_grad_x_grad, input_grad_y_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void add_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, int axis, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseBinaryGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("add_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `add_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "add_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "add_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "add_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("add_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("add_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "add_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("add_grad", kernel_data_type);
  }
  VLOG(6) << "add_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("add_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("add_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("add_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void add_triple_grad(const Tensor& grad_grad_x, const Tensor& grad_grad_y, const Tensor& grad_grad_out_grad, int axis, Tensor* grad_grad_x_grad, Tensor* grad_grad_y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(grad_grad_x, grad_grad_y, grad_grad_out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_grad_out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(grad_grad_x, grad_grad_y, grad_grad_out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "add_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "add_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("add_triple_grad", kernel_data_type);
  }
  VLOG(6) << "add_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_grad_grad_x = PrepareData(grad_grad_x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_y = PrepareData(grad_grad_y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_out_grad = PrepareData(grad_grad_out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"grad_grad_x", {
     (*input_grad_grad_x).dims()}},
     {"grad_grad_y", {
     (*input_grad_grad_y).dims()}},
     {"grad_grad_out_grad", {
     (*input_grad_grad_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("add_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(grad_grad_x_grad);
  auto kernel_out_1 = SetKernelOutput(grad_grad_y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("add_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_grad_grad_x), MakeMetaTensor(*input_grad_grad_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("add_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_grad_grad_x, *input_grad_grad_y, *input_grad_grad_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void amax_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("amax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "amax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "amax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "amax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("amax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("amax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "amax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "amax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("amax_grad", kernel_data_type);
  }
  VLOG(6) << "amax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("amax_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("amax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("amax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void amin_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("amin_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "amin_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "amin_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "amin_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("amin_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("amin_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "amin_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "amin_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("amin_grad", kernel_data_type);
  }
  VLOG(6) << "amin_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("amin_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("amin_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("amin_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void assign_out__grad(const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("assign_out__grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "assign_out__grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "assign", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "assign kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("assign_out__grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("assign_out__grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "assign_out__grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "assign", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("assign_out__grad", kernel_data_type);
  }
  VLOG(6) << "assign kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("assign_out__grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("assign_out__grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("assign_out__grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void batch_norm_double_grad(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& out_mean, const paddle::optional<Tensor>& out_variance, const Tensor& saved_mean, const Tensor& saved_variance, const Tensor& grad_out, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_scale_grad, const paddle::optional<Tensor>& grad_bias_grad, float momentum, float epsilon, const std::string& data_format, bool is_test, bool use_global_stats, bool trainable_statistics, Tensor* x_grad, Tensor* scale_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, out_mean, out_variance, saved_mean, saved_variance, grad_out, grad_x_grad, grad_scale_grad, grad_bias_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, out_mean, out_variance, saved_mean, saved_variance, grad_out, grad_x_grad, grad_scale_grad, grad_bias_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "batch_norm_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "batch_norm_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("batch_norm_double_grad", kernel_data_type);
  }
  VLOG(6) << "batch_norm_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_mean = PrepareData(out_mean, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_variance = PrepareData(out_variance, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_scale_grad = PrepareData(grad_scale_grad, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_bias_grad = PrepareData(grad_bias_grad, GetKernelInputArgDef(kernel.InputAt(9), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> out_mean_record_shapes;
     if(input_out_mean){
       out_mean_record_shapes.push_back((*input_out_mean).dims());
     }
     std::vector<phi::DDim> out_variance_record_shapes;
     if(input_out_variance){
       out_variance_record_shapes.push_back((*input_out_variance).dims());
     }
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_scale_grad_record_shapes;
     if(input_grad_scale_grad){
       grad_scale_grad_record_shapes.push_back((*input_grad_scale_grad).dims());
     }
     std::vector<phi::DDim> grad_bias_grad_record_shapes;
     if(input_grad_bias_grad){
       grad_bias_grad_record_shapes.push_back((*input_grad_bias_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"out_mean", out_mean_record_shapes},
     {"out_variance", out_variance_record_shapes},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_scale_grad", grad_scale_grad_record_shapes},
     {"grad_bias_grad",
     grad_bias_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["data_format"] = data_format;
     attrs["is_test"] = is_test;
     attrs["use_global_stats"] = use_global_stats;
     attrs["trainable_statistics"] = trainable_statistics;
     phi::RecordOpInfoSupplement("batch_norm_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("batch_norm_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(*input_x), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, float, float, const std::string&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("batch_norm_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_out_mean, input_out_variance, *input_saved_mean, *input_saved_variance, *input_grad_out, input_grad_x_grad, input_grad_scale_grad, input_grad_bias_grad, momentum, epsilon, data_format, is_test, use_global_stats, trainable_statistics, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void batch_norm_grad(const Tensor& x, const paddle::optional<Tensor>& scale, const paddle::optional<Tensor>& bias, const paddle::optional<Tensor>& mean_out, const paddle::optional<Tensor>& variance_out, const Tensor& saved_mean, const Tensor& saved_variance, const paddle::optional<Tensor>& reserve_space, const Tensor& out_grad, float momentum, float epsilon, const std::string& data_format, bool is_test, bool use_global_stats, bool trainable_statistics, Tensor* x_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, bias, mean_out, variance_out, saved_mean, saved_variance, reserve_space, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, mean_out, variance_out, saved_mean, saved_variance, reserve_space, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = scale ? MakeDistMetaTensor(*(*scale).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_mean_out = mean_out ? MakeDistMetaTensor(*(*mean_out).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_variance_out = variance_out ? MakeDistMetaTensor(*(*variance_out).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_saved_mean = MakeDistMetaTensor(*saved_mean.impl());
    auto meta_dist_input_saved_variance = MakeDistMetaTensor(*saved_variance.impl());
    auto meta_dist_input_reserve_space = reserve_space ? MakeDistMetaTensor(*(*reserve_space).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_bias, meta_dist_input_mean_out, meta_dist_input_variance_out, meta_dist_input_saved_mean, meta_dist_input_saved_variance, meta_dist_input_reserve_space, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("batch_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_scale = scale ? MakeMetaTensor(*(*scale).impl()) : phi::MetaTensor();

    phi::MetaTensor meta_dist_bias = bias ? MakeMetaTensor(*(*bias).impl()) : phi::MetaTensor();

    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*x.impl()), meta_dist_scale, meta_dist_bias, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "batch_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "batch_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "batch_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_mean_out = ReshardApiInputToKernelInput(dev_ctx, mean_out, spmd_info.first[3], "mean_out");
      auto dist_input_variance_out = ReshardApiInputToKernelInput(dev_ctx, variance_out, spmd_info.first[4], "variance_out");
      auto dist_input_saved_mean = ReshardApiInputToKernelInput(dev_ctx, saved_mean, spmd_info.first[5], "saved_mean");
      auto dist_input_saved_variance = ReshardApiInputToKernelInput(dev_ctx, saved_variance, spmd_info.first[6], "saved_variance");
      auto dist_input_reserve_space = ReshardApiInputToKernelInput(dev_ctx, reserve_space, spmd_info.first[7], "reserve_space");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[8], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_scale = dist_input_scale ? paddle::make_optional<phi::DenseTensor>((*dist_input_scale)->value()) : paddle::none;

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_mean_out = PrepareDataForDistTensor(dist_input_mean_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_mean_out = dist_input_mean_out ? paddle::make_optional<phi::DenseTensor>((*dist_input_mean_out)->value()) : paddle::none;

      dist_input_variance_out = PrepareDataForDistTensor(dist_input_variance_out, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_variance_out = dist_input_variance_out ? paddle::make_optional<phi::DenseTensor>((*dist_input_variance_out)->value()) : paddle::none;

      dist_input_saved_mean = PrepareDataForDistTensor(dist_input_saved_mean, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_mean = &dist_input_saved_mean->value();

      dist_input_saved_variance = PrepareDataForDistTensor(dist_input_saved_variance, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_variance = &dist_input_saved_variance->value();

      dist_input_reserve_space = PrepareDataForDistTensor(dist_input_reserve_space, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_reserve_space = dist_input_reserve_space ? paddle::make_optional<phi::DenseTensor>((*dist_input_reserve_space)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(8), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> scale_record_shapes;
         if(input_scale){
           scale_record_shapes.push_back((*input_scale).dims());
         }
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<phi::DDim> mean_out_record_shapes;
         if(input_mean_out){
           mean_out_record_shapes.push_back((*input_mean_out).dims());
         }
         std::vector<phi::DDim> variance_out_record_shapes;
         if(input_variance_out){
           variance_out_record_shapes.push_back((*input_variance_out).dims());
         }
         std::vector<phi::DDim> reserve_space_record_shapes;
         if(input_reserve_space){
           reserve_space_record_shapes.push_back((*input_reserve_space).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", scale_record_shapes},
         {"bias", bias_record_shapes},
         {"mean_out", mean_out_record_shapes},
         {"variance_out", variance_out_record_shapes},
         {"saved_mean", {
         (*input_saved_mean).dims()}},
         {"saved_variance", {
         (*input_saved_variance).dims()}},
         {"reserve_space", reserve_space_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["momentum"] = momentum;
         attrs["epsilon"] = epsilon;
         attrs["data_format"] = data_format;
         attrs["is_test"] = is_test;
         attrs["use_global_stats"] = use_global_stats;
         attrs["trainable_statistics"] = trainable_statistics;
         phi::RecordOpInfoSupplement("batch_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("batch_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, input_mean_out, input_variance_out, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, data_format, is_test, use_global_stats, trainable_statistics, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "batch_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "batch_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("batch_norm_grad", kernel_data_type);
  }
  VLOG(6) << "batch_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mean_out = PrepareData(mean_out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_variance_out = PrepareData(variance_out, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_reserve_space = PrepareData(reserve_space, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> scale_record_shapes;
     if(input_scale){
       scale_record_shapes.push_back((*input_scale).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<phi::DDim> mean_out_record_shapes;
     if(input_mean_out){
       mean_out_record_shapes.push_back((*input_mean_out).dims());
     }
     std::vector<phi::DDim> variance_out_record_shapes;
     if(input_variance_out){
       variance_out_record_shapes.push_back((*input_variance_out).dims());
     }
     std::vector<phi::DDim> reserve_space_record_shapes;
     if(input_reserve_space){
       reserve_space_record_shapes.push_back((*input_reserve_space).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", scale_record_shapes},
     {"bias", bias_record_shapes},
     {"mean_out", mean_out_record_shapes},
     {"variance_out", variance_out_record_shapes},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"reserve_space", reserve_space_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["data_format"] = data_format;
     attrs["is_test"] = is_test;
     attrs["use_global_stats"] = use_global_stats;
     attrs["trainable_statistics"] = trainable_statistics;
     phi::RecordOpInfoSupplement("batch_norm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("batch_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(input_scale), MakeMetaTensor(input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("batch_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_scale, input_bias, input_mean_out, input_variance_out, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, data_format, is_test, use_global_stats, trainable_statistics, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void c_embedding_grad(const Tensor& weight, const Tensor& x, const Tensor& out_grad, int64_t start_index, Tensor* weight_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(weight, x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(weight, x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_weight = MakeDistMetaTensor(*weight.impl());
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_weight, meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("c_embedding_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(weight_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::EmbeddingGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*weight.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "c_embedding_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "c_embedding_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "c_embedding_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_weight = ReshardApiInputToKernelInput(dev_ctx, weight, spmd_info.first[0], "weight");
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[1], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_weight = PrepareDataForDistTensor(dist_input_weight, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_weight = &dist_input_weight->value();

      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"weight", {
         (*input_weight).dims()}},
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["start_index"] = start_index;
         phi::RecordOpInfoSupplement("c_embedding_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::EmbeddingGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("c_embedding_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_weight, *input_x, *input_out_grad, start_index, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, weight_grad, "weight_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "c_embedding_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "c_embedding_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("c_embedding_grad", kernel_data_type);
  }
  VLOG(6) << "c_embedding_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_weight = PrepareData(weight, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"weight", {
     (*input_weight).dims()}},
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["start_index"] = start_index;
     phi::RecordOpInfoSupplement("c_embedding_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(weight_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("c_embedding_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::EmbeddingGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_weight), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int64_t, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("c_embedding_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_weight, *input_x, *input_out_grad, start_index, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void channel_shuffle_grad(const Tensor& out_grad, int groups, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("channel_shuffle_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::ChannelShuffleGradInferMeta(MakeMetaTensor(*out_grad.impl()), groups, data_format, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "channel_shuffle_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "channel_shuffle_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "channel_shuffle_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["groups"] = groups;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("channel_shuffle_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::ChannelShuffleGradInferMeta(MakeMetaTensor(*input_out_grad), groups, data_format, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("channel_shuffle_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, groups, data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "channel_shuffle_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "channel_shuffle_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("channel_shuffle_grad", kernel_data_type);
  }
  VLOG(6) << "channel_shuffle_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["groups"] = groups;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("channel_shuffle_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("channel_shuffle_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::ChannelShuffleGradInferMeta(MakeMetaTensor(*input_out_grad), groups, data_format, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("channel_shuffle_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, groups, data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void conv2d_transpose_double_grad(const Tensor& x, const Tensor& filter, const Tensor& grad_out, const Tensor& grad_x_grad, const Tensor& grad_filter_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const IntArray& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* x_grad, Tensor* filter_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, filter, grad_out, grad_x_grad, grad_filter_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_filter_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter, grad_out, grad_x_grad, grad_filter_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "conv2d_transpose_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_transpose_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d_transpose_double_grad", kernel_data_type);
  }
  VLOG(6) << "conv2d_transpose_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_filter_grad = PrepareData(grad_filter_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}},
     {"grad_filter_grad", {
     (*input_grad_filter_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size.GetData();
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d_transpose_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d_transpose_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::Conv2dTransposeDoubleGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_grad_out), MakeMetaTensor(*input_grad_x_grad), MakeMetaTensor(*input_grad_filter_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d_transpose_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_grad_out, *input_grad_x_grad, *input_grad_filter_grad, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void conv2d_transpose_grad(const Tensor& x, const Tensor& filter, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const IntArray& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* x_grad, Tensor* filter_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, filter, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_filter, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("conv2d_transpose_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::Conv2dTransposeGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*filter.impl()), MakeMetaTensor(*out_grad.impl()), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "conv2d_transpose_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "conv2d_transpose_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "conv2d_transpose_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["output_padding"] = output_padding;
         attrs["output_size"] = output_size.GetData();
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["groups"] = groups;
         attrs["dilations"] = dilations;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("conv2d_transpose_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::Conv2dTransposeGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_out_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("conv2d_transpose_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_out_grad, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "conv2d_transpose_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "conv2d_transpose_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("conv2d_transpose_grad", kernel_data_type);
  }
  VLOG(6) << "conv2d_transpose_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size.GetData();
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("conv2d_transpose_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("conv2d_transpose_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::Conv2dTransposeGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_out_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("conv2d_transpose_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_out_grad, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void deformable_conv_grad(const Tensor& x, const Tensor& offset, const Tensor& filter, const paddle::optional<Tensor>& mask, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& dilations, int deformable_groups, int groups, int im2col_step, Tensor* x_grad, Tensor* offset_grad, Tensor* filter_grad, Tensor* mask_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, offset, filter, mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, offset, filter, mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_offset = MakeDistMetaTensor(*offset.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_mask = mask ? MakeDistMetaTensor(*(*mask).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_offset, meta_dist_input_filter, meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("deformable_conv_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(offset_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_3 =
        CreateKernelDistOutput(mask_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_3 = shared_dist_out_3.get();
    phi::DenseTensor* dense_out_3 = dist_out_3 ? dist_out_3->unsafe_mutable_value() : nullptr;
    if (dense_out_3 && !rank_is_in_current_mesh && !dist_out_3->defined()) {
      *dense_out_3 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_out_3(dist_out_3);
    phi::MetaTensor meta_dist_mask = mask ? MakeMetaTensor(*(*mask).impl()) : phi::MetaTensor();

    phi::DeformableConvGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*offset.impl()), MakeMetaTensor(*filter.impl()), meta_dist_mask, MakeMetaTensor(*out_grad.impl()), strides, paddings, dilations, deformable_groups, groups, im2col_step, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr, dist_out_3 ? &meta_dist_out_3 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_3, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "deformable_conv_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "deformable_conv_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "deformable_conv_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_offset = ReshardApiInputToKernelInput(dev_ctx, offset, spmd_info.first[1], "offset");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[2], "filter");
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[3], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[4], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_offset = PrepareDataForDistTensor(dist_input_offset, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_offset = &dist_input_offset->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_mask = dist_input_mask ? paddle::make_optional<phi::DenseTensor>((*dist_input_mask)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> mask_record_shapes;
         if(input_mask){
           mask_record_shapes.push_back((*input_mask).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"offset", {
         (*input_offset).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"mask", mask_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["dilations"] = dilations;
         attrs["deformable_groups"] = deformable_groups;
         attrs["groups"] = groups;
         attrs["im2col_step"] = im2col_step;
         phi::RecordOpInfoSupplement("deformable_conv_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::MetaTensor meta_dense_out_3(dense_out_3);
      phi::DeformableConvGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_offset), MakeMetaTensor(*input_filter), MakeMetaTensor(input_mask), MakeMetaTensor(*input_out_grad), strides, paddings, dilations, deformable_groups, groups, im2col_step, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr, dense_out_3 ? &meta_dense_out_3 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("deformable_conv_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_offset, *input_filter, input_mask, *input_out_grad, strides, paddings, dilations, deformable_groups, groups, im2col_step, dense_out_0, dense_out_1, dense_out_2, dense_out_3);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
        TransDataBackend(dense_out_3, kernel_backend, dense_out_3);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, offset_grad, "offset_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, filter_grad, "filter_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_3, mask_grad, "mask_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "deformable_conv_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "deformable_conv_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("deformable_conv_grad", kernel_data_type);
  }
  VLOG(6) << "deformable_conv_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_offset = PrepareData(offset, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> mask_record_shapes;
     if(input_mask){
       mask_record_shapes.push_back((*input_mask).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"offset", {
     (*input_offset).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"mask", mask_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["dilations"] = dilations;
     attrs["deformable_groups"] = deformable_groups;
     attrs["groups"] = groups;
     attrs["im2col_step"] = im2col_step;
     phi::RecordOpInfoSupplement("deformable_conv_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(offset_grad);
  auto kernel_out_2 = SetKernelOutput(filter_grad);
  auto kernel_out_3 = SetKernelOutput(mask_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("deformable_conv_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::DeformableConvGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_offset), MakeMetaTensor(*input_filter), MakeMetaTensor(input_mask), MakeMetaTensor(*input_out_grad), strides, paddings, dilations, deformable_groups, groups, im2col_step, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, int, int, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("deformable_conv_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_offset, *input_filter, input_mask, *input_out_grad, strides, paddings, dilations, deformable_groups, groups, im2col_step, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  
}

PADDLE_API void depthwise_conv2d_transpose_grad(const Tensor& x, const Tensor& filter, const Tensor& out_grad, const std::vector<int>& strides, const std::vector<int>& paddings, const std::vector<int>& output_padding, const IntArray& output_size, const std::string& padding_algorithm, int groups, const std::vector<int>& dilations, const std::string& data_format, Tensor* x_grad, Tensor* filter_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, filter, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, filter, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_filter = MakeDistMetaTensor(*filter.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_filter, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("depthwise_conv2d_transpose_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(filter_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::Conv2dTransposeGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*filter.impl()), MakeMetaTensor(*out_grad.impl()), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "depthwise_conv2d_transpose_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "depthwise_conv2d_transpose_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "depthwise_conv2d_transpose_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_filter = ReshardApiInputToKernelInput(dev_ctx, filter, spmd_info.first[1], "filter");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_filter = PrepareDataForDistTensor(dist_input_filter, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_filter = &dist_input_filter->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"filter", {
         (*input_filter).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["output_padding"] = output_padding;
         attrs["output_size"] = output_size.GetData();
         attrs["padding_algorithm"] = padding_algorithm;
         attrs["groups"] = groups;
         attrs["dilations"] = dilations;
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("depthwise_conv2d_transpose_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::Conv2dTransposeGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_out_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("depthwise_conv2d_transpose_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_out_grad, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, filter_grad, "filter_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "depthwise_conv2d_transpose_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "depthwise_conv2d_transpose_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("depthwise_conv2d_transpose_grad", kernel_data_type);
  }
  VLOG(6) << "depthwise_conv2d_transpose_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_filter = PrepareData(filter, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"filter", {
     (*input_filter).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["output_padding"] = output_padding;
     attrs["output_size"] = output_size.GetData();
     attrs["padding_algorithm"] = padding_algorithm;
     attrs["groups"] = groups;
     attrs["dilations"] = dilations;
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("depthwise_conv2d_transpose_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(filter_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("depthwise_conv2d_transpose_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::Conv2dTransposeGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_filter), MakeMetaTensor(*input_out_grad), strides, paddings, output_padding, output_size, padding_algorithm, groups, dilations, data_format, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, int, const std::vector<int>&, const std::string&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("depthwise_conv2d_transpose_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_filter, *input_out_grad, strides, paddings, output_padding, phi::IntArray(output_size), padding_algorithm, groups, dilations, data_format, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void divide_double_grad(const Tensor& y, const Tensor& out, const Tensor& grad_out, const paddle::optional<Tensor>& grad_x, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_y_grad, int axis, Tensor* y_grad, Tensor* out_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(y, out, grad_out, grad_x, grad_x_grad, grad_y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(y, out, grad_out, grad_x, grad_x_grad, grad_y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "divide_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "divide_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("divide_double_grad", kernel_data_type);
  }
  VLOG(6) << "divide_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x = PrepareData(grad_x, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y_grad = PrepareData(grad_y_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_x_record_shapes;
     if(input_grad_x){
       grad_x_record_shapes.push_back((*input_grad_x).dims());
     }
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_y_grad_record_shapes;
     if(input_grad_y_grad){
       grad_y_grad_record_shapes.push_back((*input_grad_y_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"y", {
     (*input_y).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x", grad_x_record_shapes},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_y_grad",
     grad_y_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("divide_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(y_grad);
  auto kernel_out_1 = SetKernelOutput(out_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("divide_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_y), MakeMetaTensor(*input_out), MakeMetaTensor(*input_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("divide_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_y, *input_out, *input_grad_out, input_grad_x, input_grad_x_grad, input_grad_y_grad, axis, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void divide_grad(const Tensor& x, const Tensor& y, const Tensor& out, const Tensor& out_grad, int axis, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseBinaryGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out, meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("divide_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `divide_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "divide_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "divide_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "divide_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("divide_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("divide_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, axis, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "divide_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "divide_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("divide_grad", kernel_data_type);
  }
  VLOG(6) << "divide_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("divide_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("divide_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("divide_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out, *input_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void dropout_grad(const Tensor& mask, const Tensor& out_grad, const Scalar& p, bool is_test, const std::string& mode, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(mask, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(mask, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_mask = MakeDistMetaTensor(*mask.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_mask, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("dropout_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "dropout_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "dropout_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "dropout_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_mask = ReshardApiInputToKernelInput(dev_ctx, mask, spmd_info.first[0], "mask");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_mask = PrepareDataForDistTensor(dist_input_mask, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_mask = &dist_input_mask->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"mask", {
         (*input_mask).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
        switch (p.dtype()) {
          case DataType::FLOAT32:
              attrs["p"] = static_cast<float>(p.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["p"] = static_cast<double>(p.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["p"] = static_cast<float>(p.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["p"] = static_cast<float>(p.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["p"] = static_cast<int32_t>(p.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["p"] = static_cast<int64_t>(p.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["p"] = static_cast<int16_t>(p.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["p"] = static_cast<int8_t>(p.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["p"] = static_cast<uint16_t>(p.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["p"] = static_cast<uint8_t>(p.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["p"] = static_cast<bool>(p.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["p"] = static_cast<float>(p.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["p"] = static_cast<double>(p.to<complex128>());
              break;
          default:
              attrs["p"] = "";
              break;
        }
         attrs["is_test"] = is_test;
         attrs["mode"] = mode;
         phi::RecordOpInfoSupplement("dropout_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("dropout_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, bool, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_mask, *input_out_grad, phi::Scalar(p), is_test, mode, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "dropout_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "dropout_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("dropout_grad", kernel_data_type);
  }
  VLOG(6) << "dropout_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_mask = PrepareData(mask, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"mask", {
     (*input_mask).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
    switch (p.dtype()) {
      case DataType::FLOAT32:
          attrs["p"] = static_cast<float>(p.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["p"] = static_cast<double>(p.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["p"] = static_cast<float>(p.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["p"] = static_cast<float>(p.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["p"] = static_cast<int32_t>(p.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["p"] = static_cast<int64_t>(p.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["p"] = static_cast<int16_t>(p.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["p"] = static_cast<int8_t>(p.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["p"] = static_cast<uint16_t>(p.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["p"] = static_cast<uint8_t>(p.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["p"] = static_cast<bool>(p.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["p"] = static_cast<float>(p.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["p"] = static_cast<double>(p.to<complex128>());
          break;
      default:
          attrs["p"] = "";
          break;
    }
     attrs["is_test"] = is_test;
     attrs["mode"] = mode;
     phi::RecordOpInfoSupplement("dropout_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("dropout_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::Scalar&, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("dropout_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_mask, *input_out_grad, phi::Scalar(p), is_test, mode, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void einsum_grad(const std::vector<Tensor>& x_shape, const std::vector<Tensor>& inner_cache, const Tensor& out_grad, const std::string& equation, std::vector<Tensor*> x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x_shape, inner_cache, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x_shape, inner_cache, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_x_shape;
    for(auto& e : x_shape) {
        meta_dist_input_x_shape.push_back(MakeDistMetaTensor(*e.impl()));
    }
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_inner_cache;
    for(auto& e : inner_cache) {
        meta_dist_input_inner_cache.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x_shape, meta_dist_input_inner_cache, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("einsum_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    auto shared_dist_out = CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    std::vector<phi::distributed::DistTensor*> dist_out;
    for(auto& e: shared_dist_out){
      dist_out.push_back(e.get());
    }
    std::vector<phi::DenseTensor*> dense_out(dist_out.size());
    for (size_t i=0; i<dist_out.size(); i++) {
      dense_out[i] = dist_out[i]->unsafe_mutable_value();
      if (dense_out[i] && !rank_is_in_current_mesh && !dist_out[i]->defined()) {
        *dense_out[i] = phi::DenseTensor(
              std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
              phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    std::vector<phi::MetaTensor> dist_out_meta_vec;
    for (auto tmp : dist_out) {
      dist_out_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_meta_ptr_vec(dist_out.size());
    for (size_t i = 0; i < dist_out_meta_vec.size(); ++i) {
      dist_out_meta_ptr_vec[i] = &dist_out_meta_vec[i];
    }

    std::vector<phi::MetaTensor> x_shape_meta_vec;
    for (auto tmp : x_shape) {
      x_shape_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> x_shape_meta_ptr_vec(x_shape_meta_vec.size());
    for (size_t i=0; i < x_shape_meta_ptr_vec.size(); ++i) {
      x_shape_meta_ptr_vec[i] = &x_shape_meta_vec[i];
    }

    phi::UnchangedMultiInferMeta(x_shape_meta_ptr_vec, dist_out_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    for (size_t i = 0; i < dist_out.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out[i], current_process_mesh);
    }


    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "einsum_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "einsum_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "einsum_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x_shape = ReshardApiInputToKernelInput(dev_ctx, x_shape, spmd_info.first[0], "x_shape");
      auto dist_input_inner_cache = ReshardApiInputToKernelInput(dev_ctx, inner_cache, spmd_info.first[1], "inner_cache");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      auto dist_input_x_shape_vec = PrepareDataForDistTensor(dist_input_x_shape, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_x_shape_vec;
      for (auto tmp : dist_input_x_shape_vec) {
        dense_input_x_shape_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_x_shape_meta_vec = MakeMetaTensor(dense_input_x_shape_vec);
      std::vector<const phi::MetaTensor*> dense_input_x_shape_meta_ptr_vec(dense_input_x_shape_meta_vec.size());
      for (size_t i = 0; i < dense_input_x_shape_meta_ptr_vec.size(); ++i) {
        dense_input_x_shape_meta_ptr_vec[i] = &dense_input_x_shape_meta_vec[i];
      }

      auto dist_input_inner_cache_vec = PrepareDataForDistTensor(dist_input_inner_cache, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_inner_cache_vec;
      for (auto tmp : dist_input_inner_cache_vec) {
        dense_input_inner_cache_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_inner_cache_meta_vec = MakeMetaTensor(dense_input_inner_cache_vec);
      std::vector<const phi::MetaTensor*> dense_input_inner_cache_meta_ptr_vec(dense_input_inner_cache_meta_vec.size());
      for (size_t i = 0; i < dense_input_inner_cache_meta_ptr_vec.size(); ++i) {
        dense_input_inner_cache_meta_ptr_vec[i] = &dense_input_inner_cache_meta_vec[i];
      }

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_x_shape_vec.size());
         for (size_t i = 0; i < dense_input_x_shape_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_x_shape_vec[i]).dims());
         }
         input_shapes.emplace_back("x_shape", ddims_vec);
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_inner_cache_vec.size());
         for (size_t i = 0; i < dense_input_inner_cache_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_inner_cache_vec[i]).dims());
         }
         input_shapes.emplace_back("inner_cache", ddims_vec);
         phi::AttributeMap attrs;
         attrs["equation"] = equation;
         phi::RecordOpInfoSupplement("einsum_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      std::vector<phi::MetaTensor> dense_out_meta_vec = MakeMetaTensor(dense_out);
      std::vector<phi::MetaTensor*> dense_out_meta_ptr_vec(dense_out_meta_vec.size());
      for (size_t i = 0; i < dense_out_meta_vec.size(); ++i) {
        dense_out_meta_ptr_vec[i] = &dense_out_meta_vec[i];
      }

      phi::UnchangedMultiInferMeta(dense_input_x_shape_meta_ptr_vec, dense_out_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("einsum_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const std::string&, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, dense_input_x_shape_vec, dense_input_inner_cache_vec, *input_out_grad, equation, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "einsum_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "einsum_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("einsum_grad", kernel_data_type);
  }
  VLOG(6) << "einsum_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x_shape_vec = PrepareData(x_shape, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_x_shape(input_x_shape_vec->size());
  for (size_t i = 0; i < input_x_shape.size(); ++i) {
    input_x_shape[i] = &input_x_shape_vec->at(i);
  }
  auto input_inner_cache_vec = PrepareData(inner_cache, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_inner_cache(input_inner_cache_vec->size());
  for (size_t i = 0; i < input_inner_cache.size(); ++i) {
    input_inner_cache[i] = &input_inner_cache_vec->at(i);
  }
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_x_shape.size());
     for (size_t i = 0; i < input_x_shape.size(); ++i) {
       ddims_vec.emplace_back((*input_x_shape[i]).dims());
     }
     input_shapes.emplace_back("x_shape", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_inner_cache.size());
     for (size_t i = 0; i < input_inner_cache.size(); ++i) {
       ddims_vec.emplace_back((*input_inner_cache[i]).dims());
     }
     input_shapes.emplace_back("inner_cache", ddims_vec);
     phi::AttributeMap attrs;
     attrs["equation"] = equation;
     phi::RecordOpInfoSupplement("einsum_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(&x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("einsum_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto x_shape_meta_vec = MakeMetaTensor(input_x_shape);
  std::vector<const phi::MetaTensor*> x_shape_metas(x_shape_meta_vec.size());
  for (size_t i = 0; i < x_shape_meta_vec.size(); ++i) {
    x_shape_metas[i] = &x_shape_meta_vec[i];
  }

  auto kernel_out_meta_vec = MakeMetaTensor(kernel_out);
  std::vector<phi::MetaTensor*> kernel_out_metas(kernel_out_meta_vec.size());
  for (size_t i = 0; i < kernel_out_meta_vec.size(); ++i) {
    kernel_out_metas[i] = kernel_out[i] ? &kernel_out_meta_vec[i] : nullptr;
  }
  phi::UnchangedMultiInferMeta(x_shape_metas, kernel_out_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const phi::DenseTensor&, const std::string&, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("einsum_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, input_x_shape, input_inner_cache, *input_out_grad, equation, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void elementwise_pow_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseBinaryGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("elementwise_pow_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `elementwise_pow_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "elementwise_pow_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "elementwise_pow_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "elementwise_pow_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("elementwise_pow_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("elementwise_pow_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "elementwise_pow_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "elementwise_pow_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("elementwise_pow_grad", kernel_data_type);
  }
  VLOG(6) << "elementwise_pow_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("elementwise_pow_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("elementwise_pow_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("elementwise_pow_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void embedding_grad(const Tensor& x, const Tensor& weight, const Tensor& out_grad, int64_t padding_idx, bool sparse, Tensor* weight_grad) {
  embedding_grad_impl(x, weight, out_grad, padding_idx, sparse, weight_grad);
}
PADDLE_API void frobenius_norm_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const IntArray& axis, bool keep_dim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("frobenius_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "frobenius_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "frobenius_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "frobenius_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         attrs["keep_dim"] = keep_dim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("frobenius_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("frobenius_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(axis), keep_dim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "frobenius_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "frobenius_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("frobenius_norm_grad", kernel_data_type);
  }
  VLOG(6) << "frobenius_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keep_dim"] = keep_dim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("frobenius_norm_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("frobenius_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("frobenius_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(axis), keep_dim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fused_batch_norm_act_grad(const Tensor& x, const Tensor& scale, const Tensor& bias, const Tensor& out, const Tensor& saved_mean, const Tensor& saved_variance, const paddle::optional<Tensor>& reserve_space, const Tensor& out_grad, float momentum, float epsilon, const std::string& act_type, Tensor* x_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = MakeDistMetaTensor(*scale.impl());
    auto meta_dist_input_bias = MakeDistMetaTensor(*bias.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_saved_mean = MakeDistMetaTensor(*saved_mean.impl());
    auto meta_dist_input_saved_variance = MakeDistMetaTensor(*saved_variance.impl());
    auto meta_dist_input_reserve_space = reserve_space ? MakeDistMetaTensor(*(*reserve_space).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_bias, meta_dist_input_out, meta_dist_input_saved_mean, meta_dist_input_saved_variance, meta_dist_input_reserve_space, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fused_batch_norm_act_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*scale.impl()), MakeMetaTensor(*bias.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fused_batch_norm_act_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fused_batch_norm_act_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fused_batch_norm_act_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[3], "out");
      auto dist_input_saved_mean = ReshardApiInputToKernelInput(dev_ctx, saved_mean, spmd_info.first[4], "saved_mean");
      auto dist_input_saved_variance = ReshardApiInputToKernelInput(dev_ctx, saved_variance, spmd_info.first[5], "saved_variance");
      auto dist_input_reserve_space = ReshardApiInputToKernelInput(dev_ctx, reserve_space, spmd_info.first[6], "reserve_space");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[7], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_scale = &dist_input_scale->value();

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_bias = &dist_input_bias->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_saved_mean = PrepareDataForDistTensor(dist_input_saved_mean, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_mean = &dist_input_saved_mean->value();

      dist_input_saved_variance = PrepareDataForDistTensor(dist_input_saved_variance, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_variance = &dist_input_saved_variance->value();

      dist_input_reserve_space = PrepareDataForDistTensor(dist_input_reserve_space, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_reserve_space = dist_input_reserve_space ? paddle::make_optional<phi::DenseTensor>((*dist_input_reserve_space)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> reserve_space_record_shapes;
         if(input_reserve_space){
           reserve_space_record_shapes.push_back((*input_reserve_space).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", {
         (*input_scale).dims()}},
         {"bias", {
         (*input_bias).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"saved_mean", {
         (*input_saved_mean).dims()}},
         {"saved_variance", {
         (*input_saved_variance).dims()}},
         {"reserve_space", reserve_space_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["momentum"] = momentum;
         attrs["epsilon"] = epsilon;
         attrs["act_type"] = act_type;
         phi::RecordOpInfoSupplement("fused_batch_norm_act_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fused_batch_norm_act_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_out, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, act_type, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fused_batch_norm_act_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_batch_norm_act_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_batch_norm_act_grad", kernel_data_type);
  }
  VLOG(6) << "fused_batch_norm_act_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_reserve_space = PrepareData(reserve_space, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> reserve_space_record_shapes;
     if(input_reserve_space){
       reserve_space_record_shapes.push_back((*input_reserve_space).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"reserve_space", reserve_space_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["act_type"] = act_type;
     phi::RecordOpInfoSupplement("fused_batch_norm_act_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_batch_norm_act_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_batch_norm_act_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_out, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, act_type, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void fused_bn_add_activation_grad(const Tensor& x, const Tensor& scale, const Tensor& bias, const Tensor& out, const Tensor& saved_mean, const Tensor& saved_variance, const paddle::optional<Tensor>& reserve_space, const Tensor& out_grad, float momentum, float epsilon, const std::string& act_type, Tensor* x_grad, Tensor* z_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, out, saved_mean, saved_variance, reserve_space, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = MakeDistMetaTensor(*scale.impl());
    auto meta_dist_input_bias = MakeDistMetaTensor(*bias.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_saved_mean = MakeDistMetaTensor(*saved_mean.impl());
    auto meta_dist_input_saved_variance = MakeDistMetaTensor(*saved_variance.impl());
    auto meta_dist_input_reserve_space = reserve_space ? MakeDistMetaTensor(*(*reserve_space).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_bias, meta_dist_input_out, meta_dist_input_saved_mean, meta_dist_input_saved_variance, meta_dist_input_reserve_space, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fused_bn_add_activation_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(z_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_3 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_3 = shared_dist_out_3.get();
    phi::DenseTensor* dense_out_3 = dist_out_3 ? dist_out_3->unsafe_mutable_value() : nullptr;
    if (dense_out_3 && !rank_is_in_current_mesh && !dist_out_3->defined()) {
      *dense_out_3 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_out_3(dist_out_3);
    phi::GeneralQuaternaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*x.impl()), MakeMetaTensor(*scale.impl()), MakeMetaTensor(*bias.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr, dist_out_3 ? &meta_dist_out_3 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_3, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fused_bn_add_activation_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fused_bn_add_activation_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fused_bn_add_activation_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[3], "out");
      auto dist_input_saved_mean = ReshardApiInputToKernelInput(dev_ctx, saved_mean, spmd_info.first[4], "saved_mean");
      auto dist_input_saved_variance = ReshardApiInputToKernelInput(dev_ctx, saved_variance, spmd_info.first[5], "saved_variance");
      auto dist_input_reserve_space = ReshardApiInputToKernelInput(dev_ctx, reserve_space, spmd_info.first[6], "reserve_space");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[7], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_scale = &dist_input_scale->value();

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_bias = &dist_input_bias->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_saved_mean = PrepareDataForDistTensor(dist_input_saved_mean, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_mean = &dist_input_saved_mean->value();

      dist_input_saved_variance = PrepareDataForDistTensor(dist_input_saved_variance, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_variance = &dist_input_saved_variance->value();

      dist_input_reserve_space = PrepareDataForDistTensor(dist_input_reserve_space, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_reserve_space = dist_input_reserve_space ? paddle::make_optional<phi::DenseTensor>((*dist_input_reserve_space)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> reserve_space_record_shapes;
         if(input_reserve_space){
           reserve_space_record_shapes.push_back((*input_reserve_space).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", {
         (*input_scale).dims()}},
         {"bias", {
         (*input_bias).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"saved_mean", {
         (*input_saved_mean).dims()}},
         {"saved_variance", {
         (*input_saved_variance).dims()}},
         {"reserve_space", reserve_space_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["momentum"] = momentum;
         attrs["epsilon"] = epsilon;
         attrs["act_type"] = act_type;
         phi::RecordOpInfoSupplement("fused_bn_add_activation_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::MetaTensor meta_dense_out_3(dense_out_3);
      phi::GeneralQuaternaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr, dense_out_3 ? &meta_dense_out_3 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fused_bn_add_activation_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_out, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, act_type, dense_out_0, dense_out_1, dense_out_2, dense_out_3);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
        TransDataBackend(dense_out_3, kernel_backend, dense_out_3);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, z_grad, "z_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_3, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fused_bn_add_activation_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_bn_add_activation_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_bn_add_activation_grad", kernel_data_type);
  }
  VLOG(6) << "fused_bn_add_activation_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_reserve_space = PrepareData(reserve_space, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> reserve_space_record_shapes;
     if(input_reserve_space){
       reserve_space_record_shapes.push_back((*input_reserve_space).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"reserve_space", reserve_space_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["act_type"] = act_type;
     phi::RecordOpInfoSupplement("fused_bn_add_activation_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(z_grad);
  auto kernel_out_2 = SetKernelOutput(scale_grad);
  auto kernel_out_3 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_bn_add_activation_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);

  phi::GeneralQuaternaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_bn_add_activation_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_out, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, act_type, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);

  }
  
}

PADDLE_API void fused_softmax_mask_grad(const Tensor& out, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fused_softmax_mask_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fused_softmax_mask_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fused_softmax_mask_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fused_softmax_mask_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("fused_softmax_mask_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fused_softmax_mask_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fused_softmax_mask_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_softmax_mask_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_softmax_mask_grad", kernel_data_type);
  }
  VLOG(6) << "fused_softmax_mask_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fused_softmax_mask_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_softmax_mask_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_softmax_mask_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fused_softmax_mask_upper_triangle_grad(const Tensor& Out, const Tensor& Out_grad, Tensor* X_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(Out, Out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(Out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(Out, Out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_Out = MakeDistMetaTensor(*Out.impl());
    auto meta_dist_input_Out_grad = MakeDistMetaTensor(*Out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_Out, meta_dist_input_Out_grad);
    DebugInfoForInferSpmd("fused_softmax_mask_upper_triangle_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(X_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*Out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fused_softmax_mask_upper_triangle_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fused_softmax_mask_upper_triangle_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fused_softmax_mask_upper_triangle_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_Out = ReshardApiInputToKernelInput(dev_ctx, Out, spmd_info.first[0], "Out");
      auto dist_input_Out_grad = ReshardApiInputToKernelInput(dev_ctx, Out_grad, spmd_info.first[1], "Out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_Out = PrepareDataForDistTensor(dist_input_Out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_Out = &dist_input_Out->value();

      dist_input_Out_grad = PrepareDataForDistTensor(dist_input_Out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_Out_grad = &dist_input_Out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"Out", {
         (*input_Out).dims()}},
         {"Out_grad", {
         (*input_Out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("fused_softmax_mask_upper_triangle_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_Out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fused_softmax_mask_upper_triangle_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_Out, *input_Out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, X_grad, "X_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fused_softmax_mask_upper_triangle_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_softmax_mask_upper_triangle_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_softmax_mask_upper_triangle_grad", kernel_data_type);
  }
  VLOG(6) << "fused_softmax_mask_upper_triangle_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_Out = PrepareData(Out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_Out_grad = PrepareData(Out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"Out", {
     (*input_Out).dims()}},
     {"Out_grad", {
     (*input_Out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("fused_softmax_mask_upper_triangle_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(X_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_softmax_mask_upper_triangle_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_Out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_softmax_mask_upper_triangle_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_Out, *input_Out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void hardswish_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("hardswish_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "hardswish_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "hardswish_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "hardswish_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("hardswish_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("hardswish_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "hardswish_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hardswish_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hardswish_grad", kernel_data_type);
  }
  VLOG(6) << "hardswish_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("hardswish_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hardswish_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hardswish_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void hsigmoid_loss_grad(const Tensor& x, const Tensor& w, const Tensor& label, const paddle::optional<Tensor>& path, const paddle::optional<Tensor>& code, const paddle::optional<Tensor>& bias, const Tensor& pre_out, const Tensor& out_grad, int num_classes, bool is_sparse, Tensor* x_grad, Tensor* w_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, w, label, path, code, bias, pre_out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, w, label, path, code, bias, pre_out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_w = MakeDistMetaTensor(*w.impl());
    auto meta_dist_input_label = MakeDistMetaTensor(*label.impl());
    auto meta_dist_input_path = path ? MakeDistMetaTensor(*(*path).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_code = code ? MakeDistMetaTensor(*(*code).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_bias = bias ? MakeDistMetaTensor(*(*bias).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_pre_out = MakeDistMetaTensor(*pre_out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_w, meta_dist_input_label, meta_dist_input_path, meta_dist_input_code, meta_dist_input_bias, meta_dist_input_pre_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("hsigmoid_loss_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(w_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_bias = bias ? MakeMetaTensor(*(*bias).impl()) : phi::MetaTensor();

    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*w.impl()), meta_dist_bias, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "hsigmoid_loss_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "hsigmoid_loss_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "hsigmoid_loss_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_w = ReshardApiInputToKernelInput(dev_ctx, w, spmd_info.first[1], "w");
      auto dist_input_label = ReshardApiInputToKernelInput(dev_ctx, label, spmd_info.first[2], "label");
      auto dist_input_path = ReshardApiInputToKernelInput(dev_ctx, path, spmd_info.first[3], "path");
      auto dist_input_code = ReshardApiInputToKernelInput(dev_ctx, code, spmd_info.first[4], "code");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[5], "bias");
      auto dist_input_pre_out = ReshardApiInputToKernelInput(dev_ctx, pre_out, spmd_info.first[6], "pre_out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[7], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_w = PrepareDataForDistTensor(dist_input_w, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_w = &dist_input_w->value();

      dist_input_label = PrepareDataForDistTensor(dist_input_label, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_label = &dist_input_label->value();

      dist_input_path = PrepareDataForDistTensor(dist_input_path, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_path = dist_input_path ? paddle::make_optional<phi::DenseTensor>((*dist_input_path)->value()) : paddle::none;

      dist_input_code = PrepareDataForDistTensor(dist_input_code, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_code = dist_input_code ? paddle::make_optional<phi::DenseTensor>((*dist_input_code)->value()) : paddle::none;

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_bias = dist_input_bias ? paddle::make_optional<phi::DenseTensor>((*dist_input_bias)->value()) : paddle::none;

      dist_input_pre_out = PrepareDataForDistTensor(dist_input_pre_out, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_pre_out = &dist_input_pre_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> path_record_shapes;
         if(input_path){
           path_record_shapes.push_back((*input_path).dims());
         }
         std::vector<phi::DDim> code_record_shapes;
         if(input_code){
           code_record_shapes.push_back((*input_code).dims());
         }
         std::vector<phi::DDim> bias_record_shapes;
         if(input_bias){
           bias_record_shapes.push_back((*input_bias).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"w", {
         (*input_w).dims()}},
         {"label", {
         (*input_label).dims()}},
         {"path", path_record_shapes},
         {"code", code_record_shapes},
         {"bias", bias_record_shapes},
         {"pre_out", {
         (*input_pre_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["num_classes"] = num_classes;
         attrs["is_sparse"] = is_sparse;
         phi::RecordOpInfoSupplement("hsigmoid_loss_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_w), MakeMetaTensor(input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("hsigmoid_loss_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_w, *input_label, input_path, input_code, input_bias, *input_pre_out, *input_out_grad, num_classes, is_sparse, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, w_grad, "w_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "hsigmoid_loss_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "hsigmoid_loss_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("hsigmoid_loss_grad", kernel_data_type);
  }
  VLOG(6) << "hsigmoid_loss_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_w = PrepareData(w, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_label = PrepareData(label, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_path = PrepareData(path, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_code = PrepareData(code, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pre_out = PrepareData(pre_out, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> path_record_shapes;
     if(input_path){
       path_record_shapes.push_back((*input_path).dims());
     }
     std::vector<phi::DDim> code_record_shapes;
     if(input_code){
       code_record_shapes.push_back((*input_code).dims());
     }
     std::vector<phi::DDim> bias_record_shapes;
     if(input_bias){
       bias_record_shapes.push_back((*input_bias).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"w", {
     (*input_w).dims()}},
     {"label", {
     (*input_label).dims()}},
     {"path", path_record_shapes},
     {"code", code_record_shapes},
     {"bias", bias_record_shapes},
     {"pre_out", {
     (*input_pre_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["num_classes"] = num_classes;
     attrs["is_sparse"] = is_sparse;
     phi::RecordOpInfoSupplement("hsigmoid_loss_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(w_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("hsigmoid_loss_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_w), MakeMetaTensor(input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, int, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("hsigmoid_loss_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_w, *input_label, input_path, input_code, input_bias, *input_pre_out, *input_out_grad, num_classes, is_sparse, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void logsumexp_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const std::vector<int64_t>& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("logsumexp_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "logsumexp_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "logsumexp_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "logsumexp_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("logsumexp_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("logsumexp_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "logsumexp_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "logsumexp_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("logsumexp_grad", kernel_data_type);
  }
  VLOG(6) << "logsumexp_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("logsumexp_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("logsumexp_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("logsumexp_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, axis, keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void matmul_double_grad(const Tensor& x, const Tensor& y, const Tensor& grad_out, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_y_grad, bool transpose_x, bool transpose_y, Tensor* x_grad, Tensor* y_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, grad_out, grad_x_grad, grad_y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, grad_out, grad_x_grad, grad_y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "matmul_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matmul_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matmul_double_grad", kernel_data_type);
  }
  VLOG(6) << "matmul_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y_grad = PrepareData(grad_y_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_y_grad_record_shapes;
     if(input_grad_y_grad){
       grad_y_grad_record_shapes.push_back((*input_grad_y_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_y_grad",
     grad_y_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["transpose_x"] = transpose_x;
     attrs["transpose_y"] = transpose_y;
     phi::RecordOpInfoSupplement("matmul_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matmul_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matmul_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_grad_out, input_grad_x_grad, input_grad_y_grad, transpose_x, transpose_y, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void matmul_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, bool transpose_x, bool transpose_y, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::MatmulGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad, transpose_x, transpose_y);
    DebugInfoForInferSpmd("matmul_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `matmul_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "matmul_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "matmul_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "matmul_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["transpose_x"] = transpose_x;
         attrs["transpose_y"] = transpose_y;
         phi::RecordOpInfoSupplement("matmul_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("matmul_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, transpose_x, transpose_y, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "matmul_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "matmul_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("matmul_grad", kernel_data_type);
  }
  VLOG(6) << "matmul_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["transpose_x"] = transpose_x;
     attrs["transpose_y"] = transpose_y;
     phi::RecordOpInfoSupplement("matmul_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("matmul_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, bool, bool, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("matmul_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, transpose_x, transpose_y, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void max_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const IntArray& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ReductionGradInferSpmd(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad, axis.GetData(), keepdim, reduce_all);
    DebugInfoForInferSpmd("max_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `max_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "max_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "max_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "max_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("max_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("max_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "max_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "max_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("max_grad", kernel_data_type);
  }
  VLOG(6) << "max_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("max_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("max_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("max_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void maximum_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseBinaryGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("maximum_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `maximum_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "maximum_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "maximum_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "maximum_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("maximum_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("maximum_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "maximum_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "maximum_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("maximum_grad", kernel_data_type);
  }
  VLOG(6) << "maximum_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("maximum_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("maximum_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("maximum_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void mean_grad(const Tensor& x, const Tensor& out_grad, const IntArray& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ReductionGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad, axis.GetData(), keepdim, reduce_all);
    DebugInfoForInferSpmd("mean_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `mean_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "mean_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "mean_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "mean_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("mean_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("mean_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "mean_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mean_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mean_grad", kernel_data_type);
  }
  VLOG(6) << "mean_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("mean_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mean_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mean_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void min_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const IntArray& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("min_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "min_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "min_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "min_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("min_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("min_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "min_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "min_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("min_grad", kernel_data_type);
  }
  VLOG(6) << "min_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("min_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("min_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("min_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void minimum_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("minimum_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "minimum_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "minimum_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "minimum_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("minimum_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("minimum_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "minimum_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "minimum_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("minimum_grad", kernel_data_type);
  }
  VLOG(6) << "minimum_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("minimum_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("minimum_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("minimum_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void mish_grad(const Tensor& x, const Tensor& out_grad, float lambda, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("mish_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "mish_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "mish_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "mish_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["lambda"] = lambda;
         phi::RecordOpInfoSupplement("mish_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("mish_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, lambda, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "mish_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "mish_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("mish_grad", kernel_data_type);
  }
  VLOG(6) << "mish_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["lambda"] = lambda;
     phi::RecordOpInfoSupplement("mish_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("mish_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, float, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("mish_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, lambda, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void multiply_double_grad(const Tensor& x, const Tensor& y, const Tensor& grad_out, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_y_grad, int axis, Tensor* x_grad, Tensor* y_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, grad_out, grad_x_grad, grad_y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, grad_out, grad_x_grad, grad_y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "multiply_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiply_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply_double_grad", kernel_data_type);
  }
  VLOG(6) << "multiply_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y_grad = PrepareData(grad_y_grad, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_y_grad_record_shapes;
     if(input_grad_y_grad){
       grad_y_grad_record_shapes.push_back((*input_grad_y_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_y_grad",
     grad_y_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("multiply_double_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);
  auto kernel_out_2 = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multiply_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_grad_out), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multiply_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_grad_out, input_grad_x_grad, input_grad_y_grad, axis, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void multiply_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, int axis, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseBinaryGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("multiply_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `multiply_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "multiply_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "multiply_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "multiply_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("multiply_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("multiply_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "multiply_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiply_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply_grad", kernel_data_type);
  }
  VLOG(6) << "multiply_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("multiply_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multiply_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multiply_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void multiply_triple_grad(const Tensor& x, const Tensor& y, const Tensor& fwd_grad_out, const paddle::optional<Tensor>& fwd_grad_grad_x, const paddle::optional<Tensor>& fwd_grad_grad_y, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_y_grad, const paddle::optional<Tensor>& grad_grad_out_grad, int axis, Tensor* x_grad, Tensor* y_grad, Tensor* fwd_grad_out_grad, Tensor* fwd_grad_grad_x_grad, Tensor* fwd_grad_grad_y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, fwd_grad_out, fwd_grad_grad_x, fwd_grad_grad_y, grad_x_grad, grad_y_grad, grad_grad_out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(fwd_grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, fwd_grad_out, fwd_grad_grad_x, fwd_grad_grad_y, grad_x_grad, grad_y_grad, grad_grad_out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "multiply_triple_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "multiply_triple_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("multiply_triple_grad", kernel_data_type);
  }
  VLOG(6) << "multiply_triple_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fwd_grad_out = PrepareData(fwd_grad_out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fwd_grad_grad_x = PrepareData(fwd_grad_grad_x, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_fwd_grad_grad_y = PrepareData(fwd_grad_grad_y, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y_grad = PrepareData(grad_y_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_grad_out_grad = PrepareData(grad_grad_out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> fwd_grad_grad_x_record_shapes;
     if(input_fwd_grad_grad_x){
       fwd_grad_grad_x_record_shapes.push_back((*input_fwd_grad_grad_x).dims());
     }
     std::vector<phi::DDim> fwd_grad_grad_y_record_shapes;
     if(input_fwd_grad_grad_y){
       fwd_grad_grad_y_record_shapes.push_back((*input_fwd_grad_grad_y).dims());
     }
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_y_grad_record_shapes;
     if(input_grad_y_grad){
       grad_y_grad_record_shapes.push_back((*input_grad_y_grad).dims());
     }
     std::vector<phi::DDim> grad_grad_out_grad_record_shapes;
     if(input_grad_grad_out_grad){
       grad_grad_out_grad_record_shapes.push_back((*input_grad_grad_out_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"fwd_grad_out", {
     (*input_fwd_grad_out).dims()}},
     {"fwd_grad_grad_x", fwd_grad_grad_x_record_shapes},
     {"fwd_grad_grad_y", fwd_grad_grad_y_record_shapes},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_y_grad", grad_y_grad_record_shapes},
     {"grad_grad_out_grad",
     grad_grad_out_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("multiply_triple_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);
  auto kernel_out_2 = SetKernelOutput(fwd_grad_out_grad);
  auto kernel_out_3 = SetKernelOutput(fwd_grad_grad_x_grad);
  auto kernel_out_4 = SetKernelOutput(fwd_grad_grad_y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("multiply_triple_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_3(kernel_out_3, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_4(kernel_out_4, kernel_result.is_stride_kernel);

  phi::GeneralQuinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(*input_fwd_grad_out), MakeMetaTensor(input_fwd_grad_grad_x), MakeMetaTensor(input_fwd_grad_grad_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr, kernel_out_3 ? &meta_out_3 : nullptr, kernel_out_4 ? &meta_out_4 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("multiply_triple_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_fwd_grad_out, input_fwd_grad_grad_x, input_fwd_grad_grad_y, input_grad_x_grad, input_grad_y_grad, input_grad_grad_out_grad, axis, kernel_out_0, kernel_out_1, kernel_out_2, kernel_out_3, kernel_out_4);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);
    TransDataBackend(kernel_out_3, kernel_backend, kernel_out_3);
    TransDataBackend(kernel_out_4, kernel_backend, kernel_out_4);

  }
  
}

PADDLE_API void norm_grad(const Tensor& x, const Tensor& norm, const Tensor& out_grad, int axis, float epsilon, bool is_test, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, norm, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, norm, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_norm = MakeDistMetaTensor(*norm.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_norm, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_norm = ReshardApiInputToKernelInput(dev_ctx, norm, spmd_info.first[1], "norm");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_norm = PrepareDataForDistTensor(dist_input_norm, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_norm = &dist_input_norm->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"norm", {
         (*input_norm).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         attrs["epsilon"] = epsilon;
         attrs["is_test"] = is_test;
         phi::RecordOpInfoSupplement("norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, float, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_norm, *input_out_grad, axis, epsilon, is_test, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("norm_grad", kernel_data_type);
  }
  VLOG(6) << "norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_norm = PrepareData(norm, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"norm", {
     (*input_norm).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     attrs["epsilon"] = epsilon;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("norm_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, float, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_norm, *input_out_grad, axis, epsilon, is_test, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pad_double_grad(const Tensor& grad_x_grad, const std::vector<int>& paddings, const Scalar& pad_value, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "pad_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pad_double_grad", kernel_data_type);
  }
  VLOG(6) << "pad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["paddings"] = paddings;
    switch (pad_value.dtype()) {
      case DataType::FLOAT32:
          attrs["pad_value"] = static_cast<float>(pad_value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["pad_value"] = static_cast<double>(pad_value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["pad_value"] = static_cast<float>(pad_value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["pad_value"] = static_cast<float>(pad_value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["pad_value"] = static_cast<int32_t>(pad_value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["pad_value"] = static_cast<int64_t>(pad_value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["pad_value"] = static_cast<int16_t>(pad_value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["pad_value"] = static_cast<int8_t>(pad_value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["pad_value"] = static_cast<uint16_t>(pad_value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["pad_value"] = static_cast<uint8_t>(pad_value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["pad_value"] = static_cast<bool>(pad_value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["pad_value"] = static_cast<float>(pad_value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["pad_value"] = static_cast<double>(pad_value.to<complex128>());
          break;
      default:
          attrs["pad_value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pad_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pad_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::PadInferMeta(MakeMetaTensor(*input_grad_x_grad), paddings, pad_value, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pad_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_grad_x_grad, paddings, phi::Scalar(pad_value), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pad_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int>& paddings, const Scalar& pad_value, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("pad_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pad_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pad_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pad_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
    auto dist_input_x = x.impl();
    auto input_x = &(static_cast<phi::distributed::DistTensor*>(dist_input_x.get())->value());

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["paddings"] = paddings;
        switch (pad_value.dtype()) {
          case DataType::FLOAT32:
              attrs["pad_value"] = static_cast<float>(pad_value.to<float>());
              break;
          case DataType::FLOAT64:
              attrs["pad_value"] = static_cast<double>(pad_value.to<double>());
              break;
          case DataType::FLOAT16:
              attrs["pad_value"] = static_cast<float>(pad_value.to<float16>());
              break;
          case DataType::BFLOAT16:
              attrs["pad_value"] = static_cast<float>(pad_value.to<bfloat16>());
              break;
          case DataType::INT32:
              attrs["pad_value"] = static_cast<int32_t>(pad_value.to<int32_t>());
              break;
          case DataType::INT64:
              attrs["pad_value"] = static_cast<int64_t>(pad_value.to<int64_t>());
              break;
          case DataType::INT16:
              attrs["pad_value"] = static_cast<int16_t>(pad_value.to<int16_t>());
              break;
          case DataType::INT8:
              attrs["pad_value"] = static_cast<int8_t>(pad_value.to<int8_t>());
              break;
          case DataType::UINT16:
              attrs["pad_value"] = static_cast<uint16_t>(pad_value.to<uint16_t>());
              break;
          case DataType::UINT8:
              attrs["pad_value"] = static_cast<uint8_t>(pad_value.to<uint8_t>());
              break;
          case DataType::BOOL:
              attrs["pad_value"] = static_cast<bool>(pad_value.to<bool>());
              break;
          case DataType::COMPLEX64:
              attrs["pad_value"] = static_cast<float>(pad_value.to<complex64>());
              break;
          case DataType::COMPLEX128:
              attrs["pad_value"] = static_cast<double>(pad_value.to<complex128>());
              break;
          default:
              attrs["pad_value"] = "";
              break;
        }
         phi::RecordOpInfoSupplement("pad_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pad_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const phi::Scalar&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, paddings, phi::Scalar(pad_value), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pad_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pad_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pad_grad", kernel_data_type);
  }
  VLOG(6) << "pad_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = x.impl();
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["paddings"] = paddings;
    switch (pad_value.dtype()) {
      case DataType::FLOAT32:
          attrs["pad_value"] = static_cast<float>(pad_value.to<float>());
          break;
      case DataType::FLOAT64:
          attrs["pad_value"] = static_cast<double>(pad_value.to<double>());
          break;
      case DataType::FLOAT16:
          attrs["pad_value"] = static_cast<float>(pad_value.to<float16>());
          break;
      case DataType::BFLOAT16:
          attrs["pad_value"] = static_cast<float>(pad_value.to<bfloat16>());
          break;
      case DataType::INT32:
          attrs["pad_value"] = static_cast<int32_t>(pad_value.to<int32_t>());
          break;
      case DataType::INT64:
          attrs["pad_value"] = static_cast<int64_t>(pad_value.to<int64_t>());
          break;
      case DataType::INT16:
          attrs["pad_value"] = static_cast<int16_t>(pad_value.to<int16_t>());
          break;
      case DataType::INT8:
          attrs["pad_value"] = static_cast<int8_t>(pad_value.to<int8_t>());
          break;
      case DataType::UINT16:
          attrs["pad_value"] = static_cast<uint16_t>(pad_value.to<uint16_t>());
          break;
      case DataType::UINT8:
          attrs["pad_value"] = static_cast<uint8_t>(pad_value.to<uint8_t>());
          break;
      case DataType::BOOL:
          attrs["pad_value"] = static_cast<bool>(pad_value.to<bool>());
          break;
      case DataType::COMPLEX64:
          attrs["pad_value"] = static_cast<float>(pad_value.to<complex64>());
          break;
      case DataType::COMPLEX128:
          attrs["pad_value"] = static_cast<double>(pad_value.to<complex128>());
          break;
      default:
          attrs["pad_value"] = "";
          break;
    }
     phi::RecordOpInfoSupplement("pad_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pad_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, const phi::Scalar&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pad_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, paddings, phi::Scalar(pad_value), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pool2d_double_grad(const Tensor& x, const Tensor& grad_x_grad, const IntArray& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "pool2d_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool2d_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pool2d_double_grad", kernel_data_type);
  }
  VLOG(6) << "pool2d_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size.GetData();
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["ceil_mode"] = ceil_mode;
     attrs["exclusive"] = exclusive;
     attrs["data_format"] = data_format;
     attrs["pooling_type"] = pooling_type;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["padding_algorithm"] = padding_algorithm;
     phi::RecordOpInfoSupplement("pool2d_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pool2d_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::Pool2DInferMeta(MakeMetaTensor(*input_grad_x_grad), kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pool2d_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_grad_x_grad, phi::IntArray(kernel_size), strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pool2d_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const IntArray& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("pool2d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pool2d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pool2d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pool2d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["kernel_size"] = kernel_size.GetData();
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["ceil_mode"] = ceil_mode;
         attrs["exclusive"] = exclusive;
         attrs["data_format"] = data_format;
         attrs["pooling_type"] = pooling_type;
         attrs["global_pooling"] = global_pooling;
         attrs["adaptive"] = adaptive;
         attrs["padding_algorithm"] = padding_algorithm;
         phi::RecordOpInfoSupplement("pool2d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pool2d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(kernel_size), strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pool2d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool2d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pool2d_grad", kernel_data_type);
  }
  VLOG(6) << "pool2d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size.GetData();
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["ceil_mode"] = ceil_mode;
     attrs["exclusive"] = exclusive;
     attrs["data_format"] = data_format;
     attrs["pooling_type"] = pooling_type;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["padding_algorithm"] = padding_algorithm;
     phi::RecordOpInfoSupplement("pool2d_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pool2d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pool2d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(kernel_size), strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void pool3d_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const std::vector<int>& kernel_size, const std::vector<int>& strides, const std::vector<int>& paddings, bool ceil_mode, bool exclusive, const std::string& data_format, const std::string& pooling_type, bool global_pooling, bool adaptive, const std::string& padding_algorithm, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("pool3d_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "pool3d_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "pool3d_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "pool3d_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["kernel_size"] = kernel_size;
         attrs["strides"] = strides;
         attrs["paddings"] = paddings;
         attrs["ceil_mode"] = ceil_mode;
         attrs["exclusive"] = exclusive;
         attrs["data_format"] = data_format;
         attrs["pooling_type"] = pooling_type;
         attrs["global_pooling"] = global_pooling;
         attrs["adaptive"] = adaptive;
         attrs["padding_algorithm"] = padding_algorithm;
         phi::RecordOpInfoSupplement("pool3d_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("pool3d_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "pool3d_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "pool3d_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("pool3d_grad", kernel_data_type);
  }
  VLOG(6) << "pool3d_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["kernel_size"] = kernel_size;
     attrs["strides"] = strides;
     attrs["paddings"] = paddings;
     attrs["ceil_mode"] = ceil_mode;
     attrs["exclusive"] = exclusive;
     attrs["data_format"] = data_format;
     attrs["pooling_type"] = pooling_type;
     attrs["global_pooling"] = global_pooling;
     attrs["adaptive"] = adaptive;
     attrs["padding_algorithm"] = padding_algorithm;
     phi::RecordOpInfoSupplement("pool3d_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("pool3d_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, bool, bool, const std::string&, const std::string&, bool, bool, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("pool3d_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, kernel_size, strides, paddings, ceil_mode, exclusive, data_format, pooling_type, global_pooling, adaptive, padding_algorithm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void prod_grad(const Tensor& x, const Tensor& out, const Tensor& out_grad, const IntArray& dims, bool keep_dim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("prod_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "prod_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "prod_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "prod_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[1], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["dims"] = dims.GetData();
         attrs["keep_dim"] = keep_dim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("prod_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("prod_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(dims), keep_dim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "prod_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "prod_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("prod_grad", kernel_data_type);
  }
  VLOG(6) << "prod_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["dims"] = dims.GetData();
     attrs["keep_dim"] = keep_dim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("prod_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("prod_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("prod_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out, *input_out_grad, phi::IntArray(dims), keep_dim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void repeat_interleave_grad(const Tensor& x, const Tensor& out_grad, int repeats, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("repeat_interleave_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "repeat_interleave_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "repeat_interleave_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "repeat_interleave_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["repeats"] = repeats;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("repeat_interleave_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("repeat_interleave_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, repeats, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "repeat_interleave_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "repeat_interleave_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("repeat_interleave_grad", kernel_data_type);
  }
  VLOG(6) << "repeat_interleave_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["repeats"] = repeats;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("repeat_interleave_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("repeat_interleave_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("repeat_interleave_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, repeats, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void repeat_interleave_with_tensor_index_grad(const Tensor& x, const Tensor& repeats, const Tensor& out_grad, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, repeats, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, repeats, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_repeats = MakeDistMetaTensor(*repeats.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_repeats, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("repeat_interleave_with_tensor_index_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "repeat_interleave_with_tensor_index_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "repeat_interleave_with_tensor_index_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "repeat_interleave_with_tensor_index_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_repeats = ReshardApiInputToKernelInput(dev_ctx, repeats, spmd_info.first[1], "repeats");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_repeats = PrepareDataForDistTensor(dist_input_repeats, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_repeats = &dist_input_repeats->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"repeats", {
         (*input_repeats).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("repeat_interleave_with_tensor_index_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("repeat_interleave_with_tensor_index_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_repeats, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "repeat_interleave_with_tensor_index_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "repeat_interleave_with_tensor_index_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("repeat_interleave_with_tensor_index_grad", kernel_data_type);
  }
  VLOG(6) << "repeat_interleave_with_tensor_index_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_repeats = PrepareData(repeats, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"repeats", {
     (*input_repeats).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("repeat_interleave_with_tensor_index_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("repeat_interleave_with_tensor_index_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("repeat_interleave_with_tensor_index_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_repeats, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void reshape_double_grad(const Tensor& grad_out, const Tensor& grad_x_grad, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(grad_out, grad_x_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_x_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(grad_out, grad_x_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "reshape_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reshape_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reshape_double_grad", kernel_data_type);
  }
  VLOG(6) << "reshape_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", {
     (*input_grad_x_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reshape_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reshape_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_grad_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reshape_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_grad_out, *input_grad_x_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void reshape_grad(const Tensor& xshape, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(xshape, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_backend = ParseBackend(out_grad);

    kernel_layout = ParseLayout(out_grad);

    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(xshape, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_xshape = MakeDistMetaTensor(*xshape.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ReshapeGradInferSpmd(meta_dist_input_xshape, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("reshape_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::KernelWithXShapeInferMeta(MakeMetaTensor(*xshape.impl()), MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `reshape_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "reshape_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "reshape_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "reshape_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
    auto dist_input_xshape = xshape.impl();
    auto input_xshape = &(static_cast<phi::distributed::DistTensor*>(dist_input_xshape.get())->value());

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("reshape_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("reshape_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "reshape_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "reshape_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("reshape_grad", kernel_data_type);
  }
  VLOG(6) << "reshape_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_xshape = xshape.impl();
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("reshape_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("reshape_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::KernelWithXShapeInferMeta(MakeMetaTensor(*input_xshape), MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("reshape_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void rnn_grad(const Tensor& x, const std::vector<Tensor>& pre_state, const std::vector<Tensor>& weight_list, const paddle::optional<Tensor>& sequence_length, const Tensor& out, const Tensor& dropout_state_out, const Tensor& reserve, const Tensor& out_grad, const std::vector<Tensor>& state_grad, float dropout_prob, bool is_bidirec, int input_size, int hidden_size, int num_layers, const std::string& mode, int seed, bool is_test, Tensor* x_grad, std::vector<Tensor*> pre_state_grad, std::vector<Tensor*> weight_list_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, pre_state, weight_list, sequence_length, out, dropout_state_out, reserve, out_grad, state_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, pre_state, weight_list, sequence_length, out, dropout_state_out, reserve, out_grad, state_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_pre_state;
    for(auto& e : pre_state) {
        meta_dist_input_pre_state.push_back(MakeDistMetaTensor(*e.impl()));
    }
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_weight_list;
    for(auto& e : weight_list) {
        meta_dist_input_weight_list.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto meta_dist_input_sequence_length = sequence_length ? MakeDistMetaTensor(*(*sequence_length).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_dropout_state_out = MakeDistMetaTensor(*dropout_state_out.impl());
    auto meta_dist_input_reserve = MakeDistMetaTensor(*reserve.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    std::vector<phi::distributed::DistMetaTensor> meta_dist_input_state_grad;
    for(auto& e : state_grad) {
        meta_dist_input_state_grad.push_back(MakeDistMetaTensor(*e.impl()));
    }
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_pre_state, meta_dist_input_weight_list, meta_dist_input_sequence_length, meta_dist_input_out, meta_dist_input_dropout_state_out, meta_dist_input_reserve, meta_dist_input_out_grad, meta_dist_input_state_grad);
    DebugInfoForInferSpmd("rnn_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    auto dist_out_1 = SetKernelDistOutput(pre_state_grad);
    std::vector<phi::DenseTensor*> dense_out_1(dist_out_1.size());
    for (size_t i = 0; i < dist_out_1.size(); i++) {
      dense_out_1[i] = const_cast<phi::DenseTensor*>(&dist_out_1[i]->value());
      if (dense_out_1[i] && !rank_is_in_current_mesh && !dist_out_1[i]->defined()) {
        *dense_out_1[i]= phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
      }
    }

    auto dist_out_2 = SetKernelDistOutput(weight_list_grad);
    std::vector<phi::DenseTensor*> dense_out_2(dist_out_2.size());
    for (size_t i = 0; i < dist_out_2.size(); i++) {
      dense_out_2[i] = const_cast<phi::DenseTensor*>(&dist_out_2[i]->value());
      if (dense_out_2[i] && !rank_is_in_current_mesh && !dist_out_2[i]->defined()) {
        *dense_out_2[i]= phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
      }
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    std::vector<phi::MetaTensor> dist_out_1_meta_vec;
    for (auto tmp : dist_out_1) {
      dist_out_1_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_1_meta_ptr_vec(dist_out_1.size());
    for (size_t i = 0; i < dist_out_1_meta_vec.size(); ++i) {
      dist_out_1_meta_ptr_vec[i] = &dist_out_1_meta_vec[i];
    }

    std::vector<phi::MetaTensor> dist_out_2_meta_vec;
    for (auto tmp : dist_out_2) {
      dist_out_2_meta_vec.emplace_back(phi::MetaTensor(tmp));
    }
    std::vector<phi::MetaTensor*> dist_out_2_meta_ptr_vec(dist_out_2.size());
    for (size_t i = 0; i < dist_out_2_meta_vec.size(); ++i) {
      dist_out_2_meta_ptr_vec[i] = &dist_out_2_meta_vec[i];
    }

    std::vector<phi::MetaTensor> pre_state_meta_vec;
    for (auto tmp : pre_state) {
      pre_state_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> pre_state_meta_ptr_vec(pre_state_meta_vec.size());
    for (size_t i=0; i < pre_state_meta_ptr_vec.size(); ++i) {
      pre_state_meta_ptr_vec[i] = &pre_state_meta_vec[i];
    }

    std::vector<phi::MetaTensor> weight_list_meta_vec;
    for (auto tmp : weight_list) {
      weight_list_meta_vec.emplace_back(MakeMetaTensor(*tmp.impl()));
    }
    std::vector<const phi::MetaTensor*> weight_list_meta_ptr_vec(weight_list_meta_vec.size());
    for (size_t i=0; i < weight_list_meta_ptr_vec.size(); ++i) {
      weight_list_meta_ptr_vec[i] = &weight_list_meta_vec[i];
    }

    phi::RnnGradInferMeta(MakeMetaTensor(*x.impl()), pre_state_meta_ptr_vec, weight_list_meta_ptr_vec, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1_meta_ptr_vec, dist_out_2_meta_ptr_vec);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    for (size_t i = 0; i < dist_out_1.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out_1[i], current_process_mesh);
    }

    for (size_t i = 0; i < dist_out_2.size(); ++i) {
        SetReplicatedDistAttrForOutput(dist_out_2[i], current_process_mesh);
    }


    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "rnn_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "rnn_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "rnn_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_pre_state = ReshardApiInputToKernelInput(dev_ctx, pre_state, spmd_info.first[1], "pre_state");
      auto dist_input_weight_list = ReshardApiInputToKernelInput(dev_ctx, weight_list, spmd_info.first[2], "weight_list");
      auto dist_input_sequence_length = ReshardApiInputToKernelInput(dev_ctx, sequence_length, spmd_info.first[3], "sequence_length");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[4], "out");
      auto dist_input_dropout_state_out = ReshardApiInputToKernelInput(dev_ctx, dropout_state_out, spmd_info.first[5], "dropout_state_out");
      auto dist_input_reserve = ReshardApiInputToKernelInput(dev_ctx, reserve, spmd_info.first[6], "reserve");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[7], "out_grad");
      auto dist_input_state_grad = ReshardApiInputToKernelInput(dev_ctx, state_grad, spmd_info.first[8], "state_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      auto dist_input_pre_state_vec = PrepareDataForDistTensor(dist_input_pre_state, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_pre_state_vec;
      for (auto tmp : dist_input_pre_state_vec) {
        dense_input_pre_state_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_pre_state_meta_vec = MakeMetaTensor(dense_input_pre_state_vec);
      std::vector<const phi::MetaTensor*> dense_input_pre_state_meta_ptr_vec(dense_input_pre_state_meta_vec.size());
      for (size_t i = 0; i < dense_input_pre_state_meta_ptr_vec.size(); ++i) {
        dense_input_pre_state_meta_ptr_vec[i] = &dense_input_pre_state_meta_vec[i];
      }

      auto dist_input_weight_list_vec = PrepareDataForDistTensor(dist_input_weight_list, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_weight_list_vec;
      for (auto tmp : dist_input_weight_list_vec) {
        dense_input_weight_list_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_weight_list_meta_vec = MakeMetaTensor(dense_input_weight_list_vec);
      std::vector<const phi::MetaTensor*> dense_input_weight_list_meta_ptr_vec(dense_input_weight_list_meta_vec.size());
      for (size_t i = 0; i < dense_input_weight_list_meta_ptr_vec.size(); ++i) {
        dense_input_weight_list_meta_ptr_vec[i] = &dense_input_weight_list_meta_vec[i];
      }

      dist_input_sequence_length = PrepareDataForDistTensor(dist_input_sequence_length, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_sequence_length = dist_input_sequence_length ? paddle::make_optional<phi::DenseTensor>((*dist_input_sequence_length)->value()) : paddle::none;

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_dropout_state_out = PrepareDataForDistTensor(dist_input_dropout_state_out, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_dropout_state_out = &dist_input_dropout_state_out->value();

      dist_input_reserve = PrepareDataForDistTensor(dist_input_reserve, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_reserve = &dist_input_reserve->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(7), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      auto dist_input_state_grad_vec = PrepareDataForDistTensor(dist_input_state_grad, GetKernelInputArgDef(kernel.InputAt(8), kernel_backend), {}, kernel_result.is_stride_kernel);
      std::vector<const phi::DenseTensor*> dense_input_state_grad_vec;
      for (auto tmp : dist_input_state_grad_vec) {
        dense_input_state_grad_vec.emplace_back(&tmp->value());
      }
      std::vector<phi::MetaTensor> dense_input_state_grad_meta_vec = MakeMetaTensor(dense_input_state_grad_vec);
      std::vector<const phi::MetaTensor*> dense_input_state_grad_meta_ptr_vec(dense_input_state_grad_meta_vec.size());
      for (size_t i = 0; i < dense_input_state_grad_meta_ptr_vec.size(); ++i) {
        dense_input_state_grad_meta_ptr_vec[i] = &dense_input_state_grad_meta_vec[i];
      }

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> sequence_length_record_shapes;
         if(input_sequence_length){
           sequence_length_record_shapes.push_back((*input_sequence_length).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"sequence_length", sequence_length_record_shapes},
         {"out", {
         (*input_out).dims()}},
         {"dropout_state_out", {
         (*input_dropout_state_out).dims()}},
         {"reserve", {
         (*input_reserve).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         std::vector<phi::DDim> ddims_vec;
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_pre_state_vec.size());
         for (size_t i = 0; i < dense_input_pre_state_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_pre_state_vec[i]).dims());
         }
         input_shapes.emplace_back("pre_state", ddims_vec);
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_weight_list_vec.size());
         for (size_t i = 0; i < dense_input_weight_list_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_weight_list_vec[i]).dims());
         }
         input_shapes.emplace_back("weight_list", ddims_vec);
         ddims_vec.clear();
         ddims_vec.reserve(dense_input_state_grad_vec.size());
         for (size_t i = 0; i < dense_input_state_grad_vec.size(); ++i) {
           ddims_vec.emplace_back((*dense_input_state_grad_vec[i]).dims());
         }
         input_shapes.emplace_back("state_grad", ddims_vec);
         phi::AttributeMap attrs;
         attrs["dropout_prob"] = dropout_prob;
         attrs["is_bidirec"] = is_bidirec;
         attrs["input_size"] = input_size;
         attrs["hidden_size"] = hidden_size;
         attrs["num_layers"] = num_layers;
         attrs["mode"] = mode;
         attrs["seed"] = seed;
         attrs["is_test"] = is_test;
         phi::RecordOpInfoSupplement("rnn_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      std::vector<phi::MetaTensor> dense_out_1_meta_vec = MakeMetaTensor(dense_out_1);
      std::vector<phi::MetaTensor*> dense_out_1_meta_ptr_vec(dense_out_1_meta_vec.size());
      for (size_t i = 0; i < dense_out_1_meta_vec.size(); ++i) {
        dense_out_1_meta_ptr_vec[i] = &dense_out_1_meta_vec[i];
      }

      std::vector<phi::MetaTensor> dense_out_2_meta_vec = MakeMetaTensor(dense_out_2);
      std::vector<phi::MetaTensor*> dense_out_2_meta_ptr_vec(dense_out_2_meta_vec.size());
      for (size_t i = 0; i < dense_out_2_meta_vec.size(); ++i) {
        dense_out_2_meta_ptr_vec[i] = &dense_out_2_meta_vec[i];
      }

      phi::RnnGradInferMeta(MakeMetaTensor(*input_x), dense_input_pre_state_meta_ptr_vec, dense_input_weight_list_meta_ptr_vec, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1_meta_ptr_vec, dense_out_2_meta_ptr_vec);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("rnn_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, float, bool, int, int, int, const std::string&, int, bool, phi::DenseTensor*, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, dense_input_pre_state_vec, dense_input_weight_list_vec, input_sequence_length, *input_out, *input_dropout_state_out, *input_reserve, *input_out_grad, dense_input_state_grad_vec, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "rnn_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rnn_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rnn_grad", kernel_data_type);
  }
  VLOG(6) << "rnn_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_pre_state_vec = PrepareData(pre_state, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_pre_state(input_pre_state_vec->size());
  for (size_t i = 0; i < input_pre_state.size(); ++i) {
    input_pre_state[i] = &input_pre_state_vec->at(i);
  }
  auto input_weight_list_vec = PrepareData(weight_list, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_weight_list(input_weight_list_vec->size());
  for (size_t i = 0; i < input_weight_list.size(); ++i) {
    input_weight_list[i] = &input_weight_list_vec->at(i);
  }
  auto input_sequence_length = PrepareData(sequence_length, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_dropout_state_out = PrepareData(dropout_state_out, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_reserve = PrepareData(reserve, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(7), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_state_grad_vec = PrepareData(state_grad, GetKernelInputArgDef(kernel.InputAt(8), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  std::vector<const phi::DenseTensor*> input_state_grad(input_state_grad_vec->size());
  for (size_t i = 0; i < input_state_grad.size(); ++i) {
    input_state_grad[i] = &input_state_grad_vec->at(i);
  }
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> sequence_length_record_shapes;
     if(input_sequence_length){
       sequence_length_record_shapes.push_back((*input_sequence_length).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"sequence_length", sequence_length_record_shapes},
     {"out", {
     (*input_out).dims()}},
     {"dropout_state_out", {
     (*input_dropout_state_out).dims()}},
     {"reserve", {
     (*input_reserve).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     std::vector<phi::DDim> ddims_vec;
     ddims_vec.clear();
     ddims_vec.reserve(input_pre_state.size());
     for (size_t i = 0; i < input_pre_state.size(); ++i) {
       ddims_vec.emplace_back((*input_pre_state[i]).dims());
     }
     input_shapes.emplace_back("pre_state", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_weight_list.size());
     for (size_t i = 0; i < input_weight_list.size(); ++i) {
       ddims_vec.emplace_back((*input_weight_list[i]).dims());
     }
     input_shapes.emplace_back("weight_list", ddims_vec);
     ddims_vec.clear();
     ddims_vec.reserve(input_state_grad.size());
     for (size_t i = 0; i < input_state_grad.size(); ++i) {
       ddims_vec.emplace_back((*input_state_grad[i]).dims());
     }
     input_shapes.emplace_back("state_grad", ddims_vec);
     phi::AttributeMap attrs;
     attrs["dropout_prob"] = dropout_prob;
     attrs["is_bidirec"] = is_bidirec;
     attrs["input_size"] = input_size;
     attrs["hidden_size"] = hidden_size;
     attrs["num_layers"] = num_layers;
     attrs["mode"] = mode;
     attrs["seed"] = seed;
     attrs["is_test"] = is_test;
     phi::RecordOpInfoSupplement("rnn_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(&pre_state_grad);
  auto kernel_out_2 = SetKernelOutput(&weight_list_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rnn_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }

  auto pre_state_meta_vec = MakeMetaTensor(input_pre_state);
  std::vector<const phi::MetaTensor*> pre_state_metas(pre_state_meta_vec.size());
  for (size_t i = 0; i < pre_state_meta_vec.size(); ++i) {
    pre_state_metas[i] = &pre_state_meta_vec[i];
  }

  auto weight_list_meta_vec = MakeMetaTensor(input_weight_list);
  std::vector<const phi::MetaTensor*> weight_list_metas(weight_list_meta_vec.size());
  for (size_t i = 0; i < weight_list_meta_vec.size(); ++i) {
    weight_list_metas[i] = &weight_list_meta_vec[i];
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);

  auto kernel_out_1_meta_vec = MakeMetaTensor(kernel_out_1);
  std::vector<phi::MetaTensor*> kernel_out_1_metas(kernel_out_1_meta_vec.size());
  for (size_t i = 0; i < kernel_out_1_meta_vec.size(); ++i) {
    kernel_out_1_metas[i] = kernel_out_1[i] ? &kernel_out_1_meta_vec[i] : nullptr;
  }
  auto kernel_out_2_meta_vec = MakeMetaTensor(kernel_out_2);
  std::vector<phi::MetaTensor*> kernel_out_2_metas(kernel_out_2_meta_vec.size());
  for (size_t i = 0; i < kernel_out_2_meta_vec.size(); ++i) {
    kernel_out_2_metas[i] = kernel_out_2[i] ? &kernel_out_2_meta_vec[i] : nullptr;
  }
  phi::RnnGradInferMeta(MakeMetaTensor(*input_x), pre_state_metas, weight_list_metas, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1_metas, kernel_out_2_metas);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, const std::vector<const phi::DenseTensor*>&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<const phi::DenseTensor*>&, float, bool, int, int, int, const std::string&, int, bool, phi::DenseTensor*, std::vector<phi::DenseTensor*>, std::vector<phi::DenseTensor*>);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rnn_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, input_pre_state, input_weight_list, input_sequence_length, *input_out, *input_dropout_state_out, *input_reserve, *input_out_grad, input_state_grad, dropout_prob, is_bidirec, input_size, hidden_size, num_layers, mode, seed, is_test, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void rrelu_grad(const Tensor& x, const Tensor& noise, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, noise, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, noise, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_noise = MakeDistMetaTensor(*noise.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_noise, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("rrelu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::RReluGradInferMeta(MakeMetaTensor(*out_grad.impl()), MakeMetaTensor(*noise.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "rrelu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "rrelu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "rrelu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_noise = ReshardApiInputToKernelInput(dev_ctx, noise, spmd_info.first[1], "noise");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_noise = PrepareDataForDistTensor(dist_input_noise, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_noise = &dist_input_noise->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"noise", {
         (*input_noise).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("rrelu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::RReluGradInferMeta(MakeMetaTensor(*input_out_grad), MakeMetaTensor(*input_noise), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("rrelu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_noise, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "rrelu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "rrelu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("rrelu_grad", kernel_data_type);
  }
  VLOG(6) << "rrelu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_noise = PrepareData(noise, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"noise", {
     (*input_noise).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("rrelu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("rrelu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::RReluGradInferMeta(MakeMetaTensor(*input_out_grad), MakeMetaTensor(*input_noise), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("rrelu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_noise, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void set_value_grad(const Tensor& out_grad, const IntArray& starts, const IntArray& ends, const IntArray& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("set_value_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "set_value_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "set_value_with_scalar_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "set_value_with_scalar_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["starts"] = starts.GetData();
         attrs["ends"] = ends.GetData();
         attrs["steps"] = steps.GetData();
         attrs["axes"] = axes;
         attrs["decrease_axes"] = decrease_axes;
         attrs["none_axes"] = none_axes;
         phi::RecordOpInfoSupplement("set_value_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("set_value_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "set_value_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "set_value_with_scalar_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("set_value_grad", kernel_data_type);
  }
  VLOG(6) << "set_value_with_scalar_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["steps"] = steps.GetData();
     attrs["axes"] = axes;
     attrs["decrease_axes"] = decrease_axes;
     attrs["none_axes"] = none_axes;
     phi::RecordOpInfoSupplement("set_value_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("set_value_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("set_value_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void set_value_with_tensor_grad(const Tensor& values, const Tensor& out_grad, const IntArray& starts, const IntArray& ends, const IntArray& steps, const std::vector<int64_t>& axes, const std::vector<int64_t>& decrease_axes, const std::vector<int64_t>& none_axes, Tensor* x_grad, Tensor* values_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(values, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(values, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("set_value_with_tensor_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(values_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::SetValueGradInferMeta(MakeMetaTensor(*out_grad.impl()), MakeMetaTensor(*values.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "set_value_with_tensor_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "set_value_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "set_value_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
    auto dist_input_values = values.impl();
    auto input_values = &(static_cast<phi::distributed::DistTensor*>(dist_input_values.get())->value());

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["starts"] = starts.GetData();
         attrs["ends"] = ends.GetData();
         attrs["steps"] = steps.GetData();
         attrs["axes"] = axes;
         attrs["decrease_axes"] = decrease_axes;
         attrs["none_axes"] = none_axes;
         phi::RecordOpInfoSupplement("set_value_with_tensor_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::SetValueGradInferMeta(MakeMetaTensor(*input_out_grad), MakeMetaTensor(*input_values), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("set_value_with_tensor_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, values_grad, "values_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "set_value_with_tensor_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "set_value_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("set_value_with_tensor_grad", kernel_data_type);
  }
  VLOG(6) << "set_value_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_values = values.impl();
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["steps"] = steps.GetData();
     attrs["axes"] = axes;
     attrs["decrease_axes"] = decrease_axes;
     attrs["none_axes"] = none_axes;
     phi::RecordOpInfoSupplement("set_value_with_tensor_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(values_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("set_value_with_tensor_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::SetValueGradInferMeta(MakeMetaTensor(*input_out_grad), MakeMetaTensor(*input_values), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("set_value_with_tensor_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(steps), axes, decrease_axes, none_axes, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void slice_grad(const Tensor& input, const Tensor& out_grad, const std::vector<int64_t>& axes, const IntArray& starts, const IntArray& ends, const std::vector<int64_t>& infer_flags, const std::vector<int64_t>& decrease_axis, Tensor* input_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(input, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(input, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_input = MakeDistMetaTensor(*input.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::SliceGradInferSpmdDynamic(meta_dist_input_input, meta_dist_input_out_grad, axes, starts.GetData(), ends.GetData(), infer_flags, decrease_axis);
    DebugInfoForInferSpmd("slice_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(input_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*input.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `slice_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "slice_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "slice_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "slice_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_input = ReshardApiInputToKernelInput(dev_ctx, input, spmd_info.first[0], "input");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_input = PrepareDataForDistTensor(dist_input_input, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_input = &dist_input_input->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"input", {
         (*input_input).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axes"] = axes;
         attrs["starts"] = starts.GetData();
         attrs["ends"] = ends.GetData();
         attrs["infer_flags"] = infer_flags;
         attrs["decrease_axis"] = decrease_axis;
         phi::RecordOpInfoSupplement("slice_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("slice_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, axes, phi::IntArray(starts), phi::IntArray(ends), infer_flags, decrease_axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, input_grad, "input_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "slice_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "slice_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("slice_grad", kernel_data_type);
  }
  VLOG(6) << "slice_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_input = PrepareData(input, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"input", {
     (*input_input).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["infer_flags"] = infer_flags;
     attrs["decrease_axis"] = decrease_axis;
     phi::RecordOpInfoSupplement("slice_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(input_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("slice_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_input), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int64_t>&, const phi::IntArray&, const phi::IntArray&, const std::vector<int64_t>&, const std::vector<int64_t>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("slice_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_input, *input_out_grad, axes, phi::IntArray(starts), phi::IntArray(ends), infer_flags, decrease_axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void softmax_grad(const Tensor& out, const Tensor& out_grad, int axis, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::SoftmaxGradInferSpmd(meta_dist_input_out, meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("softmax_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `softmax_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "softmax_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "softmax_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "softmax_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[0], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("softmax_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("softmax_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, axis, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "softmax_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "softmax_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("softmax_grad", kernel_data_type);
  }
  VLOG(6) << "softmax_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("softmax_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("softmax_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("softmax_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out, *input_out_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void strided_slice_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int>& axes, const IntArray& starts, const IntArray& ends, const IntArray& strides, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::StridedSliceGradInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad, axes, starts.GetData(), ends.GetData(), strides.GetData());
    DebugInfoForInferSpmd("strided_slice_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `strided_slice_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "strided_slice_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "strided_slice_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "strided_slice_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axes"] = axes;
         attrs["starts"] = starts.GetData();
         attrs["ends"] = ends.GetData();
         attrs["strides"] = strides.GetData();
         phi::RecordOpInfoSupplement("strided_slice_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("strided_slice_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, axes, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(strides), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "strided_slice_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "strided_slice_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("strided_slice_grad", kernel_data_type);
  }
  VLOG(6) << "strided_slice_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axes"] = axes;
     attrs["starts"] = starts.GetData();
     attrs["ends"] = ends.GetData();
     attrs["strides"] = strides.GetData();
     phi::RecordOpInfoSupplement("strided_slice_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("strided_slice_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const phi::IntArray&, const phi::IntArray&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("strided_slice_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, axes, phi::IntArray(starts), phi::IntArray(ends), phi::IntArray(strides), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void subtract_double_grad(const Tensor& y, const Tensor& grad_out, const paddle::optional<Tensor>& grad_x_grad, const paddle::optional<Tensor>& grad_y_grad, int axis, Tensor* grad_out_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(y, grad_out, grad_x_grad, grad_y_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(grad_out.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(y, grad_out, grad_x_grad, grad_y_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  VLOG(6) << "subtract_double_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "subtract_double_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("subtract_double_grad", kernel_data_type);
  }
  VLOG(6) << "subtract_double_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_out = PrepareData(grad_out, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_x_grad = PrepareData(grad_x_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_grad_y_grad = PrepareData(grad_y_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> grad_x_grad_record_shapes;
     if(input_grad_x_grad){
       grad_x_grad_record_shapes.push_back((*input_grad_x_grad).dims());
     }
     std::vector<phi::DDim> grad_y_grad_record_shapes;
     if(input_grad_y_grad){
       grad_y_grad_record_shapes.push_back((*input_grad_y_grad).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"y", {
     (*input_y).dims()}},
     {"grad_out", {
     (*input_grad_out).dims()}},
     {"grad_x_grad", grad_x_grad_record_shapes},
     {"grad_y_grad",
     grad_y_grad_record_shapes}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("subtract_double_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(grad_out_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("subtract_double_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_grad_out), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const paddle::optional<phi::DenseTensor>&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("subtract_double_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_y, *input_grad_out, input_grad_x_grad, input_grad_y_grad, axis, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void subtract_grad(const Tensor& x, const Tensor& y, const Tensor& out_grad, int axis, Tensor* x_grad, Tensor* y_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ElementwiseBinaryGradInferSpmd(meta_dist_input_x, meta_dist_input_y, meta_dist_input_out_grad, axis);
    DebugInfoForInferSpmd("subtract_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh, spmd_info.second[1]);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    // API `subtract_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "subtract_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "subtract_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "subtract_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[2], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis;
         phi::RecordOpInfoSupplement("subtract_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("subtract_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, dense_out_0, dense_out_1);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "subtract_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "subtract_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("subtract_grad", kernel_data_type);
  }
  VLOG(6) << "subtract_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis;
     phi::RecordOpInfoSupplement("subtract_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("subtract_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);

  phi::GeneralBinaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, int, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("subtract_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, *input_out_grad, axis, kernel_out_0, kernel_out_1);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);

  }
  
}

PADDLE_API void sum_grad(const Tensor& x, const Tensor& out_grad, const IntArray& axis, bool keepdim, bool reduce_all, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::ReductionGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad, axis.GetData(), keepdim, reduce_all);
    DebugInfoForInferSpmd("sum_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `sum_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sum_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sum_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sum_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["axis"] = axis.GetData();
         attrs["keepdim"] = keepdim;
         attrs["reduce_all"] = reduce_all;
         phi::RecordOpInfoSupplement("sum_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sum_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sum_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sum_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sum_grad", kernel_data_type);
  }
  VLOG(6) << "sum_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["axis"] = axis.GetData();
     attrs["keepdim"] = keepdim;
     attrs["reduce_all"] = reduce_all;
     phi::RecordOpInfoSupplement("sum_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sum_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, bool, bool, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sum_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(axis), keepdim, reduce_all, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void swish_grad(const Tensor& x, const Tensor& out_grad, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("swish_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "swish_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "swish_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "swish_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         phi::RecordOpInfoSupplement("swish_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("swish_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "swish_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "swish_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("swish_grad", kernel_data_type);
  }
  VLOG(6) << "swish_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     phi::RecordOpInfoSupplement("swish_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("swish_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::GeneralUnaryGradInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("swish_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void sync_batch_norm_grad(const Tensor& x, const Tensor& scale, const Tensor& bias, const Tensor& saved_mean, const Tensor& saved_variance, const paddle::optional<Tensor>& reserve_space, const Tensor& out_grad, float momentum, float epsilon, const std::string& data_format, bool is_test, bool use_global_stats, bool trainable_statistics, Tensor* x_grad, Tensor* scale_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, scale, bias, saved_mean, saved_variance, reserve_space, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, scale, bias, saved_mean, saved_variance, reserve_space, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_scale = MakeDistMetaTensor(*scale.impl());
    auto meta_dist_input_bias = MakeDistMetaTensor(*bias.impl());
    auto meta_dist_input_saved_mean = MakeDistMetaTensor(*saved_mean.impl());
    auto meta_dist_input_saved_variance = MakeDistMetaTensor(*saved_variance.impl());
    auto meta_dist_input_reserve_space = reserve_space ? MakeDistMetaTensor(*(*reserve_space).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_scale, meta_dist_input_bias, meta_dist_input_saved_mean, meta_dist_input_saved_variance, meta_dist_input_reserve_space, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("sync_batch_norm_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(scale_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*scale.impl()), MakeMetaTensor(*bias.impl()), dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "sync_batch_norm_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "sync_batch_norm_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "sync_batch_norm_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_scale = ReshardApiInputToKernelInput(dev_ctx, scale, spmd_info.first[1], "scale");
      auto dist_input_bias = ReshardApiInputToKernelInput(dev_ctx, bias, spmd_info.first[2], "bias");
      auto dist_input_saved_mean = ReshardApiInputToKernelInput(dev_ctx, saved_mean, spmd_info.first[3], "saved_mean");
      auto dist_input_saved_variance = ReshardApiInputToKernelInput(dev_ctx, saved_variance, spmd_info.first[4], "saved_variance");
      auto dist_input_reserve_space = ReshardApiInputToKernelInput(dev_ctx, reserve_space, spmd_info.first[5], "reserve_space");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[6], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_scale = PrepareDataForDistTensor(dist_input_scale, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_scale = &dist_input_scale->value();

      dist_input_bias = PrepareDataForDistTensor(dist_input_bias, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_bias = &dist_input_bias->value();

      dist_input_saved_mean = PrepareDataForDistTensor(dist_input_saved_mean, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_mean = &dist_input_saved_mean->value();

      dist_input_saved_variance = PrepareDataForDistTensor(dist_input_saved_variance, GetKernelInputArgDef(kernel.InputAt(4), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_saved_variance = &dist_input_saved_variance->value();

      dist_input_reserve_space = PrepareDataForDistTensor(dist_input_reserve_space, GetKernelInputArgDef(kernel.InputAt(5), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_reserve_space = dist_input_reserve_space ? paddle::make_optional<phi::DenseTensor>((*dist_input_reserve_space)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(6), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> reserve_space_record_shapes;
         if(input_reserve_space){
           reserve_space_record_shapes.push_back((*input_reserve_space).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"scale", {
         (*input_scale).dims()}},
         {"bias", {
         (*input_bias).dims()}},
         {"saved_mean", {
         (*input_saved_mean).dims()}},
         {"saved_variance", {
         (*input_saved_variance).dims()}},
         {"reserve_space", reserve_space_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["momentum"] = momentum;
         attrs["epsilon"] = epsilon;
         attrs["data_format"] = data_format;
         attrs["is_test"] = is_test;
         attrs["use_global_stats"] = use_global_stats;
         attrs["trainable_statistics"] = trainable_statistics;
         phi::RecordOpInfoSupplement("sync_batch_norm_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("sync_batch_norm_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, data_format, is_test, use_global_stats, trainable_statistics, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, scale_grad, "scale_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "sync_batch_norm_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "sync_batch_norm_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("sync_batch_norm_grad", kernel_data_type);
  }
  VLOG(6) << "sync_batch_norm_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_scale = PrepareData(scale, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_bias = PrepareData(bias, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_mean = PrepareData(saved_mean, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_saved_variance = PrepareData(saved_variance, GetKernelInputArgDef(kernel.InputAt(4), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_reserve_space = PrepareData(reserve_space, GetKernelInputArgDef(kernel.InputAt(5), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(6), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> reserve_space_record_shapes;
     if(input_reserve_space){
       reserve_space_record_shapes.push_back((*input_reserve_space).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"scale", {
     (*input_scale).dims()}},
     {"bias", {
     (*input_bias).dims()}},
     {"saved_mean", {
     (*input_saved_mean).dims()}},
     {"saved_variance", {
     (*input_saved_variance).dims()}},
     {"reserve_space", reserve_space_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["momentum"] = momentum;
     attrs["epsilon"] = epsilon;
     attrs["data_format"] = data_format;
     attrs["is_test"] = is_test;
     attrs["use_global_stats"] = use_global_stats;
     attrs["trainable_statistics"] = trainable_statistics;
     phi::RecordOpInfoSupplement("sync_batch_norm_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(scale_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("sync_batch_norm_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::GeneralTernaryGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_scale), MakeMetaTensor(*input_bias), kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, float, float, const std::string&, bool, bool, bool, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("sync_batch_norm_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_scale, *input_bias, *input_saved_mean, *input_saved_variance, input_reserve_space, *input_out_grad, momentum, epsilon, data_format, is_test, use_global_stats, trainable_statistics, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void tile_grad(const Tensor& x, const Tensor& out_grad, const IntArray& repeat_times, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::TileGradInferSpmd(meta_dist_input_x, meta_dist_input_out_grad, repeat_times.GetData());
    DebugInfoForInferSpmd("tile_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `tile_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "tile_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "tile_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "tile_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["repeat_times"] = repeat_times.GetData();
         phi::RecordOpInfoSupplement("tile_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("tile_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(repeat_times), dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "tile_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tile_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tile_grad", kernel_data_type);
  }
  VLOG(6) << "tile_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["repeat_times"] = repeat_times.GetData();
     phi::RecordOpInfoSupplement("tile_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tile_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::IntArray&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tile_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, phi::IntArray(repeat_times), kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void trans_layout_grad(const Tensor& x, const Tensor& out_grad, const std::vector<int>& perm, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("trans_layout_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::TransLayoutGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*out_grad.impl()), perm, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "trans_layout_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "trans_layout_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "trans_layout_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[1], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["perm"] = perm;
         phi::RecordOpInfoSupplement("trans_layout_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::TransLayoutGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), perm, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("trans_layout_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, perm, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "trans_layout_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "trans_layout_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("trans_layout_grad", kernel_data_type);
  }
  VLOG(6) << "trans_layout_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["perm"] = perm;
     phi::RecordOpInfoSupplement("trans_layout_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("trans_layout_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TransLayoutGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_out_grad), perm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("trans_layout_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_out_grad, perm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void transpose_grad(const Tensor& out_grad, const std::vector<int>& perm, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::TransposeGradInferSpmd(meta_dist_input_out_grad, perm);
    DebugInfoForInferSpmd("transpose_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::TransposeGradInferMeta(MakeMetaTensor(*out_grad.impl()), perm, &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `transpose_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "transpose_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "transpose_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "transpose_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["perm"] = perm;
         phi::RecordOpInfoSupplement("transpose_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::TransposeGradInferMeta(MakeMetaTensor(*input_out_grad), perm, &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("transpose_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, perm, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "transpose_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "transpose_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("transpose_grad", kernel_data_type);
  }
  VLOG(6) << "transpose_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["perm"] = perm;
     phi::RecordOpInfoSupplement("transpose_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("transpose_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::TransposeGradInferMeta(MakeMetaTensor(*input_out_grad), perm, &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const std::vector<int>&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("transpose_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, perm, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void tril_grad(const Tensor& out_grad, int diagonal, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("tril_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "tril_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "tril_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "tril_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["diagonal"] = diagonal;
         phi::RecordOpInfoSupplement("tril_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("tril_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, diagonal, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "tril_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "tril_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("tril_grad", kernel_data_type);
  }
  VLOG(6) << "tril_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["diagonal"] = diagonal;
     phi::RecordOpInfoSupplement("tril_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("tril_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("tril_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, diagonal, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void triu_grad(const Tensor& out_grad, int diagonal, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::TriuGradInferSpmd(meta_dist_input_out_grad, diagonal);
    DebugInfoForInferSpmd("triu_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh, spmd_info.second[0]);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    // API `triu_grad` does not need to set DistAttr for output.

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "triu_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "triu_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "triu_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["diagonal"] = diagonal;
         phi::RecordOpInfoSupplement("triu_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("triu_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, diagonal, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "triu_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "triu_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("triu_grad", kernel_data_type);
  }
  VLOG(6) << "triu_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["diagonal"] = diagonal;
     phi::RecordOpInfoSupplement("triu_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("triu_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("triu_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, diagonal, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void disable_check_model_nan_inf_grad(const Tensor& out_grad, int unsetflag, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("disable_check_model_nan_inf_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "disable_check_model_nan_inf_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "check_model_nan_inf", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "check_model_nan_inf kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["unsetflag"] = unsetflag;
         phi::RecordOpInfoSupplement("disable_check_model_nan_inf_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("disable_check_model_nan_inf_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, unsetflag, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "disable_check_model_nan_inf_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "check_model_nan_inf", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("disable_check_model_nan_inf_grad", kernel_data_type);
  }
  VLOG(6) << "check_model_nan_inf kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["unsetflag"] = unsetflag;
     phi::RecordOpInfoSupplement("disable_check_model_nan_inf_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("disable_check_model_nan_inf_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("disable_check_model_nan_inf_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, unsetflag, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void enable_check_model_nan_inf_grad(const Tensor& out_grad, int unsetflag, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(out_grad);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_out_grad);
    DebugInfoForInferSpmd("enable_check_model_nan_inf_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*out_grad.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "enable_check_model_nan_inf_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "check_model_nan_inf", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "check_model_nan_inf kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[0], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["unsetflag"] = unsetflag;
         phi::RecordOpInfoSupplement("enable_check_model_nan_inf_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("enable_check_model_nan_inf_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_out_grad, unsetflag, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "enable_check_model_nan_inf_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "check_model_nan_inf", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("enable_check_model_nan_inf_grad", kernel_data_type);
  }
  VLOG(6) << "check_model_nan_inf kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["unsetflag"] = unsetflag;
     phi::RecordOpInfoSupplement("enable_check_model_nan_inf_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("enable_check_model_nan_inf_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_out_grad), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, int, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("enable_check_model_nan_inf_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_out_grad, unsetflag, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}

PADDLE_API void fused_gemm_epilogue_grad(const Tensor& x, const Tensor& y, const paddle::optional<Tensor>& reserve_space, const Tensor& out_grad, bool trans_x, bool trans_y, const std::string& activation, Tensor* x_grad, Tensor* y_grad, Tensor* bias_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, y, reserve_space, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, y, reserve_space, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_y = MakeDistMetaTensor(*y.impl());
    auto meta_dist_input_reserve_space = reserve_space ? MakeDistMetaTensor(*(*reserve_space).impl()) : phi::distributed::DistMetaTensor();
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_y, meta_dist_input_reserve_space, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("fused_gemm_epilogue_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_0 =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_0 = shared_dist_out_0.get();
    phi::DenseTensor* dense_out_0 = dist_out_0 ? dist_out_0->unsafe_mutable_value() : nullptr;
    if (dense_out_0 && !rank_is_in_current_mesh && !dist_out_0->defined()) {
      *dense_out_0 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_1 =
        CreateKernelDistOutput(y_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_1 = shared_dist_out_1.get();
    phi::DenseTensor* dense_out_1 = dist_out_1 ? dist_out_1->unsafe_mutable_value() : nullptr;
    if (dense_out_1 && !rank_is_in_current_mesh && !dist_out_1->defined()) {
      *dense_out_1 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out_2 =
        CreateKernelDistOutput(bias_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out_2 = shared_dist_out_2.get();
    phi::DenseTensor* dense_out_2 = dist_out_2 ? dist_out_2->unsafe_mutable_value() : nullptr;
    if (dense_out_2 && !rank_is_in_current_mesh && !dist_out_2->defined()) {
      *dense_out_2 = phi::DenseTensor(
          std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
          phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out_0(dist_out_0);
    phi::MetaTensor meta_dist_out_1(dist_out_1);
    phi::MetaTensor meta_dist_out_2(dist_out_2);
    phi::MetaTensor meta_dist_reserve_space = reserve_space ? MakeMetaTensor(*(*reserve_space).impl()) : phi::MetaTensor();

    phi::FusedGemmEpilogueGradInferMeta(MakeMetaTensor(*x.impl()), MakeMetaTensor(*y.impl()), meta_dist_reserve_space, MakeMetaTensor(*out_grad.impl()), trans_x, trans_y, activation, dist_out_0 ? &meta_dist_out_0 : nullptr, dist_out_1 ? &meta_dist_out_1 : nullptr, dist_out_2 ? &meta_dist_out_2 : nullptr);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out_0, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_1, current_process_mesh);
    SetReplicatedDistAttrForOutput(dist_out_2, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "fused_gemm_epilogue_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "fused_gemm_epilogue_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "fused_gemm_epilogue_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_y = ReshardApiInputToKernelInput(dev_ctx, y, spmd_info.first[1], "y");
      auto dist_input_reserve_space = ReshardApiInputToKernelInput(dev_ctx, reserve_space, spmd_info.first[2], "reserve_space");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_y = PrepareDataForDistTensor(dist_input_y, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_y = &dist_input_y->value();

      dist_input_reserve_space = PrepareDataForDistTensor(dist_input_reserve_space, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      paddle::optional<phi::DenseTensor> input_reserve_space = dist_input_reserve_space ? paddle::make_optional<phi::DenseTensor>((*dist_input_reserve_space)->value()) : paddle::none;

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<phi::DDim> reserve_space_record_shapes;
         if(input_reserve_space){
           reserve_space_record_shapes.push_back((*input_reserve_space).dims());
         }
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"y", {
         (*input_y).dims()}},
         {"reserve_space", reserve_space_record_shapes},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["trans_x"] = trans_x;
         attrs["trans_y"] = trans_y;
         attrs["activation"] = activation;
         phi::RecordOpInfoSupplement("fused_gemm_epilogue_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out_0(dense_out_0);
      phi::MetaTensor meta_dense_out_1(dense_out_1);
      phi::MetaTensor meta_dense_out_2(dense_out_2);
      phi::FusedGemmEpilogueGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(input_reserve_space), MakeMetaTensor(*input_out_grad), trans_x, trans_y, activation, dense_out_0 ? &meta_dense_out_0 : nullptr, dense_out_1 ? &meta_dense_out_1 : nullptr, dense_out_2 ? &meta_dense_out_2 : nullptr);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("fused_gemm_epilogue_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, bool, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_y, input_reserve_space, *input_out_grad, trans_x, trans_y, activation, dense_out_0, dense_out_1, dense_out_2);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out_0, kernel_backend, dense_out_0);
        TransDataBackend(dense_out_1, kernel_backend, dense_out_1);
        TransDataBackend(dense_out_2, kernel_backend, dense_out_2);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_0, x_grad, "x_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_1, y_grad, "y_grad");
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out_2, bias_grad, "bias_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "fused_gemm_epilogue_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "fused_gemm_epilogue_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("fused_gemm_epilogue_grad", kernel_data_type);
  }
  VLOG(6) << "fused_gemm_epilogue_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_y = PrepareData(y, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_reserve_space = PrepareData(reserve_space, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<phi::DDim> reserve_space_record_shapes;
     if(input_reserve_space){
       reserve_space_record_shapes.push_back((*input_reserve_space).dims());
     }
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"y", {
     (*input_y).dims()}},
     {"reserve_space", reserve_space_record_shapes},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["trans_x"] = trans_x;
     attrs["trans_y"] = trans_y;
     attrs["activation"] = activation;
     phi::RecordOpInfoSupplement("fused_gemm_epilogue_grad", input_shapes, attrs);
  }

  auto kernel_out_0 = SetKernelOutput(x_grad);
  auto kernel_out_1 = SetKernelOutput(y_grad);
  auto kernel_out_2 = SetKernelOutput(bias_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("fused_gemm_epilogue_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out_0(kernel_out_0, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_1(kernel_out_1, kernel_result.is_stride_kernel);
  phi::MetaTensor meta_out_2(kernel_out_2, kernel_result.is_stride_kernel);

  phi::FusedGemmEpilogueGradInferMeta(MakeMetaTensor(*input_x), MakeMetaTensor(*input_y), MakeMetaTensor(input_reserve_space), MakeMetaTensor(*input_out_grad), trans_x, trans_y, activation, kernel_out_0 ? &meta_out_0 : nullptr, kernel_out_1 ? &meta_out_1 : nullptr, kernel_out_2 ? &meta_out_2 : nullptr);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const paddle::optional<phi::DenseTensor>&, const phi::DenseTensor&, bool, bool, const std::string&, phi::DenseTensor*, phi::DenseTensor*, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("fused_gemm_epilogue_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_y, input_reserve_space, *input_out_grad, trans_x, trans_y, activation, kernel_out_0, kernel_out_1, kernel_out_2);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out_0, kernel_backend, kernel_out_0);
    TransDataBackend(kernel_out_1, kernel_backend, kernel_out_1);
    TransDataBackend(kernel_out_2, kernel_backend, kernel_out_2);

  }
  
}

PADDLE_API void unpool_grad(const Tensor& x, const Tensor& indices, const Tensor& out, const Tensor& out_grad, const std::vector<int>& ksize, const std::vector<int>& strides, const std::vector<int>& padding, const IntArray& output_size, const std::string& data_format, Tensor* x_grad) {
  // Kernel Key Construction
  Backend kernel_backend = Backend::UNDEFINED;
  DataLayout kernel_layout = DataLayout::UNDEFINED;
  DataType kernel_data_type = DataType::UNDEFINED;

  bool run_auto_parallel = AllInputsAreDistTensor(x, indices, out, out_grad);
  bool rank_is_in_current_mesh = true;
  if (run_auto_parallel) {
    auto mesh = std::static_pointer_cast<phi::distributed::DistTensor>(out_grad.impl())->dist_attr().process_mesh();
    rank_is_in_current_mesh = phi::distributed::IsCurRankInMesh(mesh);
  }
  if (rank_is_in_current_mesh) {
    kernel_data_type = ParseDataType(x);

    if (kernel_backend == Backend::UNDEFINED
          || kernel_layout == DataLayout::UNDEFINED
          || kernel_data_type == DataType::UNDEFINED ) {
      auto kernel_key_set = ParseKernelKeyByInputArgs(x, indices, out, out_grad);
      auto kernel_key = kernel_key_set.GetHighestPriorityKernelKey();
      if (kernel_backend == Backend::UNDEFINED) {
        kernel_backend = kernel_key.backend();
      }
      if (kernel_layout == DataLayout::UNDEFINED) {
        kernel_layout = kernel_key.layout();
      }
      if (kernel_data_type == DataType::UNDEFINED) {
        kernel_data_type = kernel_key.dtype();
      }
    }
  }

  // Kernel Dispatch Body
  // Auto Parallel condition
  if (run_auto_parallel) {
    // 1. InferSpmd (Infer DistAttr of Inputs&Outputs)
    auto meta_dist_input_x = MakeDistMetaTensor(*x.impl());
    auto meta_dist_input_indices = MakeDistMetaTensor(*indices.impl());
    auto meta_dist_input_out = MakeDistMetaTensor(*out.impl());
    auto meta_dist_input_out_grad = MakeDistMetaTensor(*out_grad.impl());
    auto spmd_info = phi::distributed::VariadicReplicatedInferSpmdDynamic(meta_dist_input_x, meta_dist_input_indices, meta_dist_input_out, meta_dist_input_out_grad);
    DebugInfoForInferSpmd("unpool_grad", spmd_info);

    // 2. Create Temporary Output & Prepare Dist and Dense Output
    phi::DeviceContext* dev_ctx = nullptr;
    std::shared_ptr<phi::distributed::DistTensor> shared_dist_out =
        CreateKernelDistOutput(x_grad, !rank_is_in_current_mesh);
    phi::distributed::DistTensor* dist_out = shared_dist_out.get();
    phi::DenseTensor* dense_out = dist_out->unsafe_mutable_value();
    if (dense_out && !rank_is_in_current_mesh && !dist_out->defined()) {
      *dense_out = phi::DenseTensor(
            std::make_shared<phi::Allocation>(nullptr, 0, phi::distributed::GetDefaultPlace()),
            phi::DenseTensorMeta());
    }

    // 3. Infer DistTensor's Global Shape
    phi::MetaTensor meta_dist_out(dist_out);
    phi::UnchangedInferMeta(MakeMetaTensor(*x.impl()), &meta_dist_out);


    // 4. Set Output Dist Attr For Default Impl
    auto current_process_mesh = paddle::holds_alternative<phi::distributed::TensorDistAttr>(spmd_info.first[0]) ?
               paddle::get<0>(spmd_info.first[0]).process_mesh() : paddle::get<1>(spmd_info.first[0]).at(0).process_mesh();
    SetReplicatedDistAttrForOutput(dist_out, current_process_mesh);

    if (rank_is_in_current_mesh) {
      // 5. Select Kernel
      VLOG(6) << "unpool_grad API dist branch: kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
      auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
          "unpool_grad", {kernel_backend, kernel_layout, kernel_data_type});
      const auto& kernel = kernel_result.kernel;
      VLOG(6) << "unpool_grad kernel: " << kernel;
      dev_ctx = GetDeviceContextByBackend(kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend);

      // 6. Reshard Input
      auto dist_input_x = ReshardApiInputToKernelInput(dev_ctx, x, spmd_info.first[0], "x");
      auto dist_input_indices = ReshardApiInputToKernelInput(dev_ctx, indices, spmd_info.first[1], "indices");
      auto dist_input_out = ReshardApiInputToKernelInput(dev_ctx, out, spmd_info.first[2], "out");
      auto dist_input_out_grad = ReshardApiInputToKernelInput(dev_ctx, out_grad, spmd_info.first[3], "out_grad");

      // 7. PrepareData (DataTransform & Prepare Dense Input)
      dist_input_x = PrepareDataForDistTensor(dist_input_x, GetKernelInputArgDef(kernel.InputAt(0), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_x = &dist_input_x->value();

      dist_input_indices = PrepareDataForDistTensor(dist_input_indices, GetKernelInputArgDef(kernel.InputAt(1), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_indices = &dist_input_indices->value();

      dist_input_out = PrepareDataForDistTensor(dist_input_out, GetKernelInputArgDef(kernel.InputAt(2), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out = &dist_input_out->value();

      dist_input_out_grad = PrepareDataForDistTensor(dist_input_out_grad, GetKernelInputArgDef(kernel.InputAt(3), kernel_backend), {}, kernel_result.is_stride_kernel);
      auto input_out_grad = &dist_input_out_grad->value();

      // 8. RecordOpInfoSupplement
      if(phi::RecordOpInfoSupplement::IsEnabled()){
         std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
         {"x", {
         (*input_x).dims()}},
         {"indices", {
         (*input_indices).dims()}},
         {"out", {
         (*input_out).dims()}},
         {"out_grad", {
         (*input_out_grad).dims()}}};
         phi::AttributeMap attrs;
         attrs["ksize"] = ksize;
         attrs["strides"] = strides;
         attrs["padding"] = padding;
         attrs["output_size"] = output_size.GetData();
         attrs["data_format"] = data_format;
         phi::RecordOpInfoSupplement("unpool_grad", input_shapes, attrs);
      }
      // 9. Infer Local DenseTensor Meta
      phi::MetaTensor meta_dense_out(dense_out);
      phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_dense_out);

      // 10. DenseTensor Kernel Call
      phi::RecordEvent* kernel_record_event = nullptr;
      if(phi::RecordEvent::IsEnabled()){
        kernel_record_event = new phi::RecordEvent("unpool_grad dist compute", phi::TracerEventType::OperatorInner, 1);
      }
      using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, phi::DenseTensor*);
      auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
      (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out, *input_out_grad, ksize, strides, padding, phi::IntArray(output_size), data_format, dense_out);
      if(kernel_record_event != nullptr){
        delete kernel_record_event;
      }

      // 11. Fallback
      if (kernel_result.has_fallback_cpu) {
        TransDataBackend(dense_out, kernel_backend, dense_out);
      }
    }
    // 12. Reshard Kernel Output to API output
      ReshardKernelOutputToApiOutput(dev_ctx, shared_dist_out, x_grad, "x_grad");

    // 13. Return
    return;
  }

  VLOG(6) << "unpool_grad API kernel key: [" << kernel_backend << ", " << kernel_layout << ", "<< kernel_data_type << "]";
  auto kernel_result = phi::KernelFactory::Instance().SelectKernelOrThrowError(
      "unpool_grad", {kernel_backend, kernel_layout, kernel_data_type}, true);
  const auto& kernel = kernel_result.kernel;
  if (FLAGS_low_precision_op_list) {
    phi::KernelFactory::Instance().AddToLowPrecisionKernelList("unpool_grad", kernel_data_type);
  }
  VLOG(6) << "unpool_grad kernel: " << kernel;
  // add actual_kernel_backend to select actual kernel backend after a potential falling-back to CPU
  Backend actual_kernel_backend = kernel_result.has_fallback_cpu ? Backend::CPU : kernel_backend;
  auto* dev_ctx = GetDeviceContextByBackend(actual_kernel_backend);

  auto input_x = PrepareData(x, GetKernelInputArgDef(kernel.InputAt(0), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_indices = PrepareData(indices, GetKernelInputArgDef(kernel.InputAt(1), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out = PrepareData(out, GetKernelInputArgDef(kernel.InputAt(2), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  auto input_out_grad = PrepareData(out_grad, GetKernelInputArgDef(kernel.InputAt(3), actual_kernel_backend), {}, kernel_result.is_stride_kernel);
  if(phi::RecordOpInfoSupplement::IsEnabled()){
     std::vector<std::pair<const char*, std::vector<phi::DDim>>> input_shapes{
     {"x", {
     (*input_x).dims()}},
     {"indices", {
     (*input_indices).dims()}},
     {"out", {
     (*input_out).dims()}},
     {"out_grad", {
     (*input_out_grad).dims()}}};
     phi::AttributeMap attrs;
     attrs["ksize"] = ksize;
     attrs["strides"] = strides;
     attrs["padding"] = padding;
     attrs["output_size"] = output_size.GetData();
     attrs["data_format"] = data_format;
     phi::RecordOpInfoSupplement("unpool_grad", input_shapes, attrs);
  }

  auto kernel_out = SetKernelOutput(x_grad);

  phi::RecordEvent *infer_shape_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    infer_shape_record_event = new phi::RecordEvent("unpool_grad infer_meta", phi::TracerEventType::OperatorInner, 1);
  }
  phi::MetaTensor meta_out(kernel_out, kernel_result.is_stride_kernel);

  phi::UnchangedInferMeta(MakeMetaTensor(*input_x), &meta_out);

  if(infer_shape_record_event != nullptr){
    delete infer_shape_record_event;
  }
  using kernel_signature = void(*)(const phi::DeviceContext&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const phi::DenseTensor&, const std::vector<int>&, const std::vector<int>&, const std::vector<int>&, const phi::IntArray&, const std::string&, phi::DenseTensor*);
  auto* kernel_fn = kernel.GetVariadicKernelFn<kernel_signature>();
  phi::RecordEvent* kernel_record_event = nullptr;
  if(phi::RecordEvent::IsEnabled()){
    kernel_record_event = new phi::RecordEvent("unpool_grad compute", phi::TracerEventType::OperatorInner, 1);
  }
    (*kernel_fn)(*dev_ctx, *input_x, *input_indices, *input_out, *input_out_grad, ksize, strides, padding, phi::IntArray(output_size), data_format, kernel_out);
  if(kernel_record_event != nullptr){
    delete kernel_record_event;
  }

  if (kernel_result.has_fallback_cpu) {

    TransDataBackend(kernel_out, kernel_backend, kernel_out);

  }
  
}


}  // namespace experimental
}  // namespace paddle
